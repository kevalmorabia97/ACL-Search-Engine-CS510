AUTHOR INDEX  
Sponsored by the Association for Computational Linguistics  
Natural Language Generation Journeys to Interactive 3D Worlds*  Interactive 3D worlds offer an intriguing testbed for the natural language generation community. To complement interactive 3D worlds&apos; rich visualizations, they ~equire significant linguistic flexibility and communicative • power. We explore the major functionalities and ~rchitectural implications of natural language generation for three key classes of interactive 3D worlds: self-. .&quot; explaining 3D environments, habitable 3D learning environments , and interactive 3D narrative worlds. These are illustrated with .empirical investigations underway in our laboratory with severalsuch systems. • • •. . • Introduction Natural language generation (NLG) has witnessed great strides over the past decade. Our theoretical Underpinnings are firming up, our systems building activities are proceeding quickly, and we are beginning to see significant empirical results. As a result of this mat-uration, the field is now well positioned to attack the • &quot;challenges pose&apos;d by a new family of computing envi-• ronments: interactive 3D worlds, which continuously • •render the activities playing out in rich 3D scenes in realtime. Because of these worlds&apos; compelling visual properties and their promise of a high degree of mul-timodal interactivity, they will soon form the basis for applications ranging from learning environments for ed, ucation and training to interactive fiction systems for •entertainment. Interactive 3D worlds offer an intriguing testbed for the NLG •community for several reasons. They may portray scenes with complicated spatial relationships, Science; and a Corporate gift from Novell, Inc. • such as those found in the domain of electricity and magnetism in physics. They may include multiple dynamic objects tracing out complex motion paths, such as water particles traveling through xylem tissue in Virtual plants. They might be inhabited by user-directed avatars that manipulate objects in the world and lifelike agents that will need to coordinate speech, gesture, and locomotion as they explain and demonstrate complex phenomena. In 3D interactive fiction systems, user-directed avatars and lifelike autonomous agents may navigate through complex cityscapes and interact with users and with one another to create new forms of theater. As the visual complexities of interactive 3D worlds grow, they will place increasingly heavy demands on the visual channel. To complement their rich visualiza-tions, interactive 3D worlds will require the linguistic flexibility and •communicative power that only NLG can provide. In interactive learning environments, the spatial complexities and dynamic phenomena that characterize physical devices must be clearly explained. NLG delivered with speech synthesis will need to be carefully coordinated with 3D graphics generation to create interactive presentations that are both coherent and interesting. In a similar fashion, lifelike agents roaming • around the same 3D worlds through which users guide their avatars will require sophisticated NLG capabilities , and 3D interacti.ve fiction systems will-benefit considerably from virtual narrators that are articulate and can generate interesting commentary in realtime. In this talk, we will •explore the major issues, func-tionalities, and architectural implications of •natural language generation for interactive 3D worlds. Our discussion will examine NLG issues for three interesting classes of interactive 3D worlds: • Self-Explaining 3D Environments: In response to users&apos; questions, Self-explaining environments dynamically generate spoken natural language and 3D animated visualizations and produce vivid explana-. • • :
COMMUNICATIVE GOAL-DRIVEN NL GENERATION AND DATA-DRIVEN GRAPHICS GENERATION: AN ARCHITECTURAL SYNTHESIS FOR MULTIMEDIA PAGE GENERATION  • In this paper we presen t a system for automatically producing multimedia pages of information that draws both from results in data-driven aggregation in information visualization and from results in communicative-goal oriented natural language generation. Our system constitutes an architectural synthesis of these two directions, allowing a beneficial cross-fertilization of research methods. We suggest that data-driven visualization provides a general approach to aggregation in NLG, and that text planning allows higher user-responsiveness in visualization via automatic diagram design.
• A PRINCIPLED REPRESENTATION OF ATTRIBUTIVE DESCRIPTIONS FOR GENERATING INTEGRATED TEXT AND INFORMATION GRAPHICS PRESENTATIONS  This paper describes a media-independent, compositional,)lan-based approach to representing attributive descriptions for use in integrated text and graphics generation. An attributive description&apos;s main function is to convey information directly contributing to the communicative goals of a discourse, Whereas a referential description&apos;s only function is to enable the audience to identify a particular referent. This approach has been implemented as part of an architecture for generating integrated text and information graphics. Uses of referential and attributive descriptions are represented as two distinct types of communicative acts in a media-independent plan. It is particularly important to distinguish the two types of acts, since theyhave different consequences for dialogue and text generation, and for graphic design, •1 Introduction This paper describes a media-independent, compositional, plan-based approach to representing attributiv e descriptions for use in integrated text and graphics generat!on. An attributive descrip-tion&apos;s main function is to convey information directly contributing to the communicative goals of a discourse, whereas a referential description&apos;s only function is to enable the audience to identify a particular referent [Donnellan1977, Kronfeld1986]. While the generation of referential descriptions has received considerable attention in text and multimedia generation, the generation of attributive descriptions has received • relatively little attention in computational linguistics. : However, such descriptions are pervasive in the type of presentations which is the focus of our research. We are developing systems that automatically generate presentations consisting • of coordinated text and information graphics (graphics for presenting abstract, quantitative or relational information as opposed to depictions of real-world objects or processes). For example in our current implementation, the •system produces analyses and summarizations of large amounts of data created by a transportation scheduling program. In this domain, it is necessary to generate descriptions of aggregate quantities of complex attributes such as total port capacity of all ports and 90~ of the total weight• of the cargo arriving by day 25. Furthermore, in this genre both referential and attributive uses of descriptions occur. .. in our approach, presentations are &quot;generated •using .an architecture that integrates hierarchical Planning to achieve media-independen t communicative goals with task-based graphic design. This architecture has been implemented in a prototype system. The focus of this paper is on the representation and role of attributive descriptions in the architecture. First, we describe the referential-attributive distinction and its importance in dialogue and text generation. Next, we discuss its importance in task-based graphic design. After providing an overview of our architecture, we describe how attributive descriptions are planned. We conclude with a survey of related work. 18
AN ARCHITECTURE FOR OPPORTUNISTIC TEXT GENERATION  We describe the architecture of the ILEX system, • which supports opportunistic text generation. In • web-based text generation, the SYstem cannot plan the entire multi-page discourse because the user&apos;s browsing path is unpredictable. For this reason, • the system must be ready opportunistically to take • advantage of whatever path the user chooses. We describe both the nature of opportunism in ILEX&apos;s museum domain, and then show how ILEX has been designed to function in this environment. The architecture presented addresses opportunism in both content determination and sentenceplanning.
Controlled Realization of Complex Objects by Reversing the Output of a Parser  This paper Is astudy in the tactics of content selection and realization at the micro-planning level. It presents a .technique for controlling the content and phrasing of complex sentences through• the use of data derived from a parser that has read through a corpus and taken note of which variations do and do not occur in the realization of the concepts in the genre the corpus is taken from. These findings are entered as annotations on a new representational• device, a &apos;saturation lattice&apos;, that provides a systematic way to define partial information and is the jumping off point for the micro-planner. The generator and parser are both based •on a declarative, bi-directional representation of realization relationship between concepts and text.
De-Constraining Text Generation  We argue that the current, predominantly task-oriented, approaz~hes to modularizing text • generation, while plausible and useful conceptually, set up spurious conceptual and operational constraints. We propose a data-driven approach to modularization and illustrate how it eliminates • •the previously ubiquitous constraints on combination of evidence across modules and on • control. We also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts.-1 Introduction This paper addresses the area of text generation known as microplanning [Levelt1989, Panaget1994, Huang and Fiedler1996], or sentence planning [Rambow and Korelsky1992]; [Wanner and Hovy1996]. Microplanning involves low-level discourse structuring and marking, sentence boundary planning, clause•internal structuring and all of the varied subtasks involved in lexical choice, These complex tasks are often modularized and treated separately. The general argument is that since sentence planning tasks are not single-step operations, since they do not have to be performed in strict sequence , and since the planner&apos;s operation is non-deterministic, each sentence planning task should be implemented by a separate module or by several modules (see, e.g., [Wanner and Hovy1996]). Such an argument is natural if generation is viewed as a set of coarse-grained tasks. Indeed, with the exception of a few researchers ([Elhadad et a1.1997] and the incrementalists listed below), the task-oriented view is standard in the generation community. Unfortunately, task-oriented generation sets up barriers among the components of the generation process, primarily because, in a realistic scenario, the tasks are intertwined to a high degree. Overcoming these barriers has become a central topic in generation research (see below). In our approach the basis of modularization is sought in the nature of the input data to the generation process, in our case, a text meaning representation, formulated largely in terms of an ontology. This data-oriented approach is similar to that taken by many incremental generators [De Smedt1990, Reithinger1992], although these tend to concentrate on syntactic processing. But see [Kilger1997], who explicitly addresses microplanning. We feel that our work provides an optimal path between task-oriented generators (which face problems due to the interrelationships between the tasks) and traditional incremental generation (which does not take advantage of problem decomposition as discussed below). In what follows we describe our ontology-based modularization, the kind of constraints which can be automatically set up within such a paradigm, and the control mechanism we employ to process it. We focus on the task of lexicalization, but other microplanning tasks have been handled similarly. We conclude with a discussion of the avoidable barriers inherent in most current approaches, along • 48
AUTOMATIC GENERATION OF SUBWAY DIRECTIONS: SALIENCE GRADATION AS A FACTOR FOR \ DETERMINING MESSAGE AND FORM  A frequently encountered problem in urban life is navigation. In order to get to some place we use private means or public transportation, and if we lack clear directions we tend to ask for help. We will deal in this paper with the descriptions of subway routes and their automatic generation. In particular, we will try to show how the relative importance of a given piece of information can effect not only the message but also the form.
INTRODUCING MAXIMAL VARIATION IN TEXT PLANNING FOR SMALL DOMAINS This work on text planning is part of a project that is concerned with investigating Dutch prosody by implementing a concept-to-speech system. The project focuses on the prosodic module, which predicts the pitch accents and the prosodic boundaries of an utterance on the basis •of its semantic and syntactic •structure and its discourse context. The key idea is that a natural language generator, as opposed to a parser, generates extensive and reliable information about the liriguistic structure of an utterance, and is therefore particularly suitable to provide input to the prosodic •module. This approach requires at least two things from the generator. First, it should generate all information that the prosodic module needs for deriving the prosodic structure of an utterance. Second, it should generate as much variation as• possible, in order to put the prosodic module to the test. Given a conventional architecture consisting of a text planner followed by a surface generator, these requirements affect the text planner. For instance, it should keep track of the information status of concepts, because the distinction between old and new information is important for pitch accent placement. With respect to the second requirement, it should be able to paraphrase one and the same conceptual structure as different semantic structures, which are in turn realized as different •sentences by the surface generator.• This paper describes a text planner that meets these requirements. It is described on the basis of an application of concept-to-speech in which train table information is taken as input to generate a spoken description, in Dutch, of how to get from one placeto another by train. The approach, however, is easily adaptable to similar domains. Since we are primarily interested in generating linguistically rich and maximally varied input for the prosodic module, the text planner is rather uncomplicated and ignores many other aspects of text planning like rhetorical "Thanks to Peter-Arno Coppen, Wire Claassen, Carlos Gussenhoven, and two anonymous reviewers for their useful comments and corrections. • Figure 1: Example of an input structure structuring of the text or tailoring information to the user. In fact, there is no real dialogue with the user in the sense that the system is capable •of reacting on feedback from the user. Also, efficiency considerations (real time behaviour) have not played a role. The interesting points, however, are that the text planner employs a constraint-based approach to produce variation and that its implementation is completely grammar-based within the framework of Functional Unification Grammar. • This work describes a method for text planning that is suitable to small domains like train table information. Our aim is to introduce maximal variation in the packaging of • information and in the linear order of its presentation. To this end, we regard text planning as a goal-driven process that dynamically constructs a text plan. The goal is a state where all information in the input is shared with the user; the means to achieve this goal are utterances. The application of utterances is limited by constraints that refer to the user&apos;s current state of knowledge. This approach to text planning can be conven!ently implemented as a Functional Unification Grammar. In addition, we show how optional or inferable information can be accountedfor, how focus can be distributed, and how the generation of anaphoric expressions can be constrained by looking at the form and content of a previous utterance.
A NEW APPROACH TO EXPERT SYSTEM EXPLANATIONS 1 Expert System Explanation  Expert systems were one of the first applications to emerge from initial research in artificial intelligence , and the explanation of expert system reasoning was one of the first applications of natural language generation3 This is because the need for explanations is obvious, and generation from a knowledge-based application such as reasoning should be relatively straightforward. However, while explanation has been universally acknowledged as a desirable functionality in expert systems, natural language generation has not taken a central place in contemporary expert system development. For example, a popular.text book about expert systems such as (Giarratano and Riley, 1994) stresses twice in the introduction the importance of explanation, but provides no further mention of explanation in the remaining 600 pages. (The book is based on the popular CLIPS framework.) In this paper, we present a new approach to enhancing an expert system with an explanation facility. The approach comprises both software components and a methodology for assembling the components. The methodology is minimally intrusive into existing expert system development practice. This paper is structured as follows. In Section• 2, we discuss previous work and identify shortcomings. We present our analysis of knowledge • types in Section 3. Section 4 presents the •Security Assistant and its explanation facility. Finally, we sketch a general methodology for explainable expert system engineering in Section 5. 1The work reported inthis paper was carried out while all authors were at CoGenTex, Inc., and is in part supported by contract F30602-96-C-0076 awarded by the Information Directorate of the Air Force Research Laboratory at the Rome Research Site. We would like to thank Rob Flo, project engineer, for his support and feedback. We would also like to thank Joe McEnerney for help in integrating the explanation facility with the SA, and Mike White and two anonymous reviewers for useful comments. 78
Macroplanning with a Cognitive Architecture for the Adaptive Explanation of Proofs A )erson who explains to another person a technical device or a logical line of reasoning adapts his explanations to the addressee's knowledge. A computer program designed to take over the explaining part should also adopt this principle.Assorted systems take into account the intended audience's knowledge in the generation of explanations (see e.g. [Cawsey, 1990, Paris, 1991, Wahlster et al., 1993). Most of them adapt to the addressee by choosing between different discourse strategies: Since proofs areinherently rich in inferences, their explanation must also consider which inferences the audience can make [Hora~ek, 1997, Zukerman andMcConachy, 1993]. However, because of the constraints of the human memory, inferences are not chainable without costs. The explicit representation of the addressee's cognitive states proves to be useful in choosing the information to convey [Walker and Rambow, 1994]. While a mathematician communicates a proof on a level of abstraction that is tailored to the audience, state-of-the-art proof presentation systems such as PROVERB [Huang and Fiedler, 1997] verbalize proofs in a nearly textbook'like style on a fixed degree of abstraction given by the initial representatio n of the proof. Nevertheless, PROVERB is not restricted to the presentation on a certain level of abstraction. Adaptation to the reader's knowledge may still take place by providing the appropriate level of abstraction in the initial representation of the proof.Drawing on results from cognitive science, we are Currently developing an interactive proof explanation system~ called P. rez (for proof explainer). In this paper, we propose an architecture for its dialog planner based on the theory of human cognition AcT-R [Anderson, 1993]. The latter explicitly represents the addressee's knowledge in a declarative memory and his cognitive skills in procedural production rules. This cognitive model enables the dialog planner to trace the addressee's cognitive states during the explanation. Hence, it can choose for each proof step as an appropriate explanation its most abstract justification known by the addressee.The architecture of P. rex, which is sketched in Section 3, is designed to allow for multimodal generation. The dialog planner is described in detail in Section 4. Since it is necessary to know some of the concepts in ACT-R to understand the macroplanning process, the cognitive architecture is •first introduced in the next section. In order to generate high quality explanations in technical or mathematical domains, the presentation must be adapted to the knowledge of the intended audience. Current proof presentation systems only communicate proofs on a fixed degree of abstraction independently of the addressee&apos;s knowledge. in this paper that describes ongoing research, we propose an architecture for an interactive proof explanation system, called P. rex. Based on the theory of human cognition AcT-R, its dialog planner exploits a cognitive model, in which both the user&apos;s knowledge and his cognitive processes are modeled. By this means, his cognitive states are traced during the explanation. The explicit representation of the user&apos;s cognitive states in AcT-R allows the dialog planner to choose a degree of abstraction tailored to the user for each proof step to be explained.
EXPERIMENTS USING STOCHASTIC SEARCH FOR TEXT PLANNING  • Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? We describe experiments with a number of heuristic Search methods for this task. &apos; 1 • Introduction: Text Planning 1.1 The Task This paper presents some initial experiments using stochastic search methods for aspects of text planning. The work was motivated by the needs of the ILEX system for generating descriptions of museum artefacts (in particular, 20th Century jewellery) [Mellish e t al 98]. We present results on examPles semi-automatically generated from datastructures that exist within ILEX. Forming a set of facts about a piece of jewellery into a structure that yields a coherent text is a non-trivial problem. Rhetorical Structure Theory[Mann and Thompson 87] claims that a text is coherent just in case it can be analysed hierarchically in terms of relations between text spans. Much work in NLG makes the assumption that constructing something like an RS tree is a necessary step in the planning of a text. This work takes as its starting point Marcu&apos;s [Marcu 97] excellent • formalisation of RST and the problem of building legal RST trees, and for the purposes of this paper the phrase &quot;text planning&quot; will generally denote the task characterised by him. In this task, one is given a set of facts all of which should be included in a text and a set of relations between facts, some of which canbe included in the text. The task is to produce a legal RS tree using the facts and some relations (or the &quot;best&quot; such tree). Following the original work on RST and assumptions that have been commonly made in subsequent work, we wil ! assume that there is a fixed set of possible relations (we include &quot;joint&quot; as a second-class relation which can be applied to any two facts, but whose use is not preferred). Each relation has a nucleus and a satellite (we don&apos;t consider multiple nuclei or satellites here, apart from the case of &quot;joint&quot;, which is essentially multinuclear). Each relation may be indicated by a distinctive &quot;cue phrase&quot;, with the nucleus and satellite being realised in some fashion around it. Each relation has applicability conditions which can be tested between two atomic facts. For two complex text spans, a relation holds exactly when that relation holds between the nuclei of those spans. Relations can thus hold between text spans Of arbitrary size. Figure 1 shows an example of the form of the input that is used for the experiments •reported here. Each primitive &quot;fact&quot; is represented in terms of a subject, verb and complement (as well as a unique identifier). The &quot;subject&quot; is assumed to be the entity that the fact is &quot;about&quot;. The approaches reported here have not yet been linked to a realisation component, and so the entities a 80 South Bridge, Edinburgh EH1 1HN.
ABDUCTIVE REASON G FOR SYNTACTIC REALIZATION*  Abductive reasoning is •used in a bidirectional framework for syntactic realization and semantic interpretation. The use of the framework is illustrated in a case study of sentence generation, where different syntactic forms are generated depending on the status of discourse information. Examples are given involving three differen t syntactic constructions in German root clauses. 1 Pragmatics in Natural Language Generation The computational treatment of pragmatics in natural language generation is often-directly or indirectly-• oriented around the Gricean maxims [Grice 7•5]. Their effects emerge from the pragmatic model of the generation system so that the generated text s satisfy these maxims. The texts should be a true characterization of a state of affairs, they should be as informative as possible, relevant, and perspicuous. While the first three maxims are related to what is said, the last maxim is related to how it is said. The category of perspicuity principles includes constraints on avoiding obscurity and ambiguity, or being brief and orderly. It is anything but clear how these principles should be interpreted precisely. Several attempts have been • made to remedy this in computational work on generating texts that best satisfy these maxims, especially with respect to the generation of referring expressions (e.g. [Dale et al. 95]). However, there is•more to pragmatics than satisfying Gricean maxims. In particular, the category of perspicuity principles does not usually cover the important fact that texts are tailored toa specific addressee, not only in content,• i.e., with respect to her or his informational needs, but also in the linguistic form, i.e., word order, syntactic constructions, the choice of lexical items, and eventually prosodic information. This tailoring of the linguistic form to the listener is termed &quot;information structuring&quot;. In generating texts, information Structuring requires, among other things, the use of some listener model, which may include information about the listener&apos;s knowledge, goals, properties, etc. LinguiStic approaches to describing the principles of information structuring have sometimes characterized information structure as an instruction to the listener about how to construct a model of the communicated state of affairs [Prince 81]~ In AI and Computational Linguistics, tailoring the message to the •-. Usercomprises very often solely content planning, which only indirectly determines the linguistic output.
GENERATING WARNING INSTRUCTIONS BY PLANNING ACCIDENTS AND INJURIES We •present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances • , that is able to automatically generate safety Warnings to the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of the device, and these "injury sub-plans" are then used to instruct the user to avoid these situations. Thus, unlike other instruction generation systems, our •system tells the user what not to do as well as what to do. We will show how knowledge about a device that is assumed to already exist as part of the engineering effort, together with adequate, domain-independent knowledge about .the environment, can be used for this. We also put forth the notion that actions are performed on the materials that thedevice operates upon, that the states of these materials may change as a result of these actions, and that the goal of the system should be defined in • terms of the final states of the materials.We take the stand that a complete natural language instruction generation system for a device should have, at the top level, knowledge of the device (as suggested by Delin et al. (1993)). This is one facet of instruction generation that many NLG systems have largely ignored by instead incorporating the knowledge •of the task at their top level, i.e., the basic content of the• instructions is assumed to already exist and does not need to be planned for. In our approach, all the knowledge necessary for the planning stage of a system i s contained (possibly in a more abstract form) in the knowledge of the artifact together with the world knowledge. The kinds of knowledge that Should .be sufficient for this planning are device knowledge •(topological, kinematic, electrical, thermodynamic, and electronic) and world knowledge.The IDAS project of Reiter et al. (1992;1995) served as a key motivation for this work. One of the primary goals of the IDAS project was to automatically generate technical documentation !   I  i  I  I  I  I  I  I  I  I  I  I  I  I  I  I  |   I from a domain knowledge base containing design information (such as that produced by an advanced computer-aided design tool) using NLG techniques. IDAS turned outto be successful in demonstrating the usefulness, from a cost and benefits perspective, of applying NLG technology to partially automate the generation of documentation. If work in qualitative process theory, using functional-specifications such as those in e.g., (Iwasaki et al., 1993), can yield the device and world knowledge that are required for text pianning, then the need for cost effectiveness would be met. We present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances; that is able to automatically generate safety warnings tO the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of the device, and these &quot;&apos;injury sub-plans, are then used to instruct the user to avoM these situations.
DISCOURSE MARKER CHOICE IN SENTENCE PLANNING When a coherence relation ties two adjacent portions of text together, it is often lexically signalled on the linguistic surface with a suitable word--most Often a conjunction, but also a preposition, a prepositional phrase or an adverb [Quirk et al. 1972]. The set of words from these grammatically heterogeneous classes that can signal coherence relations we call discourse markers. For example, :the CONCESSION relation in English can be signalled with the subordinator although, the adverb Still, the preposition despite, and a number of other words. For most coherence relations, language offers quite a variety of such markers, as several studies Of individual relations have demonstrated (see references in Section 2). Accordingly, from the generation perspective, a serious choice task arises if the produced • text is not only to simply signal :the coherence relation, but moreover to reflect pragmatic goals, stylistic considerations, and the different connotations markers have. The importance of these factors was stressed by Scott and de Souza [1990], who gave a number of informal heuristics for when and how to signal the presence of coherence relations in text. Fleshing out the choice task in order to come up with a computational model, though, reveals two sources of difficulty. For one thing, in addition to syntactic variety, the precise semantic and pragmatic differences between similar markers can be quite difficult to determine. For instance, the CONCESSION markers although and even though differ merely in emphasis; the CAUSE markers because and since differ in whether they mark the following information as given or not; the German CAUSE markers weil and denn differ in the illocution type of the conjuncts (proposition versus statement  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  i  I  I  I the dependencies between marker choice and other generation decisions are rather intricate. The idea of avoiding them is, presumably, the reason for the simplistic treatment of marker choice in typical generators to-date: They regard discourse markers as mere consequences of the structural decisions, hence do not perform any choice. We wish to demonstrate, however , that this strategy, which is typical for dealing with closed-class lexical items in general, is too great .a simplification in these cases.In this paper, we propose to use a discourse marker lexicon as a declarative resource for the sentence planning stage of the text generation process. The paper is organized as follows. Section 2 examines the role of discourse markers in NLG and reviews the state of the art. Section 3 briefly summarizes the ideas on sentence planning that have arisen in the past few years and argues that for a sophisticated treatment of discourse marker choice, a dedicated lexicon is to be used as one information resource in sentence planning. Section 4 •introduces the discourse marker lexicon we are currently developing, and Section 5 describes how this lexicon can be usefully employed in the sentence planning phase to realize more flexible marker production. In text, discourse markers signal the kind of coherence relation holding between adjacent text spans; for example, because, since; and for this reason are different markers for causal relations. For any but the most simple applications of text generation, marker Selection is an important aspect of producing cohesive text. However, present systems use markers in fairly simplistic ways andcannot make use of the full potential of markers that language offers for a given relation. • To improve this situation, we propose a specialized lexicon for discourse markers, which holds the relevant constraints and preferences associated with the markers, and which can be used by a text generator to make an informed choice among alternative ways of expressing a relation in the given context, we demonstrate how .the lexicon can be employed in the generation • process and propose to perform discourse marker choice in the sentence planning stage, where the interactions with other generation• decisions can be accounted for.
Clause Aggregation Using Linguistic Knowledge An expression is more concise than another expression if it conveys the same amount of information in fewer words. Complex sentences generated by combining clauses are more concise than corresponding simple sentences because multiple references to the recurring entities are removed. For example, clauses like "Jones is a patient" and "Jones has hypertension" can be combined into a more concise sentence "Jones is a hypertensive patient. ' ~ To illustrate the common occurrence of such repeated entities in generation, let us take a shipping company's database as an example. Each database tuple being conveyed is transformed into one or multiple propositions or clauses (we use these terms interchangeably throughout the paper). Each proposition refers to a piece of information which usually corresponds to a simple sentence. The database might Contain multiple shipments to the same location possibly on the same day. Generating a sentence for each tuple separately would containrepetitive and potentially redundant references to the same location Or date. Though we used a relational database as the example, the observation about recurring entities in the input is also valid for other types of input, such as execution traces from expert systems. CASPER (Clause Aggregation in Sentence PlannER) is a sentence planner which focuses on generating concise sentences. Clause aggregation can happen at three levels: inferential, rhetorical, and linguistic. At the inferential level, user modeling, domain knowledge, and common sense reasoning are used to reduce the number of concepts to convey. Such operations are implemented in the content planner and clauses are combined without consulting lexical resources. Text summarization is an application which uses inferential operators extensively. For example, the two sentences "John hit Mary" and "Mary kicked John" might imply that "John and Mary fought." To define a set of inferential operators for unrestricted text is beyond the state-of-art. Because it is unlikely that the inferential operators for our domains (medical briefings and telephone network plan descriptions ) will be reusable for other applications, we have directed our effort into aggregation operations at other levels. At the rhetorical level, clauses are combined based on their rhetorical relationships [Mann and Thompson, 1986], such as CONTRAST and CONDITION. We will take advantage of such information in future aggregation work. At the linguistic level, lexical and Syntactic information are used to combine clauses. In this paper, we concentrate on two types The patient's past medical history is significant for bladder carcinoma1, status post cystectomy with a urostomy tube insertion2, left nephrolithiasis~, status post surgery4, recurrent syncopes, questionable vagovagal6, a neurological workup was negativer, and the EPS was negatives, abdominal aortic aneurysm approximately 5 cmg, high cholesterol10, exertional anginan, past tobacco smoker, quit about one year ago12. Figure 1: The sentence with maximum number of propositions in the corpus Of linguistic aggregation operators: hypotactic and paratactic. The term, hypotaxis, describes the relation between a dependent element and its dominant element. Hypotactic operators transform one clause into a modifier and attach the modifier to the dominant clause. In contrast, paratactic aggregation operators combine clauses together using constructions of equal status, such as.coordination. CASPER is used in two separate projects, MAGIC (Multimedia Abstract Generation for Intensive Care) and PLANDoc, to increase the fluency of the generated text.MAGIC [Dalal et al., 1996, MeKeown et al., 1997 automatically generates multimedia briefings to describe the post-operative status of a patient after undergoing Coronary Artery Bypass Graft (CABG) surgery. It uses the existing computerized information infrastructure in the operating rooms at Columbia Presbyterian Medical Center. PLANDoc[Kukich et al, 1994 generates English summaries based on somewhat cryptic traces of the interaction between planning engineers and LEIS-PLAN TM. It documents the timing, placement and cost of new facilities for routes in telephone networks.In Section 2, we present a corpus analysis to identify the complexity of the target output in MAGIC. Section 3 describes the semantic representation used in CASPER. Details of hypotactic operators are presented in Section 4. Paratactic operators are described in Section 5. Section 6 describes related work. By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.
ATTENTION DURING ARGUMENT GENERATION AND PRESENTATION* In this paper, we describe the operation of our argument-generation system, NAG (Nice Argument Generator). We consider its content planning and argument presentation processes, and discuss the attentional mechanism used in both of these processes.Given a goal proposition, NAG's objective is to generate a nice argument for it, by which we mean one which achieves a balance between what is normatively justifiable and what persuades the interlocutor. To this end, NAG consults a normative model, which contains our best understanding of the domain of discourse, and a model of the user's beliefs. The main modules of the system are shown in Figure 1.The Strategist drives the argumentation process. During argument generation, it activates a generationanalysis cycle as follows (Section 3). First, it invokes the Attentional Mechanism (Section 4) to activate salient propositions, which are used to construct an initial Argument Graph for an argument, or to extend an already existing Argument Graph. (An Argument Graph is a network with nodes that represent propositions, and links that represent the inferences that connect these propositions.) The Strategist then calls the Generator to continue the argument building process (Section 5). The Generator in turn fleshes out the Argument Graph by activating Reasoning Agents, which consult several sources of information, and incorporating the inferences and propositions returned by these agents into the Argument Graph. This Argument Graph• is returned to the Strategist, which passes it to the Analyzer in order to evaluate its niceness and check for reasoning flaws (Section 6). If the Analyzer indicates that the argument is not nice enough, i.e., there is not sufficient belief in the•goal in the user or the normative model, then the Strategist re-activates the Generator in order to find further support for the premises in the argument, and so on.-The generation-analysis cycle continues until a sufficiently nice Argument Graph is generated. This graph is then passed to the Argument Presenter, which selects an argumentation strategy and determines propositions to be removed from the argument, aiming to produce a simpler, enthymematic argument. After each removal, the Presenter activates the Analyzer to check whether theargument is still nice and the Attentional Mechanism to determine whether the argument can still be followed by the user.Thus, the Attentional Mechanism is used in two different stages of the argumentation process. During argument generation, it focuses the argument construction process on concepts which are related to the "This research was supported in part by Australian Research Council grant A49531227. User Argument/Inquiry/Goal Proposition Figure 1: System Architecture goal, avoiding some distractions. During argument presentation, the Attentional Mechanism supports the generation of enthymematic arguments. We describe the operation of our argumentation system, and discuss its use of attentional focus during both content planning and argument presentation. During content planning, attentional focus guides an abductive process used to build-up arguments. This process is applied to a model of a user&apos;s • beliefs and a normative model. During argument presentation, attentional focus supports the generation of enthymematic arguments.
PLANNING DIALOGUE CONTRIBUTIONS WITH NEW •INFORMATION The question that we will investigate is the starting point of generation, and we argue that this is NewInfo, the piece of new information exchanged in interaction, with which the mutual context gets updated. This may sound like sliding on the '!slippery slope" of (McDonald, 1993), who points out that the answer to the question 'how far back does generation go?' is tied to the proportional amounts of linguistic and contextual information in the specification which serves as the source of generation. However, we would like to stress the separation of communicative knowledge from the minimal information units used as the basis for generation, and that the selection of the information units and the way they axe actually communicated are subject to conditions which may require changes in one of the tasks before the other one is properly completed. In (spoken) interaction, utterances consist of fine-grained information units to which the listener immediately reacts by giving feedback or a response, and this feedback then directs the speaker to modify her utterance accordingly. Hence, we might as well slide the slippery slope all the way down, and conclude that generation starts simultaneously with interpretation, as a reaction to the presented information. The initial 'messag e' is then gradually specified into a linguistic expression with respect to languagespecific knowledge and communicative • requirements of the situation.Consequently, in this paper we focus on the three rs in generation: Incrementality, Immediacy, and Interactivity. The research is still ongoing, so we pose questions more than provide answers. After introducing the three I's, the specific questions we will discuss are: (1) What are suitable utterance• units for exchanging information in spoken dialogues? (2) What is the relationship between NewInfo and organisation of the task/d0main information? (3) What kind of requirements are imposed on the generator? 1 aThroughout the paper we use generator to refer to the whole system that generates rather than analyses natural language. The component of the generation system that mostly deals with world-knowledge, tasks, plans and communicative goals, is called planner, while realiser is the component which concerns lexico-semantic and syntactic information. Sentence planning is also called micro-planning. The paper discusses a framework for planning contributions in a spoken dialogue system, and focuses especially on the three/&apos;s: Incrementality, Immediacy, and Interactivity. The emphasis is on communicative principles and the notion of NewInfo, or the information focus of the utterance. NewInfo provides a natural way of to conceptualize the planning process and to generate utterances on the level of granularity •required in spoken interaction.
GENERATION OF NOUN COMPOUNDS IN HEBREW: CAN SYNTACTIC KNOWLEDGE BE FULLY ENCAPSULATED? Over the past three years,• we have started developing HUGG, a syntactic realization component for Hebrew. One of our objectives is to investigate constraints on the design of the input specification language to a syntactic realization component through a contrastive analysis of the requirements of English and Hebrew. By design, we are attempting to keep the input to HUGG as similar as possible to the one we defined in the SURGE syntactic realization for English [7]. A detailed analysis of syntactic constructs specific to Hebrew becomes, therefore, critical to evaluate to which extent the input specification language can abstract away from knowledge of the syntax.We investigate in this paper one such construct: the Hebrew noun-compounding form known as srnixut. Because smixut is morphologically marked and remarkably productive in Hebrew, there exists a vast tradition of work in descriptive grammar of Hebrew providing functional analysis of the phenomenon [11] [10] [13]. This previous work has served as a fertile ground for our own generation-specific purposes.The specific issue we discuss in this paper is: what information in the input specification to the syntactic realization component can license the selection of a smixut construct. The classical objectives of modularity and knowledge encapsulation indicate that this decision should be a private decision of the syntactic realization component. Because there are so many syntactic constraints on the use of smixut, the objective of encapsulation is made even more desirable.After a thorough analysis of the different functions of the smixut construct and the constraints over its use, our conclusion, however, is that this reductionist strategy fails: we cannot explain the selection of a smixut construct without considering simultaneously lexical, semantic and pragmatic factors.Theoretically, in order to allow the syntactic realization component to select a smixut construct adequately, we are, therefore, left with two options: (1) either provide full, detailed access from the syntactic realization component to the complex semantic and pragmatic features that can impact on the decision; or else, (2) allow the other components to request the use of a smixut construct when they deem it adequate. In either case, modularity and encapsulation suffer. This analysis informs us in our design of a bilingual realization component: if a feature like use-smixut is required in the input to the syntactic component, this level of abstraction cannot be appropriate as a bilingual construction. It also informs us in the general ongoing debate over the design of reusable syntactic components and their place in the architecture of generators.From a more pragmatic perspective, however, we also provide a set of simple defaults for the generation of smixut based on a simple semantic classification of the relations head-modifier. We evaluate the validity of this classification by constructing input specifications for a corpus of more than 800 comp!ex, noun phrases and regenerating from them. The validation process includes two aspects: (1) we test that human coders agree on the semantic relations they use to label complex NPs; and (2) we verify that the generator's decision to produce a smixut construction corresponds to that observed in the corpus. Preliminary results are provided in Section 4.3. They encourage us to view in the set of semantic relations we propose a useful basis for the design of an interlingual input specification language.In the rest of the paper, we first briefly review the main approaches to the treatment of nouncompounds in English and in Hebrew. In Section 3 we provide descriptive data on the use of smixut in Hebrew. We then describe in Section 4 a first approach to the generation of smixut based on a simple semantic classification similar to that found in [12]. In Section 5, we identify the limitations of such an approach, illustrating that an explanation based on recoverable semantic relations cannot provide sufficient nor necessary conditions for the generation of smixut. However, the preliminary empirical evaluation we present in Section 4.3 demonstrates that the semantic relation approach provides a useful default that works "most of the time." Hebrew •includes a very productive noun-compounding construction called smixut. Because smixut is marked morphologically and is restricted by many syntactic constraints, it has been the focus of many descriptive studies in Hebrew grammar. We present the treatment of smixut in HUGG, a FUF-based syntactic realization system capable of producing complex noun phrases in Hebrew. We contrast the treatment of smixut with noun-compounding in English and illustrate the potential for paraphrasing it introduces. We Specifically address the issue of determining when a smixut construction can be generated as opposed to other semantically equivalent constructs. We investigate several competing hypotheses-smixut is lexically, semantically and/or pragmatically determined. For each hy: pothesis, we explain why the decision to produce a smixut construction cannot be reduced to a computation Over features produced by an outside module that Would not need to know about the smixut phenomenon. We conclude that smixut provides yet another theoretical example where the interface that a syntactic realization component presents to the other components of a generation architecture cannot be made as isolated as we would hope. While the syntactic constraints on smixut are encapsulated within HUGG, the input Specification language to HUGG must contain a feature that specifies that smixut is requested if possible. • However, because smixut accounts for close to half the cases of NP modifiers observed on a corpus of complex NPs, and it •appears to be the unmarked realization form for some frequent semantic relations, we empirically evaluate a default setting strategy for the feature use-smixut based on a simple semantic Classification of the relations head-modifier in the NP. This study provides a Solid ground for the definition of a small set of predicates in the input specification language to HUGG, that has applications beyond the selection of smixut-for the determination of the order of modifiers in the NP and the use of stacking vs. conjunction-and for the definition of a bilingual input specification language.
TEXTUAL ECONOMY THROUGH CLOSE COUPLING OF SYNTAX AND SEMANTICS • Matthew Stone Th e problem we address is that of producing efficient descriptions of objects, collections, acti0ns, events, etc. (i.e., any generalized individual from a rich ontology for Natural Language such as those described in [2] •and advocated in [9]). We are interested in a particular kind of efficiency that we call textual economy, which presupposes a view of sentence generation as goal-directed activity that has broad support in Natural Language Generation (NLG) research [1,5,15,17]. According to this view, a system has certain communicative intentions that it aims to fulfill in producing a description. For example, the system might have the goal of identifying an individual or action o~ to the hearer, or ensuring that the hearer knows that has property P. Such goals can be satisfied explicitly by assembling appropriate syntactic constituents--for example, • satisfying the goal of identifying an individual using a noun phrase that refers to it or identifying an action using a verb phrase that specifies it. Textual economy refers to satisfying such goals implicitly, by exploiting the hearer's (or reader's) recognition of inferential links to material elsewhere in the Sentence that is there to satisfy independent communicative goals. Such material is therefore overloaded in the sense of [18]. l While there are other ways of increasing the efficiency of descriptions (Section 5), our focus is • on the efficiency to be gained by viewing a large part of generation in terms of describing (generalized) individuals.Achieving this however places strong requirements on the representation and reasoning used in generating sentences. The representation must support the generator's proceeding incrementally through the syntax and semantics of the sentence as a whole. The reasoning used must enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its (incomplete) syntax and semantics. Only by evaluating the status of such key questions as• what (generalized) individuals could the sentence (or its parts) refer to?1Pollack used the term overloading to refer to cases where a single intention to act is used to wholly or partially satisfy several of an agent's goals simultaneously.i 78 • what (generalized) individuals would the hearer take the sentence to refer to?• what would the sentence invite the hearer to conclude about those individuals?• how can this sentence be modified or extended?can the generator recognize and exploit an opportunity for textual economy. These representational and reasoning requirements are met in the SPUD system for sentence planning and realization [26,27]. SPUD draws on earlier work by Appelt [1] in building sentences using planning techniques, sPUD plans the syntax and semantics of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above. In this paper, we describe the intermediate representations that allow SPUD to do so, since these representations have been glossed over in earlier presentations [26,27]. Reasoning in SPUD is performed using a fast modal theorem prover [24,25] to keep track both of what the sentence entails and what the sentence requires in context. By reasoning about the predicated relationships withinclauses and the informational relationships [16] between clauses, sPUD is able to generate sentences that exhibit two forms of textual economy: referential interdependency among noun phrases within a single clause, and pragmatic overloading of clauses in instructions [7].For an informal example of the textual economy to be gained by taking advantage of predicated relationships within clauses, consider the scene pictured in Figure 1 and the goal of getting the hearer to take the rabbit currently in the hat out of the hat it's currently in. Even though there are several rabbits, several hats, and even a rabbit in a bathtub and a flower in a hat, it would be sufficient here to issue the command:(1) Remove the rabbit from the hat.It suffices because one of the semantic features of the verb remove--that its object (here, the rabbit) starts out in the source (here, the hat). distinguishes the intended rabbit and hat in Figure 1 from the other ones.Pragmatic overloading [7] illustrates how an informational relation between clauses can support textual economy in the clauses that serve as its "arguments". In [7], we focused on describing (complex) actions, showing how a clause interpreted as conveying the goal13 or termination condition r of an action a partially specified in a related clause forms the basis of a constrained inference that provides additional information about a. For example,  Here, the two clauses (2a) and (2b) are related by purpose---specifically, enablement. The action t~ described in (2a) will enable the actor to achieve the goal/3 described in (2b). While a itself does not specify the orientation of the cup under the spigot, its purpose fl can lead the hearer to an appropriate choice---4o fill a cup with coffee, the cup must be held vertically, with its concavity pointing upwards. As-noted in [7], this constraint depends crucially on the purpose for which a is performed. The purpose specified i n (3b) does not constrain cup orientation in the same way: Examples like (1) and• (2) suggest that the natural locality for sentence planning is in a description of a generalized individual. Even though such descriptions may play out over several clauses (or even sentences), the predications within clauses and the informational relations across clauses of a description give rise to similar textual economies, that merit a similar treatment.2 SPUD• An NLG system must satisfy at least three constraints in mapping the content planned for a sentence onto the string of words that realize it [4,13,20]. Any fact to be communicated must be fit into an abstract grammatical structure, including lexical items. Any reference to a domain entity must be elaborated into a description that distinguishes the entity from its distractors--the salient alternatives to it in context. Finally, a surface form must be found for this conceptual material.In one architecture for NLG Systems that is becoming something of a standard [22], these tasks are performed in Separate stages. For example, to refer to a uniquely identifiable entity x from the common ground, first a set of concepts is identified that together single out x from its distractors in context. Only later is the syntactic structure that realizes those concepts derived.SPUD [26,27] integrates these processes in generating a description--producing both syntax and semantics simultaneously, in stages, as illustrated in (4).• ( Each such tree is paired with logical formulae that, by referring to a rich discourse model, characterize the semantic and pragmatic contribution that it makes to the sentence. We give a detailed example of SPUD's processing in Section 3 and describe in Section 4 the reasoning methods we use to derive computational representations like the set of distractors shown in (4). For now, a general understanding of SPUD suffices--this is provided by the summary in Figure 2. The procedure in Figure 2 is sufficiently general so that SPUD• can use similar steps to construct both definite and indefinite referring forms. The main difference lies how alternatives are evaluated. When an indefinite referring form is used to refer tO a brand-new generalized individual [19] (an object, for example,• Start with a tree with one node (e.g., s, uP) and one or more referential or informational goals.• While the current tree is incomplete, or its references are ambiguous to the hearer, or its meaning does not fully convey the informational goals (provided progress is being made):consider the trees that extend the current one by the addition (using LTAG operations) of a true and appropriate lexicalized descriptor;-rank the results based on local factors (e.g., completeness of meaning, distractors for reference, unfilled substitution sites, specificity of licensing conditions);-make the highest ranking new tree the current tree. or an action in an instruction), the object is marked as new and does not have to be distinguished from others because the hearer creates afresh "file card" for it. However, because the domain typically provides features needed in an appropriate description for the object, SPUD continues its incremental addition of content to convey them, When an indefinite form is used to refer to an old object that cannot be distinguished from other elements of a uniquely identifiable set (typically an inferrable entity [ 19]), a process like that illustrated in (4) must build a description that identifies this set, based on the known common properties of its elements. Several advantages of using LTAG in such an integrated system are described in [27] (See also previous work on using TAG in NLG such as [11] and [29]). These advantages include:• Syntactic constraints can be handled early and naturally. In the problem illustrated in (4), SPUD directly encodes the syntactic requirement that a description should have a head noun--missing from the concept-level account--using the NP substitution site.• The order of adding content is flexible. Because an LTAG derivation allows modifiers to adjoin at any step (unlike a top-down CFG derivation), there is no tension between providing what the syntax requires and going beyond what the syntax requires.• Grammatical knowledge is Stated once only. All operations in constructing a sentence are guided by LTAG's lexicalized grammar; by contrast, with separate processing, the lexicon is split into an inventory of concepts (used for organizing content orconstructing descriptions) and a further inventory of concepts in correspondence With some syntax (for surface realization).This paper delineates a less obvious, but equally significant advantage that follows from the ability to consider multiple goals in generating descriptions, using a representation and a reasoning process in which syntax and semantics are more closely linked:• It naturally supports textual economy. We focus on the production of efficient descriptions of objects, actions and events. We define a type of efficiency, textual economy, that exploits the hearer&apos;s recognition of inferential links to material elsewhere within a sentence. Textual economy leads to efficient descriptions because the material that supports such inferences has been included to satisfy independent communicative goals, and is therefore overloaded in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements on the representation and reasoning used in generating sentences. • The representation must support the generator&apos;s simultaneous consideration of syntax and semantics. Reasoningmust enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its-(inc0mplete)syntax,andsemantics. We show that these representational and reasoning requirements are met in the SPUD system for sentence planning and realization.
A Language-Independent System for Generating Feature Structures from Interlingua Representations Interlingua approach to machine t.ranslation (MT) aims at achieving the translation task by using an intermediate, language-independent meaning representation [Nirenburg et al., 1992]. The use • of such an artificial language, interlingua, makes the design of analysis and generation components separate in interlingua-based systems. Analysis is responsible for representing the input source text in interlingua, and generation produces the target text from those previously constructed representations. In other Words, the source and the target language are never in direct contact in such systems.Generation in such systems should at least perform lexical selection, syntactic structure creation, morpho!ogical inflection, and word order determination if planning (determination of overall text structure and sentence boundaries) is not considered. One approach to the design of • generation modul e in inter!ingua-ba.sed MT systems is to handle the first two tasks in a separate architecture, get a form of syntactica!ly represented target sentences, and achieve the last two tasks with a tactical generator. In this way, only the interlingua dependen t tasks are handled in processing interlingua representations.The aim of this paper is t.o present a computational architecture for generation which performs the tasks of lexical selection [Dorr, 1993] and syntactic structure • determination [Mitamura and Nyberg, 1992] in interlingua approach.The system is designed to take the interlingua representations ofindividual sentences .and produce their frame-based syntactic representations in which selected lexemes are included [Temizsoy, 1997]. A knowledge-based approach is utilized in the developed architecture such that information about, the t.arget language is taken from knowledge resources. In other words, its architecture is language-independent. The utilized interlingua is mainly based on an ontology, a hierarchical world model, to represent propositional content.It also utilizes special frames to represent semantic and pragmatic phenomena encountered in analysis. The arclfitecture uses Ontology while processing inter!ingua representation in addition to lexicon, map-rules (relation between interlingua and. target language syntactic structure), and target language's Syntax representation formalism. The architecture of the designed system is given in Figure 1. ....... The implemented system is used to generate the syntactic structure representations of Turkish sentences from their corresponding interlingua representations. The syntax representation formalism of Turkish is taken from a Turkish tactical generator previously developed by Hakkani [Hakkani, 1996]. The output of the system can be directly fed into this generator to produce the final reafizations of Turkish sentences. Although input resources do not providefull coverage of Turkish, special consideration is givento linguistic phenomena encountered in Turkish such as free word-order and narrative tense [Temizsoy, 1997].The rest of this paper is organized as follows. In Section 2, the interlingua formalism utilized in this work and its use of ontology is presented. Then, knowledge resources that provide information about the target language to the developed model are described in Section 3. Computational architecture of the system is presented in Section 4, and some specific examples from Turkish are given to demonstrate the system usage in Section 5. Finally, conclusion and and some possible future works are given in Section 6, Two main problems in natural language generation are lexical selection and syntactic • structure determination. In interlingua approach to machine translation, determining sentence structures becomes more difficult, especially when the interlingua does not contain any syntactic information, in this paper, a knowledge-based computational model which handles these two problems in interlingua approach is presented. The developed system takes interlingua representations of individual sentences, performs lexical selection, and produces frame-based syntactic structures. The system takes all the information about the target language from knowledge resources, in other words its architecture is language-independent. The implemented system is tested with Turkish through small-sized resources such that its output can be fed into a previously developed tactical generator to produce the final realizations of Turkish sentences.
Towards Multilingual Protocol Generation For Spontaneous Speech Dialogues* VERBMOBIL is a research project aiming at a Speech-To-Speech translation system for Face-toFace dialogues [Bub and Schwinn 1996, Bub, Wahlster, and Waibel 1997, Wahlster 1993. In its first phase (1993 -1996), a bilingual (English-German) system for time scheduling negotiation dialogues was developed. For its current second phase (1997 -2000), a third language, Japanese, has been incorporated. Additionally, more than two people should be able to participate in the dialogue, a setting we call multi-party. For the second phase, a novel system feature is currently under development -protocol generation. The idea is to provide the user(s ) with three kinds of protocols:Progress Protocol The most salient parts of the dialogue are summarized. Result Protocol The result of the negotiation is summarized. Status Protocol The current status of the negotiation is summarized.The first two are generated after the dialogue is finished. For the result protocol, a summarization of the goal reached in the negotiation is generated. The progress protocol serves as a simplified recapitulation of the progress of the dialogue, whereas the status protocol will, on demand, be generated during the ongoing dialogue. Its task is to deliver a brief description of the current status of the dialogue. All protocols must be generated in all three languages, putting extra requests on the protocol generation component; particularly, being as language independent as possible. In this paper we focus on the first of the protocol types listed above -the progress protocol."The research within VBRBMOBIL presented here is funded by the German Ministry of Research and Technology under grant 01IV101K/1. The authors would like to thank Amy Demeisi, Michael Kipp, Norbert Reithinger and the anonymous reviewers for comments on earlier drafts on this paper. This paper presents a novel multilingual progress protocol generation module. The module is used within the speech-to-speech translation system VERBMOBIL. The task of the protocol is to give the dialogue partners a brief description of the content of their dialogue. We utilize an .abstract representation describing, for instance, thematic information and dialogue acts of the dialogue utterances. From this representation we generate simplified paraphrases of the individual turns of the dialogue which together make up the protocol. Instead of writing completely new software, the protocol generation component is almost exclusively composed of already existing modules in the system which are extended by planning and formatting routines for protocol formulations. We describe how the abstract information is extracted from user utterances in different languages and how the abstract thematic representation is used to generate a protocol in one specific language. Future directions are given.
Fully Lexicalized Head-Driven Syntactic Generation  We describe a new approach to syntactic generation with Head-Driven Phrase Structure Grammars (HPSG) that uses an extensive off-line preprocessing step. Direct generation algo-• rithms apply the phra~se-structure rules (schemata) of the grammar on:line which is an com-putationally expensive step. Instead, we collect off-line for every lexical type of the HPSG grammar all minimally complete projections (called elementary trees) that can be derived with the schemata. This process is known as &apos;compiling HPSG to TAG&apos; and derives a Lexicalized Tree-Adjoining Grammar (LTAG). The representation as an LTAG is &apos;fully lexicalized&apos; in the sense that all grammatical information is directly encoded with the lexical item (as a set of elementary trees) and the combination operations are reduced from schema applications to the TAG primitives of adjunction and substitution. Given this LTAG, the generation task has a very different search space that Can be traversed very efficiently, avoiding the costly on-line applications of HPSG unification. The entire generation task from a semantic representation to a surface string is split into two tasks, a microplanner and a syntactic realizer. This paper discusses the syntactic generator and the preprocessing steps as implemented in the Verbmobil system.
APPROACHES TO SURFACE REALIZATION WITH HPSG In work on natural language generation, tile most influential linguistic framework has l)robably been Systemic Functional Grammar (SFG). However, in other areas of computational linguistics the most widely used grammatical framework appears to be Head-driven Phrase Structure Grarnmar (HPSG). Why is it, then, that using HPSG for generation has been almost as unpopular as using SFG for parsing? Without making any claim that HPSG is better t.han SFG for generation, we will review some plausible approaches to surface realization with HPSG. We will show that there are indeed some fundamental difficulties in using HPSG for generation, but also that there are some solutions to these difficulties.The first approach to mention is the radical one of converting HPSG into something else before generation, such as Tree Adjoining Grammar ( Kasper et al., 1995)..Though this seems to support the view that HPSG is unsuitable for generation, it is in fact a valuable contribution to work on compiling HPSG grammars for efficient processing, whether for parsing (Torisawa and Tsujii, 1996) or for generation.However, we will not be concerned with efficiency, but with more basic problems in the relations between HPSG and generation algorithmS. The question is, can existing algorithms be used with HPSG grammars at all? For clarity, we use the simplest versions of the algorithms, which were originally developed for use with definite clause (DCG) grammars and categorial grammar. For uniformity~ the algorithms are implemented m Prolog and the grammars are implemented in ProFIT (Erbach, 1995)... HPSG is widely Used in theoretical and computational linguistics, but rarely in natural language generation. The paper describes some approaches to surface realization in which HPSG can be used. The implementation of all the approaches combines generation algorithms in Prolog and HPSG grammars in ProFIT. It is natural to combine a head-driven HPSG grammar with a head-driven generation algorithm. We show how a simple head-driven generator can easily be adapted for use with HPSG. This works well with simplified semantics, but if we implement the full HPSG textbook semantics this approach does not work. In a second approach to head-driven generation, we implement some recent revisions of HPSG, and show that head-driven generation with HPSG • is in fact possible. We then switch to non-head-driven approaches. We show how a bag generation algorithm, developed for use with categorial grammar and indexed QLF, can be used with HPSG and MinimalRecursion Semantics. We describe an approach to incremental generation with HPSG, noting a difficulty for highly incremental generation with HPSG and proposing a solution. Finally we briefly mention a few other plausible approaches.
The Multex generator and its environment: application and development 1  
A FLEXIBLE SHALLOW APPROACH TO TEXT GENERATION in order to Support the efficient development of NL generation systems, two orthogonal methods are currently pursued with emphasis: (1) reusable, general, and linguistically motivated surface realization components, and (2) simple, task-oriented template-based techniques. Surface realization components impose a layer of intermediate representations that has become fairly standard, such as the Sentence Plan Language (SPL) [Kasper and Whitney, 1989]. This layer allows for the use of existing software with well-defined interfaces, often reducing the development effort for surface realization considerably. Template-based •techniques recently had some sort of revival through several application-oriented projects such as IDAS [Reiter et al.;, that combine pre-defined surface expressions with freely generated text in one or another way. However, the benefits of both surface realization components and template-based techniques are still limited from an application-oriented perspective. Surface realization components are difficult to use because of the differences between domain-oriented and linguistically •motivated ontologies (as in SPL), and existing template-based • techniques are too inflexible. In this paper we Suggest and evaluate flexible shallow methods for report generation applications requiring limited linguistic resources that are adaptable with little effort. We advise a close connection between domain-motivated and linguistic ontologies, and we suggest a layer of intermediate representation that is oriented towards the domain and the given task. This layer may contain representations of different granularity, some highly implicit, others very elaborate. We show how this is used by the processing components in a beneficial way. 1This work has beensupported by a grant for theproject TEMSIS from the European Union (Telematics Applications Programme, Sector C9, contract no. 2945). In order to support the efficient development of NL generation systems, two orthogonal • methods are currently pursued with emphasis: (i) reusable, general, and linguistically •motivated surface realization components, and (2) Simple, task-oriented template-based techniques. In this paper we argue that, from an application-oriented perspective, the benefits of both are still limited, lax order to improve this situation, we suggest and evaluate shallow generation methods associated with increased flexibility. We advise a close connection between domain-motivated and linguistic ontologies that Supports the quick adaptation to new tasks and domains, rather than the reuse of general resources. Our method is especially designed for •generating reports with limited linguistic variations.
THE PRACTICAL VALUE OF N&apos;GRAN IS IN GENERATION Langkilde and Knight (1998) introduced Nitrogen, a system that implements a new style of generation in which corpus-based ngram statistics are used in place of deep, extensive symbolic knowledge to provide Very large-scale generation (lexicons and knowledge bases on the order of 200,000 entities), and simultaneously simplify the input and improve robustness for sentence generation. Nitrogen's generation occurs in two stages, as shown in Figure 1. First the input is mapped tO a word lattice, a compact representation of multiple generation possibilities. Then, a statistical extractor selects the most fluent path through the lattice.The word lattice encodes alternative English expressions for the input when the symbolic knowledge is unavailable (whether from the input, or from the knowledge bases) for making realization decisions. The Nitrogen statistical extractor ranks these alternative s using bigram (adjacent word pairs) and unigram (single word) statistics Collected from two years of the Wall Street Journal. The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995).   Knight and Hatzivassil0glou, 1995).In essence, Nitrogen uses ngram statistics to robustly make a wide variety of decisions, from tense to word choice• to syntactic subcategorization, that traditionally are handled either with defaults (e.g., assume present tense, use the alphabetically-first synonyms, use nominal arguments), explicit input specification, or by using deep, detailed knowledge bases.However, in scaling up a generator system, these methods become unsatisfactory. Defaults are too rigid and limit quality; detailed input specs are difficult or complex to construct, or m~' be unavailable; and : : 248 I  t  t  I  I  I  I  I  I  I  I  large-scale comprehensive knowledge bases of detailed linguistic relations do not exist, and even those on a smaller scale tend to be brittle. This paper examines the synergy between symbolic and statistical language processing and discusses Nitrogen's performance in practice. The analysis provides insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves sealability. It also discusses the limits of bigram statistical knowledge. It is organized around specific examples of Nitrogen's output. We examine the practical s~&apos;nergy between symbolic and statistical language processing in a generator called Nitrogen. The analysis provides insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves scalability.. We also discuss the limits of bigram statistical knowledge. We focus on specific examples of Nitrogen&apos;s output.
GENERATION AS A SOLUTION TO ITS OWN PROBLEM  Natural language generation technology is now ripe for commercial exploitation, but one of the remaining bottlenecks is that of providing NLG •systems with user-friendly interfaces for Specifying the content of documents to be generated. We present here a new technique we have developed for providing such interfaces: WYSIWYM editing. WYSIWYM (What You See Is What You Meant) makes novel use of the system&apos;s generator to provide a natural language input device which requires no NL interpretation. .. .-only NL generation.
-EXEMPLARS: A Practical, Extensible Framework For Dynamic Text Generation In "NLG vs. Templates," Reiter [Reiter 95] points out that while template-based text generation tools and techniques suffer from many well-known drawbacks, they do nevertheless enjoy numerous practical advantages over most tools and techniques developed in the NLG community in many circumstances. These advantages include, among others, efficiency, simplified system architectures~ -full control over Output, and much reduced demands on knowledge .acquisition and representation. This leads Reiter to suggest that, from a practical perspective, one should use NLG techniques in hybrid systems that mix template-style and more sophisticated techniques; moreover, to facilitate adoption, NLG technologies should be developed so that they Can be used without "getting in the way."In line with this thinking, we have. been developing EXEMPLARS, an object-oriented, rule-based framework for dynamic text generation, with an emphasis on ease-of-use, programmatic extensibility and run-time efficiency. Exemplars [Rambow et al. 98] are schenm-like text planning rules that are so called because they are meant to capture an exemplary way of achieving a communicative goal in a given communicative context, as determined by the system designer. Each exemplar contains a specification of the designer' s intended method for achieving the Communicative goal. In the general case envisioned in Rainbow et al., these specifications can be given at the level of intentional-rhetorical, conceptual, lexicosyntactic, or formatting/hypertext structures. The present framework currently supports specifications only at the level of formatting/hypertext structures --using any SGML-based representation, such as HTML --or RealPro abstract syntactic structures [Lavoie &amp; Rambow 97]. A more complete range of specifications is instead Supported in PRESENTOR [Lavoie &amp; Rambow 98], a parallel implementation of the general approach with a complementary emphasis; while PRESENTOR emphasizes representation, we 266 .J~ have instead emphasized extensibility and classification-based planning. In future work, we plan to merge the best of the two implementations.In comparison to existing hybrid systems (e.g. [Reiter et al. 95]; [Milosavljevic et al. 96]; [Knott et al. 96]), we believe the present framework offers the following novel features:• Extensible classification-based text planning mechanism: The text planner's rule selection mechanism involves a decision tree-style traversal of the exemplar specialization hierarchy, where the applicability conditions associated with each exemplar in the hierarchy are successively evaluated in order to find the most specific exemplar for the current context. Viewed in this way, the rule selection mechanism naturally forms the •basis of an efficient, deterministic approach to text planning, where communicative actions are classified in context and then recursively executed, much as in [Reiter &amp; Mellish 92]. In contrast to Reiter and Mellish's approach, however, we emphasize extensibility, supporting inter alia discourse-sensitive conditions.• Java-based definition language: Exemplars are defined using a superset of Java, and then compiled down to pure Java. This approach makes it possible to (i) reuse basic Java constructs as well as •Java's inheritance mechanism, exceptions, threads, etc., (ii) directly and efficiently integrate with other application objects, and (iii) take advantage of advanced Java-based system architectures.• Advanced HTML/SGML support: With exemplars, the designer can bootstrap the authoring process using existing HTML or (normalized) SGML, then annotate the specification to produce dynamic content. Moreover, in contrast to other HTML template approaches (e.g. that provided with JavaSoft's Java Web Server [Sun 98]), we allow the designer to generate HTML in a truly hierarchical fashion.To date we have developed three systems 1 with the framework at CoGenTex --•namely the Project Reporter [CoGenTex 97], CogentHelp [Caldwell &amp; White 97] and EMMA systems [McCullough et al. 98] --and are currently engaged in using it to develop a natural language query tool for a large data warehousing company. The framework has benefited substantially from feedback received during its use with these projects.The rest of this paper is organized as follows. In Section 2, we describe how the EXEMPLARS framework can be used to dynamically generate HTML using objects from an application object model. In Section 3, we focus on the role of specialization and extensibility in managing textual variation, In Section 4, we compare our classification-based approach to text planning to that of [Reiter &amp; Mellish 92], as well as to systemic and schema-based approaches, plus HTML template approaches taken Outside the NLG community. In Section 5, we conclude with a discussion Of the types of generation systems for which we consider the framework to be appropriate.'Project Reporter is currently in the pre-beta release stage of development; CogentHelp and EMMA are operational prototypes. In this paper, we present EXEMPLARS, an object-oriented, rule-based framework designed to support practical, dynamic text generation, emphasizing its novel features compared to .existing hybrid systems that mix template-style and more sophisticated techniques. These features-.include an extensible classification-based text planning mechanism, a definition language that is a superset of the Java language, and advanced support for HTMIdSGML templates.
SYSTEM DEMONSTRATION NATURAL LANGUAGE GENERATION WITH ABSTRACT MACHINE  &quot; We present a system for Natural Language Generation based on an Abstract Machine approach. Our abstract machine operates on grammar s encoded in a unification-based Typed Feature Structure formalism, and is capable of both generation and parsing. For efficient generation , grammars are first inverted to a suitable form, and then compiled into abstract machine instructions. A dual compiler translates the same input grammar into an abstract machine program for parsing. Both generation and parsing programs are executed under the same (chart-based) evaluation strategy: This results in an efficient, bidirectional (parsing/generation) System for Natural Language Processing. Moreover, the system possesses ample debugging features , and thus can serve as a user-friendly environment for bidirectional grammar design and development. 1 Overview An input for the generation 1 task is a logical form which represents a meaning, and a grammar to govern the generation process. The output consists of one or more phrases in the language of the grammar whose meaning is (up to logical equivalence) the given logical form. The system robe demonstrated applies an Abstract.Machine (AM) approach for Natural Language Generation, within the framework of Typed Feature Structures (Carpenter, 1992b). Such a machine is an abstraction over an ordinary computer, lying somewhere between regular high-level languages and common hardware architectures. Programming an Abstract Machine has proved-fruitful in previous research, reaching a peak as a highly efficient technique to build Prolog compilers (Ait-Kaci, 199!). AMALIA 2 has two compilers of grammars into Abstract Machine instructions; the outputs of compilation are AM programs which perform either chart generation or chart parsing, both according to the given grammar. Both tasks use an auxiliary table (chart) to store intermediate processing results..AMALIA has a uniform core engine for bottom-up chart processing, which interprets the given (abstract machine) program, and realizes the generation or parsing task. In the case of generation it is the given semantic meaning whose components are consumed in the process. The only differences between the two processing directions are in the nature of chart items and interpretation of the final results. Thereby, AMALIA makes dual use of its chart and forms a complete bidirectional natural language system, which is considered an advantage in the literature (Strzalk0wski, 1994). The system is capable of very efficient processing, since grammars are precompiled directly into abstract machine instructions, which are subsequently executed over and over. lln this work we mean by &quot;generation&quot; what is sometimes known also as &quot;syntactic generation&quot;. Thus, no text. plannin$, speaker intentions and the like are considered here. / 2The acronym stands for &quot;Abstract MAchine for Llnguistic Applications&quot;. .. 276
CONTENT PLANNING AS THE BASIS FOR AN • •INTELLIGENT•TUTORING SYSTEM The negative feedback loop which maintains a steady blood pressure in the human body is one of the more difficult topics for first-year medical students to master. CIRCSIM-Tutor v. 3 is the latest in a series of dialogue-based intelligent tutoring systems intended to help students master the concepts involved.CIRCSIM-Tutor v. 3 differs from many other ITSs in that text planning is an integral part of the system rather than part of a natural-language front-end. It contains a global planner whose fundamental goal is "generate a conversation resulting in the student knowing &lt;concepts&gt;" rather than "teach &lt;concepts&gt;." Constraints on the plan operators can be used to take a variety Of knowledge Sources into account, including the tutorial history, the domain knowledge base and a student model. To ensure that CIP, CSiM-Tutor is useful in the classroom, we have paid particular attention to broad coverage of the domain, maintenance of a coherent conversation and fast response time. We often need to model what human tutors do without a deep model of why they say what they say. As a result our content planner uses a schema-based.representation which allows us to control the decomposition and sequence of goals. Through the use of a reactive planning architecture, we can update our plan based on the student's answers. Text realization is accomplished via a high-level template mechanism based on a mini-syntax of potential answers. Botl! the content schemata and the text realization templates are based on detailed modeling of the dialogu e patterns of expert human tutors. 
SYSTEM DEMONSTRATION FLAUBERT: AN USER FRIENDLY SYSTEM FOR MULTILINGUAL TEXT GENERATION . . .FLAUBERT is an engine for text generation. Its first applications has been for instructional texts, both in French and in English, in software and aeronautics domains. It is an implementation of G-TAG, a -formalism for generation inspired from TAG ( [Danlos &amp; Meunier 96], [Meunier 97] 
SYSTEM DEMONSTRATION OVERVIEW OF GBGEN* This paper presents an overview of the GBGen system, a sentence realizer currently developped for French. The system is strictly deterministic, i.e. it maps semantic structures to surface forms without either simulating parallelism or using backtracking, and the performances are accordingly extremely satisfying. It is procedural, based on Government &amp; Binding Theory (Chomsky 1981): several levels of syntactic representation are defined, on which configurational searches and transformations apply.GBGen is large-scale, based on a lexicon of approximately 185.000 entries (more or less 24.000 lexemes together with inflected word forms). The system covers simple and complex sentences, complex grammatical phenomena like unbounded dependencies, raising and control structures, intrasentential coreference, cliticization , modifiers (both clausal and prepositional) and main cases of coordination. It also computes Several morphosyntactic phenomena hke agreement, contractions or pronoun lexicalization. In what follows, we present the general characteristics of the software and detailits majors components. 
SYSTEM DEMONSTRATION GOALGETTER: GENERATION OF SPOKEN SOCCER REPORTS  In this paper we describe a demonstration Of the GoalGetter system, which generates spoken soccer reports (in Dutch) on the basis of tabular data. Two types of speech output are available. The demo runs via the web. It includes the possibility of !creating your own match&apos; and having GoalGetter generate a report on this match. •. ._
System Demonstration Multilingual Weather Forecast Generation System  The MLWFA (Multilingual Weather Forecasts Assistant) system will be demonstrated. It is developed to generate the multilingual text of the weather forecasts automatically. The raw data from the weather observation can be used to generate the weather element chart. According to the weather change trend, the forecasters can directly modify the value Of the element on the chart, such as the center .point value, the isoline and the isocircle. After •that, the modified data are stored as the input for the system. The system can select a schema depending on the input or the requirement from the users. The schema library can be conveniently maintained, such as the schema modification or extension. Through optimizing and mapping the schema •tree, the microplanner constructs the brief and coherent internal text structure for the surface generator. After the processing of the generator, the muitilingual •weather forecasts used for the broadcast program are generated. The MLWFA ~system is developed as the first application of the project ACNLG [Huang et al. 97a &amp; b] which is an international •cooperation between the German Research Center for Artificial Intelligence (DFKI) and Shanghai••Jiao Tong University (SJTU). The system mainly consists of four components: the graphic processor, the macroplanner, the microplanner and the surface generator. The graphic, •processor is used to adjust weather forecasts data by the forecasters. The technique •adopted for the macroplanner is based on the schema approach [McKeown 85], but we expand the operator of schema. The microplanner is based on the sentence structure optimizing which is independent of the language and language resource mapping-which is associated with the language. On the basis of the FB-LTAG (Feature-based Lexicalized-Tree Adjoining Grammar) [Joshi 85, XTAGRG 95], the surface generator identifies the feature of the nodes, compounds the grammar-trees and finally generates Chinese, •English and German weather forecasts. 2 Architecture The architecture of the system is shown in figure 1. The macroplanner obtains original predicted weather data either from the interface or from the users. These data mainly include weather status, wind direction, wind force, temperature and so on. In the stag e of macroplanning, we adopt the schema approach and introduce the &quot;It&quot; operator • which indicates that the order of the predicates can be exchanged. The macroplanning procedure 296 II I I iJ I
SYSTEM DEMONSTRATION INTERACTIVE GENERATION AND KNOWLEDGE ADMINISTRATION IN MULTIMETEO  
ROMVOX-EXPERIMENTS REGARDING UNRESTRICTED TEXT- TO-SPEECH SYNTHESIS FOR THE ROMANLAN LANGUAGE Speech synthesis systems are expected to play important roles in advanced user-friendly human-machine interfaces. Wishing to realize an as good as possible text-to-speech system for the Romanian language the research started with the •development of the software for monotonous speech synthesis, which simply concatenated the elements of the speech database. Prosodic aspects need to impose a correspondent modification of the synthesized speech signal, modification performed in the second version based on LPC. The experimental results using the classical LPC synthesis method proved that the quality of the synthesized signal is limited and it cannot be considerably improved by rising the prediction order, the sampling frequency or the parameters' refreshing frequency. The following chapters present the language specific aspects of the ROMVOX system and our last approach regarding the used synthesis technique.2. The Building Elements of the••ROMVOX Text-to-Speech System 2.1 Text-preprocessing. We need a text-preprocessing module on the grapheme level in order to convert the incoming orthography into some linguistically reasonable s~andard forms. There are many phenomena encountered in normal orthography like: underlining, the occurrence of capitals, abbreviation containing periods, abbreviations containing no vowels, numbers, fractions, Roman numerals, dates, times, formulas and a wide variety, of punctuation including periods, commas, question marks, parentheses, quotation marks and hyphens. In our system the abbreviations are stored in a vocabulary, which can be extended by the user, so field specific abbreviation can be built into the system.2.2 Speech sound set. We used a set of 31 phonemes for Romanian language. As internal representation for special Romanian sounds we used the following symbols: gl (in ge, gi), g (in ghe, ghi), c (in ce,ei), k (in the, ehi),al (for Romanian letter ~), il (for7 and $) sl (for ~), tl (for t).2.3 Conversion of graphemes. In our system the grapheme-to-phoneme conversion rules are alphabetized according to the first letter of the sequence. Each letter of the alphabet represents a separate rule block in the to the rules of the prosody preparation module. Where _ means pause, j I means special short i.2.4 Word accent. For Romanian language the word accent is free, choosing between the last two syllables of the word, and there are many words with other place of accent. Semantically different words have the same orthography. For example:We are thinking of the possibility to formalize these kinds of problems.2.5 Intonation. For obtaining acceptable intonation for unrestricted texts, a set of rules has to be formulated which produces natural sounding pitch contours for utterances that may have never been spoken.In sentence intonation, one serious problem is to find such rules that make the monotonous speech more natural, so that listening to long texts would not be uncomfortable. We studied experimentally the pitch contour for different kinds of sentences (declaratives, questions, and exclamations). For declarative sentences, the fundamental fi-equency raises for the first word (from 100% to 140% of its value and slows down to 125% for the last part of this word), and slows down until the end of the sentence, except the last word: Here it falls at 70% and remains Constant.Questions can be with Q-word (specific word for interrogation) or without. For the former, the fundamental frequency raises on this word from 100% to 160% and comes down to 100%. For the last type of questions we adopted a conventional pitch contour, but very subtle intonation effects cannot be handled. The ROMVOX Text-toaSpeech synthesis system developed by our team is the first one that allowed the synthesis of any unrestricted Romanian text with intonation facilities on 1BM-PC compatible computers. During the last years of research several version of text-to-speech systems were achieved, trying to enhance their facilities. Our paper describes the present stage of our experiments performed in order to improve the naturalness of the generated voice.
WYSIWYM: knowledge editing with natural language feedback ~, WYSIWYM (What You See Is What You Meant) is a user interface technique which allows anauthor to create and edit in a natural and simple way the knowledge contained in a generated document. More generally, WYSIWYM editing provides a self-documenting, multi-lingual approach to maintaining knowledge bases.We demonstrate here the use of WYSIWYM knowledge editing in the DRAFTER-If system. DRAFTER-If is an interactive software tool designed to assist the production of technical documents in several languages at once. The prototype system allows a technical author or domain expert to create software manual instructions in English, French and Italian. Our interactive 'Symbolic Authoring' approach avoids the difficulty and cost of translation between languages and also the problems of attempting fully automatic generation of documents. 
Sponsored by The Association for Computational Linguistics ACL/SIGDAT Edited by  
Dynamic Coreference-Based Summarization \Ve ha.ve developed a query-sensitive text summarization technology \Vell suited for the task of determining whether a. document is relevant to &lt;' -" query. Enoug;h of the docurnent is displayed for the user to determine whether the document should l:H~ read in its entirety. Evaluations indicate that sununarics are classif-ied for relevauce uearly as well as full documents. This approach i.s based on the concept that a good SltJnrnary will repn-)sent each of the topics in the query and is n'alized by ~electing smltcnc&lt;-!S from the document until all the phrases in the query which are represented in the sumiHa.ry are (covered.' A phrase in th&lt;:; docu-. ' Jllcnt is considered to cover a phraf:le in the qu(~r:Y if it is cmeferent \Vith it. This approach maxirnizes the space of &lt;!ntities reta.incd in th(:: summ&lt;Jxy with minimal rednnda.ncy. 'rhe software is built upon the CAMP NLP spt.cm [2]. 
Multilingual robust anaphora resolution  Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and timehconsuming task. This paper presents a robust , knowledge-poor approach to resolving pronouns in technical manuals. This approach is a modification of the practical approach (Mitkov 1998a) and operates on texts pre-processed by a part-of-speech taggcr. Input is checked against agreement and a nlonhber of antecedent indicators. Candidates are assigried scores by each indicator and the candidate with the highest ag~ gregate score is returned as the antecedent. We pro~ pose this approach as a platform for multilingual pronoun resolution. The robust approach was initially de~ veloped and tested for English, but we have also adapted and tested it for Polish and Arabic. For both languages, we found that adaptation required minimum modification and that further, even if used un~ modified, the approach delivers acceptable success rates. Preliminary evaluation reports high success rates in the range of and over 90%
Aligning Clauses in Parallel Texts The availability of large collections of texts in electronic fom1, has given rise to a wide range of applications aim~ ing at the elicitation of linguistic resources such as tTanslation dictionaries, transfer grammars and retTieval of translation examples (Dagan et al., 1991;Matsumoto et al., 1993), or even the building of fully-blown machine translation systems ( Brown et al., 1990). The pmpose of this paper is to describe a technique for extracting translation correspondences at bellow sentence level by employing statistical techniques coupled with shallow linguistic processing catering for the segmentation of sentences into clauses.Statistical processing has proved powerful for the exh·action of translation equivalences at sentence and intra-sentence level. Brown et al. (1991) described a method based on the number of words contained in sentences. The general idea is that the closer in length two sentences are, the most likely they are to align. Moreover, certain anchor points and paragraph markers are considered. Dynamic progra111111ing and HMMs are pipelined to produce alignments at sentence level. The method has been applied to the Hansard-Corpus, achieving an accuracy of 96%-97%. Gale and Church ( 1991) proposed a method that relies on a simple statistical model of character lengths. The model is based on the observation that longer sentences in one language tend to be translated into longer sentences in the other language while shorter ones tend to be translated into shorter ones. A probabilistic score is assigned to each pair of proposed sentence pairs, and a dynamic programming framework calculates the most probable alignment. Although the apparent efficacy of the Gale-Church algorithm is undeniable and validated on different pairs of languages, rt faces problems when handling complex alignments(l-0, 1-2, 2-2).' Simard et al. (1992) argue that a small amount of linguistic information is necessary in order to overcome the inherited weaknesses of the purely statistical techniques. They proposed using cognates, which are pairs of tokens of different languages sharing 11 0bvioUS 11 phonological or orthographic and semantic properties, since these are likely to be used as mutual translations. Papageorgiou et al. (1994) proposed a generic alignment scheme invoking surface linguistic information coupled with information about possible unit delimiters depending on the level at which alignment is sought. Each unit, sentence, clause or phrase, is represented by the sum of its content part of speech (POS) tags. The results are then fed into a dynamic programming framework that computes the optimum alignment of text units. Brown (1988) uses a probabilistic measure to estimate word similarity of two languages in the context of statistically-based machine translation. Kay and Ro- escheisen (1993) present an algorithm for aligning bilingual texts on the basis of internal evidence. Processing is pcrfom1ed in many iterations and each new iteration uses the results of the previous one in order to calculate more accurate word and sentence correspondences. In each iteration, processing consists of calculating correspondences between sentences on the basis of their relative positions, and then calculating word correspondences on the basis of word co-occunences in related sentences. The Dice coefficient is used as the similarity measure between words of two languages in an attempt to secure the conectness of the alignment of parallel texts at sentence level. Kitamura and Matsumoto (1995) have used the same Dice coefficient to calculate the word similarity between Japanese-English parallel corpora. Single word correspondences have also been investigated by Gale and Church (1991 b) using a statistical evaluation of contingency tables. Piperidis et al. ( 1997) and Boutsis and Piperidis ( 1996) describe methods for extracting single and multi-word equivalences based on a parallel corpus statistically aligned at sentence level and employing a similarity metric along the lines of the Dice coefficient with comparable performance.Collocational conespondences have been studied by Smadja (1992) and Smadja et al. (1996), in an attempt to find h·anslation patterns for continuous and discontinuous collocations in English and French. Meaningful collocations are first extracted in the source language while their corresponding French ones are found by calculating the mutual information between instances of the English collocation and various single word candidates in English-French aligned corpora. Recent work has broadened the scope identifying correspondences between word sequences. Kupiec (1993) proposes a method for extracting translation patterns of noun phrases from English-French parallel corpora. The corpus is tagged at partof~spcech (POS) level and then finite-state recognizers specified by regular expressions defined in tenns of POS categories detect noun phrases on either side. Probabilities of correspondences are then calculated using an iterative EM-like algorithm. Kumano and Hirakawa (1994) presuppose an ordinary bilingual dictimmy and non-parallel corpora, attempting to find bilingual conespondences in a Japanese-English setting at word, noun phrase and unknown word level. Extending previous work, Kitamura and Matsumoto (1996) apply the Dice coefficient on word sequence correspondence extraction. This paper describes a method for the automatic alignment of parallel texts at clause level. Texts are first aligned at sentence level using statistical techniques. Part-of-speech tagging takes place next annotating each word form with the appropriate part of speech. Processing in this step and the next one is monolingual, so each language side of the text is treated independently of the other. Surface syntactic analysis is performed next on the basis of regular grammars. Shallow parsing results in the recognition of clauses. Statistical processing follows taking into account different sources of information, aiming at identifying intra-sentence alignments formed by the clauses of the parallel sentences of the bitext. The 18 method caters for alignments of type 1-0, 1-1, 1-2, 2-1, and 2-2. A first pass through the text computes occurrence and co-occunence probabilities for content words on both language sides. A probabilistic score, expressing the probability that a clause (or a pair of clauses) of the source language is h·anslated into a clause (or a pair of clauses) of the target language, is computed on the basis of the previously calculated word probabilities, and a model of character lengths. Possible clause alignments are examined by a dynamic programming framework deciding on the best alignment. Avoiding combinatorial explosion requires that large sentences be channeled into a module that approximates the optimal alignment through simulated amrealing, operating in polynomial time. EM iterative training caters for the estimation of the model's parameters, given the lack of hand-aligned training material. The overview of the processing is pictured in Figure 1. This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.
Automatic Insertion of Accents m French Text Even in this era of flashy, high-speed multimC::dia information, unaccented French texts (i.e texts without diacritics) are still routinely encountered in electronic media. Two factors account for this: first, the computer field has long suffered from a lack of sufficiently widespread standards for encoding accented characters, which has resulted in a plethora of problems in the electronic transfer and processing of French texts. Even now, it is not uncommon for one of the software links in an E-mail distribution chain to deliberately remove accents in order to avoid subsequent problems. Secondly, when nsing a computer keyboard that is not specifically designed for French, keying in French accented characters can turn out to be a laborious activity. This is a matter of both standards and ergonomics. As a result, a large number of French-speaking users systematically avoid using accented characters, at least in informal communication.If this situation remains tolerable in practice, it is essentially because it is extremely rare that the absence of accents renders a French text incomprehensible to the human reader. Cases of ambiguity do nonetheless occur: for instance, "Ce chantier ferme a cause des emeutes" could be interpreted as '\Ce chan tier ferme a cause des CmeuteS 11 C'This work-site is closing because of the riots") or \\Ce chan tier fermi a cause des Cmeutesn ("This closed work-site [more naturally put, this work-site closure] has caused riots',). From a linguistic point of view, the lack of accents in French simply increases the relative degree of ambiguity inherent in the language. At worst, it slows down reading and proves awkward, much as a text . written entirely in capital letters might do. Automatic accent inser·tion (AAI) is the problem of re-inserting accents (diacritics) into a text where they are missing. Unaccented French texts are still quite common in electronic media, as a result of a long history of character encoding problems and the lack of well-established conventions for typing accented characters on computer keyboards. We present an AAI method for French, based on a stochastic language model. This method was implemented into a program and G library of functions, which are now commercially available. Our experiments show that French text processed with this program contains less than one accent error per 130 words. We also show how our AAI method can be nsed to do on-the-fly accent insertions within a word-processing environment, which makes it possible to write in French without having to type accents. A prototype of such a_ s&apos;ystem was integrated into the Emacs editor, and iS now available to all students and employees of t}1e Universite de Montreal&apos;s computer science department.
Valence Induction with a Head-Lexicalized PCFG Either directly or indirectly, the lexicon for a natural language specifies complementation frames or valences for open-class words such as verbs and nouns. Constructing a lexicon of complementation fram&lt;:~s for larg&lt;-'! vocabularies constitutes a challenge of scale, with the further complication that frame usage, like vocabulary, varies with genre and undergoes ongoing: innovation in a living language. This paper addresses this problem by means of a learning tech·· niquc baswi on probabilistic lexicalized context free grammars and the expectation-maximi~";ation (EM) algorithm. Given a hand-written grammar and a text corpus, frequencies of a head word accompanied by a frame are estimated using the inside-outside algorithm, and such frequencies are used to compute probability para.meters characterizing subcategorization. The procedure can be iterated for improved models. \Nc show that the scheme is practical for large vocabularies and accurate enough to capture differences in usage, such as those characteristic of different domains. 
Measures for corpus similarity and homogeneity  How similar are two corpora? A measure of corpus similarity would be very useful for NLP for many purposes , such as estimating the work involved in porting a system from one domain to another. First, we discuss difficulties in identifying what we mean by &apos;corpus similariti: human similarity judgements are not fine-grained enough, corpus similarity is inherently multi-dimensional, and similarity can only be interpreted in the light of corpus homogeneity. We then present an operational definition of corpus similarity \vhich addresses or circumvents the problems, using purpose-built sets of aknown-similarity corpora&quot;. These KSC sets can be used to evaluate the measures. We evaluate the measures described in the literature, including three variants of the information theoretic measure &apos;perplexity&apos;.
Word-Sense Distinguishability and Inter-Coder Agreement * It is common in Natural Language Processing (NLP) that the categories into which text is classified do not have fully objective definitions. Examples of such categories are lexical distinctions such as part-of-speech tags and word-sense distinctions, sentence level distinctions such as phrase attachment, and discourse level distinctions such as topic or speech-ac.t categorization. This paper presents an approach to analyzing the agreement among human judges for the purpose of formulating a refined 1~his research was supported by the Of-fice of Naval Research under grant number N00014-95-l-0776. It. is common in NLP that the categories into which text is classified do not have fully objective definitions. Examples of such categories are lexical distinctions such as part-of-speech tags and word-sense distinctions, sentence level distinctions such as phrase attachment, and discourse level distinct .icms such as topic or speech-act categorization. This p&gt;1per presents an approach to analy?-ing the agrcen1ent arnong lnnnan judges for the purpose of formulating a refined and more reliable set of category designations. We use these techniques to analyze the sense tags assigned by five judgps to the noun intcr·est. The initial tag set is takmi from Longman&apos;s Dictionary of Contemporary }i:nglish. Through this process of analysis, we automatically identify and assign a revised set of sense tags for the data. The revised tags exhibit high reliability as measured by Cohen&apos;s r;.. Such techniques are important for formulating and evaluating both human and automated classification systems.
Category Levels in Hierarchical Text Categorization  We consider the problem of assigning level numbers (weights) to hierarchically organized categories during text categorization. These levels control the ability of the categories to attract documents during the catego-rization process. The levels are adjusted to obtain a balance between recall and precision for each category. If a category&apos;s recall exceeds its precision, the category is too strong and its level is reduced. Conversely, a cat-egory&apos;s level is increased to strengthen it if its prelision exceeds its recall. ·&apos; The categorization algorithm used is a su~ervised learning procedure that uses a linear classifier hewed on the category levels. We are given a set of categories: organized hierarchically. \Ve are also given a training corpus of documents already placed in one or more categories. From these, we extract vocabulary, words that appear with high frequency within a given category, characterizing each subject area. Each node 1 s vocabulary is filtered and its words assigned weights with respect to the specific category. Tben, test documents are scanned and categories ranked based on the presence of vocabulary terms. Documents are assigned to categories based on these rankings. We demonstrate that precision and recall can be significantly improved by solving the categorization problem taking hierarchy into account. Specifically, we show that by adjusting the category levels in a principled way, that precision can be significantly improved, from 84% to 91%, on the much-studied Reuters-21578 corpus organized in a three-level hierarchy of categories.
An Empirical Approach to Text Categorization based on Term Weight Learning \~Vith increasing numbers of machine readable doctiments becoming availa.ble, an automatic text categorization which is the classification of text with respect t.o a. set. of pre-categori;.:ed texts) has become a trend in IR and NLP studies. In this paper) we propose a method for text categoriza-Lion task using term weight learning. In our approach, learning is to learn true keywords from the error of clustering results. Parameters of term weighting are then estimated so as to maximize the true keywords and minimize the other words in the text. The characteristic of our approach is that the degree of context dependency is used in order to judge whether a word in a text is a true keyvv·ord or not. The experiments using Wall Street Journal corpus demonstrate the effectiveness of the method.
An Empirical Evaluation on Statistical Parsing of Japanese Sentences using Lexical Association Statistics In the statistical parsing literature) it has alread:y been established that statistics of lexical association have real potential for improvement of disambiguation performance. The question is how lexical association statistics should be incorporated into the overall statistical parsing framework. In exploring this issue) we consider the following four basic requirements: o \Ve are proposing a new framework of statistical language modeling which integrates lexical .association statistics with syntactic preference, while maintaining the modularity of those different statistics types, facilitating both training of the model and analysis of its behavior. In this paper, we report the result of an empirical evaluation of our model, where the model is applied to disambiguation of dependency structures of Japanese sentences. We also discussed the room remained for further improvement based on our error analysis.
Japanese Dependency Structure Analysis based on Lexicalized Statistics  \Ve JH&apos;t&apos;::wnt statistical rnodels of Japanese dependency analysis aud report results of some experiments to investigate the pnfonuancc of the models for the use fo a partical parsing system. The statistical modeb &lt;:u&apos; &lt;&apos; rather simple compared with the recent complex models &lt;wd intesivcl,y usc lexical level information 1 such a.s morplH:mcs 1 and part-of-speech tags .. \Ve conducted several expcrimcllts to show the following properties of the modeb: lb performance of the models according to feature selection
A Comparison of Criteria for Maximum Entropy / Minimum Divergence Feature Selection Maximum entropy / minimum divergence (MEMD) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology [5] and language modeling [6,11,3,9]. The heart of the method is to ~ljoose a collection of informative features, each encodi'llg some linguistically significant event, and then to incorporate these features into a family of conditional models.A fundamental issue in applying this technique is the criterion used to select features. The work described in (3], for instance, incorporates every feature which either appears with above-threshold count in a training corpus, or which exhibits high mutual information. In [11] and [1], the authors select features based on a mutual information statistic. As we argue below, both these methods have drawbacks.In this paper, we examine a statistic for selecting MEMD model features, called the gain. The gain was introduced in [4], and studied in greater detail in [5] and [2]. We present intuition, theory and experimental results for this statistic, as a criterion for selecting features for an MEMD language model. We believe our work marks the first time it has been used in MEMD language modeling, and the first side-by-side comparison with other selection criteria. Though our experimental results concern language models exclusively, we note that the gain can be used to select features for any MEMD model on a discrete space.The language model we present is based on dependency grammars. It is similar to, but extends upon, the work reported in [3]. Two important differences between that work and ours are that ours is a true 97 In this paper we study the gain, a naturally-arising statistic from the theory of MEMD modeling [2], as a figure of merit for selecting features for an MEMD language model. We compare the gain with two popular alternatives-empirical activation and mutual information-and argue that the gain is the preferred statistic, on the grounds that it directly measures a fea-ture&apos;s contribution to improving upon the base modeL
AUTHOR INDEX  
THE 11™ NORDIC CONFERENCE ON COMPUTATIONAL LINGUISTICS NOD ALIDA &apos;98 PROCEEDINGS  
LMT at Tivoli Gardens  IB M T. J. W a ts o n R e s e a r c h C e n t e r P .O .B. 7 0 4 , Y o rk to w n H e ig h ts, N Y 1 0 5 9 8 a re n d se @ w a tso n. ibm. c o m m c c o rd @ w a tso n. ib m. c o m Abstract T h is p a p e r re p o rts o n c e rta in a s p e c ts o f E n g Iish-&gt; D a n is h m a c h in e tra n sla tio n , u sin g a n ew ly re d e sig n e d v e rs io n o f L M T th a t is im p le m e n te d in C a n d is c a p a b le o f tra n s la tin g W e b p ag es. W e u s e th e tra n s la tio n o f a W eb p a g e a b o u t T iv o li G a rd e n s as a sp rin g b o a rd f o r d isc u s sio n o f L M T &apos;s tre a tm e n t o f so m e p h e n o m e n a o f E n g l i s h ^ D a n i s h tra n s la tio n , w ith e m p h a sis o n the c o n trib u tio n s o f a n e w tra n s fo rm a tio n a l s y ste m fo r L M T .
Drug terminology A multilingual term database. The AVENTINUS project  T h is p a p e r starts w ith a b r ie f o v e ra ll p re se n ta tio n o f th e A V E N T IN U S p ro je c t, m e re ly a list o f th e d iffe re n t in c lu d e d m o d u le s a n d so m e co m m en ts. T h e n fo llo w s a d is c u s s io n a b o u t d ru g te rm i­ n o lo g y a n d fin a lly a d e s c rip tio n o f th e d e s ig n an d im p le m e n ta tio n o f a ta ilo re d m u ltilin g u a l d ru g te rm in o lo g y d a tab ase in M S A c c e ss. T h e u se d ta g s a n d lin k s are p re s e n te d a n d d isc u sse d and th e in p u ttin g situ a tio n is d e sc rib e d. T h e n so m e n u m e ric a l d e ta ils o f th e d a ta b a se a n d a sh o rt c o n ­ c lu d in g re m a rk are given. Project description T h e A V E N T IN U S Project* a im s a t s u p p o rtin g an A d v a n c e d In fo rm a tio n S y ste m fo r M u ltin a ­ tio n a l D ru g E n fo rce m en t. It is fu n d e d b y th e E u ro p e a n U n io n in th e L in g u is tic E n g in e e rin g (L E) P ro g ra m , a n d h a s se v e ra l d e v e lo p m e n t a n d u se r p artn e rs. T h e g o a l o f th e p ro je c t is to su p p o rt d m g e n fo rc e m e n t w ith m u ltilin g u a l lin g u istic ex p ertise. A V E N T IN U S w ill s u p p o rt c o m m u n ic a ­ tio n b y p ro v id in g lin g u is tic to o ls to o v e rc o m e la n g u ag e c o m m u n ic a tio n b a rrie rs. U se rs sh o u ld b e a b le to a c c e ss in fo rm a tio n a n d re c e iv e re su lts o f se a rc h re q u e s ts in th e ir o v ra n a tiv e lan g u ag e, e v e n i f th e in fo rm a tio n is d e riv e d fro m fo re ig n la n g u ag e so u rces. T h e la n g u a g e s d e a lt w ith in th e firs t p h a s e a re E n g lish , G erm an , S p a n ish , a n d S w e d ish. A V E N T IN U S w ill p ro v id e m o d u le s a n d co m p o n e n ts th a t c a n b e lin k e d to a n d in te g ra te d in to th e u se rs d o m e stic e n v iro n m e n ts. M o d u la rity a n d in te g ra ta b ility a re th e m o s t p ro m in e n t featu res o f th e so ftw a re so lu tio n s to b e p ro v id e d. T h e p a rtic ip a tin g u sers are d o m e stic p o lic e o rg a n isa tio n s a n d in te llig e n c e a g e n c ie s a n d th e E u ro-p o l D ru g U n it (E D U). In te re st fro m o th e r a u th o ritie s h a v e a lso b e e n n o tic e d , th o u g h. &apos; For an exhaustive description o f the project, see [THUR97] or take a look at our AVENTINUS web
AVENTINUS, GATE and Swedish Lingware T h is re p o rt p re se n ts a n o u tlin e o f S w e d ish L in g w a re c o m p o n e n ts in te g ra te d in th e L a n g u a g e E n g in e e rin g a p p lic a tio n d e v e lo p m e n t e n v iro n m e n t o f G A T E . T h is is o n g o in g w o rk w ith in th e fra m e w o rk o f th e L a n g u a g e E n g in e e rin g p ro je c t A V E N T IN U S . F irs t, a b r ie f o v e rv ie w o f th e A V E N T IN U S p ro jec t, its g o a ls a n d c h a ra c te ristic s a re p re se n te d . T h e n a d e s c rip tio n o f G A T E a n d th e in fo rm a tio n e x tra c tio n , IE , ta s k o f th e p ro je c t w ill b e d isc u sse d . F in a lly , th e d e sc rip tio n o f S w e d is h m o d u le s, c o m p ris in g th e n e c e ss a ry lin g w a re fo r th e IE ta s k w ill b e g iv en . T h ese m o d u le s a n d th e in te ra c tio n b e tw e e n th e m w ill b e illu stra te d w ith c o n c re te e x am p les. F u r th e n n o r e th e so ftw a re /lin g w a re sp e c ific a tio n s, re q u ire m e n ts a n d , w h e n a p p ro p ria te , so m e p re lim in a ry p e rfo rm a n c e m e a su re s w ill b e g iv e n . T h is re p o rt p re se n ts a n o u tlin e o f th e S w e d ish L in g w a re c o m p o n e n ts in te g ra te d in th e L an g u a g e E n g in e e rin g a p p lic a tio n d e v e lo p m e n t e n v iro n m e n t o f G A T E. T h is is a n o n g o in g w o rk w ith in th e fra m e w o rk o f th e L a n g u a g e E n g in e e rin g p ro je c t A V E N T IN U S. A V E N T IN U S , A d v a n c e d In fo r m a tio n S y s te m f o r M u ltilin g u a l D r u g E n fo rc em e n t, is a m u ltilin g u a l in fo rm a tio n p ro c e s s in g p ro je c t fin an c ed b y th e E u ro p e a n U n io n , L a n g u a g e E n g in e e rin g p ro g ra m , w ith c o n tra c t re fe re n c e L E l-2 2 3 8 1 0 3 3 5/0. T h e A V E N T IN U S p ro je c t is a u se r-a n d d a ta-o rien ted p ro je c t, a d d re s s in g th e n eed s o f E u ro p e a n p o lic e a n d la w e n fo rc e m e n t a g e n c ie s o n p re v e n tio n a n d d e te c tio n o f d ru g-re la te d o ffe n se s, s u c h as m o n e y la u n d erin g , d ru g tra ffic k in g , d ru g ab u se etc. A V E N T IN U S is also e x p e c te d to b e e x te n d e d a n d u se d in re la te d to d ru g e n fo rc e m e n t field s su c h as o rg a n iz e d crim e. T h e re p o rt w ill c o n c e n tra te o n a c o n c ise d e s c rip tio n o f th e o n g o in g d e v e lo p m e n t, in te g ra tio n a n d e v a lu a tio n o f S w e d is h lin g w a re c o m p o n e n ts in G A T E. T h e se c o m p o n e n ts a s w e ll as th e te x tu a l m a te ria l u sed , th a t is co rp o ra , o f th e n a rc o tic a s u b c o rp o ra d o m a in w ill b e d isc u sse d. F irst, a b r ie f o v e rv ie w o f th e A V E N T IN U S p ro je c t, its g o a ls a n d c h a ra c te ristic s are p resen te d. T h e n a d e sc rip tio n o f G A T E a n d th e in fo rm a tio n e x tra c tio n , IE , ta s k o f th e p ro je c t w ill b e d isc u sse d. F in ally , th e d e s c rip tio n o f S w e d ish m o d u le s, c o m p ris in g th e n e c e ss a ry lin g w a re fo r th e IE ta sk w ill b e g iv e n. T h e se m o d u le s a n d th e in te ra c tio n b e tw e e n th e m w ill b e illu stra te d w ith c o n c re te e x am p les. F u rth e rm o re th e so ftw a re /lin g w a re sp e c ific a tio n s, re q u ire m e n ts an d , w h e n ap p ro p ria te , so m e p re lim in a ry p e rfo rm a n c e m e a su re s w ill b e g iv e n .
Norwegian Computational Lexicon {N orK om pL eks)  
Structural Lexical Heuristics in the Automatic Analysis of Portuguese  D e p a rtm e n t o f L in g u is tic s, A rh u s U n iv e rsity , W ille m o e sg a d e 15 D , D K-8 2 0 0 Å rh u s N tel: + 4 5-8 9 4 2 2 1 3 1 , fax: + 45-86 2 8 1 3 9 7 , e-m a il: lin e b @ h u m .a a u .d k h ttp ://v isl.h u m .o u .d k /L in g u istic s.h tm l Abstract T h e p a p e r d isc u sse s, o n th e le x ic a l lev el, th e in te g ra tio n o f h e u ris tic so lu tio n s in to a le x ic o n b a se d a n d ru le g o v e rn e d s y ste m fo r th e a u to m a tic a n a ly sis o f u n re s tric te d P o rtu g u e se te x t. In p a rtic u la r, a m o rp h o lo g y b a se d a n a ly tic a p p ro a c h to le x ic a l h e u ris tic s is p re se n te d a n d e v a lu a te d. T h e ta g g e r in v o lv e d u se s a 5 0 .0 0 0 en try b ase fo rm le x ic o n as w e ll as p re fix-, su ffix-a n d in fle x io n e n d in g s le x ic a to a s s ig n p a rt o f sp e e c h a n d o th e r m o rp h o lo g ic a l ta g s to e v e ry w ordform. in th e tex t, w ith re c a ll ra te s b e tw e e n 9 9 .6 % a n d 9 9 .7 %. M u ltip le re a d in g s are s u b se q u e n tly d isa m b ig u a te d b y u s in g g ra m m a tic a l ru le s fo rm u la te d in th e C o n s tra in t G ra m m a r fo rm alism. O n th e n e x t lev el o f a n a ly sis , ta g s fo r sy n ta c tic a l fo rm a n d fu n c tio n a lte rn a tiv e s a re m a p p e d o n to th e w o rd fo n n s a n d d is a m b ig u a te d in a sim ila r w ay. In sp ite o f u sin g a h ig h ly d iffe re n tia te d ta g set, th e p a rse r y ie ld s c o rre c tn e ss ra te so n ru n n in g u n re stric te d a n d u n k n o w n te x to f o v e r 9 9 % fo r m o rp h o lo g y /P o S a n d 9 7-9 8 % fo r sy n tax. A te st site w ith a v a rie ty o f a p p lic a tio n s (p a rsin g , c o rp u s se arch es, in te ra c tiv e g ra m m a r te a c h in g a n d-e x p e rim e n ta l-M T h a s b e e n e sta b lish e d a t h ttp ://v is l.h u m .o u .d k /L in g u is tic s .h tm l. 1 Background In c o rp u s lin g u istic s, m o s t sy ste m s o f a u to m a tic a n a ly sis c a n b e c la ssifie d b y m e a su rin g th e m a g a in st th e b ip o la rity o f ru le b a se d v e rsu s p ro b a b ilis tic a p p ro a c h e s. T h u s K a rlsso n (1 9 9 5) d istin g u ish e s b e tw e e n &quot; p u re &quot; ru le b a se d o r p ro b a b ilis tic s y ste m s, h y b rid s y ste m s a n d c o m p o u n d sy ste m s, i.e. ru le b a s e d s y ste m s su p p le m e n te d w ith p ro b a b ilis tic m o d u le s, o r p ro b a b ilis tic sy ste m s w ith ru le b a s e d &quot;b ia s &quot; o r p o stp ro c e ss in g. A s a se c o n d p a ra m e te r, le x ic o n d e p e n d e n c y m ig h t b e ad d ed , s in c e b o th ru le s b a s e d a n d p ro b a b ilistic s y ste m s d iffe r in te rn a lly a s to h o w m u c h u se th e y m a k e o f e x te n siv e le x ic a , b o th in te rm s o f le x ic a l c o v e ra g e a n d g ra n u la rity o f le x ic a l in fo rm atio n. T h e c o n s tr a in t g r a m m a r (C G) fo rm a lis m (e.g. K a rls so n e t al., 1 9 9 5), w h ic h I h a v e b e e n u s in g in m y o w n s y s te m &apos; fo r th e a u to m a tic a n a ly sis o f u n re s tric te d P o rtu g u e s e te x t (B ic k , 1996 [1] a n d 1997 [2]), is b o th ru le g o v e rn e d a n d le x ic o n b a se d , fo c u s in g o n d isa m b ig u a tio n o f m u ltip ly &apos; The system was developed in the framework o f a Ph.D.-project at Arhus University, over a period o f three years, drawing on lexicographic research on Portuguese from an earlier Master&apos;s Thesis.
An HP S G Marking Analysis of Danish Determiners and Clausal Adverbials C ostanza N avarretta and Anne N eville Center for Sprogteknologi D a n is h d e te r m in e r s , lik e d e te r m in e r s in o th e r la n g u a g e s , c o m b in e in a fix ed o r d e r w ith in th e n o u n p h r a s e . T h e o r d e r o f d e te r m in e r s is n o t a n issu e t h a t h a s re c e iv e d m u c h a t t e n t i o n in H P S G 9 4 [9 ]. In H P S G 9 4 t h e class o f d e te r m in e r s fo rm s a u n ifo rm c a te g o r y h a v in g n o s u b ­ c a te g o rie s . D e te r m in e r s a r e le x ic a lly a s s ig n e d t h e fu n c tio n s p e c ifie r w h ic h is r e s tr ic te d to t h e c a te g o r y o f fu n c tio n a ls . F u n c tio n a ls in c lu d e t h e ty p ic a l m in o r c a te g o rie s la c k in g p h r a s a l p r o je c tio n s , b u t a r e a lso m e a n t t o in c lu d e w o rd s t h a t p r o je c t [9, c h a p t e r 9]. T h is a n a ly s is c a n n o t a c c o u n t fo r th e c a te g o r ia l d iv e rs ity o f d e te r m in e r s w h ich is e s s e n tia l t o a n y d e s c r ip tio n o f t h e i r o r d e r . S in c e d e te r m in e r s in H P S G 9 4 b e lo n g t o th e s a m e c a te g o ry , t h e c o m b in a tio n o f d e te r m in e r s c a n n o t b e c o n s tr a in e d b y l e t t i n g d e te r m in e r s o r n o u n s se le c t s u b c a te g o r ie s o f d e te r m in e r s . A lso , c e r t a in d e te r m in e r s m a y f u n c tio n a s e ith e r s p e c ifie r o r a d j u n c t d e p e n d in g o n th e c o n t e x t th e y o c c u r in . S in ce fu n c tio n -s p e c if ic a tio n ta k e s p la c e in t h e le x ic o n irre s p e c ­ tiv e o f c o n te x t, th i s a n a ly s is w o u ld re q u ir e tw o le x ic a l e n trie s fo r e a c h o f th e s e d e te r m in e r s , o n e f o r w h e n t h e y f u n c tio n a s sp e c ifie r a n d o n e f o r w h e n th e y f u n c tio n a s a d j u n c t. T h is p r o p e r t y o f a d v e r b ia ls is n o t d e a l t w ith in [9] e ith e r . I n (2)^ w e p r e s e n t D a n is h d a t a i l l u s t r a t i n g th e m u t u a l o r d e r o f d e te r m in e r s in n o u n p h ra s e s a n d c la u s a P a d v e rb ia ls in m a in a n d s u b o r d in a t e c la u se s. T h e n , in (3 ), A lle g ra n z a 's ([2] a n d [3] ) a l te r n a t iv e a n a ly s is o f I t a l i a n d e te r m in e r s , w ith in t h e fra m e w o rk o f H P S G , is p re s e n te d . In (4) w e d e s c rib e h p w w e h a v e a d a p t e d A lle g ra n z a 's a p p ro a c h t o fo rm a liz e D a n is h d e te r m in e r s w h ic h d o n o t e x h ib it t h e s a m e c o o c c u rre n c e p a t t e r n s a s I t a l i a n d e te r m in e r s . W e f u r th e r sh o w t h a t t h e a p p ro a c h c a n b e e x te n d e d to c o n s tr a in t h e m u t u a l o r d e r o f c la u s a l a d v e rb ia ls ^ . [10]). T h e c la ss ific a tio n is b a s e d o n t h e i r p o s itio n in t h e n o u n p h r a s e w ith re s p e c t to e a c h o th e r , p re -a n d p o s td e te r m in e r s p iv o tin g o n t h e c e n tr a l d e te r m in e r . Q u a n tif ic a tio n a l d e te r m in e r s c u t a c ro s s a ll th r e e c lasses a n d w e m a y s u b d iv id e th e m in to q u a n tif ic a tio n a l p r e -, c e n tr a l a n d p o s td e te r m in e r s . A rtic le s , d e m o n s tr a tiv e s a n d p o sse ssiv e s a r e c e n tr a l d e te r m in e r s , c a r d in a ls a n d o r d in a ls a r e p o s td e t e r ­ m in e rs . (2) sh o w s th e su c c e ssiv e a t t a c h m e n t o f a p re -, c e n tr a l a n d p o s td e te r m in e r . ^The work described in this paper in part originates from research carried out within the framework of two EU funded projects, MLAP93-09 [11] and LSGRAM (LRE 61029 [7]). ^By clausal adverbials we mean adverbials which are placed in the so-called Actualization Field (in Danish "Nexus Felt" following Diderichsen[4j) i.e. they follow the finite verb or the finite verb and the subject in main clauses, while in subordinate clauses they occur in-between the subject and the finite verb. ®In this paper we focus exclusively on the syntax of determiners and adverbials.T h e c la s s ific a tio n in [10]   ^Note that the subject follows the finite verb in main clauses with an initial topicalized element. ®In main clauses the subject is postpositioned in simple interrogative clauses and in comment clauses in addition to the previously mentioned clauses with a topicalized element. in in i t i a l p o s itio n a n d in t h e A c tu a lis a tio n field. A few a d v e r b ia ls h a v e d iffe re n t m e a n in g s d e p e n d in g o n w h e th e r t h e y o c c u r in t h e A c tu a lis a tio n field o r n o t. D e te r m in e r s a n d a d v e r b ia ls in m a n y la n g u a g e s o c c u r in a fix ed o r d e r w h ic h is n o t a n issu e t h a t h a s re c e iv e d a g r e a t d e a l o f a t te n t i o n in H P S G 9 4 [9 ]. T o d e a l w ith t h e o rd e r o f I t a l i a n d e t e r ­ m in e r s A lle g ra n z a ([2] a n d [3]) p ro p o s e s a re v is io n o f H P S G 9 4. W e h a v e a d o p te d A lle g ra n z a &apos;s a p p r o a c h to a c c o u n t fo r t h e o rd e r o f D a n is h d e te r m in e r s a n d we h a v e e x te n d e d i t to d e s c rib e t h e o r d e r o f c o o c c u rrin g c la u s a l a d v e rb ia ls. O u r e x te n s io n p ro v id e s e v id e n c e t h a t A lle g ra n z a &apos;s re v is io n o f H P S G c a n b e u s e d n o t o n ly to a c c o u n t fo r d e te r m in e r s in o th e r la n g u a g e s t h a n I t a l ia n , b u t a lso o t h e r p h e n o m e n a t h a t e x h ib it a s im ila r c o m b in a to r ia l b e h a v io u r.
A Chart-Based Framework for Grammar Checking Initial Studies A la n g u a g e c h e c k e r, ty p ic a lly , h a s tw o b a sic c o m p o n e n ts, a sp e ll-c h e c k e r a n d a g ra m m a r c h ec k er. W h e re a s th e sp e ll-c h e c k e r, u su a lly , lim its its o p e ra tio n to th e in sp e c tio n a n d c o rre c tio n o f in d iv id u a l te x t w o rd s, th e g ra m m a r c h e c k e r h a s to c o p e w ith e rro rs th a t c a n o n ly b e d e te c te d in c o n te x ts th a t a re la rg e r th a n th e w o rd . A m o n g th e la tte r w e fin d n o t o n ly s y n ta c tic e rro rs in th e tra d itio n a l se n se , b u t a lso p u n c tu a tio n erro rs, s o ft lo w e r c a se e rro rs , a n d o th e r v io la tio n s o f g rap h ic al c o n v e n tio n s (W e d b je r R a m b e ll 1998). H e re w e w ill o n ly d isc u ss erro rs th a t m a y be d e n o te d c o n s tru c tio n e rro rs, i.e. sy n ta c tic e rro rs a n d c e rta in ty p e s o f p u n c tu a tio n e rro rs, e.g . se p a ra to rs su c h a s co rm n a . S p e llin g e n o r s th a t re s u lt in p ro p e r w o rd s b u t w ro n g c o n stru c tio n s a n d th u s c a n n o t b e c a p tu re d by a sp e ll-c h e c k e r also b e lo n g to th is g ro u p . F or th e h a n d lin g o f s o ft lo w e r ca se e rro rs a 
CP-UDOG An Algorithm for the Disambiguation of Compound Participles in Danish  T h is p a p e r d e sc rib e s so m e a s p e c ts o f th e lin g u istic a n a ly sis w h ic h h a s b een ap p lied to d isa m b ig u a te D a n is h c o m p o u n d p a rtic ip le s (C P s) (sect. 1-3), fo llo w e d b y a n in tro d u c tio n to th e im p le m e n tio n o f th e d is a m b ig u a tio n p ro c e ss (sect. 4-6).
Evaluation of the Syntactic Parsing Performed by the ENGCG Parser Klas Prytz  
CATCH: A Program for Developing World Wide Web CALL Material C o m p u te r-A s s is te d L a n g u a g e L e a rn in g , in s h o rt C A L L , is th e re s e a rc h a re a w h ic h d eals w ith th e d e v e lo p m e n t a n d e v a lu a tio n o f c o m p u te r so ftw a re an d h a rd w a re th a t is u s e d in a la n g u a g e le a rn in g e n v iro n m e n t. T h e re c e n tly p u b lish e d h o o k {\it C o m p u te r-A s siste d L a n g u a g e L e a rn in g -C o n te x t a n d C o n c e p tu a liz a tio n } b y M ic h a e l L e v y fo c u se s o n e v a lu a tin g C A L L sy ste m s a n d p ro v id in g a h ig h -le v e l d e s c rip tio n fo r th e m . W e a re in te re ste d in o b ta in in g a n sw e rs to m o re b a sic q u e s tio n s: W h a t e d u c a tio n a l m e th o d s c a n b e u sed in C A L L m a te ria l? A n d w h a t so ftw a re c a n b e u s e d f o r d e v e lo p in g C A L L m a te ria l? C A L L m a te ria l lo n g e r th a n u n til th e n e x t so ftw a re o r h a rd w a re update^. 
Assembling a Balanced Corpus from the Internet  
Peeking Into the Danish Living Room Internet access to a large speech corpus O u r n e w ly o p e n e d In te rn e t site o ffers a v ie w to a &gt;10* w o rd c o rp u s o f in fo rm a l D a n is h c o n v ersatio n s. T h e c o rp u s a n d th e search e n g in e situ a te d a t lA A S c a n n o w b e re a c h e d a n d u se d as e a sily as a n y h o m e p a g e o n th e W o rld W id e W eb , o ffe rin g a to o l fo r serio u s in v e stig a tio n s in to in fo rm al sp eech .A fte r a fe w in tro d u c to ry re m a rk s, w e sh all p re se n t th e c o rp u s a n d th e se a rc h en g in e a s seen fro m th e u s e r's p o in t o f v ie w , fo llo w in g u p th e p re s e n ta tio n w ith a fe w ex a m p le q u erie s. In c o n c lu sio n , so m e re fle c tio n s o n th e p o ssib ilitie s th a t th e In te rn e t h a s to o ffe r in u tilisa tio n , m a in te n a n c e , a n d c o n tro l o f la rg e c o rp o ra o f s e m i-c o n fid e n tia l data. 
Extraction of Translation Equivalents from Parallel Corpora se n te n c e a lig n m e n t, s tric t tra n s la tio n s , a n d h isto rical re la tio n s b etw ee n c o n sid e re d la n g u a g e p a irs. T h e y ta k e a d v a n ta g 
The Construction of a Tagged Danish Corpus  D e p t, o f L in g u istic s, U n iv e rsity o f A a rh u s, D e n m a rk , b ilg ra m @ lin g .a a u .d k Britt Keson D S L , C h ristia n s B ry g g e 1,1. 1219 K ø b e n h a v n K , D e n m a rk , p a ro le @ c o c o .ih i.k u .d k Abstract T h e o b je c t o f th is p a p e r is to p re s e n t o n g o in g w o rk o n th e c o n s tru c tio n o f a m o rp h o sy n ta c tic a lly ta g g e d D a n is h c o rp u s, w h ic h is a n in te g ra l ste p in th e m a k in g o f a C o n s tra in t G ra m m a r (C G) p a rs e r f o r D a n is h a n d a lso c o n s titu te s a p a rt o f th e D a n is h c o n trib u tio n to th e E u ro p e a n P A R O L E p ro ject. T h is p a p e r d is c u s se s v a rio u s a sp e c ts o f th e m o rp h o lo g ic a l d e s c rip tio n o f D a n ish u s e d h e re as w e ll as s o m e o f th e g u id e lin e s d e v e lo p e d fo r th e m a n u a l d is a m b ig u a tio n p ro c e ss. F in a lly , i t a lso b rie fly g iv e s a n o v e rv ie w o f th e o b je c tiv e s o f th e tw o p ro je c ts in v o lv e d .
Linguistics isn&apos;t always the answer: Word comparison in computational linguistics^  
Logic for Part-of-Speech Tagging and Shallow Parsing  I n-fliis p a p e r, a p u re ly lo g ical ap p ro ac h to p a rt-o f-s p e e c h ta g g in g a n d s h a llo w p a rs in g is e x p lo re d. It h a s a lo t in c o m m o n w ith re d u c tio n is t p a rs in g stra te g ie s su c h as th o s e e m p lo y e d in C o n s tra in t G ra m m a r (K a rls s o n e t al. 1994) a n d F in ite-S ta te In te rs e c tio n G ra m m a r (K o sk e n n ie m i 1990), b u t ru le s a re fo rm u la te d e n tire ly in lo g ic , a n d a m o d e l g e n e ra tio n th e o re m p ro v e r is u s e d fo r p a rt-o f-s p e e c h ta g g in g a n d p arsin g .
A statistical and structural approach to extracting collocations likely to be of relevance in relation to an LSP sub&quot;domain text Bjarne Blom  
A Reasonably Language Independent, Heuristic Algorithm for the Marking of Names in Running Texts  
Som e R esu lts R egarding Tree H om om orphic Feature Structure Grammar and the E m pty String*  I n th is a rtic le we focus o n som e re stric tio n s we m ay im p o se o n T ree H o m o m o rp h ic F e a tu re S tru c tu re G ra m m a r (THFSG) re g a rd in g th e e m p ty strin g. A fte r d efin in g th e re s tric tio n s , we prove som e closure p ro p e rtie s o f th e tw o new classes o f lan g u ag es th ese re s tric tio n s give us. F u rth e rm o re , we e sta b lish th a t th e m e m b ersh ip p ro b le m is P S P A C E-c o m p le te fo r T H FSG s w ith o u t e m p ty p ro d u c tio n s in th e p h ra se s tru c tu re ru les.
Teaching and learning computational linguistics in an international setting  
Reference and Pragmatics An Integrated Approach to Reference and Presupposition Resolution Knowledge-Lean Coreference Resolution andits Relation to Textual Cohesion and Coherencebased Approaches Positing and Resolving Bridging -Anaphora in Deverbal NPs Elena Not, Lucia Tovena, and Massimo Zancanaro - Discourse Structure and Co-reference: An Empirical Study Building a Tool for Annotating Reference in Discourse Reference and Natural Language Generation Generating Anaphoric Expressions: Pronoun or Definite Description?  fax acl~aclweb.org ii 0 0 0 0 0 0 ~0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 O • Monday, June 21 8:35-8:45 Welcome
  
 When a system has to assign a referent to a referring expression, it is almost always the case that there is more than one candidate referents and one has to resort to some Way of eliminating the wrong candidates in order to choose the right one.There are two main ways of doing this. First, candidate referents can be ranked on the basis of accessibility (Erlm &amp; Gundel 1987;Fretheim and Gundal 1996;C~mbacher andHargreaves 1988, Grosz et al. 1995;Sidner 1983a, b;Walker et al. 1998). Alternatively, they can be checked against the accessibility of contextual assumptions (F'mcher-Kiefex 1993;Kintsch 1988, Magliano et al. 1993McKoon &amp; Ratcliff 1992;Sanford &amp; Garrod 1981;Sharkey &amp; Sharkey 1987;Singer 1993Singer , 1995. In either case, once a candidate is singled out, the acceptability of the referent needs to be tested against some pragmatic criteria. Currently, we seem to have at least three distinct criteria available:i. Truth-based criterion-i.e. whether the overall, interpretation is likely to be factually plausible (Clark 1977;Clark &amp; Haviland 1977;Erku &amp; Gundel 1987;Sanford &amp; Gan~ 1981;Sidner 1983a); iL Coherence-based criterion -i.e. whether the overall interpretation is likely to be coherent (Asher &amp; Lascaries 1993;Grosz et al. 1995Grosz et al. , 1998Hobbs 1979;Lascarides &amp; Asher 1993;Sanders et aL 1992;Walker et aL 1994Walker et aL , 1998); Hi. Relevance-based criterion -i.e. whether the overall interpretation is likely to be optimally relevant (Matsui 1993(Matsui , 1995(Matsui , 1998Wilson 1992;Wilson &amp; Matsui 1998). In this paper, I will focus on one version of coherence-based criterion, namely, centering theory. One of the goals of centering theory is to sort out the various mechanisms used to maintain discourse coherence, and the use of various referring expressions is regarded as one such mechanism. Among the various hypotheses put forward by centering theory, what concerns us most is the following: that 'each utterance [except the initial utterance] in a coherent discourse segment contains a single semantic entity -the backward-looking center [or Cb] -that provides a link to the previous utterance, and an ordered set of entities -the forward-looking centers [or Cf] -that offer potential links to the next utterance'( Gordon et al. 1993:311). There are two rules to provide constraints on choosing centers, which are shown in (1):( l)Rule 1: If any element of Cf (Un) is realised by a pronoun in Un+l then the Cb (Un+l) must he realised by a pronoun also. Rule 2: Sequences of continuation are preferred over sequences of retaining; and sequences of retaining are to be preferred over sequences of shifting.The first rule states that the most highly ranked element of the forward-looking center of a previous utterance is the backwardlooking center of the current utterance, and must be realised by a pronoun if any element of the Cf of the previous utterance is realised by a pronoun in the current utterance. The following example from Gordon et ai.(1993 Here, the first utterance has no Cb because it is the initial sen'tence of a discourse. Its Cf includes the referents off'Susan' and 'Betsy' and the semantic interpretation of 'a hamster', ranked in that order. The second utterance has Susan as the Cb and a Cf with Susan as its most highly ranked element. The third utterance preserves the Cb and prominent Cf from the previous utterance, therefore it pronominalises the Cb successfully. By contrast, utterance [3 1, in which Susan is realised by a name and Betsy is realised by a pronoun, leads to stylistic infelicity. According to Gordon et al., this is due to violation of Rule 1 mentioned above.The ranking of forward-looking centers is generally based on the discourse salience of each candidate entity. According to Grosz et aL (1995), although an ultimate criteria for deciding the ranking has not been worked out yet, there are evidences to support the idea that grammatical role such as SUBJECT, OBJECT, etc., can affect the Cf ranking. Thus, I will simply assume here the following preference in ranking forward-looking center shown in (3), as suggested by Grosz et al. (ibid.):The highest ranked member of the set of forward-looking centers is called the 'preferred center; or Up" As mentioned above, Cp is regarded as the most likely candidate for Cb in the following utterance.Mother important claim made by centering theory is that discourse segments are more coherent if they share the same Cb. On the basis of this idea, different degrees of coherence are proposed. For example, Walker et al. adopt the following 4 types of transition between discourse segments, each corresponding to different degree of coherence, using the notion of Cb and Cp;. namely. 'continue. 'retain; 'smooth-shift" and "rough-shift'. These are shown in (4). When two utterances, say Ui-I and U share the same Cb, and the same entity is also the highest-ranked Cf, i.e. Cp, in Ui-l, the transition from Ui-I to Ui is called 'continue: When Ui-1 and Ui share the same Cb, but thesame entity is not the highest-ranked Cf in Ui, the transition is called 'retain '. When Ui-1 and Ui do not share the same Cb, there are two possibilities: if Cb in Ui is the same as Cp in Ui, the transition is 'smooth-shift'; if Cb in Ui is not the same as Cp in Ui, it is 'rough-shift '. It is claimed that when the hearer has to choose one from several possible interpretations, the one based on the most coherent transition should be chosen. The 4 transition states are ordered in the following way according to their preference:The second of the two rules in (1) is about this ordered preference of transition states.' 2. Walker et al. (1994) and Japanese zero pronoun resolution Walker et ai. (1994) propose the following ranking order of forward-looking centers to deal with Japanese:As you can see, they add two new grammatical roles, namely, topic and empathy, to the list of factors affecting pronoun resolution. Since the notion of topic will become important in the discussion which follows, I will briefly describe the Japanese topic marker below. For empathy-loaded verbs, please refer to Kuno 1987 andKuno &amp; Kaburaki 1977. In Japanese, both in written and spoken discourse, lqPs which can be recovered from context are often omitted. The omitted NPs are often called 'zero pronouns'. It is widely agreed that missing NPs in JapaneSe behave like pronouns in other languages such as There were only three aTtn:idon statea in t&amp;e origml [o~ulmion by Gron et ~ (1983, /99S), namely, CONTINUE, RETAIN and SHIFZ. Fhe distinction between SMOOTH-SHIFT and ROUGII-SHIFF w~r [u~ proposed in Brenn~ (1987).English. Japanese zero pronouns should be distinguished from missing NPs in 'pro-drop' languages such as Italian, since in 'pro-drop' languages, information to recover missing NPs is morphologically encoded elsewhere in the sentence, e.g. in the form of verb inflection, whereas Japanese lacks such an overt encoding.Japanese zero pronouns are used equally often as inter-sentential discourse anaphors and intra-sentential anaphors. Here, I will concentrate on their use as discourse anaphors, where the role of context and adequate pragmatic criterion is crucial. For those who are interested in intra-sentential use of zero pronouns, please refer to e.g. Hasegawa 1985 andKameyama 1986. In Japanese, there are several postpositional particles. The one we are interested in here is 'wa; which is often called a ~opic marker: As the name suggests, 'via' is typically used to construct a grammatical topic of a sentence, which is characterised as an entity whose existence is presupposed. The function of 'wa" might become clearer when it is compared with another particle 'ga; which marks a hrP in the subject position, which typically conveys a new information. Compare (6a) and (6b): TOP flowers A~ bought "John bought flowers.' (6a) and (6b) share the same propositional content. However, only (6a) is acceptable as an answer to the question such as Who bought flowers?" while only (6b) is acceptable as an answer to the question such as What did John buy?'. There are various suggestions about how to characterise functions of the two particles (see e.g. Shibatani 1990;Tanaka 1991), and although it is an interesting question on its own, it shouldn't concern us here. This is because Walker et al. are only interested in the surface form of 'lqP+wa; which is automatically given the highest accessibility ranking in their framework.Walker et al. claim that a topic NP is more likely to be realised as a zero pronoun in the subsequent discourse than any other NPs due to its highest degree of accessibility. Moreover, in Walker et al. 's framework, topic NP is given two further advantages: they suggest (a) that a NP marked by %va' becomes the backward-looking center even at the onset of the discourse; and (b) that once topic NP is realised as the backward-looking center ,as long as it continues to be realised as a zero pronoun in subsequent discourse, it could continue to be the backward-looking center. The second advantage given to the topic NP is called 'zero topic assignment', which is defined as in (7):When a zero in Ui+l representsan entity that was Cb (Ui), and when no other CONTHqUE transition is available, that zero may be interpreted as the ZERO TOPIC of Ui+lWalker et al. demonstrate that their framework, including an optional rule of zero topic assignment, can successfully explain the preference in the interpretation of the last sentence in (8)  [4] According to the questionnaire carried out by Walker et al., the preferred interpretation of [4] is that Hanako invited Michiko to lunch. As you can see, in fact, there are two possible ways of ranking forward-looking center in [3], and two possible ways of deciding both the backward-looking center and the ranking of forward-looking center in [4]. In their analysis of (8), the preference in the interpretation in [4] is explained by zero topic assignment in [3] and preference on 'continue' transition in [4].In summary, Walker et aL's account of Japanese zero pronoun is based on two independent preference mechanisms: the first one is the forward center farting, and the second is the ordered transition states. However, as Walker et al. themselves point out, each transition state between discourse segments is determined by the ordering of forward looking centers, the predictions of the theory tend to depend more largely on the forward center ranking.3. Problems with Walker et al. Now I would like to discuss some problems With Walker et al. "s framework. Needless to say. it has great advantages, such as relative ease of computational imPlementation. Moreover, I agree that accessibility ofdiscourse entities plays an important role in reference assignment, and their forward center ranking is an adequate enough approximation of accessibility of discourse entities in different grammatical categories in Japanese. However, as I mentioned before, there are cases whose interpretation process cannot poss!bly be explained by the accessibility factor alone. Accounts of reference assignment which are largely based on accessibility of discourse entities tend to exhibit their weaknesses when they face cases which require some pragmatic inferences, and Walker et al. is not an exception here. I will illustrate two problems they need to solve below. 
 Human face&lt;o-face communication is an ideal model for humm-computa-interface. One of the major features of face-to-face communication is its multiplicity of communication channels that acts on multiple modalities. By providing a number of channels through which information may pass between a user and a computer, a multi-modal dialogue system gives the user a • more convenient and natural interface than a language-only dialogue system. In the system, a user often uses a variety of anaphodc expressions like this, the red item, it, etc. User's intention is passed to the system through multiple channels, e.g., the auditory channel (carrying speech) and the visual channel (c~nrying gestures and/or facial expressions). For example, a user can say utterance (4) in Figure  ! ! while touching an item on the screen. The user may also say utterance (8) without touching the screen when there is only one red item displayed on the screen. Moreover, the user can use anaphoric expression to refer to an entity in previous utterances as in utterance (10). 
  
  
Discourse Structure and Co-Reference: An Empirical Study Most current anaphora resolution systems implement a pipeline architecture with three modules CLappin and Leass," 1994;Mitkov, 1997;Kameyama, 1997).1. A COLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it, 2. A FILTER module eliminates referees incompatible with the anaphor f~m the LPA.3. A PREFEI~NCE module detennm" es the most likely antecedent on the basis of an Ordering policy.In most cases,, the COLLECT module determines an LPA by enumerating all antecedents in a window of text that pLeced__es the anaphor under scrutiny (Hobbs, 1978;Lappin and Leass, 1994;Mitkov, 1997;Kameyama, 1997;Ge et al., 1998). This window can be as small as two or three sentences or as large as the entire preceding text.The FILTER module usually imposes semantic constraints by requiring that the anaphor and potential antecedents have the same number and gender, that selectional restrictions are obeyed, etc. The PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978;Lappin and Leass, 1994;Mitkov, 1997;Kameyama, 1997). In other cases, these modules are integrated by means of statistical ( Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in Order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981;Gmsz and Sidner, 1986;Grosz et aL, 1995;Fox, 1987;Vonk et al., 1992;Azzam et al., 1998;Hitzeman and P .oesio, 1998). These studies claim, on the one hand, that the use of referents in naturally occurring texts imposes constmints on the interpretation of discourse; and, on the other, that the structure of discourse constrains the HAs to which anaphors can be resolved. The oddness of the situation can be explained by the fact that both groups seem primafacie to be righL Empkical experiments studies that employ linear techniques for determining the LPAs of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and I.eass, 1994;Ge et al., 1998). Empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of discourse one has the potential of determining correct co-referential links for more than 80% of the referential expressions (Fox, 1987;Cristea et al., 1998) although to date, no discourse-based anaphora resolution system has been implemented. Since no di- of these two classes of approaches  has been made, • it is difficult to determine which  group is right, and what method is the best. In this paper, we attempt to fill this gap by empirically comparing the potential of linear-and hierarchical models of discourse to correctly establish co-referential links in texts, and hence, their potentiai to correctly resolve anaphors. Since it is likely that both linear-and discourse-based anaphora resolution systems can implement similar FILTER and PREFERENCE strategies, we focus here only on the strategies that can be used to COLLECT lists of potential antecedents. Specifically, we focus on determining whether discourse theories can help an anaphora resolution system determine LPAs that are "better" than the LPAs that can be computed from a linear interpretation of texts. Section 2 outlines the theoretical as.~umptions of our empirical investigation. Section 3 describes our experiment. We conclude with a discussion of the results. We compare the potential of two classes of finear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference&apos;and discourse structure.
Building a Tool for Annotating Reference in Discourse  We discuss the development of a system for marking several types of reference to facilitate the analysis of reference in discourse. The tool is designed to be used in three applicationsi generating training data for machine learning of co-reference relations, evaluating iheories of referring expression generation and resolution in texts, and developing theories for understanding reference in dialogs. The need to mark any of a broad set of relations which may span several levels of discourse structure drives the system architecture. The system has the abilityto collect statistics over encoded relations and meastwe inter-coder reliability, and includes tools to increase the accuracy of the user&apos;s markings by highlighting the di.u:rep-ancies between two sets of markings. Using parsed corpora as the input further reduces the human workload and increases reliability. 1 Mo~a~n To examine the phenomenon of reference in discourse, and to analyze how discourse structure and reference interact , we need a tool,which allows several kinds of func-tionality including markup , visualization, and evaluation. Before desitming slach a tool, we must ~y analyze the kinds of information each application requires. Three applications have driven the design of the system. These are: 1) the creation of training data for automatic derivation of reference resolution algorithms (/.e., machine learning), 2) the formation ofa testhed for evaluating proposed reference generation and anaphera resolution theories, and 3) the development of theories about understanding reference in dialog. The influence that these three areas have upon the functional requirements of an annotation system are discussed below. In this paper we fn~t describe the requirements that each of these three related applications demand from a discourse annotation tool geared to aid in answering questions concerning reference. We next discuss some of the theoretical implications and decisions concerning the tool development that have arisen from these requirements. Next we describe the tool itself. F&apos;mally, we discuss related work, future directions of this work, and some conclusions.
Q @ @ Generating Anaphoric Expressions: Pronoun or Definite Description? Anaphoric expressions are an important component to generating coherent discourses. While there has been some work on generating appmwiate refening expressions, little attention has been given to the problem of when a pronoun should be used to ref= to an object. In most instances the assumption has been that a pronoun should be generated when referring to a discourse entity that is highly prominent (accessible). However, a study of naturally occurring texts reveals that factors beyond accessibility must be ~g*n into account in order to explain the patterns of pronoun use found.Other researchers have indicated that fuller descriptions tend to bbe found at the beginning of discourse segments (Grosz &amp; Sidaer, 1986;Reichman, 1985), even when the object being referred to is extzemely prominent in the preceding sentence. There may be two reasons for this: (1) it could be that the item is not accessible since the "focus space" associated with the previous sentence is '*popped" at the boundary (and perhaps an older focus space, with respect to which accessibility should be judged, is restored in its place ( Passonneau. 1996)), (2) the use of a fuller definite description is actually marking the discourse segment boundary. If (2) is the case, because other methods for marking boundaries are also possible (and the writer need not use multiple markings), this may help explain why the correlation between discourse segment boundaries and fuller referring expressions is not perfecL Supposing that discourse, segments are an important factor in choosing anaphoric expressions, in order to take advantage of them we must have a clear definition-Of what a discourse segment is. For generation, the discourse segment boundary must be part of the input to a sentence generator (which, we assume, is responsible for making the referring expression choice). To evaluate proposals for generating referring expressions, the discourse segment boundary must also be recognizable.While discourse segment boundaries may be important, note that fuller noun phrases are sometimes used when there is no discourse segment boundary (under any reasonable definition of boundary). This may occur when the referent is not accessible because the nearest antecedent is too far away or because it is confusable with another referent. We attempt to explain such instances as well.In order to determine the circuntctances under which to choose a pronoun versus a definite descripfiont0 our tack has been to study naturally occurring examples and to try to hypothesize rules that explain the reference forms in those examples. To date we have concentrated our study on New York Times news articles. We hope to generalize some of our findings to other types of text genres as well.Consider the following passage from the first several lines of one.of the stories we analyzed:When Kenneth L. Curtis was wheeled into court nine years ago, mute, dull-eyed and crippled, it seemed clear to nearly everyone involved that it would be pointless t o put him on trial for the murder of his former girlfriend, 'We use the term definite description to mean either a definite noun phrase or a name.. In order to produce coherent text. natural language generation systems must have the ability to generate pronouns in the appropriate places. In the past, pronoun usage was primarily investigated with respect to the accessibility of ref-erents. We.argue that generating appropriate referring expressions requires looking at factors beyond accessibility. Also important are sentence boundaries, distance from last mention , discourse structure and ambiguity. We present an algorithm for generating appropriate anaphoric expressions which takes the tent+ poral structure of texts and knowledge about ambiguous contexts into account. We back up our hypotheses with some empirical results indicating that our algorithm chooses the right referring expression in 85% of the cases.
Cb or not Cb? Centering theory applied to NLG  Centering theory (CT) has been mostly discussed from the point of view of interpretation rather than generation, and research has tended to concentrate on problems of anaphora resolution. This paper examines how centering could fit into the generation task, separating out components of the theory which are concerned with planning and lexical choice. We argue that it is a mistake to define a total ordering on the transitions CONTINUE, PJZTAm, SHIFT and that they are in fact epiphenomenal; a partia/ordering emerges from the interaction between cohesion (maintaining the same center) and salience (re-all.qing the center as Subject). CT has generally been neglected by NLG practitioners, possibly because it appears to assume that the center is determined according to feedbsch from the surface grammar, to text planning, but we argue that this is an artefactual problem which can be eliminated on an appropriate interpretation of the CT rules.
Comprehension of Coreferential Expressions The study of coreference in generative linguistics has led to a very strong emphasis on how the hierarchical structure of sentences interacts with the form of referring expressions to constrain coreference (Chomsky, 1986;Reinhart, 1976). The resulting principles, embodied in the Binding Theory, provide rules that are of some use to researchers in naturallanguage processing because they provide information about disjoint reference -what an expression cannot refer to in certain circumstances. However, beyond that use theoretical work on the Binding Theory does not directly bear on questions central to language processing. Questions of how to resolve potentially ambiguous expression such as pronouns, or how meaning more generally is built up incrementally from linguistic expressions in context, are beyond its scope. The ways in which the form of referring expressions interacts with the structure of language are reviewed. Evidence from a number of different methods-quantitatively analyzed judgements of acceptable coreference, reading time, and corpus frequency of different types of coreferenfial expressions-converges on a fairly simple description of patterns of coreference.-A model is presented which integrates important aspects of Discourse Representation Theory and of Centering Theory in order to provide an account of how referential expressions are interpreted as part of constructing a discourse universe from a series of utterances.
Reference-based Discourse Structure for Reference Resolution 0 @ O @ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 O 0 O 0 0 0 0 0 0 @ 0 0 0 0 0 0 0 O 0 0 0 0 0 @ 0  The connection between discourse structure and reference is well recognmed. In the system of language understanding described , discourse structure is exploited for the purposes of reference resolution and then continu/ty of reference is exploited to build the discomle ~etructure for the purposes of subsequent reference resohtion. The role of discourse structure in derefer-encing, pronouns, other NPs, and temporal references is explored, as is the role of these in constraining discourse structure. I Introduction, The role of discourse structure in reference resolution , in particular pronoun and temporal reference resolution, is well recognised. However both Rhetorical Structure Theory (Mann and Thompson, 1988) and the intention-based approach to diacourse strut-ture associated with C~ntering Theory (GROSS and Sidner, 1986) involve high-level reamning. Such tea-sorting itself presupposes the availability of interpretations in which references have already been resolved. This rakes the question of the legitimacy of assuming such a discourse segmentation for the purposes of reference resolution. From a practical point of view a number of questions arise. Firstly, to what extent can We obtain the discourse structure we need for reference resolution without recourse to such higher-level reasoning? Secondly, having done our reference resolution, what further higher-level reasoning needs to be done if we are to adequately capture the meaning of the discourse, v Being able to label our discourse structure with discourse relations will only be useful insofar as these relations have implications for language understanding. Recognising an e.~hnafion can be seen to contribute to our understanding in concrete ways. It allows us to infer, for instance, the fact that one event caused another together with the fact that the first event preceded the second. Given that this information is implicit in the discourse, it is clear that our understanding would be impaired without it. With other discourse relations, however, the contribution to inference and so discourse understanding is le~ dear. We have implemented s unified approach to reference resolution and discourse structure in the sys-tern of language understanding described in (Ram-say, 1999). Having used the existing discourse structure to remlve references in the utterance and anchor it in the discourse, we then use information about continuity of reference to attach the new discourse state generated by the utterance to the discourse structure, or discourse tree. Attachment is by means of a number of interchangeable attachment rules which we have used to explore how different referential cues contribute to discourse structure (Seville, 1999). The kinds of r~er~tial information we use include: ~eme I (as this term is used by (Hal-l/day, 1985)), since this is what the dismurse at any one point is about; reference ~/me ~, since sometimes discourse structure reflects the temporal structure of the events described; and pronommali~ation in general, since this recognisably contributes to the overall coherence of the discourse. There are further referential cues we have yet to explore, including ad-verbiais like &quot;Then&quot; and &quot;Before that&quot; and bridging descriptions like those in the following examples: (1) John I/~ Shirley. (2) H~ mo~er doesn&apos;t. (I) The house was grand. (2) The door was carved with gilded cherubs. 2 Tim is ~ailar in many ways to the center. 2Asain, this is ,;,,,;lax to the notion of tmpomi center (Kameyama, Pa~onean, and Poeslo, 1993), but with important differences; for instance, re]crones time is an instant, while the tempond ~ is an interval.
Reference Hashed Discourse referents are represented quite differently by current diso~urse theories. Discourse Representation Theory (DRT), for example, employs a rather unstructured data structure for the domain of discourse referents: a set ( Kamp and Reyle, 1993). A DRT-implementation by Asherand Wada (1988), however, employs a more complex data type: a tree representation. In further work by Asher (i 993) referents are grouped together into segments depending on Ihe discourse structure. His Segmented DRT (SDRT) uses a tree-like representation for the discourse sUuctui~ I Centering Theory (CD proposes a//st structure for the entities one preferably refers to in subsequent sentences. In order to cover coreference over discourse segments the centering model was extended by a stack mechanism ( Grosz and Sidner, 1986). Recently, these data structures have been criticized by Walker (1998), because they seem to be too restrictive. She proposes a cache storage for the referenis in the focus of attention.! propose instead a novel data structure for the representation of discourse referents. A/rushing list tSimilarly, Rhetorical Structure Theory (RST) makes use of a tree representation ( Mann et aL,1988). 100 is used to distinguish between the different types of referents according to their grammatical features wrt. number or gender. This list structure is furthermore combined with a hierarchical tree structure.The remaining pan of the paper is organised as follows. Section 2 introduces the main claims made by past theories. Focusing on SDRT and CT, I will highlight the (dis.-...) advantages of these two approaches. Section 3 provides the reader with an introduction to hashing lists and how they can be used for linguistic dam. Section 4 discusses how the different advantages of former approaches can be combined. First, DRT will be amended by using a hashing list for the discourse referents instead of a simple set. Second, the centering model will be applied to the representation gained. Finally, the shortcomings of a flat representation are presented and the introduction of discourse segments is discussed. Subsequently, section 5 describes a detailed formalisation of one example sequences by the representation proposed, before section 6 concludes. This paper argues for a novel data structure for the representation of discourse referents. A so-called hashing list is employed to store discourse referents according to their grammatical features. The account proposed combines insights from several the-odes of discourse comprehension. Segmented Discourse Representation Theory (Asher, 1993) is enriched by the ranking system developed in centering theory (Grosz et al., 1995). In addition, a tree logic is used to represent underspecification within the discourse structure (Schilder, 1998).
Logical Structure and In thl. paper, we use a semantic theory based on Dynamic Quantifier Logic (van den Berg 1992 to present an approach to discom~ anaphora resolution under the Linguistic DL~ourse Model (Polanyi (1985, 1986, 1988, 1996) Pohmyi and Scha (1984), Scha and Pclawfi (1988), Prfmt, H., It Scha and M. H. van den Berg, 1994; Po~q~ L. and M. IL van den Berg 1996; van den Berg, M. H. 1996b). Our treatment integrates the imights of the Center~g framework (Jmbi audK,,h-1979(Jmbi audK,,h- , 1981Grosz et~l. 1983Grosz et~l. , 1986Grosz et~l. , 1995Gundel 1998;Walker et.al. 1998b) into a -n~Sed theory of discourse level structufa/and semantic relations. In our account, discourse level aaaphora resolution effects fall out of a general theory of discourse quantification. Scope orderinge in the logical representation of the antecedent utterance result in d|fferences in " The authors dedkate this paper to the memory of Megumi Kameyama ), a dedicated researcher and a very dear friend. accessibility for potential referents in a target utterance. No additional c~ntering mechanisms are required, the centering predictions follow from this theory.Our treatment is universal: explanations of relative coherence do not depend on conventions that might differ in different languages. Furthermore, we provide a treatment for the resolution of multiple anaphors, resulting from a range of possible antecedents including plurals and multiple antecedents.The approach to discourse anaph°ra resolution we take in this paper integrates a rigorous formal semantic machinery within a theory Of discourse strtlcture. Before giving a detailed account of our treatmeat of di~murse reference resolution, we would llke to address explicitly some of the positions towards rdereace resolution and discour~ ~mcture which inform our work. 
Coreference and Its Applications Proceedings of the Workshop Published by the Association for Computational Linguistics  
Cross-Document Event Coreference: Annotations, Experiments, and Observations Events form the backbone of the reasons why people communicate to one another. News is interesting and important because it describes actions, changes of state and new relationships between individuals. While the communicative importance of described events is evident, the phenomenon has proved difficult to recognize and manipulate in automated ways (example: MUC information extraction efforts).We began this research program by developing algorithms to determine whether two mentions of a name, example "John Smith", in different documents actually referred to the same individual in the world. The system that we built was quite successful at resolving cross-document entoty coreference (Bagga, 98b). We, therefore, decided to extend the system so that it could handle events as well. Our goal was to determine whether events in separate documents, example "resignations", referred to the same event in the world (is it the same person resigning from the same company at the same time). This new classof coreference has proved to be more challenging.Below we will present our approach and results as follows: First we discuss how this research is different from Information Extraction and Topic Detection and Tracking. Then we present the core algorithm for cross document person coreference and our method of scoring the the system's output. The method for determining event reference follows with presentation and discussion of results. We finish with an interannotator agreement experiment and future work. 1 Abstract We have developed cross document event tracking technology that extends our earlier efforts in cross document person coreference. The software takes class of events, like &quot;resignations&quot; and clusters documents that mention resignations into equivalence classes. Documents belong to the same equivalence class if they mention the same &quot;resignation&quot; event, i.e. resignations involving the same person, time, and organization. Other events evaluated include &quot;elections&quot; and &quot;espionage&quot; events. Results range from 45-90% F-measure scores and we present a brief interannotator study for the &quot;elections&quot; data set. 2
Is Hillary Rodham Clinton the President? Disambiguating Names across Documents The need to identify and extract important concepts in online text documents is by now commonly acknowledged by researchers and practitioners in the fields of information retrieval, knowledge management and digital libraries. It is a necessary first step towards achieving a reduction in the ever-increasing volumes of online text. In this paper we focus on the identification of one kind of concept -names and the entities they refer to. Context is crucial in identifying the intended mapping. A document usually defines a single context, in which it is quite unlikely to find several entities corresponding to the same variant. For example, if the document talks about the car company, it is unlikely to also discuss Gerald Ford. Thus, within documents, the problem is usually reduced to a many-to-one mapping between several variants and a single entity. In the few cases where multiple entities in the document may potentially share a name variant, the problem is addressed by careful editors, who refrain from using ambiguous variants. If Henry Ford, for example, is mentioned in the context of the car company, he will most likely be referred to by the unambiguous Mr. Ford.Much recent work has been devoted to the identification of names within documents and to linking names to entities within the document. Several research groups [DAR95, DAR98], as well as a few commercial software packages [NetOw197], have developed name identification technologyk In contrast, few have investigated named entities across documents. In a collection of documents, there are multiple contexts; variants may or may not refer to the same entity;i among them our own research group, whose technology is now embedded in IBM's Intelligent Miner for Text [IBM99]. and ambiguity is a much greater problem. Cross-document coreference was briefly considered as a task for the Sixth Message Understanding Conference but then discarded as being too difficult [DAR95].Recently, Bagga and Baldwin [BB98] proposed a method for determining whether two names (mostly of people) or events refer to the same entity by measuring the similarity between the document contexts in which they appear. Inspired by their approach, we have revisited our current cross-document coreference heuristics and have devised an improved algorithm that seems promising. In contrast to the approach in [BB98], our algorithm capitalizes on the careful intra-document name recognition we have developed. To minimize the processing cost involved in comparing contexts we define compatible names --groups of names that are good candidates for coreference --and compare their internal structures first, to decide whether they corefer. Only then, if needed, we apply our own version of context comparisons, reusing a tool --the Context Thesaurus --which we have developed independently, as part of an application to assist users in querying a collection of documents.Cross-document coreference depends heavily on the results of intra-document coreference, a process which we describe in Section 1. In Section 2 we discuss our current cross-document coreference. One of our challenges is to recognize that some "names" we identify are not valid, in that they do not have a single referent. Rather, they form combinations of component names. In Section 3 we describe our algorithm for splitting these combinations. Another crossdocument challenge is to merge different names. Our intra-document analysis stipulates more names than there are entities mentioned in the collection. In Sections 4-5 we discuss how we merge these distinct but eoreferent names across documents. Section 4 defines compatible names and how their internal structure determines coreference. Section 5 describes the Context Thesaurus and its use to compare contexts in which names occur. Section 6 describes preliminary results and future work. A number of research and software development groups have developed name identification technology, but few have addressed the issue of cross-document coreference, or identifying the same named entities across documents. In a collection of documents, where there are multiple discourse contexts, there exists a many-to-many correspondence between names and entities, making it a challenge to automatically map them correctly. Recently, Bagga and Baldwin proposed a method for determining whether two names refer to the same entity by measuring the similarity between the document contexts in which they appear. Inspired by their approach, we have revisited our current cross-document coreference heuristics that make relatively simple decisions based on matching strings and entity types. We have devised an improved and promising algorithm, which we discuss in this paper.
Identification of Coreference Between Names and Faces In multimedia contents retrieval, almost all of researches have ibcused on information extracted from single media, e.g.( Han and Myaeng, 1996) (Smeaton and Quigley, 1996). These methods don't take into account semantic relations, like coreference between faces and names, holding between the contents of individual media. In order to retrieve multimedia contents with this kind of relations, it is necessary to find out such relations.In this research, we use photograph news articles distributed on the Internet (Mai, 1997) and develop a system which identifies a person's name in texts of this type of news articles and her/his face on the accompanying photograph image, based on 1) the machine learning technology applied to individual media contents to build decision trees which extract face regions and human names, and 2) hypothesis based combining method for the results extracted by decision trees of 1). Since, in general, the number of candidates from image and that from language are more than one, the output of our system is the coreference between a set of face regions and a set of names.There are many researches in the area of human face recognition ( Rowley et al., 1996) (Hunke, 1994)( Yang et al., 1997) (Turk and Pentland, 1991) and human name extraction, e.g. (MUC, 1995). However, almost all of them deal with the contents of single media and don't take into account the combination of multimedia contents. As a case of combining multimedia contents, there is a research of captioned images (Srihari and Burhans, 1994) (Srihari, 1995). Their system analyzes an image and the corresponding caption to identify the coreference between faces in the image and names in the caption. The text in their research is restricted to captions, which describes contents of the corresponding images. However, in newspapers or photo news, captions don't always exist and long captions like the captions used in their research are rare. Therefore, in general, we have to develop a method to capture effective linguistic expressions not from captions but from the body of text itself.In the research field of the video contents retrieval, although there are many researches ((Flickner et al., 1995),etc), few researches have been done to combine image and language media ( ) ) (Smith and Kanade, 1997)( Wactlar et al., 1996)(Smoliar andZhang, 1994) . In this field, as language media, there are soundtracks or captions in the video or sometimes in its transcriptions. For analysis of video contents, the information which consists along the time axis is effective and is used in such systems. On the other hand, for analysis of still images, some other methods that are different from the methods for video contents retrieval are required because the relatively small amount of and limited information than information from videos are provided.In section 2, the background and our system's overview are stated. In section 3 and 4, we describe the language module and the image module, respectively. Section 5 describes the com-bining method of the results of the language module and the image module. In section 6, the experimental results are shown. Section 7 is our conclusions.2 System architecture for combining To find coreferences between names in the text and faces in the image of the same photograph news article, we have to extract human names from the text and recognize faces in the image (Figure 1).  The problem is that the face of the person whose name is appearing in a text is not always appearing in the image, and vice versa. Therefore, we have to develop a method by which we automatically extracts a person whose name appears in the text and simultaneously his/her face appears on the image of the same article. For the convenience, we define common person, common name and common face as follows. To retrieve multimedia contents by their meaning , it is necessary to use not only the contents of distinct media, such as image or language, but also a certain semantic relation holding between them. For this purpose, in this paper, we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph. The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces. Our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media.
Automatic Slide Presentation from Semantically Annotated Documents A presentation of information content must be adapted to the context. A problem arises here because of diverse types of contexts mainly due to the audience's idiosyncratic needs, backgrounds, and so forth. Adaptation by learning [Perkovitz and Etzioni, 1997;1998] cannot provide a full solution here, because individual information seekers' profiles and contexts are unpredictable from past experiences. It is essentially necessary to dynamically customize a presentation through interactions with the audience, as human presenters normally do.In the present paper we discuss how to automatically generate slide shows from semantically annotated documents, in such a way that the presentation can be dynamically adapted to the audience. The reported presentation system detects important topics in the input document and composes a slide for each topic by extracting and paraphrasing relevant Sentences. This whole process takes into consideration not only the semantic structure of the given document but also interactions with the audience. So the slide show can be dynamically customized by reflecting requests and queries from the audience during the presentation.Each slide is typically an itemized summary of a topic in the original document. Generating such slides and coordinating them to meet the audience's needs involves a lot more drastic reformation of the original document than mere extraction of sentences in traditional summarization, so that accurate semantic structure of the document is necessary. We hence assume that the input documents come with GDA (Global Document Annotation) tags [Hasida, 1997;Nagao and Hasida, 1998] embedded. The GDA tagset is an XML (eXtensible Markup Language) tagset which allows machines to automatically infer the semantic structures (including pragmatic structures) underlying the raw documents.Under the current state of the art, GDA-tagging can be only semiautomatic and calls for manual correction. The cost involved here pays, because an annotated document is a generic form of information content from which to compose diverse types of presentations, potentially involving summarization, narration, visualization, translation, information retrieval, information extraction, and so forth. The slide presentation system reported below addresses a core technology in this broad setting. In the rest of the paper, we first outline the GDA tagset, and discuss how to extract topics from the input document and to generate slides for them by exploiting the tags.2 The GDA Tagset GDA is a project to make WWW texts machineunderstandable on the basis of a. linguistic tag set, and to develop applications such as content-based presentation, retrieval, question-answering, summarization, and translation with much higher quality than before. GDA thus proposes an integrated global platform for electronic content authoring, presentation, and reuse. The GDA tagset 1 is based on XML, and designed as compatible as possible with HTML, and TEI 2, etc., incorporatlhttp ://~w. etl. go. jp/et I/nl/GDA/t agset, html 2http://www.uic.edu:80/orgs/tei/ ing insights from EAGLES s, Penn TreeBank [Marcus et al., 1993], and so forth.Described below is a minimal outline of the GDA tagset necessary for the rest of the discussion. Parsetree bracketing, semantic relation, and coreference are essential for slide presentation, as with many other applications such as translation. Further details, concerning coordination, scoping, illocutionary act, and so on, are omitted. This paper discusses how to automatically generate slide shows. The reported presentation system inputs documents annotated with the GDA tagset, an XML tagset which allows machines to automatically infer the semantic structure underlying the raw documents. The system picks up important topics in the input document on the basis of the semantic dependencies and coreferences identified from the tags. This topic selection depends also on interactions with the audience, leading to dynamic adaptation of the presentation. A slide is composed for each topic by extracting relevant sentences and paraphrasing them to an itemized summary. Some heuristics are employed here for paraphrasing and layout. Since the GDA tagset is independent of the domain and style of documents and applicable to diverse natural languages, the reported system is also do-main/style independent and easy to adapt to different languages.
Resolution of Indirect Anaphora in Japanese Sentences Using Examples &quot;X no Y (Y of X)&quot; A noun phrase can indirectly refer to an entity that has already been mentioned. For example, "I went into an old house last night. The roof was leaking badly and ..." indicates that "The roof' is associated with "an old house," which has already been mentioned. This kind of reference (indirect anaphora) has not been thoroughly studied in natural language processing, but is important for coherence resolution, language understanding, and machine translation. We propose a method that will resolve the indirect anaphora in Japanese nouns by using the relationship between two nouns.When we analyze indirect anaphora, we need a case frame dictionary for nouns that contains information about the relationship between two nouns. For instance, in the above example, the knowledge that "roof" is a part of a "house" is required to analyze the indirect anaphora. But no such noun case frame dictionary exists at present. We considered using the example-based method to solve this problem. In this case, the knowledge that "roof" is a part of "house" is analogous to "house of roofi" Therefore, we use examples of the form "X of Y" instead. In the above example, we use linguistic data such as "the roof of a house." In the case of verbal nouns, we do not use "X of Y" but a verb case frame dictionary. This is because a noun case frame is similar to a verb case frame and a verb case frame dictionary does exist.The next section describes a method for resolving indirect anaphora. A noun phrase can indirectly refer to an entity that has already been mentioned. For example, &quot;I went into an old house last night. The roof was leaking badly and ...&quot; indicates that &quot;the roof&apos; is associated with &quot;an old house&quot;, which was mentioned in the previous sentence. This kind of reference (indirect anaphora) has not been studied well in natural language processing, but is important for coherence resolution, language understanding, and machine translation. In order to analyze indirect anaphora, we need a case frame dictionary for nouns that contains knowledge of the relationships between two nouns but no such dictionary presently exists. Therefore, we are forced to use examples of &quot;X no Y&quot; (Y of X) and a verb case frame dictionary instead. We tried estimating indirect anaphora using this information and obtained a recall rate of 63% and a precision rate of 68% on test sentences. This indicates that the information of &quot;X no Y&quot; is useful to a certain extent when we cannot make use of a noun case frame dictionary. We estimated the results that would be given by a noun case frame dictionary, and obtained recall and precision rates of 71% and 82% respectively. Finally, we proposed a way to construct a noun case frame dictionary by using examples of &quot;X no Y.&quot;
Pronoun Resolution in Japanese Sentences Using Surface Expressions and Examples  In this paper, we present a method of estimating ref-erents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using exam-pies, surface expressions, topics and loci. Unlike conventional work which was semantic markers for semantic constraints, we used examples for semantic constraints and showed in our experiments that examples are as useful as semantic markers. We also propose many new methods for estimating referents of&apos;pronouns. For example , we use the form &quot;X of Y&quot; for estimating referents of demonstrative adjectives. In addition to our new methods , we used many conventional methods. As a result, experiments using these methods obtained a precision rate of 87% in estimating referents of demonstrative pronouns , personal pronouns, and zero pronouns for training sentences, and obtained a precision rate of 78% for test sentences. 1 Overview This paper describes how to resolve the referents of pronouns: demonstrative pronouns, personal pronouns, and zero pronouns. Pronoun resolution is especially important for machine translation. For example, if the system cannot resolve zero pronouns 1, it cannot translate sentences containing them from Japanese into English. When the word order of sentences is changed and the pronominalized words are changed in translation into English, the system must detect the referents of the pronouns. A lot of work has been done in Japanese pronoun resolution (Kameyama 86) (Yamamuraet al. 92) (Walker et al. 94) (Takada &amp; Doi 94) (Nakaiwa &amp; Ikehara 95). The main distinguishing features of our work are as follows: • In conventional pronoun resolution methods, semantic markers have been used for semantic constraints. On the other hand, we use examples for semantic constraints and show in our experiments that examples are as useful as semantic markers. This is an important result because the cost of constructing the case frame using semantic markers is generally higher than the cost of constructing the case frame using examples. • We use examples in the form &quot;X no Y&quot; (Y of X) for estimating referents of demonstrative adjectives.
Corpus-Based Anaphora Resolution Towards Antecedent Preference Coreference information is relevant for numerous NLP systems. Our interest in anaphora resolution is based on the demand for machine translation systems to be able to translate (possibly omitted) anaphoric expressions in agreement with the morphosyntactic characteristics of the referred object in order to prevent contextual misinterpretations.So far various approaches 1 to anaphora resolution have been proposed. In this paper a machine learning approach (decision tree) is combined with a preference selection method based on the frequency information of non-/coreferential pairs tagged in the corpus as well as distance features within the current discourse. The advantage of machine learning approaches is that they result in modular anaphora resolution systems automatically trainable from a corpus with no 1See section 4 for a more detailed comparison with related research.or only a minimal amount of human intervention. In the case of decision trees, we do have to provide information about possible antecedent indicators (syntactic, semantic, and pragmatic features) contained in the corpus, but the relevance of features for the resolution task is extracted automatically from the training data.Machine learning approaches using decision trees proposed so far have focused on preference selection criteria directly derived from the decision tree results. The work described in (Conolly et al., 1994) utilized a decision tree capable of judging which one of two given anaphor-antecedent pairs is "better". Due to the lack of a strong assumption on "transitivity", however, this sorting algorithm is more like a greedy heuristic search as it may be unable to find the "best" solution.The preference selection for a single antecedent in (Aone and Bennett, 1995) is based on the maximization of confidence values returned from a pruned decision tree for given anaphor-candidate pairs. However, decision trees are characterized by an independent learning of specific features, i.e., relations between single attributes cannot be obtained automatically. Accordingly, the use of dependency factors for preference selection during decision tree training requires that the artificially created attributes expressing these dependencies be defined. However, this not only extends human intervention into the automatic learning procedure (i.e., which dependencies are important?), but can also result in some drawbacks on the contextual adaptation of preference selection methods.The preference selection in our approach is based on the combination of statistical frequency information and distance features in the discourse. Therefore, our decision tree is not applied directly to the task of preference selection, but aims at the elimination of irrelevant candidates based on the knowledge obtained from the training data.The decision tree is trained on syntactic (lexical word attributes), semantic, and primitive discourse (distance, frequency) information and determines the coreferential relation between an anaphor and antecedent Candidate in the given context. Irrelevant antecedent candidates are filtered out, achieving a noise reduction for the preference selection algorithm. A preference value is assigned to each " potential anaphor-candidate pair depending on the proportion of non-/coreferential occurrences of the pair in the training corpus (frequency ratio) and the relative position of both elements in the discourse (distance). The candidate with the maximal preference value is resolved as the antecedent of the anaphoric expression. In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information. First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates. In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse. Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.
Coreference resolution in dialogues in English and Portuguese The problem of anaphora resolution has received a great deal of attention in theoretical linguistics, psycholinguistics and also in natural language processing.Perhaps as an inevitable consequence of such a large body of work related to the subject, the term anaphora has been used to mean a varying range of phenomena.Approaches that build on the concept of cohesion ties (Halliday and Hasan 1976) analyse anaphoric relations within a broad framework of discourse or textual cohesion. As a result, the notion of anaphora, which had been initially linked quite closely to the older concept of pronominalisation, has been expanded to include all referring expressions with some form of antecedent either explicitly introduced in the text or inferable from it.In an earlier study, Webber (1979) had already widened the scope of anaphoric relations, by including nonpronominal noun phrases which refer back to antecedents in the discourse; the so-called one-anaphora; and verb-phrase deletions. Gradually, the distinction between anaphoric and coreference relations became less and less relevant in approaches concerned with robust implementation of systems with a capacity for anaphora resolution. The present study follows the same sort of approach.Therefore, the term coreference in the present study is used to refer to all pronominal forms, anaphoric nonpronominal noun phrases, one anaphora, numerals when used as heads of noun phrases, prepositional phrases used as responses to questions or statements, responses to questions in general (including yes, no and short answers using auxiliaries), so anaphora, dophrase anaphora and whatever other elements in dialogues were thought to be referring expressions with an identifiable antecedent.The next section describes the annotation scheme used to analyse the coreference cases. The third section presents the antecedentlikelihood (henceforth, AL) theory, which is the way information collected by means of the annotation was organised so as to be used to resolve new cases of coreference in other dialogues. The decision trees which are to be built on the basis of the AL theory are explained in the subsequent section, whereas the final section concludes with a discussion of results obtained so far and an analysis of future developments. This paper introduces a methodology to analyse and resolve cases of coreference in dialogues in English and Portuguese. A four-attribute annotation to analyse cases of anaphora was used to analyse a sample of around three thousand cases in each language collected in dialogue corpora. The information thus gathered was analysed by means of exploratory and model-building statistical procedures. A probabilistic model was then built on the basis of aggregate combinations of categories across the four attributes. This model, in combination with direct observation of cases, was used to build an antecedentqikelihood theory, which is at present being organised as a decision tree for the purpose of testing with a view for automatic annotation and subsequent resolution of coreference cases in dialogues in both languages. It is thought that the findings could be extended to Spanish, Italian and possibly French.
Orthographic Co-Reference Resolution Between Proper Nouns Through the Calculation of the Relation of &quot;Replicancia&quot; The proliferation of texts on electronic form during the last two decades has livened up the interest in Information Retrieval and given rise to new disciplines such as Information Extraction or automatic summarization. These three disciplines have in common the operation of Natural Language Processing techniques (Jacobs and Rau, 1993), which thus can evolve synergically.Identification and treatment of noun phrases is one of the fields of interest shared both by Information Retrieval and Information Extraction. Such interest must be understood within the trend to carry out only partial analysis of texts so as to process them in a reasonable time (Chinchor et al., 1993).The present work proposes a new instrument designed for the treatment of proper nouns and other simple noun phrases in texts written in Spanish language. The tool can be used both in Information Retrieval and Information Extraction systems. Nowadays there is a growing research activity centred in automated processing of texts on electronic form. One of the most common problems in this field is co-reference resolution, i.e. the way of knowing when, in one or more texts, different descriptions refer to the same entity. A full co-reference resolution is difficult to achieve and too computationally demanding; moreover, in many applications it is enough to group the majority of co-referential expressions (expressions with the same referent) by means of a partial analysis. Our research is focused in co-reference resolution restricted to proper names, and we part from the definition of a new relation called &quot;replicancia&quot;. Though replicancia relation does not coincide extensionally nor intensionally with identity, attending to the tests we have carried out, the error produced when we take the replicantes as co-referents is admissible in those applications which need systems where a very high speed of processing takes priority.
Coreference-oriented Interlingual Slot Structure &amp; Machine Translation According to Mitkov (1996), the establishment of the antecedents of anaphora is of crucial importance for a correct translation. It is essential to solve the anaphoric relation when a language is translated into one that marks the pronoun gender. On the other hand, anaphora resolution is vital when translating discourse rather than isolated sentences since the anaphoric references to preceding discourse entities have to be identified. Unfortunately, the majority of Machine Translation (MT) systems do not deal with anaphora resolution and their successful operation usually does not go beyond the sentence level. Another important aspect in automatic translation of pronouns, as mentioned in Mitkov (1996), consists on the application of two possible techniques: translation or reconstruction of referential expressions. In the first technique, source language pronouns are directly translated into target language pronouns without studying their relation with other words in the text. The second technique considers that the pronouns are not autonomous in their meaning/function but dependent on other units in the text. Then, a more natural way to treat pronouns in MT would be the following: (a) analysis has to determine the reference structure of the source text, i.e. coreference/cospecification relationships between anaphora and antecedents have to be determined, (b) this is the only information that is conveyed to the target language generator, (c) the target language generator generates the appropriate target language surface expression as a function of the target equivalent of the source antecedent and/or according to the rules of this language. Mitkov et al. (1995) adopt a similar approach. In this work, we present an Interlingual (formal language without' ambiguity) mechanism proposal based on the second technique. Basically, a structure that stores the anaphora and its antecedent in the source language is used. From this structure, a similar one in the target language is generated. Using this new structure we will be able to generate the final surface structure of the original sentence.In the following section we will describe the general purpose anaphora resolution system. The following section will show the anaphora resolution module, where we will focus on the differences between English and Spanish system and we will report some evaluation results. After that, we will present our Interlingual mechanism based on the English-Spanish discrepancy analysis. Finally, we will discuss the evaluation of some commercial MT systems with their problems in pronouns translation and we will study the solution with our proposal. 
Using Coreference Chains for Text Summarization In this paper we report preliminary work which explores the use of coreference chains to construct text summaries. Sparck Jones (1993) has described summarization as a two stage process of (1) building a representation of the source text and (2) generating a summary representation from the source representation and producing an output text from this summary representation. Our source representation is a set of coreference chains -specifically those chains of referring expressions produced by an information extraction system designed to participate in the MUC-7 coreference task (DARPA, 1998). Our summary representation is a 'best chain', selected from the set of coreference chains by the application of one or more heuristics. The output summary is simply the concatenation of (some subset of) sentences from the source text which contain one or more expressions occurring in the selected coreference chain.The intuition underlying this approach is that texts are in large measure 'about' some central entity, which is effectively the topic, or focus of the discourse. This intuition may be falsethere may be more than one entity of central concern, or events or relations between entities may be the principal topic of the text. However, it is at the very least an interesting experiment to see to what extent a principal coreference chain can be used to generate a summary. Further, this approach, which we have implemented and preliminarily evaluated, could easily be extended to allow summaries to be generated from (parts of) the best n coreference chains, or from event, as well as object , coreference chains.The use of document extracts formed from coreference chains is not novel. Bagga and Baldwin (1998) describe a technique for crossdocument coreference which involves extracting the set of all sentences containing expressions in a coreference chain for a specific entity (e.g. John Smith) from each of several documents.They then employ a thresholded vector space similarity measure between these document extracts to decide whether the documents are discussing the same entity (i.e. the same John Smith). Baldwin and Morton (1998) describe a query-sensitive (i.e. user-focused) summarization technique that involves extracting sentences from a document which contain phrases that corefer with expressions in the query. The resulting extract is used to support relevancy judgments with respect to the query.The use of chains of related expressions in documents to select sentences for inclusion in a generic (i.e. non-user-focused) summary is also not novel. Barzilay and Elhadad (1997) describe a technique for text summarization based on lexical chains. Their technique, which builds on work of Morris and Hirst (1994), and ultimately Halliday and Hasan (1976) who stressed the role of lexical cohesion in text coherence, is to form chains of lexical items across a text based on the items' semantic relatedness as in-dicated by a thesaurus (WordNet in their case). These lexical chains serve as their source representation, from which a summary representation is produced using heuristics for choosing the 'best' lexical chains. From these the summary is produced by employing a further heuristic to select the 'best' sentences from each of the selected lexical chains.The novelty in our work is to combine the idea of a document extract based on coreference chains with the idea of chains of related expressions serving to indicate sentences for inclusion in a generic summary (though we explore the use of coreference between query and text as a technique for generating user-focused summaries as well).Returning to Halliday and Hasan, one can see how this idea has merit within their framework. They identify four principal mechanisms by which text coherence is achieved -reference, substitution and ellipsis, conjunction and lexical cohesion. If lexical cohesion is a useful relation to explore for getting at the 'aboutness' of a text, and hence for generating summaries, then so too may reference (separately, or in conjunction with lexical cohesion). Indeed, identifying chains of coreferential expressions in text has certain strengths over identifying chains of expressions related merely on lexical semantical grounds. For, there is no doubt that common reference, correctly identified, directly ties different parts of a text together -they are literally 'about' the same thing; lexical semantic relatedness, as indicated by an external resource, can never conclusively establish this degree of relatedness, nor indeed can the resource guarantee that semantic relatedness will be found when it exists. Further, lexical cohesion techniques ignore pronomial anaphora, and hence their frequency counts of key terms, used both for identifying best chains and best sentences within best chains, may often be inaccurate, as focal referents will often be pronominalised.Of course there are drawbacks to a coreference-based approach. Lexical cohesion relations are relatively easy to compute and do not rely on full text processing -this makes summarisation techniques based on them rapid and robust. Coreference relations tend to require more complex techniques to compute. Our view, however, is that summarisation research is still in early stages and that we need to explore many techniques to understand their strengths and weaknesses in terms of the type and quality of the summaries they produce. If coreference-based techniques can yield good summaries, this will provide impetus to make coreference technologies better and faster.The basic coreference chain technique we describe in this paper yields generic summaries as opposed to user-focused summaries, as these terms have been used in relation to the TIPSTER SUMMAC text summarization evaluation exercise ( Mani et al., 1998). That is, the summaries aim to satisfy a wide readership by supplying information about the 'most important' entity in the text. But of course this technique could also be used to generate summaries tailored to a user(group) through use with a preprocessor that analyzed a user-supplied topic description and selected one or more entities from the topic description to use in filtering coreference chains found in the full source document. The rest of this paper is organised as follows. In Section 2 we briefly describe the system we use for computing coreference relations. Section 3 describes various heuristics we have implemented for extracting a 'best' coreference chain from the set of coreference chains computed for a text; and, it discusses how we select 'best' sentences to include in the summary from those source text sentences containing referring expressions in the 'best' chain. Section 4 presents a simple example and shows the different summaries that different heuristics produce. Section 5 describes the limited evaluation we have been able to carry out to date, but more importantly introduces what we believe to be a novel and interesting way of reusing some of the MUC materials for assessing summaries. We describe the use of coreference chains for the production of text summaries, using a variety of criteria to select a &apos;best&apos; chain to represent the main topic of a text. The approach has been implemented within an existing MUC coreference system, which constructs a full discourse model of texts, including information about changes of focus, which can be used in the selection of chains. Some preliminary experiments on the automatic evaluation of summaries are also described, using existing tools to attempt to replicate some of the recent SUMMAC manual evaluations.
Using Coreference for Question Answering Search engines have become ubiquitous as a means for accessing information. When a ranking of documents is returned by a search engine the information retrieval task is usually not complete. The document, as a unit of information, is often too large for many users information needs and finding information within the set of returned documents poses a burden of its own. Here we examine a technique for extracting sentences from documents which attempts to satisfy the users information needs by providing an answer to the query presented. The system does this by modeling coreference relationships between entities and events in the query and documents. An evaluation of this system is given which demonstrates that it performs better than using a standard tf. idf weighting and that the amount of information that the user must process on average, to find an answer to their query, is reduced by an order of magnitude over document ranking alone. We present a system which retrieves answers to queries based on coreference relationships between entities and events in the query and documents. An evaluation of this system is given which demonstrates that the the amount of information that the user must process on average , tQ find an answer to their query, is reduced by an order of magnitude.
What is coreference, and what should coreference annotation be?  In this paper, it is argued that &apos;coreference an-notation&apos;, as currently performed in the MUC community, goes well beyond annotation of the relation of coreference as it is commonly understood. As a result, it is not always clear what semantic relation these annotations are actually encoding. The paper discusses a number of interrelated problems with coreference annotation and concludes that rethinking of the coreference task is needed before the task can be expanded (e.g., to cover part/whole relations) as has recently been advocated. As a step towards solution of the problems with coreference annotation , one possible simplification of the annotation task is suggested. This strategy can be summed up by the phrase &quot;Coreference annotation should annotate coreference relations, and coreference relations only&quot;.
Towards Standards and Tools for • Discourse Tagging Proceedings of the Workshop Edited by Marilyn Walker Towards Standards and Tools for u Discourse Tagging Proceedings of the Workshop Edited by Published by the Association for Computational Linguistics  
Annotation Graphs as a Framework for Multidimensional Linguistic Data Analysis  In recent work we have presented a formal framework for linguistic annotation based on labeled acyclic digraphs. These &apos;annotation graphs&apos; offer a simple yet powerful method for representing complex annotation structures incorporating hierarchy and overlap. Here, we motivate and illustrate our approach using discourse-level annotations of text and speech data drawn from the CALLHOME, COCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain specialists, we have constructed a hybrid multi-level annotation for a fragment of the Boston University Radio Speech Corpus which includes the following levels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named entity. We show how annotation graphs can represent hybrid multi-level structures which derive from a diverse set of file formats. We also show how the approach facilitates substantive comparison of multiple annotations of a single signal based on different theoretical models. The discussion shows how annotation graphs open the door to wide-ranging integration of tools, formats and corpora. 1 Annotation Graphs When we examine the kinds of speech transcription and annotation found in many existing &apos;communi-ties of practice&apos;, we see commonality of abstract form along with diversity of concrete format. Our survey of annotation practice (Bird and Liberman, 1999) attests to this commonality amidst diversity. (See [~.idc.upenn.edu/annotation] for pointers to online material.) We observed that all annotations of recorded linguistic signals require one unavoidable basic action: to associate a label, or an ordered sequence of labels, with a stretch of time in the recording(s). Such annotations also typically distinguish labels of different types, such as spoken words vs. non-speech noises. Different types of annotation often span different-sized stretches of recorded time, without necessarily forming a strict hierarchy: thus a conversation contains (perhaps overlapping) conversational turns, turns contain (perhaps interrupted) words, and words contain (perhaps shared) phonetic segments. Some types of annotation are systematically incommensurable with others: thus disfluency structures (Taylor, 1995) and focus structures (Jackendoff, 1972) often cut across conversational turns and syntactic constituents. A minimal formalization of this basic set of practices is a directed graph with fielded records on the arcs and optional time references on the nodes. We have argued that this minimal formalization in fact has sufficient expressive capacity to encode, in a reasonably intuitive way, all of the kinds of linguistic annotations in use today. We have also argued that this minimal formalization has good properties with respect to creation, maintenance and searching of annotations. We believe that these advantages are especially strong in the case of discourse annotations , because of the prevalence of cross-cutting structures and the need to compare multiple annotations representing different purposes and perspectives. Translation into annotation graphs does not magically create compatibility among systems whose semantics are different. For instance, there are many different approaches to transcribing filled pauses in English-each will translate easily into an annotation graph framework, but their semantic incompatibility is not thereby erased. However, it does enable us to focus on the substantive differences without having to be concerned with diverse formats, and without being forced to recode annotations in an agreed, common format. Therefore, we focus on the structure of annotations, independently of domain-specific concerns about permissible tags, attributes, and values. As reference corpora are published for a wider range of spoken language genres, annotation work is increasingly reusing the same primary data. For instance, the Switchboard corpus [~. Idc. upenn, edu/Cat alog/LDC93S7, html] has been marked up for disfluency (Taylor, 1995). See [~. cis. upenn, edu/&apos;treebank/swit chboard-sample .html] for an example, which also includes a
The MATE Annotation Workbench: User Requirements Many people wish to annotate spoken (li~dogu(~ (:or--I)ora with co(led information. This information can come in many forms for many different i)url)oses. Sorne ol' it will be linguisti(:; for instance, the annotation /nay tel)resent i)art-of-sl)eech information or syntactic stru(:tur(. ~, for us(. in language modelling. Some of it will be non-linguistic, representing information about the co,nmunicative situation or about events such a.s coughing or gesturing. If the dialogue being armotated is being conducted with or mediated by technology, some of it may be sl)ecilic to that technology .. for instance, showing the resuits of speech recognition in line with a human transcription of the same material in order to highlight wt~ere the dialogue model, broke down. Although some kinds of annotation, such a.s dialogue act information, come up again and again, it is impossible to prejudge what kinds of annotations people who work with corpora will find usefld even when considering a quite restricted set of coding purposes.Currently, corpus annotation is a very costly exercise not just because of the time which it takes coders to make the coding distinctions, but also because there is little etfeetive technological support for annotation. The MATE Workbench [9] is intended to address the need for technology by providing a single interface to all of the basic functionalities which corpus annotators need, but with enough flexibility that different I)rojects can to i~,',)vi(I,. ,lill'~r,.nt kinds of annotation and that inforlnation ('ktn I{¢ ~ i)()rt.¢.,I between tint Workl)en('h anti otlwr al)l)lical.i~,n,~. Tlu" workbel|ch is a standalonct t()¢)l wril.l.¢.n ill .lay;t, s,~ il, will I)e able to ru. with(,uL r,.,'tmq)ilathm ,m Illany different I,lal.l'ornm including I'( Is, Mats, ;t{ul Ill,ix lna(:hines. The M A&apos;I&apos;E project (Telemati(&apos;s L1~4-8370) aims to facilitate the re-use of spoken dialogue resour,&apos;es an,l to foster etnl)iricai invesl, igation o1&quot; (lialoguc by providing a workl)ench which ,:an l)e used to atlnotal.c and explore the. relationships among diflb.rent stru(-tures within a (lialoguc corl),m. This I)aper des~:ribe.s the hltended functionality of the workl)¢~nch by ref-eren(&apos;e to the needs of several tyl)as of l)rO.sl)ectiw~ users. It should be considered a position i)aper about what kin(l of technological supl)ort the user (:olnnni-nity requires. The workl)en(:h itself is scheduled to be relea.se(l in I)et&apos;emhe.r 19!)9, with further dew:lop-nlent likely t)eyon(I that.
Argumentation Mark-Up: A Proposal Computational linguistics has had a long interest in discourse representation, but generally on the structure of monologue or dialogue rather than on content analysis, rhetoric, or argumentation proper. There is an increasing amount of work on mark-up, that is, indicating locally in the text the linguistic information -rather than in some knowledge-representation formalism. Mark-up can be designed for the reader, for a browser (in charge of displaying the information according to some template), or for further processing (a summarizer, a semantic analyzer, a reasoning system) -or a combination of these. This is a proposal for a an XML markup of argumentation. The annotation can be used to help the reader (e.g. by means of selective highlighting or diagramming), and for further processing (summarization, critique, use in information retrieval). The article proposes a set of markers derived from manual corpus annotation, exemplifies their use, describes a way to assign them using surface cues and limited syntax for scoping, and suggests further directions, including an acquisition tool, the application of machine learning, and a collaborative DTD definition task.
Evaluation of Annotation Schemes for Japanese Discourse Japanese Discourse Tagging Working Group Linguistic corpora are now indispensable of speech and language research communities. They are used not only for examining their characteristics, but also for (semi-)automatically learning rules for speech recognition, parsing and anaphora resolution, and evaluating the performance of speech and natural language processing systems.Linguistic corpora can be used as they are, however, they are usually annotated with information such as part of speech and syntactic structures. Currently there are many large linguistic annotated corpora worldwide, but the types of annotation information are limited to morphological and syntac--tic information. While there are some corpora annotated with discourse information like speech act types and discourse structures, they are much smaller than that of the corpora, with morphological and syntactic information. One of the major reasons for this difference in the size is due to the lack of computer tools such as morphological analyzers and syntactic parsers to semi-automatically annotate information.Of course we will be able to develop such tools for discourse information, but before that, we must create a base corpora by setting standards 1 for resource sharing, which can contribute to creating large resources for discourse.To this end, the Discourse Research Initiative (DRI) was set up in March of 1996 byUS, European, and Japanese researchers to develop standard discourse annotation schemes ( Walker et al., 1996). In line with the effort of this initiative, a discourse tagging working group has started in Japan in May 1996, with the support of the Japanese Society of Artificial Intelligence. The working group consists of representatives from eight universities and four research institutes in Japan. In the first year, (1) we collected and analyzed existing annotation schemes for Japanese discourse from the viewpoints of annotation units and information types, (2) developed new annotation schemes and experimentally annotated actual data, and (3) analyzed the experimental results to ira-1 The efforts have been called 'standardization', but we must admit this naming is misleading at least. In typical standardizing efforts, as done in audio-visual and telecommunication technologies, companies try to expand the market for their products by making their products or interfaces standards, and this profit directedness leaves room for negotiation. Even if the negotiation fails, they can appeal their products or interfaces for the market to judge. The objective of standardizing efforts in discourse is to promote interactions among different discourse researcher groups and thereby provide a solid foundation for corpus-based discourse research, which makes the researchers dispense with duplicate resource making efforts and increases the resources to be shared. prove the coding schemes. In the second year, based on the examination results obtained in the first year's experiments, we have revised new annotation schemes and conducted the second round of coding experiments to verify them. This paper describes our project of standardizing annotation schemes for Japanese discourse.In the following, annotation schemes for utterance units, discourse structure, and discourse markers are discussed based on our coding experiments. This paper describes standardizing discourse annotation schemes for Japanese and evaluates the reliability of these schemes. We propose three schemes, that is, utterance unit, discourse segment and discourse markers. These schemes have shown to be in-crementally improved based on the experimental results, and the reliability of these schemes are estimated as &quot;good&quot; range.
STANDARDISATION EFFORTS ON THE LEVEL OF DIALOGUE ACT IN THE MATE PROJECT Over the last years, corpus based approaches have gained significant importance in the field of natural language processing (NLP). Large corpora for many different languages are currently being collected all over the world, like In order to use this amount of data for training and testing purposes of NLP systems, corpora have to be annotated in various ways by adding, for example, prosodic, syntactic, or dialogue act information. This annotation assumes an underlying coding scheme. The way such schemes are designed depends on the task, the domain, and the linguistic phenomena on which developers focus. The author's own style and scientific background also has its effects on the scheme. So far, standardisation in the field of dialogue acts is missing and reusability of annotated corpora in various projects is complicated. On the other hand reusability is needed to reduce the costs of corpus production and annotation time.The participating sites of the EU sponsored project MATE (Multi level Annotation Tools Engineering) reviewed the world-wide approaches, available schemes [Klein et al.1998], and tools on spoken dialogue annotation [Isard et ai.1998]. The project builds its own workbench of integrated tools to support annotation, evaluation, statistical analysis and mapping between different formats. MATE also aims to develop a preliminary form of standard concerning annotation schemes on various levels to support the reusability of corpora and schemes.In this paper we focus on the level of dialogue acts. We outline the results of the comparison of the reviewed coding schemes based on [Klein et al.1998] and discuss best practice techniques for annotation of mass data on the level of dialogue acts. These techniques are considered as a first step towards a standard on the level of dialogue acts. This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field. We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task, and language dependencies of schemes. We discuss solution strategies which have in mind the reusability of corpora. Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the Euro-pean Union funded MATE project. MATE aims to develop general methodological guidelines for the creation, annotation, retrieval and analysis of annotated corpora.
Tagging of Speech Acts and Dialogue Games in Spanish Call Home The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis. The main application of the Clarity discourse classifiers will be automatic detection of what discourse participants are doing. This includes identifying genres (e.g., lecture vs. casual conversation) [1], functional activities (e.g., planning vs. gossiping), and discourse phases (e.g., opening vs. closing of a negotiation) among other things [3]. 1 l In the work that we are reporting here we do not, however, study the notion of genre or register as brought forth in functional systemic theory. We will say more about this below. The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow , corpus-based methods of analysis. For the Clarity project, we have tagged speech acts and dialogue games in the Call Home Spanish corpus. We have done preliminary cross-level experiments on the relationship of word and speech act n-grams to dialogue games. Our results show that the label of a game cannot be predicted from n-grams of words it contains. We get better than base-line results for predicting the label of a game from the sequence of speech acts it contains, but only when the speech acts are hand tagged, and not when they are automatically detected. Our future research will focus on finding linguistic cues that are more predictive of game labels. The automatic classification of speech acts and games is carried out in a multi-level architecture that integrates classification at multiple discourse levels instead of performing them sequentially.
Experiments in Constructing a Corpus of Discourse Trees Empirical studies of discourse structure have primarily focused on identifying discourse segment boundaries and their linguistic correlates. Very little attention has been paid so far to the highlevel, rhetorical relations that hold between discourse segments. In some cases, the role of these relations was considered to fall outside the scope of a study (Flammia and Zue, 1995); in other cases, judgements were made with respect to a taxonomy of very few intention-based relations (usually dominance and satisfaction-precedence) (Grosz and Hirschberg, 1992;Nakatani et al., 1995;Hirschberg and Litman, 1987;Passonneau and Litman, 1997;Carletta et al., 1997). And in the only case in which a rich taxonomy of 29 relations was used (Moser and Moore, 1997), the corpus was small and specific to a very restricted genre: written interactions between a student and tutor on the subject of fault location and repair in electronic circuitry.In spite of many influential proposals in the linguistic of discourse structures and relations ( Ballard et al., 1971;Grimes, 1975;Halliday and Hasan, 1976;Martin, 1992;Mann and Thompson, 1988;Sanders et al., 1992;Sanders et al., 1993;Asher, 1993;Lascarides and Asher, 1993;Knott, 1995;Hovy and Maier, 1993), a number of empirical questions remain to be answered.• Can human judges construct rich discourse structures in a manner that ensures inter-judge agreement that is statistically significant?• How can one measure the agreement?#" How should judges (and programs) construct the discourse structure of texts; should they follow a top-down, bottom-up, or an incremental procedure?• How does the genre of a text influence the degree to which judges achieve agreement on the task of rhetorical tagging?In this paper, we describe an experiment designed to answer these questions. We discuss a tagging schema and a tagging tool for labeling the rhetorical structure of texts. We also propose a statistical method for measuring agreement of hierarchical structure annotations and we discuss its strengths and weaknesses. The statistical measure we use suggests that annotators can achieve good levels of agreement on the task of determining the high-level, rhetorical structure of texts. Our empirical experiments also suggest that building discourse parsers that incrementally derive correct rhetorical structures of unrestricted texts without applying any form of backtracking is unfea-sible.
Tagging Psychotherapeutic Interviews for Linguistic Analysis  A proposal is presented to create a tagsetfor linguistic features that constitute the meaningful elements to one particular theory of psy-chotherapeutic intervention, that is Grinder &amp; Bandler&apos; s Metamodel. These features include lexical, lexicogrammatical and semantic phenomena. It is proposed that determining the effectiveness of therapy could in some degree be automated by building an information system to automatically tag client interviews. An appraisal of the effectiveness of the computational processing, as well as the therapy, might be established by comparing some characteri-sation of client interviews before and after therapy based on the features of the Meta-model. Three 30 minute interviews for each of 10 clients are available, that is before and after group therapy sessions and 3-6 months later The prototype tagset is presented for appraisal. A hand analysis of some of the data gives prima facie justification for persuing this research.
The MATE meta-scheme for coreference in dialogues in multiple languages Olle of the goals of the Eu-funded MATE project is to develop tools to support some of the most popular dialogue coding activities, including annotation of syntactic information, information about 'coreference,' and information about dialogue acts. 1 A problem to be confronted when trying to develop such tools is the lack of universally agreed upon coding schemes for these 'levels'~-i.e., of specifications of a set of elements and attributes that will cover all of the information about that level that a researcher may wish to annotate, together with instructions for how to do so. What does exist at the moment is coding schemes for particular domains and/or applications: in the case of dialogue acts, for example, there are several 'specific' schemes for given applications, some of which have been shown to lead to reliable coding ( Alexandersson et al., 1997;Carletta et al., 1997). Recently, attempts to come up with standards for a few of these levels have been made at workshops organized by the Discourse Resource Initiative (DRI). The DRI did come up with a proposal concerning the dialogue act level (Dis- course Resource Initiative, 1997;Allen and Core, 1997), although there have been serious disagreements concerning the usefulness of such a 'standard' for this level, since it's not clear that it's possible to come up with a domain-independent definition of dialogue acts. No official recommendation has been made in the DRI for the so-called 'coreference' level, although the DRAMA scheme (Passonneau, 1997) has sometimes been discussed for this purpose.In this paper we report on the current proposal concerning the type of 'coreference' annotation to be supported by the MATE workbench, motivating our proposal by relation to previous proposals in tThe project's home page is at http ://mate. nis. sdu. dk/.2We use the term MARKUP LEVEL to refer to each of these types of annotation. this area. The full proposal is available on-line at hl;tp: llwww, cogsci, ed. ac. ukl'poesio/MATE/ coreference_scheme.html.2 Annotating for 'Coreference' 
A recognition-based meta-scheme for dialogue acts annotation  The paper describes a new formal framework for comparison, design and standardization of annotation schemes for dialogue acts. The framework takes a recognition-based approach to dialogue tagging and defines four independent taxonomies of tags, one for each orthogonal dimension of linguistic and contextual analysis assumed to have a bearing on identification of illocutionary acts. The advantages and limitations of this proposal over other previous attempts are discussed and concretely exemplified.
Discourse-level argumentation in scientific articles: human and automatic annotation Work on summarisation has suffered from a lack of appropriately annotated corpora that can be used for building, training and evaluating summarisation systems. Typically, corpus work in this area has taken as its starting point texts target summaries: abstracts written by the researchers, supplied by the original authors or provided by professional abstractors. Training a summarisation system then involves learning the properties of sentences in those abstracts and using this knowledge to extract similax abstract-worthy sentences from unseen texts. In this scenario, system performance or development progress can be evaluated by taking texts in a test sample and comparing the sentences extracted from these texts with the sentences in the target abstract.But this approach has a number of shortcomings. First, sentence extraction on its own is a very general methodology, which can produce extracts that are incoherent or under-informative especially when used for high-compression summarisation (i.e. reducing a document to a small percentage of its original size). It is difficult to overcome this problem, because once sentences have been extracted from the source text, the context that is needed for their interpretation is not available anymore and cannot be used to produce more coherent abstracts (Spgrck Jones, 1998).Our proposed solution to this problem is to extract sentences but also to classify them into one of a small number of possible argumentative roles, reflecting whether the sentence expresses a main goal of the source text, a shortcoming in someone else's work, etc. The summarisation system can then use this information to generate template-like abstracts: Main goal of the text:... ; Builds on work by:... ; Contrasts with:... ; etc.Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958;Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995;Mani and Bloedorn, 1998). Neither approach is very satisfactory. Relying only on your own intuitions inevitably creates a biased resource; indeed, Rath et al. (1961) report low agreement between human judges carrying out this kind of task. On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers. Even if the abstracts are written by professional abstractors, there are considerable differences in length, structure, and information content. This is due to differences in the common abstract presentation style in different disciplines and to the projected use of the abstracts (cf. Liddy, 1991). In the case of our corpus, an additional problem was the fact that the abstracts are written by the authors themselves and thus susceptible to differences in individual writing style.For the task of summarisation and relevance decision between similar papers, however, it is essential that the information contained in the gold standard is comparable between papers. In our approach, the vehicle for comparability of information is similarity in argumentative roles of the associated sentences. We argue that it is more difficult to find the kind of information that preserves similarity of argumentative roles, and that it is not guaranteed that it will occur in the abstract.: .... A related problem concerns fair evaluation Of the extraction methodology. The evaluation of extracted material necessarily consists of a comparison of sentences, whereas one would really want to compare the informational content of the extracted sentences and the target abstract. Thus it will often be the case that a system extracts a sentence which in that form does not appear in the supplied abstract (resulting in a low performance score) but which is nevertheless an abstract-worthy sentence. The mismatch often arises simply because a similar idea is expressed in the supplied abstract in a very different form. But comparison of content is difficult to perform: it would require sentences to be mapped into some underlying meaning representations and then comparing these to the representations of the sentences in the gold standard. As this is technically not feasible, system performance is typically performed against a fixed gold standard (e.g. the aforementioned abstracts), which is ultimately undesirable.Our proposed solution to this problem is to build a corpus which details not only what the abstractworthy sentences are but also what their argumentative role is. This corpus can then be used as a resource to build a system to similarly classify sentences in unseen texts, and to evaluate that system. This paper reports on the development of a set of such argumentative roles that we have been using in our work.In particular, we employ human intuition to annotate argumentatively defined information. We ask our annotators to classify every sentence in the source text in terms of its argumentative role (e.g. that it expresses the main goal of the source text, or identifies open problems in earlier work, etc). Under this scenario, system evaluation is no longer a comparison of extracted sentences against a supplied abstract, or against a single sentence that was chosen as expressing (e.g.) the main goal of the source text. Instead, every sentence in the source text which expresses the main goal will have been identified, and the system's performance is evaluated against that classification.Of course, having someone annotate text in this way may still lead to a biased or careless annotation. We therefore needed an annotation scheme which is simple enough to be usable in a stable and intuitive way for several annotators. This paper also reports on how we tested the stability of the annotation scheme we developed. A second design criterion for our annotation scheme was that we wanted the roles to be annotated automatically. This paper reports on preliminary results which show that the annotation process can indeed be automated.To summarise, we have argued that discourse structure information will improve summarisation. Other researchers ( Ono et al., 1994;Marcu, 1997) have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987). In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. Our categories are not hierarchical, and they are much less fine-grained than RST-relations. As mentioned above, we wanted them to a) provide context information for flexible summarisation, b) provide a higher degree of comparability between papers, and c) provide a fairer evaluation of superficially different sentences.In the rest of this paper, we will first describe how we chose the categories (section 2). Second, we had to construct training and evaluation material such that we could be sure that the proposed categorisation yielded a reliable resource of annotated text to train a system against, a gold standard. The human annotation experiments are reported in section 3. Finally, in section 4, we describe some of the automated annotation work which we have started recently and which uses a corpus annotated according to our scheme as its training material. In this paper we present a rhetorically defined annotation scheme which is part of our corpus-based method for the summari-sation of scientific articles. The annotation scheme consists of seven non-hierarchical labels which model prototypical academic argumentation and expected intentional &apos;moves&apos;. In a large-scale experiments with three expert coders, we found the scheme stable and reproducible. We have built a resource consisting of 80 papers annotated by the scheme, and we show that this kind of resource can be used to train a system to automate the annotation work.
A mark up language for tagging discourse and annotating documents in context sensitive interpretation environments A document comes ~om "somewhere in time and space and leads toward somewhere else" (Tonfoni, 1998). It may therefore be defined as a piece of information that has been derived t~om a dynamically evolving information flow before it is converted into a stable form, e.g., hardcopy (Tonfoni 1996(Tonfoni , 1998). Documents are derivative products of flows of conversations and various kinds of communicative intercourse, which may include a very high level of complexity and long duration. Documents and conversations, ~om which those documents were generated, are therefore two very tightly linked components that often play a crucial role in providing evidence for decision making.It is our claim that enhanced encoding procedures in the form of discourse tagging and labelling may be harmoniously linked by means of a consistent annotation system (Tonfoni 1998) to support accurate conversion of dialogues and discourse into a more stable format. This is what documentation is all about. The discourse tagging system presented here is based on and harmonious linked to an annotation system, which consists of a set of signs and symbols as follows:Discourse tagging and document annotation signs: to indicate the communicative function of a sequence of discourse, which is ultimately to become a piece of a document.Discourse tagging and document annotation symbols: to indicate the communicative s~le of each sequence of discourse, which is ultimately to become a piece of a document. roles and interplay between the discourse partners that are carried along during the information conversion process and successively attached to the resulting document.Context sensitivity may be significantly enhanced by the consistent use of interpretation devices, designed to prevent filthiness and misunderstanding from occurring. Some contextual links, if not properly handled, may be powerful enough to radically shifl the scenario. The originating context may in fact be easily modified and completely distorted, even if unintentionally. Such links need to be accurately identified and then efiminated by repositioning, either by reassessing the originating context or assessing the new and intended one. A mark up language for tagging discourse, and for converting discourse sequences into a written format,
A Two-level Approach to Coding Dialogue for Discourse Structure: Activities of the 1998 DRI Working Group on Higher-level Structures* A two-level scheme for coding discourse structure in dialogue has been proposed and undergone initial testing within the DR/effort. In particular, the higher-level structures working group of the third DR/ was charged with the task of creating a coding scheme concerned exclusively with the discourse structure of dialogue. for a consensus coding scheme for discourse structure in dialogue was a non-trivial task. Most discourse structure schemes in fact were geared toward monologue, and most dialogue coding schemes omitted the higher-level structures that were essential to the monologue schemes, or provided only genre or domain-specific higher-level structures.Given the limited amount of work in this area, it was impossible to attempt a comprehensive coding scheme for all aspects of discourse structure ill dialogue. Instead, we were guided by an analysis of what choices needed to be made in creating a coding scheme. (Traum, 1998) identifies three dimensions along which discourse structure schemes can be classifted: granularity, content, structuring mechanisms.• Granularity: how much material (time, text, turns, etc.) is covered by the units (minimum, maximum, and average)? Granularity ranges were divided roughly into three categories:Micro -roughly within a single turn Meso -roughly an exchange, IR-unit, "game", or short "sub-dialogue", Macro -coherent larger spans, related to overall dialogue purposes.• Content: what is this a structure of(e.g., intentions, accessibility, effects, etc.)?• • Structuring mechanisms: What kinds of units and structuring principles are used (e.g., fiat, set inclusion, hierarchical/CFG structuring, relational)? How many primitive types of units are allowed (one basic unit type, two or three types of units, or several types)?This multi-dimensional space was then used to classify different extant coding schemes as to which aspects they are concerned with. Guided by this principled survey of various schemes, we decided on an objective of defining a pair of coupled schemes at the meso-and macrolevels in order to create a dialogue-oriented scheme for discourse structure analysis. We felt the microlevel of analysis was addressed by the dialogue acts coding effort of DRI, and it seemed most productive to build meso-and macro-levels on top of that, in an independent manner, to see what synergy might arise. It did not seem most fruitful to code the same content at three different levels, or to code three types of content at the macro-level without making any attempt to relate that coding to other schemes in development within the DRI initiative.Thus, for our starting point we proposed two original coding schemes within this multi-dimensional space. One scheme which has as content Grounding (Clark and Schaefer, 1989;Traum, 1994), operated at a meso level of granularity, and used non-hierarchical (and possibly discontinuous ) utterance sets as its structuring principle. The second scheme concerned intentional/informational structure ( Grosz and Sidner, 1986;Nakatani et al., 1995) as content, operated at a macro level of granularity, and was structured as hierarchical trees (with annotations for capturing discontinuities). In addition, these two schemes were linked by using the resulting structures from meso-level analysis as basic input for macro-level analysis.There were several factors motivating the decision to use these particular facets of discourse structure for initial analysis. First, considering intentions, it is clear that aspects of dialogue at all levels of granularity relate to the intentions of the participants. However, not all of these intentional aspects are attuned to well-behaved plan-like structures. One issue is whose intention is under consideration: the speaker, the hearer, or the collaborative "team" of the speaker and hearer together. It is only at the level of grounded content that some sort of joint or shared intentional structure is really applicable. Below this level, one may only properly talk of individual intentions, even though those intentions may be subservient to joint goals (or goals of achieving sharedness). Thus taking grounded units (achieved at the meso-range) as a starting point for the coding of intentional structure is a natural basis for the study of joint intentional structure. Individual intentions at a lower level, especially those relating to communication management rather than task are expected to be captured within the dialogue act level of the DRI coding scheme (Discourse Resource Initiative, 1997; Allen and Core, Draft 1997). Likewise, the phenomena of grounding can occur on multiple levels. However, since macro-level phenomena (such as task summarization) differ from more local feedback phenomena (including acknowledgments and repairs), restricting the grounding-relating coding to the meso-level allows for a more tractable effort.While examining intentional structure at the macro range and grounding structure at a meso range thus had independent motivations, the coding scheme used for this subgroup was designed to test a further novel and previously untested hypothesis that the units of achieving common ground would serve as an appropriate type of basic unit for intentional analysis. Since the phenomena of grounding and intentional task-related structure are somewhat independent, there is reason to believe the structures might not align properly. However, given the utility of having an appropriate meso-level starting point for intentional structure, and lacking any compelling counter-examples, we decided to put the hypothesis to the test in the coding exercises. This paper presents a novel two-level scheme for coding discourse structure in dialogue, which was created by the authors for the discourse structure subgroup of the 1998 DR/meeting on dialogue tagging. We discuss the theoretical motivations and framework for the coding proposal , and then review the results of coding exercises performed by the 1998 DR/ discourse structure subgroup using the new manual. Finally, we provide suggestions for improving the scheme arising from the working group activities at the third DRI meeting.
Automatically Extracting The annotation scheme (AC97) developed by the Discourse Research Initiative's Backward-and Forward-Looking Group (henceforth referred to as the BF scheme) provides a set of tags that can be applied to individual utterances in a dialogue, describing the utterance's illocutionary force. The BF scheme provides a standard top-level tag set that allows researchers to reuse corpora that have been annotated for other projects, and also allows tags to be refined by individual projects to provide detail on particular phenomena being studied.There are a number of dialogue features that are of interest to researchers, and for which tagging schemes have been developed. One feature that we are concerned with is grounding, the mechanism by which dialogue participants augment their mutual beliefs. In his dissertation work (Tra94), Traum establishes a set of tags to describe grounding behavior, and then uses this taxonomy of grounding acts to describe a computational model of how dialogue participants achieve a state of mutual understanding. Traum's model describes how grounding acts can be combined to form discourse units, segments of a dialogue that correspond to individual contributions to the common ground. Clark and Schaefer define a This paper describes how to automatically extract grounding features and segment a dialogue into discourse units, once the dialogue has been annotated with the DR/ backward-and forward-looking tags. Such an approach eliminates the need for separate annotation of grounding, making dialogue annotation quicker and removing a possible source of error. A preliminary test of the mapping against a human annotator is presented.
Computer Mediated Language Assessment and Evaluation in Natural Language Processing Proceedings of a Symposium sponsored by the Association for Computational Linguistics and International Association of Language Learning Technologies Published by the Association for Computational Linguistics  
  Preface The general purpose of this symposium is to strengthen collaboration between researchers and users of language learning tools, by fostering common applied and theoretical interests in the communities of two independent conferences held at the University of Maryland: the 38 ~ annual meeting of the Association for Computational Linguistics (ACL-99) and the biennial meeting of the International Association of Language Learning Technologies (IALL-99). The call for participation solicited presentations broadly: language assessment; software for first and second language acquisition; general discussions of problems and solutions in evaluating systems, software, and people learning language; integrating technology and foreign language pedagogy; user studies; the relation between technology and language learning, or linguistic theory and tool building; discussions of what is feasible and/or desirable in language learning technology; computer adaptive testing and speech recognition in language assessment and placement; software and tool demonstrations; formulation of discussion questions for panels relating to any of the above.
ELICITING NATURAL SPEECH FROM NON-NATIVE USERS: COLLECTING SPEECH DATA FOR LVCSR As part of work in improving speech recognition performance for non-native speakers, we wanted to develop a database that captures ways in which non-native language use differs from native language use in a specific domain. Features we were interested in include pronunciation, lexical choice, syntax, expressive goals, and strategies speakers use when they are unsure of the appropriate English expression. We wanted the recorded data to be appropriate for LVCSR system training, which means that the signal quality should be good and the speech should be as close as possible in terms of style and content to speech that will be used in the target application, a tourist information query system. We also wanted to elicit data which would contain examples of systematic and unsystematic variation in the speech of low-to mid-fluency non-native speakers.One of the most interesting aspects of these experiments was the ways in which we found 5 ourselves needing to adapt our usual data collection strategies to the needs of our speakers, whose English abilities varied from beginning to near-native. It is important to be aware of a number of assumptions that are commonly made which do not necessarily hold for nonnative speakers, and which it is important to address when designing a data collection protocol.The act of speaking is not difficult. When recording native speakers speaking spontaneously for standard LVCSR projects (that is, not projects geared towards special populations or difficult tasks), it is assumed that the the act of speaking does not in and of itself represent a major cognitive load for the speaker. This can be very untrue of non-native speakers, and we had several speakers ask to quit in the middle of the recording because they felt unable to continue. The researcher needs to make a decision about what to do in such a situation, and possibly prepare an alternate task.There is little risk of alienating the community. Local communities of non-native speakers are not always large, and if it is close knit, word can quickly spread if the task is too hard or embarassing. Also, it is important to de-emphasize the fact that we are interested, among other things, in imperfections in the speaker's speech, or risk offending the community.The task is not perceived as a test. Again, when speaking spontaneously, few native speakers of nonstigmatized varieties of English would feel that they are being evaluated on the correctness of their speech. Many non-native speakers will feel tested, and as this can make them nervous and affect their speech, it is important to reassure them as far as possible that they are not being tested and that the data is being anonymized.The speaker knows what to say. Most spontaneous collection tasks are chosen because they are tasks speakers can be expected to have done before and be comfortable with. Although a non-native speaker has probably made an airplane reservation in his native language before, it is entirely possible that he has never done so in the target language, and does not have a good idea of what he should say in that situation. If he were really planning to make an airplane reservation in the target language, he would probably think about what to say in advance and might even ask someone, which he may not have a chance to do during the data collection. This undermines the representativeness of the database.We carried out a number of exploratory experiments to try to determine the format which was the most comfortable for the speaJ~ers and which resulted in elicitation of the most natural data; two of these experiments are described in Section 3. For these experiments we worked with native speakers of Japanese. The protocol that we settled on, which we feel is very effective for non-native speakers, is described in Section 4. Although transcription and analysis of this data is at the beginning stages, we have already seen patterns that will be useful for developing acoustic and language models. Examples are shown in Section 5. In this paper, we discuss the design of a database of recorded and transcribed read and spontaneous speech of semi-fluent, strongly-accented non-native speakers of English. While many speech applications work best with a recognizer that expects native-like usage, others could benefit from a speech recognition component that is forgiving of the sorts of errors that are not a barrier to communication; in order to train such a recognizer a database of non-native speech is needed. We examine how collecting data from non-native speakers must necessarily differ from collection from native speakers, and describe work we did to develop an appropriate scenario, recording setup, and optimal surroundings during recording.
SPEECH COMPARISON IN The Rosetta Stone rM Funding for this research came from the developers 1, of The Rosetta Stone TM (TRS), a highly successful interactive multimedia program for teaching foreign languages. The developers wanted to use speech recognition technology to help students of foreign languages improve their pronunciation and their active vocabulary. As of this writing TRS is available in twenty languages, which was part of the motivation to develop a language independent approach to speech recognition. Classical approaches require extensive development per language.1 FLT, 165 South Main St., Harrisonburg, VA 22801. 540-432-6166 www.trstone.com TRS provides an immersion experience, where images, movies and sounds are used to build knowledge of a language from scratch. Since there is no concession to the native language of the learner, a German speaker and a Korean speaker both learning Vietnamese have the same experience--all in Vietnamese.The most recent release of TRS includes EAR, the speech comparison system described in this paper. The input to a speech comparison system is N+I digitized utterances--in the case of TRS, that includes N utterances by native speakers recorded in a studio with quality microphones, and one utterance by a student recorded in a sometimes very noisy environment with a builtin or handheld microphone. The output is a measure of the similarity of the last utterance to each of the N others. Which language is being spoken is irrelevant.Speech comparison differs from classical speech recognition, where the input data includes one utterance, a set of expectations tuned to the particular language in use (typically digraphs or similar), and a grammar of expected words or phrases, and the output is recognition of the utterance as one of the phrases in the grammar, or rejection.The TRS CD-ROM contains tens of thousands of utterances by native speakers. Thus the TRS data set already included the necessary input for speech comparison, but not for classical speech recognition. The first application we developed was a pronunciation guide (see Fig. 1). The user clicks on a picture, hears a native speaker's utterance, attempts to mimic that utterance, sees a display of two images visually portraying the two utterances, and observes a gauge which shows a measure of the similarity between the two utterances. The system normalizes both voices (native speaker's and student's) to a The Rosetta Stone TM is a successful CD-ROM based interactive program for teaching foreign languages, that uses speech comparison to help students improve their pronunciation. The input to a speech comparison system is N+I digitised utterances. The output is a measure of the similarity of the last utterance to each of the N others. Which language is being spoken is irrelevant. This differs from classical speech recognition where the input data includes but one utterance, a set of expectations tuned to the particular language in use (typically digraphs or similar), and a grammar of expected words or phrases, and the output is recognition in the utterance of one of the phrases in the grammar (or rejection). This paper describes a speech comparison system and its application in The Rosetta Stone TM.
Multimedia Computer Technology and Performance-Based Language Testing: A Demonstration of the Computerized Oral Proficiency Instrument (COPI) The Computerized Oral Proficiency Instrument (COPI) is a multi-media, computer-administered adaptation of the tape-mediated Simulated Oral Proficiency Interview (SOPI). Both the SOPI and the COPI are oral proficiency tests based on the Speaking Proficiency Guidelines of the American Council on the Teaching of Foreign Languages (ACTFL). Oral proficiency tests like the SOPI and COPI use simulated real life tasks to elicit speech ratable by the ACTFL Guidelines' criteria. The purpose of the COPI is to use the advantages of multi-media computer technology to improve the SOPI by giving examinees more control over various aspects of the testing situation and increasing raters' efficiency in scoring the test.In this paper we primarily discuss the Spanish version of the COPI, although an Arabic and a Chinese version are also being prepared. This paper provides the context for the COPI, discusses its rationale, its components and its phases, and introduces the scoring program used by raters who assess an examinee's speech performances using the criteria of the ACTFL Guidelines. The field of language testing has long led the way in integrative, performance-based assessment. However, the use of technology in language testing has often meant limiting assessment options. We believe computer-mediated language assessment can enrich opportunities for language learners to demonstrate what they are able to do with their second language. In this paper, we describe the rationale and operation of the Computerized Oral Proficiency Instrument (COPI), a multimedia, computer-administered oral proficiency test. While at present speech performances on the COPI are evaluated by trained raters using a national standard, the COPI affords an excellent opportunity to investigate the use of Natural Language Processing for computer-assisted evaluation.
Modeling the language assessment process and result: Proposed architecture for automatic oral proficiency assessment Computer-mediated language assessment appeals to educators and language evaluators because it has the potential for making language assessment widely available with minimal human effort and limited expense. Fairly robust results (n '~ 0.8) have been achieved in the commercial domain modeling the human rater results, with both the Electronic Essay Rater (erater) system for written essay scoring ( Burstein et al., 1998), and the PhonePass pronunciation assessment (Ordinate, 1998).There are at least three reasons why it is not possible to model the human rating process. First, there is a mismatch between what the technology is able to handle and what people manipulate, especially in the assessment of speech features. Second, we lack a wellarticulated model of the human process, often characterized as holistic. Certain assessment features have been identified, but their relative importance is not clear. Furthermore, unlike automatic assessments, human raters of oral proficiency exams are trained to focus on competencies, which are difficult to enumerate. In contrast, automatic assessments of spoken language fluency typically use some type of error counting, comparing duration, silence, speaking rate and pronunciation mismatches with native speaker models.There is, therefore, a basic tension within the field of computer-mediated language assessment, between modeling the assessment process of human raters or achieving comparable, consistent assessments, perhaps through different means. Neither extreme is entirely satisfactory. A spoken assessment system that achieves human-comparable performance based only, for example, on the proportion of silence in an utterance would seem not to be capturing a number of critical elements of language competence, regardless of how accurate the assessments are. Such a system would also be severely limited in its ability to provide constructive feedback to language learners or teachers. The e-rater system has received similar criticism for basing essay assessments on a number of largely lexical features, rather than on a deeper, more humanstyle rating process.Thirdly, however, even if we could articulate and model human performance, it is not clear that we want to model all aspects of the human rating process. For example, human performance varies due to fatigue. Transcribers often inadvertently correct examinees' errors of omitted or incorrect articles, conjugations, or affixes. These mistakes are a natural effect of a cooperative listener; however, they result in an over-optimistic assessment of the speaker's actual proficiency. We arguably do not wish to build this sort of cooperation into an automated assessment system, though it is likely desirable for other sorts of human-computer interaction systems.Furthermore, if we focus on modeling human processes we may end up underutillzing the technology. Balancing human-derived features with machine learning techniques may actually allow us to discuss more about the human rating process by making the entire process available for inspection and evaluation. For example, if we are able to articulate human rating features, machine learning techniques may allow us to 'learn' the relative weighting of these features for a particular assessment value.2 Modeling the rater We outline challenges for modeling human language assessment in automatic systems, both in terms of the process and the reliability of the result. We propose an architecture
Dual Use of Linguistic Resources: Evaluation of MT Systems and Language Learners Human translators working with "embedded machine translation (MT) systems"1 on the task of filtering text documents in a foreign language often have limited training in the foreign language they encounter. For our MT system users who are also language learners, we are developing a suite of linguistic tools that enable them, on the same laptop platform, to perform their foreign language filtering tasks using a combination of Optical Character Recognition (OCR), Machine Translation (MT), Information Retrieval (IR) and language sustainment tools. 2 Thus we have begun constructing linguistic test suites that can serve the dual needs we have for the evaluation of MT systems and language learning. 3In this paper, we present our pilot work (i) defining and constructing a semantic domain of spatial expressions as a test suite, (ii) testing our MT system on the ~The term embedded MT system, adopted from Voss and Reeder (1998), refers to a computer system with several software components, including an MT engine. 2 We are creating a single interface for the MT system and the language sustainment tools that enables users to guide their own learning during MT-aided tasks, such as filtering, in contrast to single-purpose tutoring systems (e.g., Holland et al., 1995) 3 For others addressing multiple uses of linguistic resources, see NLP-IA (1998). translations of these expressions, and (iii) testing language learners' ability to translate these expressions. Our results show that, for English-to-French translation of a small set of spatial expressions, neither a commercially viable MT system nor intermediate-level students are adequately trained to identify explicit and implicit (ambiguous) paths of motion. 
FAME: a Functional Annotation Meta-scheme for multi-modal and multi-lingual Parsing Evaluation Broad coverage parsing evaluation has received growing attention in the NLP community. In particular, comparative, quantitative evaluation of parsing systems has acquired a crucial role in technology assessment. In this context, it is important that evaluation be relatively independent of, or easily parametrizable relative to the following three dimensions of variation among parsing systems:• theoretical assumptions: compared systems may be based on different theoretical frameworks;• multi-linguality: parsers are often optimally designed to deal with a particular language or family of languages;• multi-modality: systems tend to be specialized for dealing with a specific type of input, i.e. written or spoken language.As to the first point, it is important that alternative annotation schemes be evaluated (i) on the basis of the linguistic information they are intended to provide, and (ii) in terms of the utility of this information with respect to a particular task. Moreover, multi-linguality and multi-modality are crucial parameters for evaluating the robustness and portability of a given parser, with a view to the growing need for embedding NLP systems into multi-modal and multi-medial applications.An essential aspect of every evaluation campaign is the specification of an annotation scheme into which the output of the participant systems is converted and on whose basis the system performance is eventually evaluated. A suitable annotation scheme must satisfy some requirements. First of all, it should be able to represent the information relevant to a certain evaluation task in a way which is naturally conducive to quantitative evaluation. Secondly, it should easily be mappable onto different system outputs, and flexible enough to deal with multilingual phenomena and with the specific nature of both written and spoken language.The aim of this paper is to illustrate FAME, a Functional Annotation Meta-scheme for Evaluation. We will show that it complies with the above mentioned requirements, and lends itself to effectively being used in comparative evaluation campaigns of parsing systems. There are two main features of FAME that will receive particular emphasis here: it is functional and it is a meta-seheme. We claim that these two features are essential for meeting the specific requirements of comparative parsing evaluation, while tackling issues of multi-linguality and multi-modality in a principled fashion. The paper describes FAME, a functional annotation meta-scheme for comparison and evaluation of existing syntactic annotation schemes, intended to be used as a flexible yardstick in multilingual and multi-modal parser evaluation campaigns. We show that FAME complies with a variety of non-trivial methodological requirements, and has the potential for being effectively used as an &quot;interlingua&quot; between different syntactic representation formats. 1 Introduction Broad coverage parsing evaluation has received growing attention in the NLP community. In particular , comparative, quantitative evaluation of parsing systems has acquired a crucial role in technology assessment. In this context, it is important that evaluation be relatively independent of, or easily parametrizable relative to the following three dimensions of variation among parsing systems:
Modeling User Language Proficiency in a Writing Tutor for Deaf Learners of English In order for any human language tutor to be effective, he or she must have an accurate picture of the student's language acquisition status. This "picture" is used for selecting target features for tutoring and for shaping and tailoring the tutorial instruction. Automated tutoring systems emulate this desirable practice by constructing and maintaining a model of the user's knowledge, consulted at many levels of the tutorial production process. In this paper we will discuss the proposed knowledge modeling architecture of ICICLE (Interactive Computer Identification and Correction of Language Errors), a system under development (McCoy and Masterman (Michaud), 1997) whose goal is to provide deaf students with constructive tutoring on their written English.The target learner group for ICICLE is native or near-native users of American Sign Language (ASL). This population poses unique challenges for a writing instruction program: their writing contains many errors which are not made by native users of English, and students vary widely across levels of language ability, with some college-age writers producing near-nativelike English and others struggling with grammatical basics. Because of these characteristics of the learner population, it is integral to ICICLE's goal of user-tailored instruction that it account for user differences so that the instruction it provides is appropriate for a learner at any level. Since ASL is a distinct and vastly different language from English ( Baker and Cokely, 1980), we view the acquisition of written English skills to be a task in second language acquisition for these learners (Michaud and McCoy, 1998). We are therefore proposing a user model design which incorporates a representation of the language acquisition process, and we have based our design upon current research in language acquisition and in the acquisition of cognitive skills. ICICLE will consult this model to obtain specific information about the user's current language knowledge, as well as about what knowledge is likely to be learnable by the user at the current time.In the following sections, we overview the ICI-CLE system architecture in order to explain how the user model will be utilized in system operation; we then discuss our design for modeling the second language acquisition process, and overview issues involved in implementing the model within our system. In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a language tutoring application for deaf learners of written English. The model will represent the language proficiency of the user and is designed to be referenced during both writing analysis and feedback production. We motivate our model design by citing relevant research on second language and cognitive skill acquisition, and briefly discuss preliminary empirical evidence supporting the design. We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling user proficiency at a high level of granularity and specificity.
Exploiting the Student Model to Emphasize Language Teaching Pedagogy in Natural Language Processing One of the fundamental problems of any Natural Language Processing (NLP) system is the often overwhelming number of interpretations a phrase or sentence can be assigned. For example, van Noord (1997) states that the Alvey Tools Grammar with 780 rules averages about 100 readings per sentence on sentences ranging in length between 13 and 30 words. The problem is not always improved with deeper analysis, for though a semantic analysis may rule some of the possible syntactic structures, it will introduce lexical and scope ambiguity.The problem of resolving multiple interpretations is compounded in an Intelligent Language Tutoring System (ILTS) because the grammar must not only admit grammatical structures, but must also be able to navigate over ungrammatical structures and record the errors that the student has made. As a consequence, a grammar for an ILTS will not only assign structures to a grammatical sentence, but may also find analyses which interpret the sentence as ungrammatical, a set of analyses that a traditionally constrained grammar would not find.The usual method of limiting the number of parses that an ILTS grammar assigns is to examine the effects of relaxing those constraints that represent likely sources of error by students and introduce new constraints into the grammar rules to block unlikely parses (Schneider &amp; McCoy 1998). Such techniques, however, overlook individual learner differences as a key factor in language teaching pedagogy.The system introduced in this paper differs from the traditional approach by permitting the grammar to freely generate as many parses as it can and using separate pedagogic principles to select the appropriate interpretation and response. The system tightly integrates the Student Model into the process of selecting the appropriate interpretation and generating a response tailored to the student's level of expertise. The Student Model keeps a record of students' performance history which provides information essential to the analysis of multiple parses, multiple errors, and the level of interaction with the student.In the German Tutor, the ILTS described, the process leading to the creation of an instructional message in the event of an error has three stages:(1) Given a forest of parse trees created by the grammar and parser, the parse most likely representative of the intentions of the student must be selected;(2) In the cases when the parse representing a student's intentions contains several errors, one of the error must be selected as the one that will be addressed. This step is necessary because empirical studies have found that reporting all the errors in a sentence is pedagogically inappropriate. For example, in evaluating her own system Schwind (1990) reports that "[s]ometimes, however, the explanations were too long, especially when students accumulated errors."~;(3) Given an error, an instructional message must be constructed that is appropriate to the student's level of expertise and background.In Section 1, the theory behind the grammar and its formalism is briefly discussed. Section 2 describes the process leading to the selection of a particular parse and how the Student Model participates in this process. We further discuss the pedagogic role of the Student Model in handling multiple errors and deciding on the level of interaction with the student. Section 3 presents conclusions and Section 4 looks at further research. One of the typical problems of Natural Language Processing (NLP) is the explosive property of the parser and this is aggravated in an Intelligent Language Tutoring System (ILTS) because the grammar is unconstrained and admits even more analyses. NLP applications frequently incorporate techniques for selecting a preferred parse. Computational criteria, however, are insufficient for a pedagogic system because the parse chosen will possibly result in misleading feedback for the learner. Preferably, the analysis emphasizes language teaching pedagogy by selecting the sentence interpretation a student most likely intended. In the system described in this paper, several modules are responsible for selecting the appropriate analysis and these are informed by the Student Model. Aspects in the Student Model play an important pedagogic role in determining the desired sentence interpretation, handling multiple errors, and deciding on the level of interaction with the student.
A Web.Based System for Automatic Language Skill Assessment: EVALING EVALING is a Leonardo da Vinci project funded by the European Union, involving four European laboratories. The aim of the project is to build an automatic system to evaluate language skills in people's mother language. Each partner is working on his own language and building specific tests (at the present: i Association pour le traitement automatique des langues (ASSTRIL) for French, Consorzio Lexicon Ricerche from the University of Salerno, for Italian &amp; Piidagogische Hochschule Karlsruhe and Universit~t Mtinehen for German. 2 "Item banking covers any procedures that are used to create, pilot, analyse, store, manage and select test items so that multiple test forms can be created from a subset of the total 'bank' of items. " Brown (1997).French, Italian and German). In this paper we will present French.The first step consists, for each language, in determining the fields to be tested and the types of exercises which can be computerized to carry out this task. We have observed this task differs from one language to another. For example, spelling exercises are relevant in French, but they are not very interesting for German, since people make few mistakes. For French, we decided to focus on syntax, lexicon, spelling and reading comprehension. Hence, we oriented the reflection on the tests that could be automatized and on those that could not. At this point, automatization in the EVALING sytsem bears on three phases of the evaluation process:-dynamic setting up of tests (exercises are stored in tables of an exercise database). We assume that if a person has to take the test more than once, this person should not get the same set of questions twice, -automatic grading of tests (including storage of marks in a client database), -semi-automatic filling of exercise databases (with the assistance of linguistic tools).First, we will discuss some technical aspects: EVALING is a Web-based program interacting with large exercise databases and a client database. We will explain how this ' item Banking' system has been implemented on a Web Server as an ISAPI (Internet Server Application Programming for Microsoft Information Server 3) and how exercise databases were built. This last point is a key issue, because the aim is not so much to gather a fixed amount 3Chapman(1997) discuss advantages and disadvantages of ISAPI programming.. of exercises, but rather to be able to renew them easily. We designed a set of linguistic tools to satisfy this demand. The set of tools is based on the software INTEX, developed at the LADL by Max Silberztein 4. Of course, it is not always possible to automatize the creation of exercises. In certain cases, the work will have to be done manually.Second, we will present the 'Administrator side'. We call 'administrator' the person or team who needs to evaluate a large group (students, employees, applicants, etc.). An interface enables the administrator to define the parameters of the test (length, level, etc.) and to perform some statistical analysis on the client database.Finally, we will deal with the 'client side'. The client registers himself and then has access to the test session through a Web-Client software. All test forms are in HTML form. EVALING is a Leonardo da Vinci project funded by the European Union, involving four European laboratories 1. The aim of the project is to build an automatic system to evaluate language skills in people&apos;s native language. This paper focuses on native French. Other partners are working on their own language and are building specific tests (Italian and German). EVALING is an &apos;Item Banking &apos;2 system: exercise database allowing dynamic design of questionnaires. We present a technique based on the use of NPL tools that assure easy and costless updating of these databases. In addition, we underline the interest of Local Grammars (Finite State Transducers) for scoring exercises on language.
Automated Essay Scoring for Nonnative English Speakers Research and development in automated essay scoring has begun to flourish in the past five years or so, bringing about a whole new field of interest to the NLP community (Burstein, et al (1998a(Burstein, et al ( , 1998b(Burstein, et al ( and 1998c, Foltz, et al (1998), Larkey (1998), Page and Peterson (1995)). Research at Educational Testing Service (ETS) has led to the recent development of e-rater, an operational automated essay scoring system. E-rater is based on features in holistic scoring guides for human reader scoring. Scoring guides have a 6-point score scale.Six's are assigned to the "best" essays, and "l's" to the least well-written. Scoring guide criteria are based on structural (syntax and discourse) and vocabulary usage in essay responses (see http://www.gmat.org).E-rater builds new models for each topic (prompt-specific models) by evaluating approximately 52 syntactic, discourse and topical analysis variables for 270 human reader scored training essays. Relevant features for each model are based on the predictive feature set identified by a stepwise linear regression. In operational scoring, when compared to a human reader, e-rater assigns an exactly matching or adjacent score (on the 6-point scale) about 92% of the time. This is the same as the agreement rate typically found between two human readers. Correlations between erater scores and those of a single human reader are about .73; correlations between two human readers are .75.The scoring guide criteria assume standard written English. Non-standard English may show up in the writing of native English speakers of non-standard dialects.For general NLP research purposes, it is useful to have computer-based corpora that represent language variation (Biber (1993)). Such corpora allow us to explore issues with regard to how the system will handle responses that might be written in nonstandard English. Current research at ETS for the Graduate Record Examination (GRE) (Burstein, et al, 1999) is making use of essay corpora that represent subgroups where variations in standard written English might be found, such as in the writing of African Americans, Latinos and Asians (Breland, et al (1995) and Bridgeman and McHale (1996)).In addition, ETS is accumulating essay corpora of nonnative speakers that can be used for research. This paper focuses on preliminary data that show e-rater's performance on Test of Written English (TWE) essay responses written by nonnative English speakers whose native language is Chinese, Arabic, or Spanish. A small sample of the data is from US-born English speakers and a second small sample is from non-US-born candidates who report that their native language is English.The data were originally collected for a study by Frase, et al (1997) in which analyses of the essays are also discussed. The current work is only the beginning of a program of research at ETS that will examine automated scoring for nonnative English speakers. Overall goals include determining how features used in automated scoring may also be used to (a) examine the difficulty of an essay question for speakers of particular language groups, and (b) automatically formulate diagnostics and instruction for nonnative English speakers, with customization for different language groups. The e-rater system TM ~ is an operational automated essay scoring system, developed at Educational Testing Service (ETS). The average agreement between human readers, and between independent human readers and e-rater is approximately 92%. There is much interest in the larger writing community in examining the system&apos;s performance on nonnative speaker essays. This paper focuses on results of a study that show e-rater&apos;s performance on Test of Written English (TWE) essay responses written by nonnative English speakers whose native language is Chinese, Arabic, or Spanish. In addition, one small sample of the data is from US-born English speakers, and another is from non-US-born candidates who report that their native language is English. As expected, significant differences were found among the scores of the English groups and the nonnative speakers. While there were also differences between e-rater and the human readers for the various language groups, the average agreement rate was as high as operational agreement. At least four of the five features that are included in e-rater&apos;s current operational models (including discourse, topical, and syntactic features) also appear in the TWE models. This suggests that the features generalize well over a wide range of linguistic variation, as e-rater was not 1 The e-rater system TM is a trademark of Educational Testing Service. In the paper, we will refer to the e-rater system TM as e-rater. confounded by non-standard English syntactic structures or stylistic discourse structures which one might expect to be a problem for a system designed to evaluate native speaker writing.
Standardizing Lexical  
m m m m mm m m n [] WordNet 2 -A Morphologically and Semantically Enhanced Resource  Th~s paper presents an ongoing project m-tended to enhance WordNet molpholog~-cally and semanttcally The mottvatmn for th~s work steams from the current hm~ta-t~ons of WordNet when used as a hngmst~c knowledge base We enwmon a software tool that automatically parses the conceptual defining glosses, attributing part-of-speech tags and phrasal brackets The nouns, verbs, adjectives and adverbs from every defimtmn are then d~samb~guated and hnked to the corresponding synsets Th~s increases the connectlv~ty between synsets allowing the ~etneval of topically ~elated concepts Furthermore, the tool t~ansforms the glosses, first into logical forms and then into semantm fo~ms Us-mg der~vatmnal morphology new hnks are added between the synsets 1 Motivation WordNet has already been ~ecogmzed as a valuable ~esource m the human language technol-og&gt; and know, ledge processing commumtms Its apphcabfl~ty has been c~ted m mo~e than 200 papers and s~stems have been m~plemented us-mg WordNet A Wo~dNet bkbhog~aph~ ~s mamtamed at the Umve~mt) of Penns:~l~ama (http //www c~s upenn edu/~oseph~ /wn-btblw html) In Europe, WordNet ~s being u~ed to develop a multflmgual database w~th basic semantic relatmns between words for several European languages (the EuroWordNet project) Capabihties WordNet was conceived as a machine-readable dmtlonary, followmg psychohn-gmstm principles Unhke standard alphabetmal dm-t~onaHes ~hmh o~gamze vocabula~ms using mo~pho-logmal mmllm ltms, WordNet structures lex~cal reformation m terms of word meanings WordNet maps word forms m ~ord senses usmg the s)ntact~c category as a parametel Although it covers onl~ fouI paits of speech nouns verbs, adjectives and adverbs , it encompasses a large majont) of Enghsh words (http //www cogscz pmnceton edu/~..wn) Wolds of the same syntactm catego~) that can be used to express the same meamng are grouped into a smgle synonym set, called synset Words wlth multiple meanings (polysemous) belong to multiple synsets An ~mportant part of the 99 643 synsets encoded m WordNet 1 6 contain word collocatmns, thus representing complex nominals (e g the synset {manufacturer, maker, manufacturing business} , (omplex vel-bals (eg the synset {leave office, quit, step down}, complex adjectlvals (e g the ~ynset {true, dead on target} or complex adverbmls (e g the synset {out of hand, beyond control} The iep-~esentatmn of collocatmns as synset entries p~ov~des for their semantm mterp~etatmn Wolds and concepts are furthei connected through a small set of lexmo-semantm relatmns The dominant semantm relatmn is the hypernymy, xvh~ch structures the noun concepts m 11 hmr-aichms and the verb concepts into 512)he, at-chins Thlee melonym Ielatlons are encoded between noun concepts the ha~_member, the ha~_~tu]f and the has_part ~elatlons Loglcal opelatlon~ be-tx~een events or entltms ale modeled through entazl-ment and cause_to ~elatmns between verb concepts or antonymy relatmns among nouns, veibs ad)ec-t~ves or adverb words The~e are only a few mo~-phologmally motivated connectmns between x~ords known as perta~mym relatmns Llmltatmns The mare ~eaknesses of \Vo~dNet c~ted m the hte~ature ale 1 The lack oi connections between noun and verb hmrarctnes
A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation  There is a general concern within the field of word sense dusamb~guatmn about the rater-annotator agreement between human annotators. In thus paper , we examine th~s msue by comparing the agreement rate on a large corpus of more than 30,000 sense-tagged instances Thin corpus us the mtersec-tmn of the WORDNET Semcor corpus and the DSO corpus, which has been independently tagged by two separate groups of human annotators The contribution of this paper us twofold First, ~t presents a greedy search algorithm that can automatically derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a h~gher agreement rate but we stfl!mamtam as many of the original sense classes as posmble Second, the coarse sense grouping derived by the algorithm, upon verification by human, can potentially serve as a better sense inventory for evaluating automated word sense d~samb~guatmn algorithms Moreover, we examined the derived coarse sense classes and found some interesting groupings of word senses that correspond to human mtmtlve judgment of sense granularity 1 Introduction. It us widely acknowledged that word sense d~sam-blguatmn (WSD) us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching, the problem of d~s-amblguatmg word sense, or dlscermng the meamng of a word m context, must be effectively dealt with Advances in WSD v, ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s)ntactm parsing, there are relatlvely well defined and agreed-upon cnterm of what it means to have the &quot;correct&quot; part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al, 1993) pro-~ide~ ,t large repo.~tory of texts annotated w~th part-of-speech and s}ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g~ven sentence Unfortunately, th~s us not the case for word sense assignment F~rstly, it is rarely the case that any two dictionaries will have the same set of sense defim-tmns for a g~ven word Different d~ctlonanes tend to carve up the &quot;semantic space&quot; m a different way, so to speak Secondly, the hst of senses for a word m a typical dmtmnar~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d~ctmnary hke WoRDNET (Miller, 1990) tend to be rather fine Hence, two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human an-notators The agreement rate between human an-notators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance, the SENSEVAL exerclse has performed a detaded study to find out the rater-annotator agreement among ~ts lexicographers tag-grog the word senses (Kllgamff, 1998c, Kllgarnff, 1998a, Kflgarrlff, 1998b) 2 A Case Study In this-paper, we examine the ~ssue of rater-annotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus (Miller et al, 1993) and the DSO corpus (Ng and Lee, 1996, Ng, 1997), which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~VoRDNET senses, and consists of more
Supervised Learning of Lexical Semantic Verb Classes Using Frequency Distributions Recent years have witnessed a shift in grammar development methodology, from crafting large grammars, to annotation of corpora Correspondingly, there has been a change from developing rule-based parsers to developing statmUcal methods for reducing grammatmal knowledge from annotated corpus data The shift has mostly occurred because buildmg w~de-coverage grammars is ume-consummg, error prone, and difficult The same can be said for crafting the rich lexlcal representatmns that are a central component of hngmstlc knowledge, and research m automaUc lexmal acquisition has sought to address this ( (Doff andJones, 1996, Dorr, 1997), among others) Yet there have been few attempts to learn fine-grained lexical classifications from the statlsUcal analysis of dlstnbutmnal data, analogously to the induction of syntacUc knowledge (though see, e g, (Brent, 1993, Klavans and Chodorow, 1992, Resmk, 1992) In this paper, we propose such a~ approach for the automaUc classfficauon of ~erbs into lexlcal semantic classes lWe can express the Issues raised by this apploach as follows 1 Whmh hngulstlc dlstmcUons among [exlcsl classes can we expect to find m a corpus ~ 2 How easily can we extract the frequency distributions that approximate the relevant hngmstlc properttes?3 Which frequency dlstnbuUons work best to distinguish the verb classes ~In exploring these quesUons, we focus on verb classlficaUon for several reasons Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role m the orgamzatmn and use of this knowledge Knowledge about verb classes is crucml for lex,cal acqmsltton m support of language generation and machine translatmn (Dolt, 1997) and document cl~sfficatmn (Klavans and Kan, 1998), yet manual classfficauon of large numbers of verbs is a difficult and resource intensive task (Levm, 1993Miller et al , 1990, Dang et al, 1998 To address these issues, we suggest that one can tram an automatic classffier for verbs on the basts of staUstmal approxlmaUons to verb dlatheses We use dlatheses--alternatmns m the expression of the arguments of the verb--following Levm and Dorr, for two reasons Fnst, verb dlatheses are syntacuc cues 1 We are aware that a dlstnbutmnal approach rests on one strong assumptmn regarding the nature of the representatmns under study semantic notmns and syntacuc notmns are correlated, at least m part This assurapuon is under debate (Bnscoe and Copestake, 1995, Levm, 1993, Dorr and Jones, 1996, Dorr, 1997), but we adopt ~t here without further dlscussmn to semantic classes, hence they can be more easily captured by corpus-based techniques Second, using verb d~atheses reduces no,se There ~s a certain consensus ( Bnscoe and Copestake, 1995, Pustejovsky, 1995, Palmer, 1999) that verb dmtheses are regular sense extensmns Hence focussing on thin type of classfficatmn allows one to abstract from the problem of word sense dmamb,guatmn and treat remdual d~fferences m word senses as no~se m the classfficatmn task We present an m-depth case study, m which we apply machine learning techmques to automaUcally classify a set of verbs based on d~stnbutmns of grammaucal indicators of dmtheses, extracted from a very large corpus We look at three very mterestmg classes of verbs unergaUves, unaccusauves, and obJect-drop verbs (Levm, 1993) These are Interestmg classes because they all parUcapate m the trans~-uvlty alternatmn, and they are minimal parrs -that as, a small number of well-defined dmtmctmns d~ffer-entmte their trans,tlve/mtranmUve behavmr Thus, we expect the differences m their dmtnbuttons to be small, entailing a fine-grained dlscr,mmaUon task that prowdes a challenging testbed for automatic classfficatmnThe specffic theoretical questmn we mvesUgate ~s whether the factors underlying the verb class dmtmctmns are reflected m the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs m the corpus In doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh the behavtor of the classes of verbs w~th respect to the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the staUstmal dmtnbutmns are available, can be used successfully by an automatic classifier In m~ttal work (Stevenson and Merlo, 1999), ~e found that hngmstlcally motivated features that d~s-tmgmsh the verb classes can be extracted from an annotated, and m one case parsed, corpus These features are sufficient to almost halve the error rate compared to chance (45% reductmn) m automaUc verb classtficaUon, suggesting that d~stnbu-Uonal data prowdes knowledge useful to the class~-ficaUon of verbs The focus of our original stud~ was tho demonstration m prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm features, and an analysm of the relaUve contrtbutmn of the various features to learmng Th~s paper turns to the nnportant next steps of rephcatmg our findrags using other training methods and learning algorithms, and analyzing the performance on each of tbe three classes of verbs This more detailed analys~s of accuracy within each class m turn leads to the development of a new dlstrtbutmnal feature mtended to improve dlscnmmabthty among t~o of the classes The addltmn of the ne~ feature successfully reduces the error rate of out mltml results m classlficatmn by 19%, for a 56% overall reductmn m error rate compared to chance 2Determining the FeaturesIn this sectmn, we present mouvatmn for the mttml features that we mvesUgated m terms of their role m learmng the verb classes We first present the hngmstlcally den~ed features then turn to e~tdence from experimental psychohngutstlcs to e\tend the set of potentially relevant features ~ hlch the object is sm~pl~ opttonal Both unergauves and unaccusatl~es [la~e a causattve trans~u~e form, but differ m the semanuc roles that they assign to the paructpants m the e~ent described In an mtranstUve unetgaUve, the ',ubject ts an 4.gent Ithe doer of the e~ent), and m an Intransitive unaccusaUve, the subject ts a Theme (~ome-thing affected by the e~ent) The role assignments to the corresponding semanuc arguments of the ttans~u~e forms--I e, the dnect objects--a~e the ~ame with the addition of a Causal Agent (the causer of the event) as subject in both cases Object-drop verbs simply assign Agent to the subject and Theme to the optional object We expect the differing semantic role assignments of the verb classes to be reflected m their syntactic behavior, and consequently in the distributional data we collect from a corpus The three classes can be characterized by their occurrence in two alternations the transittve/mtrans~tive alternation and the causative alternation Unergatives are distinguished from the other classes m being rare in the transitive form (see (Stevenson and Merlo, 1997) for an explanation of this fact) Both unergatives and unaccusatives are dlstmgmshed from obJect-drop m being causative in their transitive form, and sundarly we expect this to be reflected in amount of detectable causative use Furthermore, since the caus&amp;tlve is a transitive use, and the transitive use of unergatlves is expected to be rare, causativity should primarily distinguish unaccusatlves from objectdrops In conclusion, we expect the defining features of the verb classes--the intransitive/transitive and causative ~lternatlons--to lead to distributional differences m the observed usages of the verbs in these alternations Vve zeport a number of computatmnal experiments m supervised learning whose goal Is to automatmally classify a set of verbs into lexmal semanUc classes, based on frequency dlstnbutmn approxlmatmns of grammatical features extracted from a very large annotated corpus DlstnbuUons of five syntactic features that approximate tranmUvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance We conclude that corpus data is a usable repository of verb class mformatmn, and that corpus-driven extraction of grammaUcal features Is a promising methodology for automatm lexmal acqum,Uon
On the concept of diathesis alternations as semantic oppositions Ana Fem~indez Ana Fernandez@uab es M a Ant6ma Martf amartl@hngua fil ub es Gl6na Vftzquez gvazquez@dal udl es Irene Castell6n castel@hngua fil ub es (U Aut6noma de Barcelona) (U de Barcelona) (U de Llelda)  In this article we present our conception of dtathesls alternations and how they intervene m the definmon of a model of lexlcal entnes We consider that dmthesls alternations are the syntactic realizations of opposmons of a more general semantic nature We will see how they interact with other components such as event structure and how different semannc classes of pre&amp;cates at~se from that interaction
Towards a Meaning-Full Comparison of Lexieal Resources The difficulty of companng lemcal resources, long a s~gnfficant challenge in computauonal hnguistlcs (Atlans, 1991), came to the fore in the recent Senseval competatton (IOlgarnff, 1998), when some systems that relied heavily on the WordNet (Miller, et al, 1990) sense inventory were faced with the necessity of using another sense inventory (Hecto0 A hasty solutaon to the problem was the " development of a map between the two inventories, but some part~cipants expressed concerns that use of flus map may have degraded their performance to an unknown degree Although there were disclaimers about the WordNet-Hector map, it nonetheless stands as a usable gold standard for efforts to compare lexical resources Moreover, we have a usable baseline (a word overlap method suggested m (Lesk, 1986)) against which to compare whether we are able to make improvements m the mapping (since flus method has been shown to perform not as well as expected (Krovetz, 1992))We first describe the lextcal resources used m the study (Hector, WordNet, other dicUonanes, and a lex~cal knowledge base), first characterizing them in terms ofpolysemy and the types of leracal mformaUon each contmns (syntacUc properties and features, semantac components and relaUons, and collocaUonal properties) We then present results of perfornung the word overlap analysis of the 18 verbs used m Senseval, analyzing the definitions m WordNet and Hector We then expand our analysis to include other dictionaries We describe our methods of analysis, particularly the methods of parsing defimtaons and identff)qng semantic relations (semrels) based on defimng patterns, essentially takang first steps m Implementing the program described by Atkms and focusmg on the use of"meamng" full mformataon rather than statistical mformaUon We identify the results that have been achieved thus far and outline further steps that may add more "meanmg" to the analysis IAll analyses described m this paper were performed automatically using functlonahty incorporated m DIMAP (Dictionary Maintenance Programs) (available for immediate download at (CL Research, 1999a)) This includes automatac extracuon of WordNet reformation for the selected words (mtegrated m DIMAP) Hector defimtlons were uploaded into DIMAP dicUonanes after use of a conversmn program Defimtlons for other The mapping from WordNet to Hector senses m Senseval provides a &quot;gold standard&quot; against wluch to judge our ability to compare lexlcal resources The &quot;gold standard&quot; is provided through a word overlap analysis (with and without a stop list) for flus mapping, achieving at most a 36 percent correct mapping (inflated by 9 percent from &quot;empty&quot; assignments) An alternaUve componenttal analysis of the defimtaons, using syntacUc, collocatmnal, and semantac component and relation identification (through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary), provides an almost 41 percent correct mapping, with an additaonal 4 percent by recogmzmg semantic components not used in the Senseval mapping Defimtion sets of the Senseval words from three pubhshed thclaonanes and Dorr&apos;s lextcal knowledge base were added to WordNet and the Hector database to exanune the nature of the mapping process between defimtton sets of more and less sco[~e The tecbauques described here consUtute only an maaal implementation of the componenUal analysis approach and suggests that considerable further improvements can be aclueved
On Some Aspects of Le rtcal Standardtzauon On Some Aspects of Lexical Standardization We envisage the standardlzauon of lexlcal resources as a three dlmenslonal process In developing, processing and using large multl-hngual and multi-purpose lexicons, a first set of dlfficult~es lies m the lack of a standard format that is flexible enough to cover many different languages and apphcaUons, but sufficiently ngld to enable the use ot a single lex~cal toolset shared across all these languages and apphcatlons A standard formahsm for encoding lex~cal knowledge enables the construction of a generic lexical toolset SGML has been used for example for pnnted dlct~onartes For computational dictionaries, a good alternative are feature structures (V6roms &amp; Ide 92, Ide &amp; V6roms 95) The second set of problems ts almost as acute as the first tt ~s very difficult to design a sound lex~cal architecture, hst the all the features that must be present for a variety of NLP apphcauons, predict the interaction between the various substructures, and predict the needs of the various NLP tools that would be accessing the dlcuonary A standard lextcal entry structure which defines the various features and provtde guldehnes to fill these features ~s a must for dictionary budders Thts level has been addressed for example m the Eagles program (Eagles 93) where it ts somettmes mtxed wtth the third dimension Finally, the problem of hngmstlc standards per se is addressed only partmlly by the definmons of gmdehnes and the use ot a standard lexlcal entry structme In a multdmgual setting, it is probably possible to dehne multdmgual types, such as a standard hst of part-of-speech However, this direction is stall very much a tesearc.h area related to the quest for a umversal grammar (see e g Cahdl &amp; Gazdar 95, 96) Current standard~zatlon ettolts such as Eagles define standards for content for particular languages onlyIn Section 2, we present a generic lex~cal architecture that addresses point one the generic structure ot lex~eal ent~es and d~cuonar~es, notions of lex~cal schema and meta-schema, and the generic lex~cal toolset Secuon 3 presents the standard structure of lex~cal entries that ~s used m structuring a number of computational d~ct~onar~es at CRL The standard structure Is layered so that a particular dictionary could implement a sub-set of the layers only, whde still implementing the standard Furthermore, the structure ~s flexible enough so that a given layer can be extended {by adding new elements through an inheritance mechamsm) for a partlcular language, but forbids the redefinmon of the lexlcal meta-structure Section 4 mention open problems and on-going research on the topic of a umversal lex~s and a parameter-based approach to the acqmslt~on of a lex~cal profile In developing and using many large mult~-hngual multt-purpose lexicons at CRL, we ~denttfied three dlstmct problem areas (1) an appropriate metalanguage (formahsm) tot representing and processing lex~cal knowledge (2) a standard generic lex~cal framework defimng a common lex~cal entry structure (names ot features and types ot content), and (3) shared umversal hngu~st~c types In th~s paper, we present the solutions developed at CRL addressing d~mens~ons 1 and 2, and we mention the ongoing research addressing dlmens~on 3 1 Introduction We envisage the standardlzauon of lexlcal resources as a three dlmenslonal process In developing, processing and using large multl-hngual and multipurpose lexicons, a first set of dlfficult~es lies m the lack of a standard format that is flexible enough to cover many different languages and apphcaUons, but sufficiently ngld to enable the use ot a single lex~cal toolset shared across all these languages and apphcatlons A standard formahsm for encoding lex~cal knowledge enables the construction of a generic lexical toolset SGML has been used for example for pnnted dlct~onartes For computational dictionaries, a good alternative are feature structures (V6roms &amp; Ide 92, Ide &amp; V6roms 95) The second set of problems ts almost as acute as the first tt ~s very difficult to design a sound lex~cal architecture, hst the all the features that must be present for a variety of NLP apphcauons, predict the interaction between the various sub-structures, and predict the needs of the various NLP tools that would be accessing the dlcuonary A standard lextcal entry structure which defines the various features and provtde guldehnes to fill these features ~s a must for dictionary budders Thts level has been addressed for example m the Eagles program (Eagles 93) where it ts somettmes mtxed wtth the third dimension Finally, the problem of hngmstlc standards per se is addressed only partmlly by the definmons of gmdehnes and the use ot a standard lexlcal entry structme In a multdmgual setting, it is probably possible to dehne multdmgual types, such as a standard hst of part-of-speech However, this direction is stall very much a tesearc.h area related to the quest for a umversal grammar (see e g Cahdl &amp; Gazdar 95, 96) Current standard~zatlon ettolts such as Eagles define standards for content for particular languages only In Section 2, we present a generic lex~cal architecture that addresses point one the generic structure ot lex~eal ent~es and d~cuonar~es, notions of lex~cal schema and meta-schema, and the generic lex~cal toolset Secuon 3 presents the standard structure of lex~cal entries that ~s used m structuring a number of computational d~ct~onar~es at CRL The standard structure Is layered so that a particular dictionary could implement a subset of the layers only, whde still implementing the standard Furthermore, the structure ~s flexible enough so that a given layer can be extended {by adding new elements through an inheritance mechamsm) for a partlcular language, but forbids the redefinmon of the lexlcal meta-structure Section 4 mention open problems and ongoing research on the topic of a umversal lex~s and a parameter-based approach to the acqmslt~on of a lex~cal profile 2 A Generic Lexical Architecture To support the development of lmge lexicon, we ~mplemented a Lex~cal Knowledge Base (LKB) called Habanera (Zajac 97) A Habanera LKB ~s composed of (1) several monohngual d~ct~onanes, (2) translation relations hnkmg these monohngual d~ct~onanes, and (3) a multdmgual d~ct~onary schema that defines a shared multdmgual inheritance h~erarchy of lex~cal types for all monohngual d~ct~onanes The system supports a variety of hngmstlc architectures Since the design of a lextcal architecture is a complex task, flex~bd~ty m des~gmng the structure of the LKB ~s an essentml feature Th~s flex~bdtty ~s provided by allowing ~o~ a multi-layered LKB schema m which each layer provides addmonal constrmnts on the structure of a lex~cal entry Thts approach ~s congruent w~th the d~stmct~on made m (Eagles 93) between meta-schemata, schemata and instances Thts 38
SIMPLE -Semantic Information for Multifunctional Plurilingual Lexica: Some Examples of Danish:Concrete Nouns The SIMPLE model is primarily based on three lexlcal flameworks ( Lencl et al, 1998) The Geneiatlve Lexicon (cf Pustejovsky, 1995), WoldNet (cf Miller and Fellbaum, 1991), and EuroWordNet (ct Vossen et al, 1998) The basic underlying assumption m the model is that word senses diffei in tel ms of their internal complexity Hence the SIMPLE model consists of three different semantic types (t) simple types, which can be characterized In terms of z The LE-PAROLE lexlcons contain 20,000 entries with corresponding morphological and syntactic mlormation tot each ot the 12 languages that parttclpated m this project, whlch was also tunded by the European Commtssmn (ct Rmmy et al, 1998) monodimensional relations, (n) unified types, which involve multidimensional information, and (ill) complex types, which identify regular polysemous classesOne of the basic tasks during the SIMPLE lexicon encoding phase is the assignment of semantic typing to the word senses to be encoded (called the semantic units or SemU's) A set of schematic structures called 'templates' constituting the SIMPLE Ontology (consisting of approx 140 semantic types in all) guides thls encoding process A template ~s a cohort of various different Information types which is primarily used by the lexicon encoder to express the semantic type of a word sense, but also to express its domain, defmlnon, predicative representation, argument structure, polysemous classes, etcThe multiple dimensions of meaning are represented In SIMPLE by the use of the Quaha Structure from the Generative Lexicon (Pustejovsky 1995) to represent lexlcal meaning expressed by means oforthogonat inheritance The Quaha Structure involves tour different roles 0) the formal role, which ptovldes reformation that distinguishes an entity wtthm a lalger set, 01) the agennve role, which concelns the orlgm of an entity, (m) the tehc role, which concerns the typical function of an entity, and 0v) the constitutive role, which expresses a variety of relations concerning the internal constitution of an entity As an illustration, consider m Figure 1 the meaning components involved m the noun pudding  (Lento et al 1998 pp 17) constitutive ~ agenttveThe central meaning aspects are m~rrored in the llngmstlc contexts surrounding the word, so for pudding we could have John refused the pudding refemng to the eat event, that's an easy pudding referring to the make event, there zs pudding on tile floor referring to the substance dimension, and that was a nice bread puddmg referrmg to the ingredients of whlch it Is madeAs an example of the semantic types explessed m the SIMPLE Ontology and of how the different dlmensmns of meaning are involved for each semantic type, consider F~gure 2 which shows a subset of the SIMPLE ontology referring to human beings Some examples of word senses encoded as simple types are russer (a Russian) under the template type 'people', 'jcde (Jew). under 'ldeo', kusme (female cousin) under l~mshlp, and yen (friend) under 'socml status' These senses may naturally involve addmonal dlmensmns ot meaning, however they are not cons,dered tytzedefining m the SIMPLE model In contrast, word senses encoded under the emphas~sed boxes above, such as borddame (female droner partner) under 'agent of temporary activity', alkohohker (an alcoholic) under 'agent of persistent actlwty', and lcege (doctor) under 'professmn', are untried types These are identified by more than one coordinate in the type h~etarchy, since they involve more than one type-defining dlmenslon of meaning Thus 'agent of temporary activity' also revolves an agennve role For borddame th~s is defined by the act of sitting next to someone at a droner 'Agent of persmtent activity' and 'profession' revolve a tehc role, which for alkohohker is the act of drinking, and for lcege ~s the act of curmg can to some extent be ~dent~fied by the argument structure of the derived verb and the internal ranking of ~ts arguments (cf Orsnes, 1995) In contrast, non-deverbal nominal compounds generally display a more arbitrary internal structure m Damsh (cf Paggm &amp; Orsnes 1993), and hence they reqmre a h~gher degree .of expresswe power m the lex~cal entries The Quaha Structure as It ~s expressed m the SIMPLE model provides a good basls for a lexlcahsed encoding of Damsh non-deverbal nominal compounds, as for example nominal compounds denoting containersThe template type 'Container' belongs to the set of templates conmtutmg the SIMPLE Core Ontology It is a unified type that has the umficat~on path 'Concrete entity + Artffact/Agenttve + Tehc' Thts indicates that the template denotes a kind of concrete ennty, and that ~t has been augmented w~th two kinds of addltmnal type-defining mformatmn (1) agentlve mformatmn (namely that these concrete entrees are man-made artifacts), and (n) tehc mformatmn (namely that these concrete entrees are used for a spemfic purpose to contain things)All Damsh contamers encoded under th~s template type have been encoded w~th Damsh mformatmn about the formal role vm an tsahlermchy As a default, beholder (container) is chosen as hypernym for the Damsh containers m the re_a-hierarchy Since containers are (manmade) artifacts, the process of their creanon ~s spectfied wa the agentlve role For Damsh containers thls is the process fremsttlle (to create) As Is apparent from the umficatmn path for th~s template, containers are also encoded w~th the type-defining tehc mformatmn that their funcnon ts to contam things For Damsh containers th~s ~s specified w~th the verb mdeholde (to contam) m the encoding of the tehc role the tehc role for such compounds as mdlebteger (measuring cup), raflebceger (ht cup for casting dice = dice cup), or drtkkebmger (dnnkmg cup) In the SIMPLE model, the encoding of the meaning of bmger can also be further specified  Id="USEM_N_vmflaske CON 1" namang="wnflaske" /wine bottle/ example=-"en vmflaske kan genbruges syv tfl otte gange" /a 1tree bottle can be reused te~en-etght ttraer/ freedefinmon="flaske til vm" /a bottle Jot ~me/ wetghtvalsemfeaturel=" WVSFTemplateContamerPROT WVSFUmficatmnPathConereteenttty-Ar tt faetAgenttve-TehePROT"&gt; &lt;RWetghtValSemU target="USEM N_flaske_CON_ 1"/bottle/ semr="SRIsa"&gt; &lt;RWelghtValSemU target='USEM V fremsttlle l"/to p:oduce/ semr="S RCreatedby"&gt; &lt;RWetghtValSemU target="USEM_V mdeholde_l" /to cm~mt~d ~emr="SRUsedfor"&gt; '~ ~, Weigh t -vai,~emi.J target="USEM_N_vm A R D_ 1" /~tne/ semr=-"SRContams" SIMPLE is a large-scale Emopean lexicon project funded by the European Commlssmn with the partlctpat~on ot 12 European countries The mm of the project is to add harmomzed semantm mtormatlon to the LE-PAROLE lexicons 1, which contain motphological and syntactic information In this paper we present some examples of concrete nouns trom the Danish SIMPLE lexicon which illustrate two central aspects of the SIMPLE model 1) the expressive power of the Quaha Structure exemphhed with a phenomenon relevant to a Scandinavian language like Damsh namely the iepresentatlon of the mternal structure of Danish non-devet bal nominal compounds, and 2) the leptesentatmn ol legular polysemy in the Damsh SIMPLE lexmon 1 Introduction The SIMPLE model is primarily based on three lexlcal flameworks (Lencl et al, 1998) The Geneiatlve Lexicon (cf Pustejovsky, 1995), WoldNet (cf Miller and Fellbaum, 1991), and EuroWordNet (ct Vossen et al, 1998) The basic underlying assumption m the model is that word senses diffei in tel ms of their internal complexity Hence the SIMPLE model consists of three different semantic types (t) simple types, which can be characterized In terms of z The LE-PAROLE lexlcons contain 20,000 entries with corresponding morphological and syntactic mlormation tot each ot the 12 languages that parttclpated m this project, whlch was also tunded by the European Commtssmn (ct Rmmy et al, 1998) monodimensional relations, (n) unified types, which involve multidimensional information, and (ill) complex types, which identify regular polysemous classes One of the basic tasks during the SIMPLE lexicon encoding phase is the assignment of semantic typing to the word senses to be encoded (called the semantic units or SemU&apos;s) A set of schematic structures called &apos;templates&apos; constituting the SIMPLE Ontology (consisting of approx 140 semantic types in all) guides thls encoding process A template ~s a cohort of various different Information types which is primarily used by the lexicon encoder to express the semantic type of a word sense, but also to express its domain, defmlnon, predicative representation, argument structure, polysemous classes, etc The multiple dimensions of meaning are represented In SIMPLE by the use of the Quaha Structure from the Generative Lexicon (Pustejovsky 1995) to represent lexlcal meaning expressed by means oforthogonat inheritance The Quaha Structure involves tour different roles 0) the formal role, which ptovldes reformation that distinguishes an entity wtthm a lalger set, 01) the agennve role, which concelns the orlgm of an entity, (m) the tehc role, which concerns the typical function of an entity, and 0v) the constitutive role, which expresses a variety of relations concerning the internal constitution of an entity As an illustration, consider m Figure 1 the meaning components involved m the noun pudding 46
Parallel Translations as Sense Discriminators It ~s well known that the most nagging issue for word sense disamblguanon (WSD) Is the definmon of just what a word sense is At its base, the problem Is a philosophical and linguistic one that is far from being resolved However, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural language processing tasks such as summarization, document retrieval, and machine translataon Several criteria have been suggested and exploited to automatically determine the sense of a word m context (see Ide and V6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, Hearst, 1991, Yarowsky, 1993, words co-occurring m global context (e g, Gale et al, 1993, Yarowsky, 1992Schutze, 1992, 1993, etc No clear criteria have emerged, however, and the problem continues to loom large for WSD workThe notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as a basis for some recent work on WSD Foi example, Brown et al (1991)and Gale et al (1992aGale et al ( , 1993 used the parallel, aligned Hansard Corpus of Canadian Parhamentary debates foi WSD, and Dagan et al (1991) and Dagan and Ital (1994) used monohngual corpora of Hebrew and German and a bilingual dictionary These studies rely on the assumption that the mapping between words and word senses vanes significantly among languages For example, the word duty in English t~anslates into French as devoir m ~ts obhgatlon sense, and tmpOt m ~ts tax sense By determining the translation ,..--.,.~ .-eqmvalent ot duty in a parallel French text, the correct sense of the Enghsh word is identified These studies exploit th~s lnformatmn m order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate new texts In related work, Dywk (1998)  Recently, Resnlk and Yarowsky (1997) suggested that fol the purposes ot WSD, the different senses of a wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally In particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages This idea would seemto p~ovtde an answer, at least m part, to the problem of determining different senses of a word mtumvely, one assumes that ff another language lexlcahzes a word m two or more ways, there must be a conceptual monvatmn If we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of a word However, th~s suggestmn raises several questions Fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the French tntdrYt and the Enghsh interest), especmlly languages that are relatively closely related Assuming this problem can be overcome, should differences found m closely related languages be given lesser (or greater) weight than those found m more distantly related languages 9 More generally, which languages should be considered for this exermse 9 All languages 9 Closely related languages9 Languages from different language famlhes ' ~ A mixture of the two 9 How many languages, and of which types, would be "enough" to provide adequate lnfotmanon tot this purpose~There ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is "lexlcahzed cross-hngu~stmally"How consistent must the d~stlnCtlOn be 9 Does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9Another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from Using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make Resmk and Yalowsky (1997)  Thls paper attempts to provide some prehmlnary answers to the questions outhned above, In order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used Given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exammanon of a small sample of parallel data can, as a first step, provide the basis and dlrectmn for more extensive studies 1 This article reports the results of a prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of George Orwell&apos;s Nmeteen Eighty-Four The goal of the study is to determine the degree to which translatmn equivalents for different meamngs of a polysemous word In Enghsh are lexlcahzed differently across a variety of languages, and to detelmme whether this information can be used to structure or create a set of sense distinctions useful in natural language processing apphcatmns A coherence Index is computed that measures the tendency for different senses o1 the same English word to be lexlcahzed differently, and flora this data a clustering algorithm is used to create sense hierat chles
An Overt Semantics with a Machine-guided Approach for Robust LKBs  In this paper, we report on our experience in build-mg computational semantic lexicons for use in NLP applications In a machine-graded approach, the computer reduces part of the semantic knowledge to be acquired by an acqulrer An overt semantics can help predict the syntactic behavior of words By overt semantics we mean applying the hnkmg or lexl-cal rules at the semantic level and not on lexlcal base forms More specffically~ we address the different strategies of acqms~tlon arguing for an application-driven, training-intensive effort We also report on how to develop lexicons using off the shelf resources, and address multlhngual issues We will try to provide an assessment of the difficulties we encountered and some directions to bypass them 1 Introduction Our experience in building computational semantic lexlcons which are used by Natural Language Pro-cessmg (NLP) systems comes from Mlkrokosmos, a knowledge-based machine translation system, 1 where texts from Spanish and Chinese are translated into Enghsh Mlkr0k~n~os adopts an xnterhngua-based approach (Nlrenfurg et al, 1992) and all lexicons can be used for multdmgual analysis and gener-atmn each word is mapped to an mterhngua structure The lexicons built for Mlkrokosmos are multipurpose multlhngual to support translation or multflmgual generatmn tasks, reusable, that is, ap-phcable to several NLP tasks~ (e g, generation, analysis , information extraction), and maintainable, that Is, supporting semi-automatic acquisition and restructuring of the lexicons The content of the Lexlcal Knowledge Base (LKB) ~s essentially the same ~rrespective of a particular application The types of information important for analysis and generation might differ, as suggested by Dale and Melhsh (1998) For instance, recording all the senses of a lexeme is more important for analysis than generation, conversely, knowing styhstlc m-formation on words such as hzghfalutm or formal m 1For a descnptmn of Mlkrokosmos, see http//crl nmsu edu/Research/Projects/nukro/mdex html important for generation (Hovy, 1988) The content of a multl-purpose LKB is apphcatlon Independent (modulo its indexing m analysis the LKB Is Indexed on lexemes whereas for generation the LKB is indexed on concepts) We argue, m section 2 that the acquisition process ~s apphcatmn-dependent Mole-over we argue that defining the meamng of a word for NLP systems requires a training-intensive effolt In other words, the fact that we, as humans, understand texts does not entail that we can determine the &quot;computational&quot; meaning of a word Chomsklan trees are linguists&apos; constructs, not Innate structures A hngmst must be trained to be able to build syntactic patterns (e g, trees) In computational semantics , the same rule applies one must be trained to build the corresponding semantics (e g frames, predicates,) for a word In order to approach the &quot;computational&quot; meaning of a word, training Is the most important means we have to date to ensure consistency among acquuers Other means are to adopt an overt semantics with a machine-guided approach which directs as much as possible the ac-qulrer (Section 3) This machine guided approach could also act behind the &apos; back&quot; of an acquner &quot;cor-recting ~&apos; some mc0nslstencms m lexmal descnptlons between acqmrers, as will be shown m Sectmn 6 In Section 4, we &amp;scuss our use of off the shelf le-sources, such as WordNet (Miller, 1990), to accelerate the machine-graded acqmsltlon of the Enghsh lexicon by taking advantage of the existing database of synsets 2 whmh provide synonym lists for a lexeme We also show how a semantic-based approach, can help predict the syntactic behavior of words Note that the reverse (predicting semantms from syntax) is not true, as some experiments on Levm&apos;s work (1993) have shown (Sectmn 4) In Sectmn 5, we address mulUlmgual issues in lexicon development 2 Application-driven Acquisition The semantics of an entry is an underspecffied Text Meaning Representation (TMR) fragment (e g, De-2Synsets represent WordNet&apos;s building blocks whmh are words, synonyms or Rear-synonyms, that can be used to refer to a given concept (Miller, 1990) 62
Statistical Matching of Two Ontologies  oglno@edr co 3P 1 Introductmn Standardizing ontologms ~s a challenging task Ontologms have been created based on different backgrounds, different purposes and different people However, standardizing them is useful not only for applications, such as Machine Translation and Information Retrmval, but also to Improve the. ontologms themselves During the process of standardization , people can find bugs or gaps in on-tologms So standardlzatmn bnngs benefits compared to just using them separately There is a committee for standardlzmg ontologaes at ANSI, the &quot;ANSI Ad-Hoc Group for Ontology Stan-dards&quot; (Hovy 1996) Although there have been a few attempts to merge and compare ontologaes, th,s work ~s still at a prehmmary stage of research (Ogmo et al 1997) attempts manual mergang of EDR (EDR 1996) (Mlyoshl et al 1996) and WordNet (Word-net) (Miller 1995), (Utlyama and Hashlda 1997) used statistical methods to merge EDR and Word-Net (Pangloss) is also working on standardizing ontologms It is certain that manual methods have great difficulty in matching the entire ontologles It would require three thousand years for a person to check all possible node pairings, if the two ontologms have 40 000 nodes each.and eachjudge-ment takes a minute So automatic methods are needed to find matches automatically or at least to narrow down the candidates for matching In this paper, we investigate a simple statistical method for matching two ontologms The method can appl~ to any ontologms which are formulated from ls-a relationships In our experiments, we used EDR and \VotdNet Tins ~ork is sumlar to the work in (UtL~ama and Hashlda 1997) They defined the task as the MWM (Maximum V~elgnt klatch) of bipartite graphs, an approach which is bas~cally common to most ontology matching schemes The information they used is partially fuzzy, i e for calculating the distance between two nodes, they used the information from each node and its neighborhood, not distinguishing between mformatmn from parent and child nodes However , since the structure of the ontologms (the relation between parent and children) is slgmficant, it might be better to utilize such structural reformation In our experiments, we will focus on this issue, rather than trying to achieve a higher performance The importance of parent, child and grandchild information will be examined We will conduct several experiments with or without some of the mformatlon It is also important to dlsco~er what welghtmg balance gives good matches 2 Ontologies First we will briefly explain the ontologms we used m our experiments 2.1 EDR The EDR Concept Dmtlonary contains 400,000 concepts hsted m the Japanese and Enghsh Word Dmtlonanes of 200,000 words each The EDR Concept Dictionary is one of the five types of EDR dictionaries, the others are the Word Dmtlonarms for English and Japanese the Blhngual Dictionary , the Coocurrence Dictionary, and the Tech-mcal Telmmology Dxctlonar} The EDR Concept Dictionary consists of three sub-dmuonanes the Headconcept Dlctxonaz} contains concept explanations m natural language (both m Engh~h and Japanese),~the Concept Classification Dmuo-nar} contains a set of ls-a relationships, and the Concept Description Dictionary contains pairs of concepts that have certain semantic relationships other than ls-a relationship 1 e object, agent 9oal, zmplement a-object (object of a particular attribute), place, scene and cause The Concept Classification Dmtlonar~ classifies all the 400 000 concepts based on their meaning A polysemous ~ord is put into several word cias-sffieatmns (concepts) As multiple inheritance l~ allowed, the entire structure is not a tree but a DAG (directed acychc graph) There are 6,000 intermediate nodes and the maximum depth is 16 2 2 WordNet WordNet (Wordnet) is an English ontology The nodes are represented by a set of synonym words (called &apos; s? nsets &apos;) WordNet contains 60,557 noun 69
A Computational Lexicon of Portuguese for Automatic Text Parsing  
Towards a Universal Index of Meaning EuroWordNet (LE2-4003, LE4-8328) develops a multflmgual database with wordnets for 8 different European languages Enghsh, Dutch, Spamsh, Italran, German, French, Czech and Estoman Further collaboratmns have been estabhshed with wordnet builders for Portuguese, Swedish, Basque, Catalan, Russmn, Greek and Damsh, who wolk according to the EuroWo~dNet specfficatmns Each of the wordnets ~s structured as the Prmceton Wordnet (Fell- baum, 1998) m terms of sets of synonymous words or so-called synsets between which basic semantic relatmns me expressed The synsets are based on the lexmahzatmns and expressions m each language Each wordnet therefore can be seen as a umque language-specffic stIuctureIn additmn to the lelatlons bet~een s:rnsets there Is also a relatmn to a so-called Inter-Lingual-Index This Inter-Lingual-Index (ILI) is an unstructmed fund of concepts, so-called ILI-records, w~th the sole purpose of hnkmg synsets across languages Synsets that are hnked to the same ILI-record can be said to be eqmvalent across two languages By means of the ILI it ts thus possible to go from one wordnet to the other and to compare the lextcahzatmn patterns across languagesThe characterxstlcs of the ILI are defined b~ ~ts functmn to provide an efficient mapping across the meanings m the wordnets for the different languages Two major reqmrements follow from this• the ILl should have a certain level of granularity,• the ILI should be the superset of concepts that occur across languagesThe first reqmrement is necessar) to make the hnkmg of meamngs easmr If many speclahzed meanmgs and Interpretations are gwen it is more dtfficult to find mappings from a language-specffic wordnet to the index The second reqmrement is necessary to be able to express an equivalence relatmn across synsets m two wordnets for which there ts no eqmvalent m other wordnets ImtmUy, the ILI has been based on WordNetl 5 It is however a well-known problem that sensedlfferentmtmn ts ver) inconsistent w~thm and across resources including WordNetl 5 On the bas~s of the above criteria and by companng the sensedlfferentiat~on across the ~ordnets we haze therefore begun to adapt the ILI Four major rex ls~ons of the ILI are derived from these• grouping sense-dlfferentlauons between which there xs a s~stematm pol~sem~ telatmn e g meton~ m~,• grouping sense-d~fferentmttons that can be represented by more general sense-group• adding sense-d~fferent~atmns ol concept~ that occur m two wordnets but not m %otd.Netl 5 * dlfferentmtmg the status of the ILl-lecold, m terms of umversaht.~, productivity, and exhaus- The Inter-Lingual-Index (ILI) m the EuroWordNet architecture is an mltmlly unstructured fund of concepts whmh functions as the hnk between the van-ous language wordnets The ILI concepts originate from WordNetl 5, and have been restructured on the basls of aspects of the internal structure of Word-Net, hnks between WordNet and other resources, and multflmgual mapping between the wordnets This leads to a dtfferentmtlon of the status of ILI concepts, a reductmn of the Wordnet polysemy, and a greater connectivity between the wordnets The restructured ILI represents the first step towards a standardized set of word meanings, ts a worhng platform for further development and testing, and can be put to use m NLP tasks such as (multdmgual) mformatmn remeval
Electronic Dictionaries and Linguistic Analysis of Italian Large Corpora  In thts paper we wdl show how Itahan electronic dictionaries have been budt within the methodological framework of Lexicon-grammar We wdl see the structure of electromc d~ctlonanes of simple and compound words, and we wdl show how to analyse texts employing these hngmst~c tools within INTEX. a morphological analyser Finally, we wdl show how electromc grammars (budt w~th INTEX) interact with dlctlonarles and allow recogmctlon of sequences of slmple and compound words m large corpora 0. Introduction We present the system of Itahan morphological dictionaries (the DELI system) which has been developed at the Department of Commumcatl0n Science of the Umverslty of Salerno We wdl see how these dictionaries can be employed m order to index a text Finally, we w~ll examine the construction of local grammars whlch, interacting with dictionaries, allow precise tagging of sequences of words 1. The Italian Electronic Dictionaries The DELI system contains several electronic dictionaries of simple words and of compound words The electromc dictionary of s~mple words, named DELAS, contains about 100 000 Itahan entries to which an alphanumencal code has been assigned This code refers to the grammatlcal category of the word and to ~ts mflect~onal paradigm What follows is an example of the DELAS d~ct~onary dottore N80 cortese A79 amare V3 dl PREP lentamente AVV the noun (N) dottore is given above m mascuhne singular canomc form The adjective (A) cortese is m the mascuhne smgular canomc form Verbs (V) are listed m the mflmtwe form, as amare Those items which do not inflect are assigned a code indicating only the grammatical category, as above shown for the preposition dt and the adverb lentamente The numerical code refers to specific mflechonal algorithms For example, code 80, assoclated to nouns, corresponds to the endings N80 ms ~ mp fp-e-essa-i-esse thus indicating that all nouns as dottore, i e campzone, professore, etc, inflect by adding to the root-e for the mascuhne smgular (ms),-essa for the feminine singular (fs),-l for the mascuhne plural (mp) and-esse for the feminine plural (fp) On the other hand, adjectives encoded A79, as cortese but also trtbale, etc, can be described by the following reflectional model 91
Empirical Methods in Natural Language Processing and Very Large Corpora  
What&apos;s Happened Since the First SIGDAT Meeting?  The first workshop on Very Large Corpora was held just before the 1993 ACL meeting in Columbus, Ohio. The turnout was even greater than anyone could have predicted (or else we would have called the meeting a conference rather than a workshop). We knew that corpus-based language processing was a &quot;hot area,&quot; but we didn&apos;t appreciate just how hot it would turn out to be.
Text-Translation Alignment: Three Languages Are Better Than Two * While bilingual text corpora have been part of the computational linguistics scene for over ten years now, we have recently witnessed the appearance of text corpora containing versions of texts in three or more languages, such as those developed within the CRATER ( McEnery et al., 1997), MULTEXT (Ide and V4ronis, 1994) and MULTEXT-EAST ( Erjavec and Ide, 1998) projects. Access to this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations?Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993;Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996;Klavans and Tzoukermann, 1995;Melamed, 1996), and cross-language information retrieval ( Nie et al., * This research was funded by the Canadian Department of Foreign Affairs and International Trade (http://~.dfait-maeci.gc.ca/), via the Agence de la francophonie (http://~. franeophonie, orE) 1998). However, the case for trilingual and multilingual alignments is not as clear. True multilingual resources such as multilingual glossaries are not widely used, and most of the time, when such resources exist, the real purpose is usually to provide bilingual resources for multiple pairs of languages in a compact way.What we intend to show here is that while multilingual correspondences may not be interesting in themselves, multilingual text alignment techniques can be useful as a means of extracting information on bilingual correspondences. Our idea is that each additional version of a text should be viewed as valuable information that can be used to produce better alignments. In other words: whatever the intended application, three languages are better than two (and, more generally: the more languages, the merrier!).After going through some definitions and preliminary material (Section 1), we present a general method for aligning three versions of a text (Section 2). We then describe some experiments that were carried out to evaluate this approach (Section 3) and various possible optimizations (Section 4). Finally, we report on some disturbing experiments (Section 5), and conclude with directions for future work. In this article, we show how a bilingual text-translation alignment method can be adapted to deal with more than two versions of a text. Experiments on a trilingual corpus demonstrate that this method yields better bilingual alignments than can be obtained with bilingual text-alignment methods. Moreover, for a given number of texts, the computational complexity of the multilingual method is the same as for bilingual alignment.
Mapping Multilingual Hierarchies Using Relaxation Labeling There is an increasing need of having available general, accurate and broad coverage multilingual lexical/semantic resources for developing NL applications. Thus, a very active field inside NL during the last years has been the fast development of generictanguage resources.Several attempts have been performed to produce multilingual ontologies. In ( Ageno et al., 1994), a Spanish/English bilingual dictionary is used to (semi)automatically link Spanish and English taxonomies extracted from DGILE (A1- var, 1987) and LDOCE (Procter, 1987). Similarly, a simple automatic approach for linking Spanish taxonomies extracted from DGILE to WordNet (Miller et al., 1991) synsets is proposed in ( Rigau et al., 1995). The work reported in (Knight and Luk, 1994) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss machine translation system. In (Okumura and Hovy, 1994) (semi)automatic methods for associating a Japanese lexicon to an English ontology using a bilingual dictionary are described. Several experiments aligning EDR and WordNet ontologies are described in (Utiyama and Hasida, 1997). Several lexical resources and techniques are combined in ( ) to map Spanish words from a bilingual dictionary to WordNet, and in (Farreres et al., 1998) the use of the taxonomic structure derived from a monolingual MaD is proposed as an aid to this mapping process.This paper presents a novel approach for merging already existing hierarchies. The method has been applied to attach substantial fragments of the Spanish taxonomy derived from DGILE ( ) to the English WordNet using a bilingual dictionary for connecting both hierarchies.This paper is organized as follows: In section 2 we describe the used technique (the relaxation labeling algorithm) and its application to hierarchy mapping. In section 3 we describe the constraints used in the relaxation process, and finally, after presenting some experiments and preliminary results, we offer some conclusions and outline further lines of research. This paper explores the automatic construction of a multilingual Lexical Knowledge Base from pre-existing lexical resources. We present a new and robust approach for linking already existing lexical/semantic hierarchies. We used a constraint satisfaction algorithm (relaxation labeling) to select-among all the candidate translations proposed by a bilingual dictionary-the right English WordNet synset for each sense in a taxonomy automatically derived from a Span-ish monolingua] dictionary. Although on average , there are 15 possible WordNet connections for each sense in the taxonomy, the method achieves an accuracy over 80~. Finally, we also propose several ways in which this technique could be applied to enrich and improve existing lexical databases.
Improved Alignment Models for Statistical Machine Translation  In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 Statistical Machine Translation The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f/= fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e 1 = argmax {Pr(e[). Pr(f/le~) } • (1) The argmax operation denotes the search problem , i.e. the generation of the output sentence in the target language. Pr(e{) is the language model of the target language, whereas Pr (ff~lel I) is the translation model. Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-to-word correspondences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are sire-ilar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j ~ i = aj from source position j to target position i = aj. The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments have shown it is difficult to handle different word order and the translation of compound nouns• In this paper, we will describe two methods for statistical machine translation extending the baseline alignment model in order to account for these problems. In section 2, we shortly review the single-word based approach described in (Tillmann et al., 1997) with some recently ira-plemented extensions allowing for one-to-many alignments. In section 3 we describe the alignment template approach which explicitly models shallow phrases and in doing so tries to overcome the above mentioned restrictions of single-word alignments. The described method is an improvement of (Och and Weber, 1998), resulting in an improved training and a faster search organization. The basic idea is to model two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words within these phrases. Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached. In section 4 we compare the two methods using the Verbmobil task. 20
Cross-Language Information Retrieval for Technical Documents Cross-language information retrieval (CLIR), where the user presents queries in one language to retrieve documents in another language, has recently been one of the major topics within the information retrieval community. One strong motivation for CLIR is the growing number of documents in various languages accessible via the Internet. Since queries and documents are in different languages, CLIR requires a translation phase along with the usual monolingual retrieval phase. For this purpose, existing CLIR systems adopt various techniques explored in natural language processing (NLP) research. In brief, bilingual dictionaries, corpora, thesauri and machine translation (MT) systems are used to translate queries or/and documents.In this paper, we propose a Japanese/English CLIR system for technical documents, focusing on translation of technical terms.Our purpose also includes integration of different components within one framework. Our research is partly motivated by the "NACSIS" test collection for IR systems ( ) 1 , which consists of Japanese queries and Japanese/English abstracts extracted from technical papers (we will elaborate on the NAC-SIS collection in Section 4). Using this collection, we investigate the effectiveness of each component as well as the overall performance of the system.As with MT systems, existing CLIR systems still find it difficult to translate technical terms and proper nouns, which are often unlisted in general dictionaries. Since most CLIR systems target newspaper articles, which are comprised mainly of general words, the problem related to unlisted words has been less explored than other CLIR subtopics (such as resolution of translation ambiguity). However, Pirkola (1998), for example, used a subset of the TREC collection related to health topics, and showed that combination of general and domain specific (i.e., medical) dictionaries improves the CLIR performance obtained with only a general dictionary. This result shows the potential contribution of technical term translation to CLIR. At the same time, note that even domain specific dictionaries do not exhaustively list possible technical terms. We classify problems associated with technical term translation as given below:(1) technical terms are often compound word~ which can be progressively created simply by combining multiple existing morphemes ("base words"), and therefore it is not entirely satisfactory to exhaustively enumerate newly emerging terms in dictionaries,(2) Asian languages often represent loanwords based on their special phonograms (primarily for technical terms and proper nouns), which creates new base words progressively (in the case of Japanese, the phonogram is called katakana).To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999), which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use "transliteration" (Chen et al., 1998;Knight and Graehl, 1998;Wan and Verspoor, 1998). Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration. Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs. This paper proposes a Japanese/English cross-language information retrieval (CLIR) system targeting technical documents. Our system first translates a given query containing technical terms into the target language, and then retrieves documents relevant to the translated query. The translation of technical terms is still problematic in that technical terms are often compound words, and thus new terms can be progressively created simply by combining existing base words. In addition, Japanese often represents loanwords based on its phono-gram. Consequently, existing dictionaries find it difficult to achieve sufficient coverage. To counter the first problem, we use a compound word translation method, which uses a bilingual dictionary for base words and collocational statistics to resolve translation ambiguity. For the second problem, we propose a translitera-tion method, which identifies phonetic equivalents in the target language. We also show the effectiveness of our system using a test collection for CLIR.
Boosting Applied to Tagging and PP Attachment Boosting is a machine learning algorithm that has been applied successfully to a variety of problems, but is almost unknown in computational linguistics. We describe experiments in which we apply boosting to part-of-speech tagging and prepositional phrase attachment. Results on both PP-attachment and tagging are within sampling error of the best previous results.The current best technique for PP-attachment (backed-off density estimation) does not perform well for tagging, and the current best technique for tagging (maxent) is below state-of-the-art on PPattachment. Boosting achieves state-of-the-art performance on both tasks simultaneously.The idea of boosting is to combine many simple "rules of thumb," such as "the current word is a noun if the previous word is the." Such rules often give incorrect classifications. The main idea of boosting is to combine many such rules in a principled manner to produce a single highly accurate classification rule.There are similarities between boosting and transformation-based learning (Brill, 1993): both build classifiers by combining simple rules, and both are noted for their resistance to overfitting. But boosting, unlike transformation-based learning, rests on firm theoretical foundations; and it outperforms transformation-based learning in our experiments.There are also superficial similarities between boosting and maxent. In both, the parameters are weights in a log-linear function. But in maxent, the log-linear function defines a probability, and the objective is to maximize likelihood, which may not minimize classification error. In boosting, the loglinear function defines a hyperplane dividing examples into (binary) classes, and boosting minimizes classification error directly. Hence boosting is usually more appropriate when the objective is classification rather than density estimation.A notable property of boosting is that it maintains an explicit measure of how difficult it finds particular training examples to be. The most difficult examples are very often mislabelled examples. Hence, boosting can contribute to improving data quality by identifying annotation errors. Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.
Applying Extrasentential Context To Maximum Entropy Based Tagging With A Large Semantic And Syntactic Tagset It appears intuitively that information from earlier sentences in a document ought to help reduce uncertMnty as to a word's correct partof-speech tag. This is especially so for a large semantic and syntactic tagset such as the roughly-3000-tag ATR General English Tagset ( Black et al., 1996;. And in fact, ) demonstrate a significant "tag trigger-pair" effect. That is, given that certain "triggering" tags have already occurred in a document, the probability of occurrence of specific "triggered" tags is raised significantly--with respect to the unigram tag probability model. Table 1, taken from ), provides examples of the tag trigger-pair effect.Yet, it is one thing to show that extrasentential context yields a gain in information with respect to a unigram tag probability model. But it is another thing to demonstrate that extrasentential context supports an improvement in perplexity vis-a-vis a part-of-speech tagging model which employs state-of-the-art techniques: such as, for instance, the tagging model of a maximum entropy tag-n-grambased tagger.The present paper undertakes just such a demonstration.Both the model underlying a standard tag-n-gram-based tagger, and the same model augmented with extrasentential contextual information, are trMned on the 850,000-word ATR General English Treebank ( Black et al., 1996), and then tested on the accompanying 53,000-word test treebank. Performance differences are measured, with the result that semantic information from previous sentences within a document is shown to help significantly in improving the perplexity of tagging Triggering Tag   Triggered Tag I.e. Words Like: Trigger Words Like: Utah, Maine, Alaska #   1 NPiLOCNM  2 JJSYSTEM  3 VVDINCHOATIVE  4 IIDESPITE  5 DD  6 PN1PERSON .,. 9 IIFROMSTANDIN  10 NNUNUM   NPISTATENM  NP1ORG  VVDPROCESSIVE  CFYET  PPHO2  LEBUT22  MPRICE  MPHONE22  MZIP  NNIMONEY   Hill, County, Bay  national, federal  caused, died, made  despite  any, some, certain  everyone, one at (sent.-final) from (sent.-final) 25%, 12", 9.4m3 Experiments are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag-n-gram part-of-speech tagger, contextual information from previous sentences of a document. The tagset employed is the roughly-3000-tag ATR General English Tagset, whose tags are both syntactic and semantic in nature. The kind of extrasentential information provided to the tagger is semantic, and consists in the occurrence or non-occurrence, within the past 6 sentences of the document being tagged, of words tagged with particular tags from the tagset, and of boolean combinations of such conditions. In some cases, these conditions are combined with the requirement that the word being tagged belong to a particular set of words thought most likely to benefit from the extrasentential information they are being conjoined with. The baseline model utilized is a maximum entropy-based tag-n-gram tagging model, embodying a standard tag-n-gram approach to tagging: i.e. constraints for tag trigrams, bigrams, and and the word-tag occurrence frequency of the specific word being tagged, form the basis of prediction. Added into to this baseline tagging model is the extrasentential semantic information just indicated. The performance of the tagging model with and without the added contextual knowledge is contrasted, training from the 850,000-word ATR General English Treebank, and testing on the accompanying 53,000-word test tree-bank. Results are that a significant reduction in testset perplexity is achieved via the added semantic extrasentential information of the richer model. The model with both long-range tag triggers and more complex linguistic constraints achieved a perplexity reduction of 21.4%.
Improving POS Tagging Using Machine-Learning Techniques The study of general methods to improve the performance in classification tasks, by the combination of different individual classifiers, is a currently very active area of research in supervised learning. In the machine learning (ML) literature this approach is known as ensemble, stacked, or combined classifiers. Given a classification problem, the main goal is to construct several independent classifiers, since it has been proven that when the errors committed by individual classifiers are uncorrelated to a sufficient degree, and their error rates are low enough, the resulting combined classifier performs better than all the individual systems (Ali and Paz- zani, 1996;Tumer and Ghosh, 1996;Dietterich, 1997).Several methods have been proposed in order to construct ensembles of classifiers that make uncorrelated errors. Some of them are general, and they can be applied to any learning algorithln, while other are specific to particular algorithms. From a different perspective, there exist methods for constructing homogeneous ensembles, in the sense that a unique learning algorithm has been used to acquire each individual classifier, and heterogeneous ensembles that combine different types of learning paradigms 1.Impressive results have been obtained by applying these techniques on the so-called unstable learning algorithms (e.g. induction of decision trees, neural networks, rule-induction systems, etc.). Several applications to real tasks have been performed, and, regarding NLP, we find ensembles of classifiers in context-sensitive spelling correction (Golding and Roth, 1999), text categorization ( Blum and Mitchell, 1998), and text filtering ( ). Combination of classitiers have also been applied to POS tagging. For instance, van Halteren (1996) combined a number of similar tuggers by way of a straightforward majority vote. More recently, two parallel works (van Halteren et al., 1998;Brill and Wu, 1998) combined, with a remarkable success, the output of a set of four tuggers based on different principles and feature modelling. Finally, in the work by MSxquez et al. (1998) the combination of taggers is used in a bootstrapping algorithm to train a part of speech tagger from a limited amount of training material.The aim of the present work is to improve an existing POS tagger based on decision trees ( Mkrquez and Rodriguez, 1997), by using ensembles of classifiers. This tagger treats separately the different types (classes) of ambiguity by considering a different decision tree for each class. This fact allows a selective construction of ensembles of decision trees focusing on the most relevant ambiguity classes, which greatly vary in size and difficulty. Another goal of the present work is to try to alleviate the problem of data sparseness by applying a method, due 1An excellent survey covering all these topics call be found in (Dietterich, 1997). to Breiman (1998), for generating new pseudoexamples from existing data. As we will see in section 4.2 this technique will be combined with the construction of an ensemble of classifiers.The paper is organized as follows: we start by presenting the two versions of the POS tagger and their evaluation on the reference corpus (sections 2 and 3). Sections 4 and 5 are, respectively, devoted to present the machine-learning improvements and to test their implementation. Finally, section 6 concludes. In this paper we show how machine learning techniques for constructing and combining several classifiers can be applied to improve the accuracy of an existing English POS tagger (MSxquez and Rodrfguez, 1997). Additionally, the problem of data sparseness is also addressed by applying a technique of generating convez pseudo-data (Breiman, 1998). Experimental results and a comparison to other state-of-the-art tuggers are reported.
Determining the specificity of nouns from text Large lexical databases such as Word- Net (see Fellbaum (1998)) are in common research use. However, there are circumstances, particularly involving domainspecific text, where WordNet does not have sufficient coverage. Various automatic methods have been proposed to automatically build lexical resources or augment existing resources. (See, e.g., Riloff and Shepherd (1997), Roark and Charniak (1998), Cara- hallo (1999), and Berland and Charniak (1999).) In this paper, we describe a method which can be used to assist in this problem.We present here a way to determine the relative specificity of nouns; that is, which nouns are more specific (or more general) than others, using only a large text corpus and no additional sources of semantic knowledge. By gathering simple statistics, we are able to decide which of two nouns is more specific to over 80% accuracy for nouns at "basic level" or below (see, e.g., Lakoff (1987)), and about 59% accuracy for nouns above basic level.It should be noted that specificity by itself is not enough information from which to construct a noun hierarchy. This project is meant to provide a tool to support other methods. See Caraballo (1999) for a detailed description of a method to construct such a hierarchy. In this work, we use a large text corpus to order nouns by their level of specificity. This semantic information can for most nouns be determined with over 80% accuracy using simple statistics from a text corpus without using any additional sources of semantic knowledge. This kind of semantic information can be used to help in automatically constructing or augmenting a lexical database such as WordNet.
Retrieving Collocations From Korean Text There have been many theoretical and applied works related to collocations. A rapidly growing awfilability of copora has attracted interests m statistical methods for automatically extractmg ¢:o]loeations from textual corpora. However, it is not easy to )dentify the central tendencies of collocation distribution and the borderlines of criteria are often fuzzy because the expressions can be of arbitrary lengths in a large variety of forms. Getting reliable collocation patterns is particularly difficult in Korean which allows arguments to scamble so freely. This paper presents a statistical method using 'interrupted bigrams' for automatically retrieving ~:ollocations and idiomatic expressions from Korean text. We suggest several statistics to account for the more flexible word order.If the distribution of a random sample is unknown, we often try to make inferences about its properties described by suitably defined measures. For the properties of arbitrary collocation distribution, four measure statistics: 'high frequency', 'condensation', 'randomness', and 'correlation' were devised.Given a morpheme, our system begins by retrieving the frequency distributions of all bigrams within window and then meaningful bigrams are extracted. We produce a-covers to extend them into n-gram collocations 1According to the definition of Kjellmer and Cowie, a fossilized phrase is a sequence, where the occurrence of one word almost predicts the rest of the phrase and one word predicts a very limited number of words in a semi-fossilized phrase (Kjellmer, 1995)  (Cowie, 1981). However, in both fossilized and semi-fossilized types there is a high degree of cohesion among the members of the phrases (Kjellmer, 1995). We consider the cohesions as a-covers that are obtained by applying a fuzzy compatibility relation, which satisfies symmetry and reflexivity, to meaningful bigrams. Namely, n-gram collocations could be interpreted as equivalent sets of the meaningful bigrams through partitioning. Here, a-covers mean the clustered sets of the meaningful bigrams. This paper describes a statistical methodology ibr automatically retrieving collocations from POS tagged Korean text using interrupted bi-grams. The free order of Korean makes it hard to identify collocations. We devised four statistics , &apos;frequency&apos;, &apos;randomness&apos;, &apos;condensation&apos;, and &apos;correlation&apos; .to account for the more flexible word order properties of Korean collocations. We extracted meaningful bigrams using an evaluation ihnction and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers. We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
Noun Phrase Coreference as Clustering Many natural language processing (NLP) applications require accurate noun phrase coreference resolution: They require a means for determining which noun phrases in a text or dialogue refer to the same real-world entity. The vast majority of algorithms for noun phrase coreference combine syntactic and, less often, semantic cues via a set of hand-crafted heuristics and filters. All but one system in the MUC-6 coreference performance evaluation (MUC, 1995), for example, handled coreference resolution in this manner. This same reliance on complicated hand-crafted algorithms is true even for the narrower task of pronoun resolution. Some exceptions exist, however. Ge et al. (1998) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus ( Marcus et al., 1993). Dagan and Itai (1991) develop a statistical filter for resolution of the pronoun "it" that selects among syntactically viable antecedents based on relevant subject-verb-object cooccurrences. Aone and Bennett (1995) and McCarthy and Lehn- ert (1995) employ decision tree algorithms to handle a broader subset of general noun phrase coreference problems.This paper presents a new corpus-based approach to noun phrase coreference. We believe that it is the first such unsupervised technique developed for the general noun phrase coreference task. In short, we view the task of noun phrase coreference resolution as a clustering task. First, each noun phrase in a document is represented as a vector of attribute-value pairs. Given the feature vector for each noun phrase, the clustering algorithm coordinates the application of context-independent and context-dependent coreference constraints and preferences to partition the noun phrases into equivalence classes, one class for each real-world entity mentioned in the text. Context-independent coreference constraints and preferences are those that apply to two noun phrases in isolation. Context-dependent coreference decisions, on the other hand, consider the relationship of each noun phrase to surrounding noun phrases.In an evaluation on the MUC-6 coreference resolution corpus, our clustering approach achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system (McCarthy and Lehnert, 1995) employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set. Furthermore, our approach has a number of important advantages over existing learning and non-learning methods for coreference resolution:• The approach is largely unsupervised, so no annotated training corpus is required.• Although evaluated in an information extraction context, the approach is domainindependent.• As noted above, the clustering approach provides a flexible mechanism for coordinating context-independent and context-dependent coreference constraints and preferences for partitioning noun phrases into coreference equivalence classes.! As a result, we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus-based coreference resolution.The remainder of the paper describes the details of our approach. The next section provides a concrete specification of the noun phrase coreference resolution task. Section 3 presents the clustering algorithm. Evaluation of the approach appears in Section 4. Qualitative and quantitative comparisons to related work are included in Section 5. This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views corer-erence resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%~ placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly , the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into corefer-ence equivalence classes.
Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence The ability to determine the named entities in a text has been established as an important task for several natural language processing areas, including information retrieval, machine translation, information extraction and language understanding. For the 1995 Message Understanding Conference (MUC-6), a separate named entity recognition task was developed and the best systems achieved impressive accuracy (with an F-measure approaching 95%). What should be underlined here is that these systems were trained for a specific domain and a particular langnage (English), typically making use of hand-coded rules, taggers, parsers and semantic lexicons. Indeed, most named entity recognizers that have been published either use tagged text, perform syntactical and morphological analysis or use semantic information for contextual clues. Even the systems that do not make use of extensive knowledge about a particular language, such as Nominator ( Choi et al., 1997), still typically use large data files containing lists of names, exceptions, personal and organizational identifiers.Our aim has been to build a maximally langnageindependent system for both named-entity identification and classification, using minimal information about the source language. The applicability of AI-style algorithms and supervised methods is limited in the multilingual case because of the cost of knowledge databases and manually annotated corpora. Therefore, a much more suitable approach is to consider an EM-style bootstrapping algorithm. In terms of world knowledge, the simplest and most relevant resource for this task is a database of known names. For each entity class to be recognized and tagged, it is assumed that the user can provide a short list (order of one hundred) of unambiguous examples (seeds). Of course the more examples provided, the better the results, but what we try to prove is that even with minimal knowledge good results can be achieved. Additionally some basic particularities of the language should be known: capitalization (if it exists and is relevant -some languages do not make use of capitalization; in others, such as German, the capitalization is not of great help), allowable word separators (if they exist), and a few frequent exceptions (like the pronoun "/" in English). Although such information can be utilised if present, it is not required, and no other assumptions are made in the general model. Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent boot-strapping algorithm based on iterative learning and re-estimation of contextual and mOrphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools.
Unsupervised Models for Named Entity Classification Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples. Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. This paper discusses the use of unlabeled examples for the problem of named entity classification.The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location. For example, a good classifier would identify Mrs. Frank as a person, Steptoe &amp; Johnson as a company, and Honduras as a location. The approach uses both spelling and contextual rules. A spelling rule might be a simple look-up for the string (e.g., a rule that Honduras is a location) or a rule that looks at words within a string (e.g., a rule that any string containing Mr. is a person). A contextual rule considers words surrounding the string in the sentence in which it appears (e.g., a rule that any proper name modified by an appositive whose head is president is a person). The task can be considered to be one component of the MUC (MUC-6, 1995) named entity task (the other task is that of segmentation, i.e., pulling possible people, places and locations from text before sending them to the classifier). Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et el. 97).At first glance, the problem seems quite complex: a large number of rules is needed to cover the domain, suggesting that a large number of labeled examples is required to train an accurate classifier. But we will show that the use of unlabeled data can drastically reduce the need for supervision. Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy. The only supervision is in the form of 7 seed rules (namely, that New York, California and U.S. are locations; that any name containing Mr. is a person; that any name containing Incorporated is an organization; and that LB.M. and Microsoft are organizations).The key to the methods we describe is redundancy in the unlabeled data. In many cases, inspection of either the spelling or context alone is sufficient to classify an example. For example, in .., says Mr. Cooper, a vice president of .. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person. Even if an example like this is not labeled, it can be interpreted as a "hint" that Mr. and president imply the same category. The unlabeled data gives many such "hints" that two features should predict the same label, and these hints turn out to be surprisingly useful when building a classifier.We present two algorithms. The first method builds on results from (Yarowsky 95) and (Blum and I Mitchell 98). (Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance. Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function. (Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training :with unlabeled examples. Our first algorithm is similar to Yarowsky's, but with some important modifications motivated by (Blum and Mitchell 98). The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criteflon.The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98). The AdaBoost algorithm was developed for supervised learning. AdaBoost finds a weighted combination of simple (weak) classifiers, where the w'eights are chosen to minimize a function that bounds the classification error on a set of training examples. Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree. The algorithm builds two classifiers iteratively: each iteration involves minimization of a continuously differential function which bounds the number of examples on which the two classifiers disagree. This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi-fier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context inwhich it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).
Hybrid Disambiguation of Prepositional Phrase Attachment and Interpretation The problem of prepositional phrase (PP) attachment ambigu!ty is one of the most famous problems in natural language processing (NLP). In recent years, many statistical solutions have been proposed: lexical associations (see (Hin- dle and Rooth, 1993)); error-driven transformation learning (see (Brill and Resnik, 1994), extensions by (Ye h and Vilain, 1998)); backedoff estimation (see (Collins and Brooks, 1995), extended to the multiple PP attachment problem by ( Merlo et al., 1997)); loglinear model (see (Franz, 1996b), (Franz, 1996a, pp. 97- 108)); maximum:entropy model (see (Ratna- parkhi, 1998;Ratnaparkhi et al., 1994)).The disambiguation method in this paper has two key features: First, it tries to solve the 1This disambiguation method was developed for an NLI in the Virtuelle Wissensfabrik ( Virtual Knowledge Factory, see (Knoll et al., 1998)), a project funded by the German state Nordrhein-Westfalen, which supported this research in part. I would like to thank Rainer Osswald and the anonymous reviewers for their useful comments and suggestions.PP attachment problem and the PP interpretation problem. Second, it is hybrid as it combines more traditional PP interpretation rules and statistical methods. In this paper, a hybrid disambiguation method for the prepositional phrase (PP) attachment and interpretation problem is presented. 1 The data needed, semantic PP interpretation rules and an annotated corpus, is described first. Then the three major steps of the disambigua-tion method are: explained. Cross-validated evaluation results&apos;, for German (88.6-94.4% correct for binary attachment ambiguities, 83.3-92.5% correct for interpretation ambiguities) show that disambiguation methods combining interpretation! rules and statistical methods might yield significantly better results than non-hybrid disambiguation methods.
HMM Specialization with Selective Lexicalization* Hidden Markov 'Models are widely used for statistical language modelling in various fields, e.g., part-of-speech tagging or speech recognition (Rabiner and Juang, 1986). The models are based on Markov assumptions, which make it possible to view the language prediction as a Markov process. 'In general, we make the firstorder Markov ass'umptions that the current tag is only dependant on the previous tag and that the current word is only dependant on the current tag. These are very 'strong' assumptions, so that the first-order Hidden Markov Models have the advantage of drastically reducing the number of its parameters. On the other hand, the assumptions restrict the model from utilizing enough constraints provided by the local context and the resultant model consults only a single category 'as the contex.A lot of effort has been devoted in the past to make up for the insufficient contextual information of the first-order probabilistic model. The second order Hidden Markov Models with " The research underlying this paper was supported t) 3" research grants fl'om Korea Science and Engineering Foundation. appropriate smoothing techniques show better performance than the first order models and is considered a state-of-the-art technique (Meri- aldo, 1994;Brants, 1996). The complexity of the model is however relatively very high considering the small improvement of the performance.Garside describes IDIOMTAG ( Garside et al., 1987) which is a component of a part-ofspeech tagging system named CLAWS. ID-IOMTAG serves as a front-end to the tagger and modifies some initially assigned tags in order to reduce the amount of ambiguity to be dealt with by the tagger. IDIOMTAG can look at any combination of words and tags, with or without intervening words. By using the IDIOMTAG, CLAWS system improved tagging accuracy from 94% to 96-97%. However, the manual-intensive process of producing idiom tags is very expensive although IDIOMTAG proved fruitful. Kupiec (Kupiec, 1992) describes a technique of augmenting the Hidden Markov Models for part-of-speech tagging by the use of networks. Besides the original states representing each part-of-speech, the network contains additional states to reduce the noun/adjective confusion, and to extend the context for predicting past participles from preceding auxiliary verbs when they are separated by adverbs. By using these additional states, the tagging system improved the accuracy from 95.7% to 96.0%. However, the additional context is chosen by analyzing the tagging errors manually.An automatic refining technique for Hidden Markov Models has been proposed by Brants (Brants, 1996). It starts with some initial first order Markov Model. Some states of the model are selected to be split or merged to take into account their predecessors. As a result, each of new states represents a extended context. With this technique, Brants reported a performance cquivalent to the second order Hidden Markov Models.In this paper, we present an automatic refining technique for statistical language models. First, we examine the distribution of transitions of lexicalized categories. Next, we break out the uncommon ones from their categories and make new states for them. All processes are automated and the user has only to determine the extent of the breaking-out. We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. &apos;Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor&apos;med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.
Why Doesn&apos;t Natural Language Come Naturally?  We have seen great success over the past 15 years in speech recognition. This success is due, largely, to the broad acceptance of Hidden Markov Models (HMMs) at the beginning of that period, which then facilitated rapid and steady improvements in speech recognition that are still continuing today. Although no one believes speech is produced by an HMM, the model affords a rich framework in which to improve the model in a rigorous and scientific manner. Could we create the same environment for a uniform probabilistic paradigm in NL? It requires several ingredients: • A uniform notational system to express meanings , • A statistical model that can represent the associations between meanings and words, * A training program that estimates parameters from annotated examples, • An understanding program that finds the most likely meaning given a word sequence, and • A substantial corpus with meanings annotated and aligned to the words. These problems are fundamental. In speech recognition , we can all agree that the desired output is a sequence of orthographic words. But in understanding , we lack agreement as to the meaning of meaning. And it gets harder from there, since the structures we must look at are not sequences, but rather trees or more complex structures. Still the goal is a worthwhile one. We attempt to formulate several different language understanding problems as probabilistic pattern recognition problems. In general, our goal is to rely heavily on corpus based methods and learning techniques rather than on human generated rules. At the same time, it is essential that we be able to incorporate our intuitions about the problem into the model. We choose probabilistic methods as our preferred form of learning technique because they have several desirable properties. First, if we can accurately estimate the posterior probability of our desired result, then we know a decision based on this posterior probability will minimize the error rate. Second, we have a large inventory of techniques for estimation of robust probabilities from finite data. Third, in contrast to classical pattern recognition problems, language deals almost exclusively with sequences (of sounds, phonemes, charaCters, words, sentences, etc.) Our goal is not to recognize or understand each of these independently, but rather to understand the sequence. Probability theory provides a convenient way to combine several pieces of evidence in making a decision. We present several language problems for which we have developed probabilistic methods that achieve accuracy comparable to that of the best rule-based systems. In each case we developed a model that is (somewhat) appropriate for the problem. These problems include Topic Classification, Information Retrieval, Extracting Named Entities, and Extracting Relations. 128
POS Tags and Decision Trees for Language Modeling For recognizing spontaneous speech, the acoustic signal is to weak to narrow down the number of word candidates. Hence, recognizers employ a language model to take into account the likelihood of word seqiaences. To do this, the recognition problem is Cast as finding the most likely word sequence l?g given the acoustic signal A (Jelinek, 1985). The last line involves two probabilities that need to be estimated--the first due to the acoustic model Pr(AIW ) and the second due to the language model Pr(W). The language model probability can be expressed as follows, where we rewrite the sequence W explicitly as the sequence of N words Wi,N. To estimate the probability distributionPr(WilWl, i-a), a training corpus is used to determine the relative frequencies. Due to sparseness of data, one must define equivalence classes amongst the contexts W~,i-1, which can be done by limiting the context to an n-gram language model (Jelinek, 1985). One can also mix in smaller size language models when there is not enough data to support the larger context by using either interpolated estimation (Jelinek and Mercer, 1980) or a backoff approach (Katz, 1987). A way of measuring the effectiveness of the estimated probability distribution is to measure the perplexity that it assigns to a test corpus ( Bahl et al., 1977). Perplexity is an estimate of how well the language model is able to predict the next word of a test corpus in terms of the number of alternatives that need to be considered at each point. The perplexity of a test set Wi,N is calculated as 2 H, where H is the entropy, defined as follows.1 N n -N Y~l°g2tSr(wilw~i-1) (3) i=1 Language models for speech recognition concentrate solely on recognizing the words that were spoken. In this paper, we advocate redefining the speech recognition problem so that its goal is to find both the best sequence of words and their POS tags, and thus incorporate POS tagging. To use POS tags effectively, we use clustering and decision tree algorithms, which allow generalizations between POS tags and words to be effectively used in estimating the probability distributions. We show that our POS model gives, a reduction in word error rate and perplexity for the Trains corpus in comparison to word and class-based approaches. By using the Wall Street Journal corpus, we show that this approach scales up when more training data is available.
An Information-Theoretic Empirical Analysis of Dependency-Based Feature Types for Word Prediction Models There are many types of features that a language model can use to predict a word in a sentence. Standard n-gram models use the immediately preceding words. Other fixed physical distance feature types may inspect word classes or parts of speech. Grammatically-based feature types may also be used, such as the suizf}@cs.ust.hk incident syntactic and semantic relations or the other words involved in those relations. Our ultimate aim is to determine which combination of feature types is optimal for language modeling. Unfortunately, the state of knowledge in this regard is very limited. Many language models have been published inspired by one or more of these feature types I11121131141151, but discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. The paper uses an information theoretic approach to select feature types for language modeling in a systematic manner. We are concerned with quantitative analysis of the information quantity, information gain and the information redundancy for various feature type combinations in both dependency grammar structure and adjacent bigram structure. The experiments yield a number of conclusions on the predictive value of various feature types and the combinations thereof, which can provide useful information on what level of performance gain can be expected in principle from a bigram model augmented with long distance dependency features. The results are expected to provide a reliable reference for feature type selection in language modeling.We have used Chinese data for the experiments in this paper. Strictly speaking, our conclusions apply only to Chinese. However, we actually expect very similar results on English, and all our preliminary experiments on English data do bear this out I61. We believe the general methodology as well as many of the specific conclusions apply tO a wide range of languages.We will begin by introducing an information theoretic framework for feature type selection and analysis. We then describe the experimental setup. Finally, we discuss a number of claims deriving from the eXperimental evidence. Over the years, many proposals have been made to incorporate assorted types of feature in language models. However, discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult to compare the models objectively. In this paper, we take an information theoretic approach to select feature types in a systematic manner. We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object. The experiments yield several conclusions on the predictive value of several feature types and feature types combinations for word prediction, which are expected to provide guidelines for feature type selection in language modeling.
Word Informativeness and Automatic Pitch Accent Modeling The production of natural, intelligible speech remains a major challenge for speech synthesis research.Recent research has focused on prosody modeling (Silverman, 1987;Hirschberg, 1990;Santen, 1992), which determines the variations in pitch, tempo and rhythm. One of the critical issues in prosody modeling is pitch accent assignment. Pitch accent is associated with the pitch prominence of a word. For example, some words may sound more prominent than others within a sentence because they are associated with a sharp pitch rise or fall. Usually, the prominent words bear pitch accents while the less prominent ones do not. A1-though native speakers of a particular language have no difficulty in deciding which words in their utterances should be accented, the general pattern of accenting in a language, such as English, is still an open question.Some linguists speculate that relative informativeness, or semantic weight of a word can influence accent placement. Ladd (1996) claims that "the speakers assess the relative semantic weight or informativeness of potentially accentable words and put the accent on the most informative point or points" (ibid, pg. 175). He also claims that "if we understand relative semantic weight, we will automatically understand accent placement" (ibid, pg. 186). Bolinger (Bolinger, 1972) also uses the following examples to illustrate the phenomenon:1. "He was arrested because he KILLED a man."2. "He was arrested because he killed a POLICEMAN."The capitalized words in the examples are accented. In (1), "man" is semantically empty relative to "kill"; therefore, the verb "kill" gets accented. However, in (2), "policeman" is semantically rich and is accented instead.However, different theories, not based on informativeness, were proposed to explain the above phenomenon. For example, Bres- nan's (1971) explanation is based on syntactic function. She suggests that "man" in the above sentence does not get accented because "man" and other words like "guy" or "person" or "thing" form a category of "semi-pronouns". Counter-examples listed below raise more questions about the usefulness of semantic informativeness. The accent pattern in the following examples cannot be explaihed solely by semantic informativeness. In intonational phonology and speech synthesis research, it has been suggested that the relative informativeness of a word can be used to predict pitch prominence. The more information conveyed by a word, the more likely it will be accented. But there are others who express doubts about such a correlation. In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness. Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment. They also show that informativeness enables statistically significant improvements in pitch accent prediction. The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily.
Learning Discourse Relations with Active Data Selection The success of corpus-based approaches to discourse ultimately depends on whether one is able to acquire a large volume of data annotated for discourse-level information. However, to acquire merely a few hundred texts annotated for discourse information is often impossible due to the enormity of the haman labor required.This paper presents a novel method for reducing the amount of data for training a decision tree classifier, while not compromising the accuracy. While there has been some work exploring the use of machine leaning techniques for discourse and dialogue (Marcu, 1997;Samuel et al., 1998), to our knowledge, no computational research on discourse or dialogue so far has addressed the problem of reducing or minimizing the amount of data for training a learning algorithm.A particular method proposed here is built on the committee-based sampling, initially proposed for probabilistic classifiers by Dagan and Engelson (1995), where an example is selected from the corpus according to its utility in improving statistics. We extend the method for decision tree classifiers using a statistical technique called bootstrapping (Cohen, 1995). With an additional extension, which we call error .feedback, it is found that the method achieves an increased accuracy as well as a significant reduction of training data. The method proposed here should be of use in domains other than discourse, where a decision tree strategy is found applicable. The paper presents a new approach to identifying discourse relations, which makes use of a particular sampling method called committee-based sampling (CBS). In the committee-based sampling, multiple learning models are generated to measure the utility of an input example in classification; if it is judged as not useful, then the example will be ignored. The method has the effect of reducing the amount of data required for training. In the paper, we extend CBS for decision tree classifiers. With an additional extension called error feedback, it is found that the method achieves an increased accuracy as well as a substantial reduction in the amount of data for training classifiers.
A Learning Approach to Shallow Parsing* Shallow parsing is studied as an alternative to full-sentence parsers. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993). Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information -has motivated the use of learning methods to recognize these patterns (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998).* Research supported by NSF grants IIS-9801638 and SBR-9873450. t Research supported by NSF grant CCR-9502540.This paper presents a general learning approach for identifying syntactic patterns, based on the SNoW learning architecture (Roth, 1998;Roth, 1999). The SNoW learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. SNoW is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large -of which NLP is a principal example. Preliminary versions of it have already been used successfully on several tasks in natural language processing (Roth, 1998;Gold- ing and Roth, 1999;Roth and Zelenko, 1998). In particular, SNoW's sparse architecture supports well chaining and combining predictors to produce a coherent inference. This property of the architecture is the base for the learning approach studied here in the context of shallow parsing.Shallow parsing tasks often involve the identification of syntactic phrases or of words that participate in a syntactic relationship. Computationally, each decision of this sort involves multiple predictions that interact in some way. For example, in identifying a phrase, one can identify the beginning and end of the phrase while also making sure they are coherent.Our computational paradigm suggests using a SNoW based predictor as a building block that learns to perform each of the required predictions, and writing a simple program that activates these predictors with the appropriate input, aggregates their output and controls the interaction between the predictors. Two instantiations of this paradigm are studied and evaluated on two different shallow parsing tasksidentifying base NPs and SV phrases. The first instantiation of this para4igm uses predictors to decide whether each word belongs to the in-terior of a phrase or not, and then groups the words into phrases. The second instantiation finds the borders of phrases (beginning and end) and then pairs !them in an "optimal" way into different phrases. These problems formulations are similar to those studied in (Ramshaw and Marcus, 1995) and (Church, 1988;Argamon et al., 1998), respectively.The experimental results presented using the SNoW based approach compare favorably with previously published results, both for NPs and SV phrases. A s important, we present a few experiments that shed light on some of the issues involved in using learned predictors that interact to produce the desired inference. In particular, we exhibit the contribution of chaining: features that are generated as the output of one of the predictors contribute to the performance of another predictor that uses them as its input. Also, the comparison between the two instantiations 0f the learning paradigm -the Inside/Outside and the Open/Close -shows the advantages of the Open/Close model over the Inside/Outside, especially for the task of identifying long sequences.The contribtition of this work is in improving the state of the art in learning to perform shallow parsing tasks, developing a better understanding for how to model these tasks as learning problems and in further studying the SNoW based computational paradigm that, we believe, can be used in many other related tasks in NLP.The rest of this paper is organized as follows: The SNoW architecture is presented in Sec. 2. Sec. 3 presents the shallow parsing tasks studled and provides details on the computational approach. Sec. 4 describes the data used and the experimental approach, and Sec. 5 presents and discusses the experimental results. A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of mod-eling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.
Guiding a Well-Founded Parser with Corpus Statistics Statistical approaches to parsing have received a great deal of attention over recent years. The availability of large tagged and syntactically bracketed corpora make the programmatic extraction of lexica and grammars feasible. Researchers have tackled parsing by substituting these automatically derived resources for hand-coded ones. While these approaches have had som e success to date (Collins, 1997;Charniak, 1997a), their usability as parsers in systems for natural language understanding is suspect. 1 The 'reconstruction of Treebank-style bracketings does not serve as an adequate basis for semantic interpretation. The phrase structure rules are too numerous, and the analyses too coarse (especially at the lower levels) to allow association of deterministic semantic rules with ph~:ase structure rules. Chaxniak himself (1997b) notes that most of the parses constructed by a "wide-coverage" grammar axe "pretty senseless". 1Collins, Ch~niak, etc. make no claims about their programs being Well suited as parsers for language understanding applications. Certainly, this type of parsing has had success t'o-date in applications such as Information Retrieval.As an example, consider the fiat NP structures that are in the Penn Treebank ( Marcus et al., 1993). Nouns, determiners, and adjectives are all sisters of each other in the syntactic annotation, e.g. (NP (DT the) (JJ mechanical) (NN engineering) (NN industry)). A parser which constructs structures such as this fails to solve an ambiguity problem that has generally been considered syntactic: Are we talking about the industry of mechanical engineering, or is the entire engineering industry perceived as mechanical? If our goal is language understanding, including semantic interpretation, the Treebank bracketings must be considered underspecified.We describe here a system which combines hand-coded linguistic resources with corpusderived probabilistic information to enable (fairly) wide-coverage syntactic parsing. Most importantly, the use of these linguistic resources allows for a better-informed probabilistic model. We present a parsing system built from a handwritten lexicon ~ and grammar, and trained on a selection of the Brown Corpus. On the sentences it can parse, the parser performs as well as purely corpus-based parsers. Its advantage lies in the fact that its syntactic analyses readily support semantic interpretation. Moreover, the system&apos;s handwritten foundation allows for a more fully lexicalized probabilistic model, i.e. one sensitive to co-occurrence of lexical heads of phrase constituents.
Exploiting Diversity in Natural Language Processing: Combining Parsers The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems. The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992;Heath et al., 1996). Their theoretical I finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. The theory has also been validated empirically.Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998;Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Niren- burg, 1994), speech recognition (Fiscus, 1997) and named entity recognition ( Borthwick et al., 1998).The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored, i The resulting parsers surpass the best previously published performance results for the Penn Treebank.
Lexical ambiguity and Information Retrieval revisited A major difficulty to experiment with lexical ambiguity issues in Information Retrieval is always to differentiate the effects of the indexing and retrieval strategy being tested from the effects of tagging errors. Some examples are:1. In ( RichardSon and Smeaton, 1995), a sophisticated retrieval system based on conceptual similarity resultled in a decrease of IR performance. It was not possible, however, to distinguish the effects of the strategy and the effects of automatic Wordl Sense Disambiguation (WSD) errors. In ( Smeaton and Quigley, 1996), a similar strategy and a combination of manual disambiguation and very short documents -image captions-pioduced, however, an improvement of IR perforinance.2. In ( Krovetz, 1997), discriminating word senses with differefit Part-Of-Speech (as annotated by the Church :POS tagger) also harmed retrieval efficiency. Krovetz noted than more than half of the words in a dictionary that differ in POS are related i n meaning, but he could not decide whether the decrease of performance was due to the loss of such semantic relatedness or to automatic POS tagging errors.3. In (Sanderson, 1994), the problem of discerning the effects of differentiating word senses from the effects of inaccurate disambiguation was overcome using artificially created pseudo-words (substituting, for instance, all occurrences of banana or kalashnikov for banana/kalashnikov) that could be disambiguated with 100% accuracy (substituting banana/kalashnikov back to the original term in each occurrence, either banana or kalashnikov).He found that IR processes were quite resistant to increasing degrees of lexical ambiguity, and that disambiguation harmed IR efficiency if performed with less that 90% accuracy. The question is whether real ambiguous words would behave as pseudo-words.4. In ( Schiitze and Pedersen, 1995) it was shown that sense discriminations extracted from the test collections may enhance text retrieval. However, the static sense inventories in dictionaries or thesauri -such as WordNet-have not been used satisfactorily in IR. For instance, in (Voorhees, 1994), manual expansion of TREC queries with semantically related words from WordNet only produced slight improvements with the shortest queries.In order to deal with these problems, we designed an IR test collection which is hand annotated with Part-Of-Speech and semantic tags from WordNet 1.5. This collection was first introduced in ( Gonzalo et al., 1998) and it is described in Section 2. This collection is quite small for current IR standards (it is only slightly bigger than the TIME collection), but offers a unique chance to analyze the behavior of semantic approaches to IR before scaling them up to TREC-size collections (where manual tagging is unfeasible).In ( Gonzalo et al., 1998), we used the manual annotations in the IR-Semcor collection to show that indexing with WordNet synsets can give significant improvements to Text Retrieval, even for large queries. Such strategy works better than the synonymy expansion in (Voorhees, 1994), probably because it identifies synonym terms but, at the same time, it differentiates word senses.In this paper we use a variant of the IR-Semcor collection to revise the results of the experiments by Sanderson (Sanderson, 1994) and Krovetz (Krovetz, 1997) cited above. The first one is reproduced using both ambiguous pseudo-words and real ambiguous words, and the qualitative results compared. This permits us to know if our results are compatible with Sanderson experiments or not. The effect of lexical ambiguity on IR processes is discussed in Section 3, and the sensitivity of recall/precision to Word Sense Disambiguation errors in Section 4. Then, the experiment by Krovetz is reproduced with automatic and manually produced POS annotations in Section 5, in order to discern the effect of annotating POS from the effect of erroneous annotations. Finally, the richness of multiwords in WordNet 1.5 and of phrase annotations in the IR-Semcor collection are exploited in Section 6 to test whether phrases are good indexing terms or not. A number of previous experiments on the role of lexical ambiguity, in Information Retrieval are reproduced on the&apos;IR-Semcor test collection (derived from Semcor), where both queries and documents are hand-tagged ;with phrases, Part-Of-Speech and WordNet 1.5 senses. Our results indicate that a) Word Sense Disambigua-tion can be more beneficial to Information Retrieval than the experiments of Sanderson (1994) with artificially ambiguous pseudo-words suggested, b) Part-Of-Speech tagging does not seem to help Improving retrieval, even if it is manually annotated, c) Using phrases as indexing terms is not a good strategy if no partial credit is given to the phrase components.
Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning  We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an opti-real combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summariza-tion problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units.
Automatic Construction of Weighted String Similarity Measures String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993;Simard et al., 1992;Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997;Borin, 1998;Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text.A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fis- cher, 1974;Stephen, 1992). The length of the LCS is usually divided by the length of the longer string of the two original tokens in order to obtain a normalized value. This score is called the longest common subsequence ratio-LCSR (Melamed, 1995).However, when it comes to different languages, a simple comparison of characters is usually not satisfactory to indicate the total correspondence between words. Different languages tend to modify loan words derived from the same origin in different ways. Swedish and English are an example for two languages with a close etymological relation but a different way of spelling for a large set of cognates. The spelling usually follows certain language specific rules, e.g. the letter 'c' in English words corresponds to the letter 'k' in Swedish in most cases of cognates. Rules like this can be used for the recognition of cognates from specific language pairs. In this paper three approaches to the automatic generation of language pair specific string matching functions axe introduced. They include comparisons at the level of characters and n-grams with dynamic length.All the three approaches presume linguistic similarities between two languages. In this study they were applied to word pairs from a Swedish/English text corpus and experimental results are presented for each of them. String similarity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented.
Taking the load off the conference chairs: towards a digital paper-routing assistant  This paper describes and extensively evaluates a system for the automatic routing of submitted papers to reviewers and area committees, without the need for any human annotation from the reviewers or the program chair. Routing is based on a profile of previous writings obtainable on-line for the reviewer pool, a generally stable and reusable resource that requires no manual adaptation for new submission streams. The paper explores a wide set of variations and extensions on the core model, and achieves system accuracy approaching that of several human judges on the same task. 1 Introduction and Problem Statement Routing submitted papers, abstracts or grant proposals to qualified reviewers is a central task of the academic enterprise, and a remarkably difficult one. Typically it is conducted under significant time pressure in a conference reviewing cycle. As the number of submissions and size of the reviewer pool grows, it becomes increasingly difficult for a conference chair to be familiar with the different expertise of all members of the program committee. It is also difficult for one person to master the subtleties of fine subject area distinctions as topic diversity in a conference becomes large. For these reasons, conferences such as ACL (the Association for Computational Linguistics) often use a hierarchical program committee structure, where submitted papers are first routed to area committees, and then more specialized area chairs have the task of assigning papers to individual reviewers in the committee. However, in a diverse and multidisciplinary field such as natural language processing, it is often difficult to define clear cut committee descriptions and the program chair still must be cognizant of the detailed expertise of the area committee members in order to route atypical or multidisciplinary papers to committees with the most appropriate pool of reviewers. The low inter-rater consistency results shown in Table 12 indicate that humans find even area committee routing to be a difficult task. The following paper focuses on a range of automated solutions to this task of routing papers to their most appropriate area committee. It presents extensive empirical investigation and evaluation of a wide range of issues related to this task. Previous published research into the problem of automatic routing of conference paper submissions is surprisingly limited. Approaches to this task can be essentially broken down into four major strategies: The first strategy is keyword based. Authors are required to specify a list of topic/subtopic areas for their papers (often from a prespecified term list), and reviewers then complete a survey of their relative level of expertise on this list of topics/subtopics. This approach is followed by AAAI conference reviewing. It suffers from the problem that authors often have a difficult time selecting keywords to adequately describe their work. It works best in conferences that are very broad, and is least effective in more focused workshops where routing distinctions in subject area and paradigm are more subtle. The second strategy is to build a statistical profile of reviewers&apos; expertise by eliciting relevance judgments on a set of abstract data. AAAI also requires its reviewers to rank (bid on) submitted abstracts , and there is currently unpublished work exploring the application of supervised routing to the ranked reviewer bids on AAAI submitted abstracts (Hirsh, personal communication). In groundbreaking work, Dumais and Nielsen (1992) developed a system for the routing of Hypertext&apos;91 abstracts using latent semantic indexing (Deerwester et al., 1990), trained from available text sources including a small set of reviewer-submitted abstracts, on-line books and ACM articles as a source for the term-by-document matrix used in their singular value de-compositions. Reviewers manually ranked their interest in all submitted abstracts, and best performance was achieved when reviewers were assigned twice their target number of abstracts and asked to choose their preferred half. One problem with the modeling of reviewer rank-ings/bids is that these may be based more on what the reviewer finds interesting rather than what 220
PP-Attachment: A Committee Machine Approach Structural ambiguity is one of the most serious problems that Natural Language Processing (NLP) systems face. This ambiguity takes place because the syntactic information alone does not suffice to make an assignment decision. Constructions such as Prepositional Phrase (PP), coordination, or relative clauses are affected. An exhaustive study about the information needed to deal with this particular structural ambiguity has not been carried out as of yet; nevertheless, in the current literature we can find several proposals.• In certain cases, it seems that the information needed to solve the attachment comes from the general context.(1.a) John has a telescope.(1.b) He saw the girl with the telescope.In this particular case, a correct attachment would require a model representing the situation in which the different entities are involved. If this were true for all of the cases, determining PP assignment would require highly complex computation.• In some other cases, the information determining the PP attachment seems to be local. Some works [Woods et al, 1972], [Boguraev, 1979], [Marcus et al. 1993] suggested several strategies that based their decision-making on the relationships existing between predicates and argumentswhat [Katz and Fodor, 1963] called selectional restrictions. Cases belonging to this group seem to be easier to handle computationally than the former ones.Regarding these different cases we can speak of two kinds of disambiguation mechanisms. One that can be called a low level mechanism which uses mainly information regarding selectional restrictions between predicates and arguments. This mechanism uses a local context in order to solve syntactic disambiguation: that which is constituted by the predicate and its arguments. The second mechanism uses higher level information such as situation models. If the low level mechanism does not solve the ambiguity, the high level mechanism, which would be activated later, should be able to do it. There are empirical data that seem to support the fact that human beings use these two mechanisms both for word sense disambiguation and syntactic disambiguation. For a review see [Sopena et al. 1998]. In this paper we use various methods for multiple neural network combination in tasks of prepo-sitional phrase attachment. Experiments with aggregation functions such as unweighted and weighted average, OWA operator, Choquet integral and stacked generalization demonstrate that combining multiple networks improve the estimation of each individual neural network. Using the Ratnaparkhi data set (the complete training set and the complete test set) we obtained an accuracy score of 86.08%. In spite of the high cost in computational time of neural net training, the response time in test mode is faster than others methods.
Cascaded Grammatical Relation Assignment When dealing with large amounts of text, finding structure in sentences is often a useful preprocessing step. Traditionally, full parsing is used to find structure in sentences. However, full parsing is a complex task and often provides us with more information then we need. For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing. For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daele- mans et al., 1996), a chunker (Veenstra, 1998;Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998). The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules?Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997;Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded).Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade. Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier. We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to verbs in text. The GR assigner uses several sources of information step by step such as several types of XP chunks (NP, VP, PP, ADJP and ADVP), and adverbial functions assigned to these chunks (e.g. temporal, local). Since not all of these entities are predicted reliably, it is the question whether each source leads to an improvement of the overall GR assignment.In the rest of this paper we will first briefly describe Memory-Based Learning in Section 2. In Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade. Section 3.2 describes the basic GR classifier. Section 3.3 presents the architecture and results of the cascaded GR assignment experiments. We discuss the results in Section 4 and conclude with Section 5. In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.
Automatically Merging Lexicons that have Incompatible Part-of-Speech Categories  We present a new method to automatically merge lexicons that employ different incompatible POS categories. Such incompatibilities have hindered efforts to combine lexicons to maximize coverage with reasonable human effort. Given an &quot;original lexicon&quot;, our method is able to merge lexemes from an &quot;additional lexicon&quot; into the original lexicon , converting lexemes from the additional lexicon with about 89% precision. This level of precision is achieved with the aid of a device we introduce called an anti-lexicon, which neatly summarizes all the essential information we need about the co-occurrence of tags and lemmas. Our model is intuitive, fast, easy to implement, and does not require heavy computational resources nor training corpus. lemma I tag apple INN boy NN calculate VB Example entries in Brill lexicon 1 Motivation We present a new, accurate method to automatically merge lexicons that contain incompatible POS categories. In this paper, we look specifically at the problem that different lexicons employ their own part-of-speech (POS) tagsets that are incompatible with each other, owing to their different linguistic backgrounds, application domains, and/or lexical acquisition methods. Consider the way that lemmas are typically marked with POS information in machine-readable lexicons. For example, here are a few entries from the lexicon in Brill&apos;s tagger (Brill, 1994) and the Moby lexicon (Ward, 1996), showing simple pairs of lemmas and POS tags:
An Iterative Approach to Estimating Frequencies over a Semantic Hierarchy Knowledge of the constraints a verb places on the semantic types of its arguments (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in many areas of natural language processing, particularly structural disambiguation. Recent treatments of selectional restrictions have been probabilistic in nature (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the number of times word senses, or concepts, appear in the different argument positions of verbs. A difficulty arises due to the absence of a large volume of sense disambiguated data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more than one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally among the alternative senses of a noun.Abney and Light (1998) have attempted to obtain selectional preferences using the Expectation Maximization algorithm by encoding WordNet as a hidden Markov model and using a modified form of the forward-backward algorithm to estimate the parameters.The approach proposed in this paper is to use a re-estimation process which relies on counts being passed up a semantic hierarchy, from the senses of nouns appearing in the data. We make use of the semantic hierarchy in WordNet (Fellbaum, 1998), which consists of word senses, or concepts, 1 related by the 'is-a' or 'is-a-kind-of' relation. If c' is a kind of c, then c is a hypernym of c', and c' a hyponym of c. Counts for any concept are transmitted up the hierarchy to all of the concept's hypernyms. Thus if eat chicken appears in the corpus, the count is transmitted up to &lt;meat &gt;, &lt; :food&gt;, and all the other hypernyms of that sense of chicken? The problem is how to distinguish the correct sense of chicken in this case from incorrect senses such as &lt;wimp&gt;. 3 We utilise the 1We use the words sense and concept interchangeably to refer to a node in the semantic hierarchy.eWe use italics when referring to words, and angled brackets when referring to concepts or senses. This notation does not always pick out a concept uniquely, but the particular concept being referred to should be clear from the context.3The example used here is adapted from Mc- Carthy (1997). There are in fact four senses of chicken in WordNet 1.6, but for ease of exposition we consider only two. The hypernyms of the fact that whilst splitting the count equally can lead to inaccurate estimates, counts do tend to accumulate in the right places. Thus counts will appear under &lt;:food&gt;, for the object of eat, but not under &lt;person&gt;, indicating that the object position of eat is more strongly associated with the set of concepts dominated by &lt;:food&gt; than with the set of concepts dominated by &lt; person &gt;. By choosing a hypernym for each alternative sense of chicken and comparing how strongly the sets dominated by these hypernyms associate with eat, we can give more count in subsequent iterations to the food sense of chicken than to the wimp sense.A problem arises because these two senses of chicken each have a number of hypernyms, so which two should be compared? The chosen hypernyms have to be high enough in the hierarchy for adequate counts to have accumulated, but not so high that the alternative senses cannot be distinguished. For example, a hypernym of the food sense of chicken is &lt;poultry&gt;, and a hypernym of the wimp sense is &lt;weakling&gt;. However, these concepts may not be high enough in the hierarchy for the accumulated counts to indicate that eat is much more strongly associated with the set of concepts dominated by &lt;poultry&gt; than with the set dominated by &lt;weakling&gt;. At the other extreme, if we were to choose &lt;entity&gt;, which is high in the hierarchy, as the hypernym of both senses, then clearly we would have no way of distinguishing between the two senses.We have developed a technique, using a X 2 test, for choosing a suitable hypernym for each alternative sense. The technique is based on the observation that a chosen hypernym is too high in the hierarchy if the set consisting of the children of the hypernym is not sufficiently homogeneous with respect to the given verb and argument position. Using the previous example, &lt;entity&gt; is too high to represent either sense of chicken because food sense are &lt;poultry&gt;, &lt;bird&gt;, &lt;meat&gt;, &lt; foodstuff &gt;, &lt; food &gt;, &lt; substance &gt;, &lt; object &gt;, &lt; entity &gt;. The hypernyms of the wimp sense are &lt; weakling &gt;, &lt; person &gt;, &lt; life_form&gt;, &lt;entity&gt;. the children of &lt;entity&gt; are not all associated in the same way with eat. The set consisting of the children of &lt;meat&gt;, however, is homogeneous with respect to the object position of eat, and so &lt;meat&gt; is not too high a level of representation. The measure of homogeneity we use is detailed in Section 5. This paper is concerned with using a semantic hierarchy to estimate the frequency with which a word sense appears as a given argument of a verb, assuming the data is not sense disambiguated. The standard approach is to split the count for any noun appearing in the data equally among the alternative senses of the noun. This can lead to inaccurate estimates. We describe a re-estimation process which uses the accumulated counts of hypernyms of the alternative senses in order to redistribute the count. In order to choose a hypernym for each alternative sense, we employ a novel technique which uses a X 2 test to measure the homo-geneity of sets of concepts in the hierarchy.
Using Subcategorization to Resolve Verb Class Ambiguity The relation between the syntactic realization of a verb's arguments and its meaning has been extensively studied in Levin (1993). Levin's work relies on the hypothesis that "the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning" (Levin, 1993, p. 1). Verbs which display the same diathesis alternations-alternations in the realization of their argument structure-are assumed to share certain meaning components and are organized into a semantically coherent class.As an example consider sentences (1)-(3) taken from Levin. Example (1) illustrates the causative/inchoative alternation. Verbs undergoing this alternation can be manifested either as transitive with a causative reading (cf. (la)) or as intransitive with an inchoative reading (cf. (lb)). Examples (2) and (3) illustrate the dative and benefactive alternations respectively. Verbs which license the former alternate between the prepositional frame NP-V-NP-PPto (cf. (2a)) and the double object frame V-NP-NP (cf. (2b)), whereas verbs which undergo the latter alternate between the double object frame b. Martha carved a toy for the baby.Verbs like crack and chip pattern with break in licensing the causative/inchoative alternation and are associated with the semantic class of BREAK verbs.Verbs make and build behave similar to carve in licensing the benefactive alternation and are members of the class of BUILD verbs, whereas sell and give undergo the dative alternation and participate in the GIVE class. By grouping together verbs which pattern together with respect to diathesis alternations Levin defines approximately 200 verb classes, which she argues reflect important semantic regularities. Levin&apos;s (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these ambiguous cases the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context. In others it is not, and an application which wants to recover this information will be forced to rely on some more or less elaborate process of inference. We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference.
IMPROVING BRILL&apos;S POS TAGGER FOR AN AGGLUTINATIVE LANGUAGE In 1992 Eric Brill presented a rule-based tagging system which differs from other rule-based systems because it automatically infers rules from a training corpus. The tagger does not use hand-crafted rules or prespecified language information, nor does the tagger use external lexicons. According to Brill (1992) 'there is a very small amount of general linguistic knowledge built into the system, but no language-specific knowledge'. The grammar is induced directly from the training corpus without human intervention or expert knowledge. The only additional component necessary is a small, manually and correctly annotated corpus -the training corpus -which serves as input to the tagger. The system is then able to derive lexical/morphological and contextual information from the training corpus and 'learns' how to deduce the most likely part of speech tag for a word. Once the training is completed, the tagger can be used to annotate new, unannotated corpora based on the tag set of the training corpus. The tagger has been trained for tagging English texts with an accuracy of 97% (Brill, 1994).In this study Brill's rule-based part of speech (PoS) tagger is tested on Hungarian, a dissimilar language, concerning both morphology and syntax, to English. The main goal is i) to find out if Brill's system is immediately applicable to a language, which greatly differs in structure from English, with a high degree of accuracy and (if not) ii) to improve the training strategies to better fit for agglutinative/inflectional languages with a complex morphological structure.Hungarian is basically agglutinative, i.e. grammatical relations are expressed by means of affixes. Hungarian is also inflectional; it is difficult to assign morphemes precisely to the different parts of the affixes. The morphotactics of the possible forms is very regular. For example, Hungarian nouns may be analyzed as a stem followed by three positions in which inflectional suffixes (for number, possessor and case) can occur. Additionally, derivational suffixes, which change the PoS of a word, are very common and productive. Verbs, nouns, adjectives and even adverbs can be further derived. Thus, a stem can get one or more derivational and often several inflectional suffixes. For example, the word taldlataiknak 'of their hits' consists of the verb stem talrl 'find, hit', the deverbal noun suffix -at, the possessive singular suffix -a 'his', the possessive plural suffix -i 'hits', the plural suffix -k 'their', and the dative/genitive case suffix -nak.In this study it is shown that Brill's original system does not work as well for Hungarian as it does for English because of the great dissimilarity in characteristics between the two languages. By adding lexical templates, more suitable for complex morphological structure (agglutination and inflection), to the lexical rule generating system, the accuracy can be increased from 82.45% up to 97%. In this paper Brill&apos;s rule-based PoS tagger is tested and adapted for Hungarian. It is shown that the present system does not obtain as high accuracy for Hungarian as it does for English (and other Germanic languages) because of the structural difference between these languages. Hungarian, unlike English, has rich morphology, is agglutinative with some inflectional characteristics and has fairly free word order. The tagger has the greatest difficulties with parts-of-speech belonging to open classes because of their complicated morphological structure. It is shown that the accuracy of tagging can be increased from approximately 83% to 97% by simply changing the rule generating mechanisms, namely the lexical templates in the lexical training module.
Corpus-Based Learning for Noun Phrase Coreference Resolution Coreference resolution refers to the process of determining if two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems. In particular, information extraction (IE) systems like those built in the DAI:tPA Message Understanding Conferences (Chinchor, 1998;Sundheim, 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (Committee, 1995).In this paper, we focus on the task of determining coreference relations as defined in MUC-6 (Committee, 1995). Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which are nouns, noun phrases, or pronouns. Thus, our coreference task resolves general noun phrases and not just pronouns, unlike in some previous work on anaphora resolution. The ability to link co-referring noun phrases both within and across sentences is critical to discourse analysis and language understanding in general. In this paper, we present a learning approach for coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just pronouns but rather general noun phrases. In contrast to previous work, we attempt to evaluate our approach on a common data set, the MUC-6 coreference corpus. We obtained encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to non-learning approaches.
Corpus-Based Approach for Nominal Compound Analysis for Korean Based on Linguistic and Statistical Information Nominal compound analysis is one of crucial issues that have been continuously studied by computational and theoretical linguists. Many linguists have dealt with nonlinal compounds in view of semantic interpretation, and tried to explain how nominal compounds are semantiThis work was partially supported by a KOSEF's postdoctoral fellowship grant.cally interpreted (Levi, 1978;Selkirk, 1982). In the field of natural language processing, various computational models have been established for syntactic analysis and semantic interpretation of nominal compounds (Finin, 1980;McDon- ald, 1982;Arens ct al. , 1987;Pustejovsky et al. , 1993;Kobayasi et al. , 1994;Van- derwerde, 1994;Lauer, 1995). Recently it has been shown that noun phrase analysis is effecrive for the improvement of the application of natural language processing such as information retrieval (Zhai, 1997).Parsing nominal compound is a basic step for ~11 problems related to it. From a bracketing point of view, structural ambiguity is also a main problem in nominal compomld analysis like in other parsing problems. Re(:ent works have shown that the corpus-b;~sed approach for nominal compound analysis makes a good result to resolve the ambiguities (Fustcjovsky et al. , 1993;Kobayasi et al. , 1994;Lauer, 1995;Zhai, 1997). Lauer (1995) has compared two diffbrent models of corpus-based approaches fbr nominal compound analysis. One was called as the adjacency model which was inspired by (Puste- jovsky et al. , 1993), and the other was referred to as the dependency model which was presented by Kobayasi ~t al. (1994) 2 and Lauer (t995). Given a nominal compound of three nouns n~'-.2'a:~, let A.s. t)e a metric used to evaluate the association of two nouns. In the adjacency model, if A.~(',,l:',J.2) &gt; A.s(n2,n3), then the structure is determined as (('hi 'n2) n3). Otherwise, ('nl (',l,~ 'n:,)). On the other hand, in 2In their work, the structure is determined l)y comparing the multiplication of the ~ssociations between all two nOuns, that is, by comImring A,s('..t, 'n2)A.s(n2, n3) and AS(nl, n3) As (n2, ',l.:~). It m~tkes similar results to the dependency model. tim dClmn(h,,ncy model, the decision is det)endent on the association strength of nt for 'rt2 and ',,::. That is. the left branching tree ((at 'n2) ha) is constructed it" A.s(nt,'u2) &gt; As(at,ha), and I:he right branching tree ('nL (n2 'ha)) is made, ~M,,,rwise. Lauer (1995) has claimed that the ~h',lmndency model makes intuitive sense and i)r~)duces t)(,,tter results.In this paper, we propose a new model tbr ~)minal comt)ound analysis on the basis of w()rd (:o-()(:cui'ren(;(?s and grannnatical relati(mshil)s ilnmanent in nominal (:ompounds. Tim grammatical relation can sometimes ma,k(,, the (tisnmbiguation more precise as wo, ll as it gives a clue of the nonfinal inl.(Ul)r('Iation.For example, in the nominal (:~nnl)ound "KYEONG JAENG (competition) YUBALa(bringing about) CHEJE(system)" whi(:h meallS system to bring about competition, tim nominal conlpound "KYEONGJAENG Cl-tEJE((:oml)etition system)" co-occurs much more fl'equently titan "KYEONGJAENG YUBAL(bringing about competition)". Howo.w;r, its structure is selected to be [[KYEONG-.IAENG YUBAL] CHEJE]. Why it is analyzed in such a way can be shown easily by transli)rming the nominal compound to the clause. Because "YUBAL(bringing about)" is the predicatiw,, noun that derives the verb with the 1)redicative suffix attached, the modifying noun phrase can be transformed to the corresponding VP which has the meaning of "to bring about competition" (Figure 1). The verb "YUBAL-HA-NEUN(to bring about)" in VP takes the "KYEONG,lAENG(competition)" as the ob-.iect. The predicative noun "YUBAL(bringing about)" also subcategorizes a noun phrase "KYEONGJAENG(competition)" in the same rammer as the verb. In the right syntactic tree of Figure 1, it should be noted that the object of a verb does not have the dependency ,elation to the noun outside the maximal 1)rojection of its head, VP. Likewise, the object "KYE()NGJAENG(competition)" does not have a,ny dependency with the other noun over the predicative noun "YUBAL(bringing a,t)out)".:WUBAL is a noun in Korean which means to cause t,o bring about something Accurate nominal compound analysis is crucial for in application of natural language processing such as information retrieval and extraction as well as nominal compound interpretation. I,n the nominal compound analysis area, some corpus-based approaches have reported successful results by using statistal co-occurrences of nouns. But a nominal compound often has the similar structure to a simple sentence , e.g. the complement-predicate structure, as well as representing compound meaning with several nouns combined. Due to the grammar-ical characteristics of nominal compounds, the fi&apos;amework based only on statistcal association between nouns often fails to analyze their structures accurately, especially in Korean. This pc-per presents a new model for Korean nominal compound analysis on the basis of linguistic and statistical knowledge. The syntactic relations often have an effect on determining the structure of nominal compounds, and we analyzed 40 million word corpus in order to acquire syntactic and statistical knowledge. The structure of a nominal compound is analyzed based on the linguistic lexical information extracted. By experiments, it is shown that our method is effective for accurate analysis of Korean nominal compounds.
  
Unsupervised Learning of Word Boundary with Description Length Gain Detecting and handling unknown words properly has become a crucial issue in today's practical natural language processing (NLP) technology. No matter how large the dictionary that is used in a NLP system, there can be many new words in running/real texts, e.g., in scientific articles, newspapers and Web pages, that the dictionary does not include. Many such words are proper names and special terminology that provide critical information. It is unreliable to rest on delimiters such as white spaces to detect new lexical units, because many basic lexical items contain one or more spaces, e.g., as in "New York", "Hong Kong" and "hot dog". It appears that unsupervised learning techniques are necessary in order to alleviate the problem of unknown words in the NLP domain.There have been a number of studies on lexical acquisition from language data of different types.Wolff attempts to infer word boundaries from artificially-generated natural language sentences, heavily relying on the co-occurrence frequency of adjacent characters [Wolff1975, Wolff 1977]. Nevill-Manning's text compression program Sequitur can also identify word boundaries and gives a binary tree structure for an identified word [Nevill-Mmming 1996]. de Marcken explores unsupervised lexical acquisition from Enghsh spoken and written corpora and from a Chinese written corpus [de Marken 1995: de Marken 1996.In this paper, we present all unsupervised approach to lexical acquisition within the minimum description length (MDL) paradigm [Rissanen 1978, Rissanen 1982 [ Rissanen 1989], with a goodness measure, namely, the description length gain (DLG), which is formulated in [Kit 1998] following classic information theory [Shannon 1948, Cover andThomas 1991]. This measure is used, following the MDL principle, to evaluate the goodness of identifying a (sub)sequence of characters in a corpus as a lexical item. In order to rigorously evaluate the effectiveness of this unsupervised learning approach, we do not limit ourselves to the detection of unknown words with respect to ally given dictionary. Rather, we use it to perform unsupervised lexical acquisition from large-scale English text corpora. Since it is a learning-via-compression approach, the algorithm can be further extended to deal with text compression and, very likely, other data sequencing problems.The rest of the paper is organised as follows: Section 2 presents the formulation of the DLG mea-sure in terms of classic information theory; Section 3 formulates the learning algorithm within the MDL framework, which aims to achieve an optimal segmentation of the given corpus into lexical items with regard to the DLG measure; Section 4 presents experiments and discusses experimental results with respect to previous studies; and finally, the conclusions of the paper are given in Section 5. This paper presents an unsupervised approach to lexical acquisition with the goodness measure description length gain (DLG) formulated following classic information theory within the minimum description length (MDL) paradigm. The learning algorithm seeks for an optimal segmentation of an utterance that maximises the description length gain from the individual segments. The resultant segments show a nice correspondence to lexical items (in particular, words) in a natural language like English. Learning experiments on large-scMe corpora (e.g., the Brown corpus) have shown the effectiveness of both the learning algorithm and the goodness measure that guides that learning.
Experiments in Unsupervised Entropy-Based Corpus Segmentation The paper presents an approach to segment a corpus into words, based on entropy. We assume that the corpus is not annotated with additional information, and that we have no information whatsoever about the corpus or the language, and no linguistic resources such as a lexicon or grammar. Such a situation may occur e.g. if there is a (sufficiently large) corpus of an unknown or unidentified language and alphabet. 1 Based on entropy, we search for separators, without knowing a priory by which symbols or sequences of symbols they are constituted.Over the last decades, entropy has frequently been used to segment corpora [Wolff, 1977, Alder, 1988, Hutchens and Alder, 1998. and it is commonly used with compression techniques. Harris [1955] proposed an approach for segmenting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: Every symbol of a word is annotated with the count of all possible successor symbols given the .substring that ends with the current symbol, and with the count of all possible predecessor symbols I Such a corpus can be electronically encoded with arbitrarily defined symbol codes.given the tail of the word that starts with the current symbol. Maxima in these counts are used to segment the word into morphemes.All steps of the present approach will be described on the example of a German corpus. In addition, we will give results obtained on modified versions of this corpus, and on an English corpus. The paper presents an entropy-based approach to segment a corpus into words, when no additional information about the corpus or the language, and no other resources such as a lexicon or grammar are available. To segment the corpus, the algorithm searches for separators, without knowing a priory by which symbols they are constituted. Good results can be obtained with corpora containing &quot;clearly perceptible&quot; separators such as blank or new-line.
Practical Bootstrapping of Morphological Analyzers The Expedition project is devoted to fast "ramp-up" of machine translation systems from less studied, so-called "low-density" languages into English. One of the components that must be acquired and built during this process is a morphological analyzer for the source low-density language. Since we expect that the source language informant will not be well-versed in computational linguistics in general or in recent approaches to building morphological analyzers (e.g., [Koskenniemi, 1983], [Antworth. 1990], [Karttunen et al., 1992], [Karttunen, 1994]) and the operation of state-of-the-art finite state tools (e.g., [Karttunen. 1993], [Karttunen and Beesley, 1992], [Karttunen et al., 1996]) in particular, the generation of the morphological analyzer component has to be accomplished almost semi-automatically. The user must be guided through a knowledge elicitation procedure for the knowledge required for the morphological analyzer. This is accomplished using the elicitation component of Expedition, the Boas system. As this task is not easy, we expect that the development of the morphological analyzer will be an iterative process, whereby the human informant will revise and/or refine the information previously elicited based on the feedback from a test runs of the nascent analyzer.The work reported in this paper describes the use of machine learning in the process of building and refining morphological analyzers. The main use of machine learning in our current approach is in the automatic learning of formal rewrite or replace rules for morphographemic changes from the examples, provided by the informant. This subtask of accounting for such phenomena is perhaps one of the more complicated aspects of building an analyzer and by automating it we expect to gain a certain improvement in productivity.There have been a number of studies on inducing morphographemic rules from a list of inflected words and a root word list. Johnson [1984] presents a scheme for inducing phonological rules from surface data, mainly in the context of studying certain aspects of language acquisition. The premise is that languages have a finite number of alternations to be handled by morphographemic rules and a fixed number of contexts in which they appear; so if there is enough data, phonological rewrite rules can be generated to account for the data. Rules are ordered by some notion of "'surfaciness", and at each stage the nmst surfacy rule --the rule with the most transparent context is selected. Golding and Thompson[1985] describe an approach for inducing rules of English word formation from a given corpus of root forms and the corresponding inflected forms. The procedure described there generates a sequence of transformation rules, l each specifying how to perform a particular inflection.More recently, Theron and Cloete [1997] have pre1Not in the sense it is used in transformation-based learning [Brill, 1995]. sented a scheme for obtaining two-level morphology rules from a set of aligned segmented and surface pairs. They use the notion of string edit sequences assuming that only insertions and deletions are applied to a root form to get the inflected form. They determine the root form associated with an inflected form (and consequently the suffixes and prefixes) by exhaustively matching against all root words. The motivation is that "real" suffixes and prefixes will appear often enough in the corpus of inflected forms, so that, once frequently occurring suffixes and prefixes are identified, one can then determine the segmentation for a given inflected word by choosing the segmentation with the most frequently occurring affix segments and considering the remainder to be the root. While this procedure seems to be reasonable for a small root word list, the potential for "noisy" or incorrect alignments is quite high when the corpus of inflected forms is large and the procedure is not given any prior knowledge of possible segmentations. As a result, selecting the "correct" segmentation automatically becomes quite nontrivial. An additional complication is that allomorphs show up as distinct affixes and their counts in segmentations are not accumulated, which might lead to actual segmentations being missed due to fragmentation. The rule induction is not via a learning scheme: aligned pairs are compressed into a special data structure and traversals over this data structure generate morphographemic rules. Theron and Cloete have experimented with pluralization in Afrikaans, and the resulting system has shown about 94% accuracy on unseen words.Goldsmith [1998] has used an unsupervised learning method based on the minimum description length principle to learn the "morphology" of a number of languages. What is learned is a set of "root" words and affixes, and common inflectional pattern classes. The system requires just a corpus of words in a language. In the absence of any root word list to use as a scaffolding, the shortest forms that appear frequently are assumed to be roots, and observed surface forms are then either generated by concatenative affixation of suffixes or by rewrite rules. 2 Since the system has no notion of what the roots and their part of speech values really are, and what morphological information is encoded by the affixes, these need to be retrofitted manually by a human (if one is building a morphological analyzer) who would have to weed through a large number of noisy rules. We feel that this approach, while quite novel, can be used to build real-world morphological analyzers only after substantial modifications are made.ZSome of which may" not make sense, but are necessaryto account for the data: for instance a rule like insert a word final y after the root "eas". is used to generate easy. This paper presents a semi-automatic technique for developing broad-coverage finite-state morphological analyzers for any language. It consists of three components-elicitation of linguistic information from humans, a machine learning bootstrapping scheme and a testing environment. The three components are applied iteratively until a threshold of output quality is attained. The initial application of this technique is for morphology of low-density languages in the context of the Expedition project at NMSU CRL. This elicit-build-test technique compiles lexical and inflectional information elicited from a human into a finite state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples. The resulting morphological analyzer is then tested against a test suite, and any corrections are fed back into the learning procedure that builds an improved analyzer.
Finding Representations for Memory-Based Language  Constructive induction transforms the representation of instances in order to produce a more accurate model of the concept to be learned. For this purpose, a variety of operators has been proposed in the literature, including a Cartesian product operator forming pair-wise higher-order attributes. We study the effect of the Cartesian product operator on memory-based language learning, and demonstrate its effect on generalization accuracy and data compression for a number of linguistic classification tasks, using k-nearest neighbor learning algorithms. These results are compared to a baseline approach of backward sequential elimination of attributes. It is demonstrated that neither approach consistently outperforms the other, and that attribute elimination can be used to derive compact representations for memory-based language learning without noticeable loss of generalization accuracy.
The #-TBL System: Logic Programming Tools for Transformation-Based Learning Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural language processing tasks, such as part-of-speech tagging [Brill, 1995], PPattachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al., 1998] and ellipsis resolution [Hardt, 1998]. Thus, TBL has proved very useful, in many different ways, and is likely to continue to do so in the future.Moreover, since Brill generously made his own TBL implementation publicly available, l many researchers in need of all off-the-shelf retrainable part-of-speech tagger have found what they were looking for. However, although very useful, Brill's original implementation is somewhat opaque, templates are not compositional, IThroughout this paper, when referring to Brill's TBL implementation, it is always his contextual-rule-learnerimplemented in C -that I have in mind. "It is available from http://www, cs. jhu. edu/~brill/, along with ~veral other learners and utility programs. and they are hard-wired into the program. Therefore, the program is difficult to modify and extend. What is more, it is fairly slow.This paper is dedicated to the design and implementation of an alternative transformation-based learner system, called "the #-TBL system" (pronounced "mutable"). The p-TBL system is designed to be theoretically transparent, flexible and efficient. Transparency is achieved by performing a 'logical reconstruction' of TBL, and by deriving the system from there. Flexibility is achieved through the use of a compositional rule and template formalism, and 'pluggable' "algorithms. As for the implementation, it turns out that transformation-based learning can be implemented very straightforwardly in a logic programming language such as Prolog. Efficient indexing of data, unification and backtracking search, as well as established Prolog programming techniques for building rule compilers and meta-interpreters, contribute to the making of a logically transparent, easily extendible, and fairly efficient system. 2 The content of the paper is presented in a bottom-up fashion, starting from the semantics of transformation rules. First, I show that, contrary to what is often assumed, transformation rules can be given a declarative, logical interpretation. I then introduce the IL-TBL system, which in a manner of speaking is derived from this interpretation of rules. The template compiler, a part of the system which translates templates into efficient Prolog programs, is described, and by w~" of examples it is shown how a particular combination of training data and templates may be 'queried' from the Prolog prompt. Next, a number of variants of all-solutions predicates are specified, that deal with notions such as scores, rankings and thresholds. Since they appear to be independently useful -even useful outside TBL -"The ~-TBL system is available from http://~w, ling. gu. se/-~lager/mutbl, html. I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I they belong in a separate library. By combining predicates from these code libraries, a number of TBL-like algorithms are assembled, and benchmarks are run that show the/~-TBL system to be quite efficient. Finally, a small experiment using transformation-based learning to induce Constraint Grammars from corpora is performed. The #u-TBL system represents an attempt to use the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. In the true spirit of logic-programming, the implementation is &apos;derived&apos; from a declarative, logical interpretation of transformation rules. The #-TBL system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including Constraint Grammar dis-ambiguators as well as more traditional &apos;Brill-taggers&apos;. Results from a number of experiments and benchmarks are presented which show that the system is both flex-&quot; ible and efficient.
Learning Transformation Rules to Find Grammatical Relations* An important level of natural language processing is the finding of grammatical relationships such as subject, object, modifier, etc.Such relationships are the objects of study in relational grammar [Perlmutter, 1983]. Many systems (e.g., the KERNEL system ) use these relationships as an intermediate, form when determining the semantics of syntactically parsed text. In the SPARKLE project  grammatical relations form the layer above the phrasal-level in a three layer syntax scheme. Grammatical relationships are often stored in some type of structure like the F-structures of lexicalfunctional grammar [Kaplan, 1994].Our own interest in grammatical relations is as a semantic basis for information extraction in the Alembic system. The extraction approach we are currently investigating exploits grammatical relations as an intermediary between surface syntactic phrases and propositional semantic interpretations. By directly associating syntactic heads with their arguments and modifiers, we are hoping that these grammatical relations will provide a high degree of generality and reliability to the process of composing semantic representations. This ability to The MITRE Corporation 202 Burlington Rd. Bedford, MA 01730 USA {lferro,mbv,asy}@mitre.org "parse" into a semantic representation is according to Charniak [Charniak, 1997, p. 42], "the most important task to be tackled now."In this paper, we describe a system to learn rules for finding grammatical relationships when just given a partial parse with entities like names, core noun m~d verb phrases (noun and verb groups) and semi-accurate estimates of the attachments of prepositions and subordinate conjunctions. In our system, the different entities, attachments and relationships are found using rule sequence processors that are cascaded together. Each processor can be thought of as approximating some aspect of the underlying grammar by finite-state transduction.We present the problem scope of interest to us, as well as the data annotations required to support our investigation. We also present a decision procedure for finding grammatical relationships. In brief, on our training mid test set, our procedure achieves 63.6% recall and 77.3% precision, for an f-score of 69.8. Grammatical relationships are an important level of natural language processing. We present a trainable approach to find these relationships through transformation sequences and-error-driven learning. Our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6% recall and 77.3% precision (f-score = 69.8).
Memory-Based Shallow Parsing Recently, there has been an increased interest in approaches to automatically learning to recognize shallow linguistic patterns in text [Ramshaw and Marcus, 1995, Vilain and Day, 1996, Argamon et al., 1998, Buchholz, 1998, Cardie and Pierce, 1998, Veenstra, 1998, Daelemans et aI., 1999a. Shallow parsing is an important component of most text analysis systems in applications such as information extraction and summary generation.It includes discovering the main constituents of sentences (NPs, VPs, PPs) and their heads, and determining syntactic relationships like subject, object, adjunct relations between verbs and heads of other constituents.Memory-Based Learning (MBL) shares with other statistical and learning techniques the advantages of avoiding the need for manual definition of patterns (common practice is to use hand-crafted regular expressions), and of being reusable for different corpora and sublanguages. The unique property of memory-based approaches which sets them apart from other learning methods is the fact that they are lazy learners: they keep all training data available for extrapolation. All other statistical and machine learning methods are eager (or greedy) learners: They abstract knowledge structures or probability distributions from the training data, forget the individual training instances, and extrapolate from the induced structures. Lazy learning techniques have been shown to achieve higher accuracy than eager methods for many language processing tasks. A reason for this is tile intricate interaction between regularities, subregularities and exceptions in most language data. and the related problem for learners of distinguishing noise from exceptions. Eager learning techniques abstract from what they consider noise (hapaxes, low-frequency events, non-typical events) whereas lazy learning techniques keep all data available, including exceptions which may sometimes be productive. For a detailed analysis of this issue, see [Daelemans et al., 1999a]. Moreover, the automatic feature weighting in the similarity metric of a memory-based learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse [Zavrel and Daelemans, 1997].In this paper, we will provide a empirical evaluation of tile MBL approach to syntactic analysis on a number of shallow pattern learning tasks: NP chunking, \'P clmnking, and the assignment of subject-verb and object-verb relations. The approach is evaluated by cross-validation on the WSJ treebank corpus [Marcus et al., 1993]. We compare the approach qualitatively and as far as possible quantitatively with other approaches. We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules. The experiments reported in this paper show competitive results, the F~=l for the Wall Street Journal (WSJ) treebank is: 93.8% for NP chunking, 94.7% for VP chunking, 77.1% for subject detection and 79.0% for object detection.
MDL-based DCG Induction for NP Identification Identification of Noun Phrases (NPs) in free text has been tackled in a number of ways (for example, [25,9,2]). Usually however, only relatively simple NPs, such as 'base' NPs (NPs that do not contain nested NPs or postmodifying clauses) are recovered. The motivation for this decision seems to be pragmatic, driven in part by a lack of technology capable of parsing large quantities of free text. With the advent of broad coverage grammars (for example [15] and attendant efficient parsers [11], however, we need not make this restriction: we now can identify 'full' NPs, NPs that might contain pre and/or post-modifying complements, in free text.Full NPs m'e more interesting than base NPs to estimate:• They are (at least) context free, unlike base NPs which are finite state. They can contain pre-and post-modifying phrases, and so proper identification can in the worst case imply full-scale parsing/grammar learning.• Recursive nesting of NPs means that each nominal head needs to be associated with each NP. Base NPs simply group all potential heads together in a flat structure.As a (partial) response to these challenges, we identify full NPs by treating the task as a special case of full-scale sentential Definite Clause Grammar (DCG) learning. Our approach is based upon the Minimum Description Length (MDL) principle. Here, we do not explain MDL, but instead refer the reader to the literature (for example, see [26,27,29,12,22]). Although a DCG learning approach to NP identification is far more computationally demanding than any other NP learning technique reported, it does provide a useful test-bed for exploring some of the (syntactic) factors involved with NP identification. By contrast, other approaches at NP identification more usually only consider lexical/part-of-speech influences.In this paper, we consider, from an estimation perspective, how dependent NPs are upon their (surrounding) syntactic context. We varied the information content of the training set and measured the effect this had upon NP identification accuracy. Results suggest that:• Use of any syntactic information, in addition" to raw text during estimation, produces better results than estimation from raw text alone.• NPs containing an internal annotation (nonterminals in addition to NPs) are harder to estimate than NPs that do not contain these additional nonterminals.• Training with NP annotated sentences and training with sentences annotated with full sentential parses produce very similar results to each other.We stress that the last finding is provisional, and further investigation is necessary to verify it. The structure of the rest of this paper is as follows. Section 2 gives an overview of our approach, whilst section 3 goes into estimation and modelling details. We do not start induction ab initio, but instead base estimation upon manually written grammars. Section 4 briefly describes the particular grammar used in this research, whilst section 5 relates our work to others. Section 6 presents an experimental evaluation of our learner. The paper ends with a discussion Of our findings. We introduce a learner capable of automatically extending large, manually written natural language Definite Clause Grammars with missing syntactic rules. It is based upon the Minimum Description Length principle , and can be trained upon either just raw text, or else raw text additionally annotated with parsed corpora. As a demonstration of the learner, we show how full Noun Phrases (NPs that might contain pre or post-modifying phrases and might also be recursively nested) can be identified in raw text. Preliminary results obtained by varying the amount of syntactic information in the training set suggests that raw text is less useful than additional NP bracketing information. However, using all syntactic information in the training set does not produce a significant improvement over just bracketing information.
EACL&apos;99 Computer and Internet Supported Education in Language and Speech Technology Proceedings of a Workshop Sponsored by ELSNET and The Association for Computational Linguistics Workshop Committee: Published by the Association for Computational Linguistics  
Keynote Talk: Diamonds on my Windshield: the Use of Computer-based Instruction in Computational Linguistics  Many disciplines currently have more or less organized activities concerned with developing resources to support Internet-and computer-based instruction. This talk will examine this area, with particular focus on the questions:
A Modern Computational Linguistics Course Using Dutch This paper describes a set of exercises in computational linguistics. The material was primarily developed for two courses: an general introduction to computational linguistics, and a more advanced course focusing on natural language interfaces. Students who enter the first course have a background in either humanities computing or cognitive science. This implies that they possess some general programming skills and that they have at least some knowledge of general linguistics. Furthermore, all students entering the course are familiar with logic programming and Prolog. The native language of practically all students is Dutch.The aim of the introductory course is to provide a overview of language technology applications, of the concepts and techniques used to develop such applications, and to let students gain practical experience in developing (components) of these applications. The second course focuses on computational semantics and the construction of natural language interfaces using computational grammars.Course material for computational linguistics exists primarily in the form of text books, such as Allen (1987), Gazdar and Mellish (1989) and Covington (1994). They focus primarily on basic concepts and techniques (finite state automata, definite clause grammar, parsing algorithms, construction of semantic representations, etc.) and the implementation of toy systems for experimenting with these techniques. If course-ware is provided, it consists of the code and grammar fragments discussed in the text-material. The language used for illustration is primarily English.While attention for basic concepts and techniques is indispensable for any course in this field, one may wonder whether implementation issues need to be so prominent as they are in the text-books of, say, Gazdar and Mellish (1989) and Covington (1994). Developing natural language applications from scratch may lead to maximal control and understanding, but is also timeconsuming, requires good programming skills rather than insight in natural language phenomena, and, in tutorial settings, is restricted to toysystems. These are disadvantages for an introductory course in particular. In such a course, an attractive alternative is to skip most of the implementation issues, and focus instead on what can be achieved if one has the right tools and data available. The advantage is that the emphasis will shift naturally to a situation where students must concentrate primarily on developing accounts for linguistic data, on exploring data available in the form of corpora or word-lists, and on using real high-level tools. Consequently, it becomes feasible to consider not only toy-systems and toyfragments, but to develop more or less realistic components of natural language applications. As the target language of the course is Dutch, this also implies that at least some attention has to be paid to specific properties of Dutch grammar, and to (electronic) linguistic resources for Dutch. Since students nowadays have access to powerful hardware and both tools and data can be distributed easily over the internet, there are no real practical obstacles.Text-books which are concerned primarily with computational semantics and natural language interfaces, such as Pereira and Shieber (1987) and Blackburn and Bos (1998), tend to introduce a toy-domain, such as a geography database or an excerpt of a movie-script, as application area. In trying to develop exercises which are closer to real applications, we have explored the possibilities of using web-accessible databases as back-end for a natural language interface program.More in particular, we hope to achieve the following:• Students learn to use high-level tools. The development of a component for morphological analysis requires far more than what can be achieved by specifying and implementing the underlying finite state automata directly. Rather, abstract descriptions of morphological rules should be possible, and software should be provided to support development and debugging. Similarly, while a programming language such as Prolog offers possibilities for relatively high-level descriptions of natural language grammars, the advant, ages of specialised languages for implementing unification-based grammars and accompanying tools are obvious. Furthermore, the availability of graphical interfaces and visualisation in tutorial situations is a bonus which should not be underestimated.• Students learn to work with real data. In developing practical, robust, wide-coverage, language technology applications, researchers have found that the use of corpora and electronic dictionaries is absolutely indispensable. Students should gain at least some familiarity with such sources, learn how to search large datasets, and how to deal with exceptions, errors, or unclear cases in real data.• Students become familiar with quantitative evaluation methods. One advantage of developing components using real data is that one can use the evaluation metrics dominant in most current computational linguistics research. That is, an implementation of hyphenatiOn-rule or a grammar for temporal expressions can be tested by measuring its accuracy on a list of unseen words or utterances. This provides insight in the difficulty of solving similar problems in a robust fashion for unrestricted text.Students develop language technology components for Dutch. In teaching computational linguistics to students whose native language is not English, it is common practice to fbcus primarily on the question how the (English) examples in the text book can be carried over to a grammar for one's own language. As this may take considerable time and effort, more advanced topics are usually skipped. In a course which aims primarily at Dutch, and which also contains material describing some of the peculiarities of this language (hyphenation rules, spelling rules relevant to morphology, word order in main and subordinate clauses, verb clusters), there is room for developing more elaborate and extended components.Students develop realistic applications. The use of tools and real data makes it easier to develop components which are robust and which have relatively good coverage. Applications in the area of computational semantics can be made more interesting by exploiting the possibilities offered by the internet. The growing amount of information available on the internet provides opportunities for accessing much larger databases (such as public transport time-tables or library catalogues), and therefore, for developing more realistic applications.The sections below are primarily concerned with a number of exercises we have developed to achieve the goals mentioned above. A accompanying text is under development. 1 This paper describes material for a course in computational linguistics which concentrates on building (parts of) realistic language technology applications for Dutch. We present an overview of the reasons for developing new material, rather than using existing textbooks. Next we present an overview of the course in the form of six exercises, covering advanced use of finite state methods, grammar development, and natural language interfaces. The exercises emphasise the benefits of special-purpose development tools, the importance of testing on realistic data-sets, and the possibilities for web-applications based on natural language processing.
Web tools for introductory computational linguistics  We introduce a notion of training methodology space (TM space) for specifying training methodologies in tile different disciplines and teaching traditions associated with computational linguistics and the human language technologies, and pin our approach to the concept of operational model; we also discuss different general levels of interactivity. A number of operational models are introduced , with web interfaces for lexical databases, DFSA matrices, finite-state phonotactics development, and DATR lexica. 1 Why tools for CL training? In computational linguistics, a number of teaching topics and traditions meet; for example: • tbrmal mathematical training, • linguistic argumentation using sources of independent evidence, • theory development and testing with empirical models, • corpus processing with tagging and statistical classification. Correspondingly, teachers&apos; expectations and teaching styles vary widely, and, likewise, students&apos; expectations and accustomed styles of learning are very varied. Teaching methods and philosophies fluctuate, too, between more behaviouristic styles which are more characteristic of practical subjects, and the more rationalistic styles of traditional mathematics training; none, needless to say, covers the special needs of all subjects. Without specifying the dimensions in detail , let us call this complex field training method space (TM space). The term training is chosen because it is neutral between teaching and learning, and implies the intensive acquisition of both theoretical and practical abilities. Let us assume, based on the variations outlined above, that we will need to navigate this space in sophisticated ways, but as easily as possible. What could be at the centre of TM space? As the centre of TM space, let us postulate a model-based training method, with the following properties: 1. The models in TM space are both formal, and with operational, empirical interpretations. 2. The empirical interpretations of models in TM space are in general operational models implemented in software. 3. The models in TM space may be under-stod by different users from several different perspectives: from the point of view of the mathematician, the programmer, the software user etc., like &apos;real life pro-grammes&apos;. 4. Typical lingware and software models are grammars, lexica, annotated corpora, op-erationalised procedures, parsers, compilers ; more traditional models are graphs, slides, blackboards, three-dimensional block or ball constructions, calculators. Why should operational models, in the sense outlined here, be at the centre of TM
Intranet learning tools for NLP Networked computers can be used to support learning in various ways. In computational linguistics, the predominant pattern of use is twofold: Learning materials are distributed using hypertext, and laboratories are conducted in which students work directly with computational linguistics processors such as parsers and generators.The 'authorware' approach to developing learning materials has not been popular in the teaching of computational linguistics because of the extensive labour involved in encoding content. Since CL is all about the use of powerful general mechanisms and expressive formalisms, the idea of writing learning materials using less expressive tools has little appeal.However, the new technologies of the internet make it easier to combine media to produce integrated learning environments in which pedagogical materials can be intimately connected to mechanisms and resources.Using such approaches can produce payoffs whether or not distance learning is involved. A better integrated set of resources for laboratory activities makes fewer demands on support staff such as graduate demonstrators. The ability to encapsulate mechanisms and tools in applets also means that the need to maintain special purpose laboratories is diminished, and it is also possible to promote CL to potential students in schools.This paper reports experience with the use of web browsers to provide practical activities to an introductory class of computational linguistics students. We concentrate on the tools developed locally, although we make use of others where appropriate. Much of the discussion focuses on what is possible with the constraints imposed by current network software. This paper describes experience with the developed of tools for CL education using Java. Some are standalone Java applets and others are clients which connect to a parsing server using a LISP-based backend. The principal benefits are platform independence and reusability rather than worldwide web access, although intranet technology reduces the need for special purpose labs.
Interactive Auditory Demonstrations  The subject matter of speech and hearing is packed full of phenomena and processes which lend themselves to or require auditory demonstration. In the past, this has been achieved through passive media such as tape or CD (e.g. Houtsma et ai, 1987; Bregman &amp; Ahad, 1995). The advent of languages such as MATLAB which suppor!s sound handling, modern interface elements and powerful signal processing routines, coupled with the availability of fast processors and ubiquitous soundcards allows tbr a more interactive style of demonstration. A significant effort is now underway in the speech and hearing community to exploit these favourable conditions (see the MATISSE proceedings (1999), for instance). Excitingly, it is now possible to allow exploratory access to part or all of the parameter space underlying each phenomenon. Over the past 18 months, more than 20 interactive auditory demonstrations have been produced at Shef-field as part of an ongoing project to provide teaching material for the diverse disciplines which contribute to speech and hearing. Many of the demonstrations are suitable for undergraduate courses, while others encode phenomena which are primarily of interest to researchers. The motivation for and design ethos behind this project has been described previously in Cooke &amp; Brown (1999) and Wrigley, Cooke &amp; Brown (1999). In this extended abstract, a gallery of screenshots which focus on the auditory (as opposed to speech) demonstrations is provided. The aim is to show the breadth of what is possible in a relatively short time and to encourage others to produce similar tools. The demonstrations can be freely downloaded via http://www.dcs.shef.ac.uk/-martin.
Web Access to Corpora: the W3Corpora Project* In this day an age, some corpus linguistics should be part of every course to do with language. But learning about corpus linguistics --its possibilities and limitations --is not just a matter of acquiring information. The best way to learn about corpus linguistics is to do it, and the best way to teach corpus linguistics is to put students into a position where they can do it ( (Leech, 1997), (Fligelstone, 1993)). This requires corpora, and tools, in addition to teaching materials. For a number of reasons, the World Wide Web offers a good method for delivering this (see below). This paper will present a resource that enables students to get a general introduction to corpus linguistics via the Web. The resource is currently available for general use. See Table 1 for URLs.No very great claims will be made for the resource in terms of being highly original or visionary in style of interaction or implementation. On the contrary, the model of learning is rather traditional, and the approach taken was very simple and straightforward. However, this in itself may be interesting as providing a baseline against which more visionary approaches can be compared --this is probably the simplest way one could go about providing Internet based education. In addition, some of the design decisions and lesson learned may be of interest.Section 2 presents the motivation for the project that produced the resource. Section 3 will give an Tile project was the joint work of Ylva Berglund, Natalia Brines-Moya, Martin Rondell and the author in the period 1996-8. The results can be seen at:http://clwww.essex.ac.uk/w3c/. The project was funded by JISC (the Joint Information Systems Committee of the UK Higher Education Funding Councils), as part of JTAP, the JISC Technonology Application Programme. Thanks also to the anonymous workshop referees for valuable comments. None of this shifts responsibility for errors and other imperfections fronl Inc. overview of the resource. Section 4 describes and compares some similar resources that are available. Section 5 describes some problems and lessons that can be learned, and notes some open questions. 
A Corpus-Based Grammar Tutor for Education in Language and Speech Technology  We describe work in progress on a corpus-based tutoring system for edu-cat, ion in traditional and formal grammar. It is mainly intended for language and speech technology students and gives them the opportunity to learn grammar and grammatical analysis from authentic language material. The exercises offered by the system are based on pedagogically adapted versions of formalisms and tools that are likely to be of relevance to the students also later in their professional life. The system will be continuously evaluated in university-level courses, both in order to assess its effectiveness as a learning aid and to provide guidance in its further development.
An Open Distance Learning Web-Course for NLP in IR There are two traditional distinctions in higher education that are currently on a period of big debate. The first one concerns industrial training vs higher education objectives, the second one is related to the role of emerging technologies for distance learning, potentially blurring the until now clear separation between conventional universities and distance learning Institutions.Changes in both Education and Training contexts are moving closer their previously separate objectives. On one hand, there is much concern about bringing educational curricula more in line with vocational needs. In a variety of disciplines, higher education courses are becoming oriented to professionally recognized qualifications, adopting approaches to integrate practice in context. On the other hand, industrial organizations are looking for a more versatile way of building personal profiles to adapt individuals to the changing needs of their organizations. Thus, acquiring more general skill is been increasingly addressed for advanced training purposes.Distance learning Universities are operating since the seventies. Most of them were based on the industrial model characterized by the production of highly effective learning materials for independent study and the use of "one-way" media, such as print, video, radio or TV broadcasting. While in traditional education the cost of education depends on the number of students involved, this is not the case with the industrial distance education model. The cost depends on a fixed part for the preparation of materials with less investment in academic staff for tutoring tasks. A clear improvement of using new technology for this model has been the delivery of course materials through CD-ROM, and later on through the Internet. Computerbased materials can integrate presentations with simulations, problem solving tools, virtual laboratories and the like, to engage students in a process of active learning. More controversial is the embedding of human-human interactive technologies. It is an open challenge to find new ways of satisfying increasing interaction between students and tutors when staff resources remain scarce. Peer collaboration is an idea to explore, but as it is the case in many industrial organizations collaborative behavior does not happen spontaneously just because the technology is available, but rather is a shift of culture to be established.The virtual campus [1] is a metaphor currently deserving a lot of attention. It is sometimes presented as a bridge for conventional universities tO extend their scope to reach distance learners, sometimes as an opportunity for distance learning institutions to provide an environment combining the strength of systematic teaching with elaborated materials to support more!efficiently distance study. But metaphors should be carefully contrasted with the demand side perspective to foresee whether technology-baged distance teaching can succeed. For example, 'there is, despite the quite short history of desl~top computer conferencing, some failed pilot experiences in real-time multipoint teleteaching. The cost of equipment was a wellknown factor,, but another very practical issue was often neglected: real distance students are in fact fully reluctant to participate regularly in synchronous events, with a fixed schedule. The existing technology has demonstrated to be powerful. Payilng due regard to usability issues in order to realize its potential for learning purposes remains an open question.Over the last decade the European Commission has launched a variety of programs to encourage international partnership and cooperation in the field of education and training. Some of the programs aimed at analyzing the current situation to recommend future action. Others were projects strongly technology oriented to develop platforms and environments specially focused on distance and flexible learning. In addition, a Set of pilot experiences and applications were also implemented.Within the :SOCRATES framework [2], ACO*HUM [3] is a thematic network including a working group on Computational linguistics and language engineering. A plan for developing open distance learning pilot courses was proposed in cooperation with ELSNET [4], and finally a proposal including six pilots was launched in February 98. We have developed one of them, on the topic of Information Retrieval (IR) and Natural Language Processing (NLP) [5].This topic is! especially well suited for a learning-by-doing web course. The Internet itself is the biggest IR testbed, and the web search engines are the most powerful applications of IR techniques. The students can be guided through on-line NLP software to manipulate, expand, translate queries, etc., and get first hand impressions on the utility of such processes.Next section discusses our approach while a detailed presentation is given in section three. A computer-based course addressing the topic of&quot; applying Natural Language resources and techniques to Information Retrieval is presented. The course provides several Internet on-line resources to support a learning by doing approach in a real world context. Rationale for the design of the course is presented and a detailed description of the course structure and content is given.
Unsupervised Learning in Natural Language Processing Proceedings of the Workshop Published by the Association for Computational Linguistics  
Hiding a Semantic Hierarchy in a Markov Model We describe here an approach to inducing selectional preferences from text corpora. In the traditional view, a predicate constrains its arguments by selecting for particular semantic classes, or concepts. Selectional restriction of the traditional sort can be characterized as a relation p(v, r, c) over predicates v, syntactic roles r, and argument concepts c. Individual instances (v, r, c) are selectional tuples. Ex of preference of v for c with respect to role r. Positive degrees of preference are intended to correlate with intuitive judgments of "plausibility" or "typicality," and negative judgments are intended to correlate with intuitive judgments of "implausibility."We have chosen to characterize such selectional preference as a side-effect of a stochastic model for generating what we will call co-occurrence tuples:triples (v, r, n) for v a predicate, r a syntactic role, and n the headword of the argument filling the role r with respect to v. An example of a co-occurrence tuple is (splatter, obj, water). Co-occurrence tuples can be obtained from text corpora, and can be used to make inferences about the probability of selectional tuples. For example, the co-occurrence tuple (splatter, obj, water) may be taken as evidence for the selectional tuple (splatter, obj, FLUID). More concretely, such co-occurrence tuples make up the training corpora, from which we train our stochastic models.For this study, we have used the British National Corpus (100M words), from which we have extracted co-occurrence tuples using the Cass parser (Abney, 1997). By way of illustration, table 2 shows the values of n in tuples (eat, obj, n) along with their frequencies in the corpus. This "subcorpus" would be used to train a stochastic model specific to the object role of the verb eat and is the first of two inputs to our induction process.There are two problems with such training data: it is noisy and it contains ambiguity. The noise is sometimes due to tagging or parsing errors, and sometimes due to metaphorical uses. Examples from   meat   45 bucket  1 ice  2  tape  1 investment 1 soup  2  proportion 2 kitchen  1 fry  4  root  4 salad  2 top  1  bread  14 feast  1 scrap  2  majority 2 sauce  1 sugar  1  principle 1 food  77 hole  2  roll  4 pack  1 bag  2  race  1 mouthful 3 dinner  11  sheep  1 salt  1 meal  46  trout  2 pasta  1 slice  7  dish  2 spaghetti 6 chicken  5  stick 1 egg 18 average 1 sandwich 13 yogurt 1 mustard 1 breakfast 30 garlic 1 However, note that the "good" examples such as food and meal are much greater in number and frequency'.Thus, the signal is stronger than the noise in most cases and most reasonably robust training methods will be able to handle the noise. The second problem, that of word sense ambiguity, is more difficult. The word bread in table 2 provides an example. Bread can be used to refer to a food, e.g., the multigrain bread in Germany is wonderlul, but it can also refer to money, e.g., I could really use some bread since my car just broke down.For this reason, it is not immediately clear which concepts the 14 tokens of bread provide evidence for.If the wrong choice is made for a high frequency word, incorrect selectional preferences will result. The model we propose represents this sort of uncertainty in a natural way: the two senses of bread are represented as different paths through a stochastic model, both of which generate the same observation. This stochastic model is a hidden Markov model (HMM) which has the shape of a given semantic hierarchy. Figure 1 shows an example hierarchy. In the work discussed here, we made use of the WordNet semantic hierarchy (Miller, 1990). This hierarchy is the second input to our induction process. We hoped that the forward-backward algorithm, an EM algorithm, would properly disambiguate word senses in the training data as a side effect of its quest to maximize the likelihood of the training data given the model. However, for reasons we will discuss in section 4, this was not the case.In the following section we discuss work on selectional preference induction that also assumes as input (i) subcorpora corresponding to predicate role pair and (ii) a semantic class hierarchy. Then we formally define our stochastic model. Next we look at a number of ultimately unsuccessful attempts to modify the forward-backward algorithm to perform effective word-sense disambiguation of the training data. Despite these problems we did obtain some encouraging results which we present at the end of the paper. We introduce a new model of selectional preference induction. Unlike previous approaches , we provide a stochastic generation model for the words that appear as arguments of a predicate. More specifically , we define a hidden Markov model with the general shape of a given semantic class hierarchy. This model has a number of attractive features, among them that selectional preference can be seen as distributions over words. Initial results are promising. However, unsupervised parameter estimation has proven problematic. A central problem is word sense ambiguity in the training corpora. We describe attempts to modify the forward-backward algorithm, an EM algorithm, to handle such disam-biguation. Although these attempts were unsuccessful at improving performance, we believe they give insight into the nature of the bottlenecks and into the behavior of the EM algorithm.
The applications of unsupervised learning to Japanese grapheme-phoneme alignment The objective of this paper is to analyse the applicability of statistical and learning methods to automated grapheme-phoneme alignment in Japanese, without reliance on pre-annotated training data or any form of supervision. The two principal models proposed herein are a simple statistical model nonreliant on learning techniques, and an incremental learning method deriving therefrom, incorporating automated "pseudo-supervision" drawing on prior alignments. The incremental learning method selects a single alignment candidate to accept at each iteration, and adjusts the statistical model accordingly to aid in the subsequent disambiguation of residue G-P tuples.Grapheme-phoneme ("G-B") alignment is defined as the task of maximally segmenting a grapheme compound into morpho-phonic units, and aligning each unit to the corresponding substring in the phoneme compound ( Bilac et al., 1999). Its main use is in portrayal of the phonological interaction between adjoining grapheme segments, and also implicit description of the range of readings each grapheme segment can take. We further suggest that a large-scale database of maximally aligned G-P tuples has applications within the more conventional task of G-P translation (Klatt, 1987;Huang et al., 1994;Divay and Vitale, 1997).Our particular interest in developing a database of G-P tuples is to apply it in the development of a kanji tester which can dynamically predict plausibly incorrect readings for a given grapheme string. For this purpose, we require as great a coverage of grapheme strings as possible, and the proposed system has thus been designed to exhaustively align the input set of G-P tuples, sacrificing precision for 100% recall.'Grapheme string' in this research refers to the maximal kanji representation of a given word or compound, and 'phoneme string' refers to the kana (hiragana and/or katakana) mora correlate. 1 By 'maximal' segmentation is meant that the grapheme string must be segmented to the degree that each segment corresponds to a self-contained component of the phonemic description of that compound, and that no segment can be further segmented into aligning sub-segments. The statement of 'maximality' of segmentation is qualified by the condition that each segment must constitute a morpho-phonic unit, in that for conjugating parts-of-speech, namely verbs and adjectives, the conjugating suffix must be contained in the same segment as the stem. By way of illustration of the alignment process, let us consider the example of the verb ka-n-syasu-ru i~--~-su-ru] "to thank/be thankful",2 a portion ot the 35 member alignment paradigm for which is given in Fig. 1. The importance of maximality of alignment is observable by way of align35, which constitutes a legal (under-)alignment of the correct solution in align1. Here, there is scope for further segmentation, as evidenced by the replaceability of by its phoneme content of ka-n in isolation of (producing the string ka-n-=~-su-ru). Thus, we are able to discount align35 on the grounds of it being non-maximal. That a segment exists between sya and su-ru, on the other hand, is a result of su-ru being a light verb and hence an independent morpheme.The overall alignment procedure is depicted in 1Our description of kana as phoneme units represents a slight abuse of terminology, in that individual kana characters are uniquely associated with a broad phonetic transcription potentially extending over multiple phones. Note, however, that in abstracting away to this meta-phonemic representation, we are freed from consideration of low-level phonological concerns such as phoneme connection constraints.2So as to make this paper as accessible as possible to readers not familiar with Japanese, hiragana and katakana characters have been transliterated into Latin script throughout this paper and are essentially treated as being identical. The graphemic kanji character set, on the other, has been provided in its original form to give the reader a feel for the significance of the kana-kanji dichotomy. For both the grapheme and phoneme strings, character boundaries are indicated by "-" and segment boundaries (which double as character boundaries) indicated by "®".ka-n-sya-su-ru In this paper, we adapt the TF-IDF model to the Japanese grapheme-phoneme alignment task, by way of a simple statistical model and an incremen-tal learning method. In the incremental learning method, grapheme-phoneme alignment paradigms are disambiguated one at a time according to the relative plausibility of the highest scoring alignment schema, and the statistical model is retrained accordingly. On limited evaluation, the learning method achieved an accuracy of 93.28%, representing a slight improvement over a baseline rule-based method.
Dual Distributional Verb Sense Disambiguation with Small Corpora and Machine Readable Dictionaries* Much recent research in the field of natural language processing has focused on an empirical, corpusbased approach, and the high accuracy achieved by a corpus-based approach to part-of-speech tagging and parsing has inspired similar approaches to word sense disambiguation. For the most successful approaches to such problems, correctly annotated materials are crucial for training learning-based algorithms. Regardless of whether or not learning is involved, the prevailing evaluation methodology requires correct test sets in order to rigorously assess the quality of algorithms and compare their performance. This seems to require manual tagging of the training corpus with appropriate sense for each occurrence of an ambiguous word. However, in marked contrast to annotated training material for part-of-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas  part-of-speech tag sets tend to differ in the detail); (b) sense annotation has a comparatively high error rate (Miller, personal communication, reports an upper bound for human annotators of around 90~ for ambiguous cases, using a non-blind evaluation method that may make even this estimate overly optimistic (Resnik, 1997)); (c) in conclusion, a sense-tagged corpus large enough to achieve broad coverage and high accuracy word sense disambiguation is not available at present. This paper describes an unsupervised sense disambiguation system using a POS-tagged corpus and a machine-readable dictionary (MRD). The system we propose circumvents the need for the sense-tagged corpus by using MRD's usage examples as the sense-tagged examples. Because these usage examples show the natural examples for headword's each sense, we can acquire useful sense disambiguation context from them. For example, open has several senses and usage examples for its each sense listed in a dictionary as shown in Table  1. The words within usage examples window, door, box, con#fence, and meeting are useful context for sense disambiguation of open. Another problem that is common for much corpusbased work is data sparseness, and the problem especially severe for work in WSD. First, enormous amounts of text are required to ensure that all senses of a polysemous word are represented, given the vast disparity in frequency among senses. In addition, the many possible co-occurrences for a given polysemous word are unlikely to be found in even a very large corpus, or they occur too infrequently to be significant. In this paper, we propose two methods that attack the problem of data sparseness in W~ using small corpus and dictionary. First, extendi word similarity measures from direct co-occurren, to co-occurrences of co-occurred words, we compl the word similarities using not co-occurred woJ but co-occurred clusters. Second, we acquire IS relations of nouns from the MRD definitions. D tionary definitions of nouns are normally written such a way that one can identify for each headw( (the word being defined), a "genus term" (a w( more general that the headword), and these are lated via an IS-A relation (Amsler, 1979). It is po~, ble to cluster the nouns roughly by the identificati of the IS-A relationship. This paper presents a system for unsupervised verb sense disambiguation using small corpus and a machine-readable dictionary (MRD) in Korean. The system learns a set of typical usages listed in the MRD usage examples for each of the senses of a polysemous verb in the MRD definitions using verb-object co-occurrences acquired from the corpus. This paper concentrates on the problem of data sparseness in two ways. First, extending word similarity measures from direct co-occurrences to co-occurrences of co-occurred words, we compute the word similarities using not co-occurred words but co-occurred clusters. Second, we acquire IS-A relations of nouns from the MRD definitions. It is possible to cluster the nouns roughly by the identification of the IS-A relationship. By these methods, two words may be considered similar even if they do not share any words. Experiments show that this method can learn from very small training corpus, achieving over 86% correct disambiguation performance without a restriction of word&apos;s senses. 1 Introduction Much recent research in the field of natural language processing has focused on an empirical, corpus-based approach, and the high accuracy achieved by a corpus-based approach to part-of-speech tagging and parsing has inspired similar approaches to word sense disambiguation. For the most successful approaches to such problems, correctly annotated materials are crucial for training learning-based algorithms. Regardless of whether or not learning is involved, the prevailing evaluation methodology requires correct test sets in order to rigorously assess the quality of algorithms and compare their performance. This seems to require manual tagging of the training corpus with appropriate sense for each occurrence of an ambiguous word. However, in marked contrast to annotated training material for part-of-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas * This work was supported in part by KISTEP for Soft Science Research project. headword : open 2 sense usage examples open Open the window a bit, please. He opened the door for me to come in. Open the box. start Our chairman opened the conference by welcoming new delegates/ Open a public meeting. Table 1: The entry of open(vt.) in OALD part-of-speech tag sets tend to differ in the detail); (b) sense annotation has a comparatively high error rate (Miller, personal communication, reports an upper bound for human annotators of around 90~ for ambiguous cases, using a non-blind evaluation method that may make even this estimate overly optimistic(Resnik, 1997)); (c) in conclusion, a sense-tagged corpus large enough to achieve broad coverage and high accuracy word sense disambigua-tion is not available at present. This paper describes an unsupervised sense disambiguation system using a POS-tagged corpus and a machine-readable dictionary (MRD). The system we propose circumvents the need for the sense-tagged corpus by using MRD&apos;s usage examples as the sense-tagged examples. Because these usage examples show the natural examples for headword&apos;s each sense, we can acquire useful sense disambiguation context from them. For example , open has several senses and usage examples for its each sense listed in a dictionary as shown in Table 1. The words within usage examples window, door, box, con#fence, and meeting are useful context for sense disambiguation of open. Another problem that is common for much corpus-based work is data sparseness, and the problem especially severe for work in WSD. First, enormous amounts of text are required to ensure that all senses of a polysemous word are represented, given the vast disparity in frequency among senses. In addition, the many possible co-occurrences for a given polyse-mous word are unlikely to be found in even a very large corpus, or they occur too infrequently to be significant. In this paper, we propose two methods 17
Unsupervised learning of derivational morphology from inflectional lexicons Development of electronic morphological resources has undergone several decades of research. The first morphologicM analyzers focussed on inflectional processes (inflection, for English, mainly covers verb conjugation, and number and gender variations). With the development of Information Retrieval, people have looked for ways to build simple analyzers which are able to recognize the stem of a given word (thus addressing both inflection and derivation1). These analyzers are known as stemmers.Faced with the increasing demand for natural language processing tools for a variety of languages, people have searched for procedures to (semi-)automatically acquire morphological resources. On the one hand, we find work fl'om the IR community aimed at building robust stemmers without much attention given to the morphologicM processes of a language. Most of this work relies on a list of affixes, usually built by the system developer, and a set of rules to stem words (Lovins, 1968;Porter, 1980). Some of these works fit within an unsupervised setting, (Hafer and Weiss, 1974;Adamson and Boreham, 1974) and to a certain extent (Jacquemin and Tzoukerman, 1997), but do not directly address the problem of learning naorphological processes. On the other hand, some researchers from the computational linguistics community have developed techniques to learn affixes of a language and software to segment words according to the identified elements. The work described in ( Daelemans et al., 1999) is a good example of this trend, based on a supervised 1The distinction between inflectional and derivational morphology is fax from clearcut. However, in practice, such a distinction allows one to divide the problems at hand and was implicitly adopted in our lexicon development plan. learning approach. However, it is difficult ill most of these studies to infer the underlying linguistic framework assumed.We present in this paper an unsupervised method to learn suffixation operations of a language from an inflectional lexicon. This method also leads to the development of a stemming procedure for the language under consideration. Section 2 presents the linguistic view we adopt on derivation. Section 3 describes the preliminary steps of our learning method and constitutes the core of our stemming procedure. Finally, section 4 describes the learning of suffixation operations. We present in this paper an unsupervised method to learn suffixes and suffixation operations from an inflectional lexicon of a language. The elements acquired with our method are used to build stemming procedures and can assist lexicographers in the development of new lexical resources.
Resolving Translation Ambiguity using Non-parallel Bilingual Corpora Choosing the correct translation of a content word in context, referred to as "translation disambiguation (of content word)", is a key task in machine translation. It is also crucial in cross-language text processing including cross-language information retrieval and abstraction.Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora ( Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area ( Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998).A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful information in the source language. For example, suppose the proper name, "Dodgers", provides good context to identify the usage of "hit" in the training corpus in English. If the translation of "Dodgers" rarely occurs in the target language corpora, it does not contribute to target word selection.The method presented in this paper solves this problem by choosing the target word that corresponds to the usage identified in the source language corpora. This method is totally unsupervised in the sense that it acquires disambiguation information from non-parallel bilingual corpora (preferably in the same domain) free from tagging.It combines two unsupervised disambiguation algorithms: one is the word sense disambiguation algorithm based on distributional clustering( Schuetze, 1997) and the other is the translation disambiguation algorithm using target language corpora (Kikui, 1998). For the given word in context, the former algorithm identifies its usage as one of several predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that best expresses the usage.The following sections are organized as follows. In Section 2, we overview the entire method. The following two sections (i.e., Section 3 and 4) then introduce the two major components of the method including the two unsupervised disambiguation algorithms. Section 5 and 6 are devoted respectively to a preliminary evaluation and discussions on related research.2 Overview of the method Figure 1 shows an overview of the entire method.Components inside the dotted line on the left represent word-sense disambiguation (WSD) in the source language. There are two sub-processes: distributional clustering and categorizing. The former automatically identifies different usages (or senses) of the given source word (shown at top center) in the  source language corpus and creates a profile, referred to as the "sense profile" for each class. The categorization process chooses the profile most relevant to the input word whose sense is implicitly given by its surrounding context. Located to the right is what we call the sense.. translation linking process. It is responsible for as--sociating each semantic profile with the most likely translation of the source word (for which the seman-. tic profile is derived). The result of this process is registered in the sense-$ranslation table.The table look-up process, bottom center, simply retrieves the translation associated with the sense identified by the categorization process. This paper presents an unsupervised method for choosing the correct translation of a word in context. It learns disambiguation information from non-parallel bilinguM corpora (preferably in the same domain) free from tagging. Our method combines two existing unsupervised disambiguation algorithms: a word sense disam-biguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpora. For the given word in context, the former algorithm identifies its meaning as one of a number of predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments.
A Computational Approach to Deciphering Unknown Scripts With surprising frequency, archaeologists dig up documents that no modern person can read. Sometimes the written characters are familiar (say, the Phoenician alphabet), but the language is unknown. Other times, it is the reverse: the written script is unfamiliar but the language is known. Or, both script and language may be unknown.Cryptanalysts also encounter unreadable documents, but they try to read them anyway. With patience, insight, and computer power, they often succeed. Archaeologists and linguists known as epigraphers apply analogous techniques to ancient documents. Their decipherment work can have many resources as input, not all of which will be present in a given case: (1) monolingual inscriptions, (2) accompanying pictures or diagrams, (3) bilingual inscriptions, (4) the historical record, (5) physical artifacts, (6) bilingual dictionaries, (7) informal grammars, etc.In this paper, we investigate computational approaches to deciphering unknown scripts, and report experimental results. We concentrate on the following case:• unfamiliar script• known language• minimal input (monolingual inscriptions only)This situation has arisen in many famous cases of decipherment--for example, in the Linear B documents from Crete (which turned out to be a "non-Greek" script for writing ancient Greek) and in the Mayan documents from Mesoamerica. Both of these cases lay unsolved until the latter half of the 20th century (Chad- wick, 1958;Coe, 1993).In computational linguistic terms, this decipherment task is not really translation, but rather text-to-speech conversion. The goal of the decipherment is to "make the text speak," after which it can be interpreted, translated, etc. Of course, even after an ancient document is phonetically rendered, it will still contain many unknown words and strange constructions. Making the text speak is therefore only the beginning of the story, but it is a crucial step.Unfortunately, current text-to-speech systems cannot be applied directly, because they require up front a clearly specified sound/writing connection. For example, a system designer may create a large pronunciation dictionary (for English or Chinese) or a set of manually constructed character-based pronunciation rules (for Spanish or Italian). But in decipherment, this connection is unknown! It is exactly what we must discover through analysis. There are no rule books, and literate informants are long-since dead. We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time.
Detecting Sub-Topic Correspondence through Bipartite Term Clustering  
Text Classification by Bootstrapping with Keywords, EM and Shrinkage When provided with enough labeled training examples, a variety of text classification algorithms can learn reasonably accurate classifiers (Lewis, 1998;Joachims, 1998;Yang, 1999;Cohen and Singer, 1996). However, when applied to complex domains with many classes, these algorithms often require extremely large training sets to provide useful classification accuracy. Creating these sets of labeled data is tedious and expensive, since typically they must be labeled by a person. This leads us to consider learning algorithms that do not require such large amounts of labeled data.While labeled data is difficult to obtain, unlabeled data is readily available and plentiful. Castelli and Cover (1996) show in a theoretical framework that unlabeled data can indeed be used to improve classification, although it is exponentially less valuable than labeled data. Fortunately, unlabeled data can often be obtained by completely automated methods. Consider the problem of classifying news articles: a short Perl script and a night of automated Internet downloads can fill a hard disk with unlabeled examples of news articles. In contrast, it might take several days of human effort and tedium to label even one thousand of these.In previous work ) it has been shown that with just a small number of labeled documents, text classification error can be reduced by up to 30% when the labeled documents are augmented with a large collection of unlabeled documents. This paper considers the task of learning text classifiers with no labeled documents at all. Knowledge about the classes of interest is provided in the form of a few keywords per class and a class hierarchy. Keywords are typically generated more quickly and easily than even a small number of labeled documents. Many classification problems naturally come with hierarchically-organized classes. Our algorithm proceeds by using the keywords to generate preliminary labels for some documents by term-matching. Then these labels, the hierarchy, and all the unlabeled documents become the input to a bootstrapping algorithm that produces a naive Bayes classifier.The bootstrapping algorithm used in this paper combines hierarchical shrinkage and ExpectationMaximization (EM) with unlabeled data. EM is an iterative algorithm for maximum likelihood estimation in parametric estimation problems with missing data. In our scenario, the class labels of the documents are treated as missing data. Here, EM works by first training a classifier with only the documents  Figure 1: A subset of Cora's topic hierarchy. Each node contains its title, and the five most probable words, as calculated by naive Bayes and shrinkage with vertical word redistribution ( Hofmann and Puzicha, 1998). Words among the initial keywords for that class are indicated in plain font; others are in italics.preliminarily-labeled by the keywords, and then uses the classifier to re-assign probahilistically-weighted class labels to all the documents by calculating the expectation of the missing class labels. It then trains a new classifier using all the documents and iterates. We further improve classification by incorporating shrinkage, a statistical technique for improving parameter estimation in the face of sparse data. When classes are provided in a hierarchical relationship, shrinkage is used to estimate new parameters by using a weighted average of the specific (but unreliable) local class estimates and the more general (but also more reliable) ancestors of the class in the hierarchy. The optimal weights in the average are calculated by an EM process that runs simultaneously with the EM that is re-estimating the class labels.Experimental evaluation of this bootstrapping approach is performed on a data set of thirty-thousand computer science research papers. A 70-leaf hierarchy of computer science and a few keywords for each class are provided as input. Keyword matching alone provides 45% accuracy. Our bootstrapping algorithm uses this as input and outputs a naive Bayes text classifier that achieves 66% accuracy. Interestingly, this accuracy approaches estimated human agreement levels of 72%.The experimental domain in this paper originates as part of the Ra research project, an effort to build domain-specific search engines on the Web with machine learning techniques. Our demonstration system, Cora, is a search engine over computer science research papers ). The bootstrapping classification algorithm described in this paper is used in Corn to place research papers into a Yahoo-like hierarchy specific to computer science. The-search engine, including this hierarchy, is publicly available at www. cora.justresearch, com. When applying text classification to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance. This paper presents an alternative approach to text classification that requires no labeled documentsi instead, it uses a small set of keywords per class, a class hierarchy and a large quantity of easily-obtained unlabeled documents. The keywords are used to assign approximate labels to the unlabeled documents by term-matching. These preliminary labels become the starting point for a bootstrap-ping process that learns a naive Bayes clas-sifier using Expectation-Maximization and hierarchical shrinkage. When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy , the keywords alone provide 45% accuracy. The classifier learned by bootstrap-ping reaches 66% accuracy, a level close to human agreement.
Unsupervised Lexical Learning with Categorial Grammars In this paper we discuss a potential solution to two problems in Natural Language Processing (NLP), using a combination of statistical and symbolic machine learning techniques. The first problem is learning the syntactic roles, or categories, of words of a language i.e. learning a lexicon. Secondly, we discuss a method of annotating a corpus with parses.The aim is to learn Categorial Grammar (CG) lexicons, starting from a set of lexical categories, the functional application rules of CG and an unannotated corpus of positive examples. The CG formalism (discussed in Section 2) is chosen because it assigns distinct categories to words of different types, and the categories describe the exact syntactic role each word can play in a sentence.This problem is similar to the unsupervised part of speech tagging work of, for example, Brill (Brill, 1997) and Kupiec (Kupiec, 1992). In Brill's work a lexicon containing the parts of speech available to each word is provided and a simple tagger attaches a complex tag to each word in the corpus, which represents all the possible tags that word can have. Transformation rules are then learned which use the context of a word to determine which simple tag it should be assigned. The results are good, generally achieving around 95% accuracy on large corpora such as the Penn Treebank.Kupiec (Kupiec, 1992) uses an unsupervised version of the Baum-Welch algorithm, which is a way of using examples to iteratively estimate the probabilities of a Hidden Markov Model for part of speech tagging. Instead of supplying a lexicon, he places the words in equivalence classes. Words in the same equivalence class must take one of a specific set of parts of speech. This improves the accuracy of this algorithm to about the same level as Brill's approach.In both cases, the learner is provided with a large amount of background knowledge -either a complete lexicon or set of equivalence classes. In the approach presented here, the most that is provided is a small partial lexicon. In fact the system learns the lexicon.The second problem -annotating the corpus -is solved because of the approach we use to learn the lexicon. The system uses parsing to determine which are the correct lexical entries for a word, thus annotating the corpus with the parse derivations (also providing less probable parses if desired). An example of another approach to doing this is the Fidditch parser of Hindle (Hindle, 1983) (based on the deterministic parser of Marcus (Marcus, 1980)), which was used to annotate the Penn Treebank (Mar- cus et al., 1993). However, instead of learning the lexicon, a complete grammar and lexicon must be supplied to the Fidditch parser.Our work also relates to CG induction, which has been attempted by a number of people. Osborne (Osborne, 1997) has an algorithm that. learns a grammar for sequences of part-of-speech tags from a tagged corpora, using the Minimum Description Length (MDL) principle -a welldefined form of compression. While this is a supervised setting of the problem, the use of the more formal approach to compression is of interest for future work. Also, results of 97% coverage are impressive, even though the problem is rather simpler. Kanazawa (Kanazawa, 1994) and Buszkowski (Buszkowski, 1987) use a unification based approach with a corpus annotated with semantic structure, which in CG is a strong indicator of the syntactic structure. Unfortunately, they do not present results of experiments on natural language corpora and again the approach is essentially supervised.Two unsupervised approaches to learning CGs are presented by Adriaans (Adriaans, 1992) and Solomon (Solomon, 1991). Adriaans, describes a purely symbolic method that uses the context of words to define their category. An oracle is required for the learner to test its hypotheses, thus providing negative evidence. This would seem to be awkward from a engineering view point i.e. how one could provide an oracle to achieve this, and implausible from a psychological point of view, as humans do not seem to receive such evidence (Pinker, 1990). Unfortunately, again no results on natural language corpora seem to be available.Solomon's approach (Solomon, 1991) uses unannotated corpora, to build lexicons for simple CG. He uses a simple corpora of sentences from children's books, with a slightly ad hoc and non-incremental, heuristic approach to developing categories for words. The results show that a wide range of categories can be learned, but the current algorithm, as the author admits, is probably too naive to scale up to working on full corpora. No results on the coverage of the CGs learned are provided.In Section 3 we discuss our learner. In Section 4 we describe experiments on three corpora containing examples of a subset of English and Section 5 contains the results, which are encouraging with respect to both problems. Finally, in Section 6, we compare the results with the systems mentioned above and discuss ways the system can be expanded and larger scale experiments may be carried out. Next, however, we describe Categorial Grammar. In this paper we report on an unsupervised approach to learning Categorial Grammar (CG) lexicons. The learner is provided with a set of possible lexical CG categories, the forward and backward application rules of CG and un-marked positive only corpora. Using the categories and rules, the sentences from the corpus are probabilistically parsed. The parses and the history of previously parsed sentences are used to build a lexicon and annotate the corpus. We report the results from experiments on a number of small generated corpora, that contain examples from subsets of the English language. These show that the system is able to generate reasonable lexicons and provide accurately parsed corpora in the process. We also discuss ways in which the approach can be scaled up to deal with larger and more diverse corpora.
NODALIDA &apos;99 \ t h Proceedings from the 12 &apos;Nordiske datalingvistikkdager  
BusTUC -A natural language bus route adviser in Prolog A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL). W ith natural language as a means of communication with a computer system, the users can make a question or a statem ent in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.The present implementation represents a a major effort in bringing na­ tural language processing into practical use. A system is developed th at can answer queries about bus routes, stated in natural language texts, and made public through the Internet World Wide Web (www. i d i . n tn u . n o /b u s tu c /).Trondheim is a small city with a university and 140000 inhabitants. The central bus system in Trondheim has 42 bus lines, serving 590 stations, with 1900 departures per day (in average). T hat gives approximately 60000 sche­ duled bus station passings per day, which is somehow represented in the route data base.The starting point is to autom ate the function of a route information agent. The following example of a system response is taken from an actual request over telephone to the local route information company: H i, I l i v e in N id a rv o ll and to n ig h t I must re a c h a t r a i n to Oslo a t 6 oclock.A typical answer would follow quickly:Bus number 54 p a sse s by N id a rv o ll school a t 1710 and a r r iv e s a t Trondheim Railway S ta tio n a t 1725.In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and databcise query processing and answer generation.One could argue th at the information content could be solved by an in­ terrogation, whereby the customer is asked to produce 4 items: d e p a rtu re s t a t i o n , a r r i v a l s t a t i o n , e a r l i e s t and l a t e s t a r r i v a l tim e. It is a myth th at natural language is better way of communication because it is "natural language" . The challenge is to prove by demonstration th at an NL system can be made that will be preferred to the interrogative mode. To do that, the system has to be correct, user friendly and almost complete within the actual domain. 
DEVELOPING A GRAMMAR CHECKER FOR SWEDISH* Software programs designated as grammar checkers have been developed since the 1980's, first and foremost for English, but also for other major European languages (Bustamante &amp; Léon 1996). Similar endeavors for the Nordic languages have been scarce, the notable exception being the Virkku system for Finnish. Virkku was developed and launched on the market in 1991 by Kielikone Ltd &lt;http://www.kielikone.fi&gt; as a side-kick of the company's long-term efforts in developing a machine translation system from Finnish to English. Despite this technical background, Virkku does not use the fiall-scale deep-syntactic parser developed for Kielikone's machine translation system, but is instead based on a lighter, unificationbased approach.^ Unfortunately, the Virkku system remains publicly undocumented.In the case of Swedish, some level of checking of noun phrase internal agreement, based on shallow parsing, was incorporated into the Swedish version of the former Inso's International ProofReader proofing tools software, developed in cooperation with IBM in the early 1990's.^ Nevertheless, it was not until the middle 1990's that several independent projects were initiated, more or less within the same timeframe, with the intent of developing a full-fledged grammar checker for Swedish, namely Granska, SCARRIE, and Grammatifix. The Granska project &lt;http://www.nada.kth.se/theory/projects/granska/&gt; was originally initiated in 1994 at the Department of Numerical Analysis and Computer Science (NADA) at the Royal Institute of Technology (KTH) in Stockholm, and has been continued on several occasions ( Domeij et al 1996). The SCARRIE project &lt;http://www.scarrie.com&gt;, which in addition to Swedish also aimed at covering the two other main written Scandinavian languages, Danish, and Norwegian Bokmål, was started in 1996, and was scheduled to end in 1999. In the SCARRIE project, the main responsibility for the Swedish component was undertaken by the Department of Linguistics at the University of Uppsala (Sågvall Hein 1998). Grammatifix is the result of a product development project initiated in 1997 and completed in 1999 at Lingsoft, Inc., a Finnish language engineering company &lt;http://www.lingsoft.fi&gt;. Lingsoft has licensed Grammatifix to Microsoft as the grammar checking component of the Swedish version of Microsoft Office 2000, launched on the market in the year 2000, and has also released Grammatifix on the Swedish market as a stand-alone product under the Grammatifix brand name. Actually, there is a fourth Swedish proofing tool on the market that covers some error types traditionally associated with grammar checkers, namely Norstedts' Skribent &lt;http://www.norstedts.se&gt;, but since it does not include any syntactic error detection, it was left outside the scope of this paper. This paper outlines the development process of Grammatifix undertaken at Lingsoft. The emphasis of this paper is on general product definition and product development issues associated with such linguistic tools as a grammar checker, whereas the actual mechanism for detecting Swedish grammar errors and its linguistic principles are covered in a separate paper by Bim in the same volume. Furthermore, this paper gives an overview of the features of Grammatifix, and compares these with the other known and documented Swedish grammar checkers, namely SCARRIE and Granska. A grammar checker for Swedish, launched on the market as Grammatifix, has been developed at Lingsoft in 1997-1999. This paper gives first a brief background of grammar checking projects for the Nordic languages, with an emphasis on Swedish. Then, the concept and definition of a grammar checker in general is discussed, followed by an overview of the starting points and limitations that Lingsoft had in setting up the Grammatifix development project. After this, the initial product development process is described, leading to an overview of the error types covered presently by Grammatifix. The error treatment scheme in Grammatifix is presented, with a focus on its relationship with the error detection rules. Finally, the error types included in Grammatifix are compared to those of two other known projects, namely SCARRIE and Granska.
DETECTING GRAMMAR ERRORS WITH LINGSOFT&apos;S SWEDISH GRAMMAR CHECKER The purpose of this paper is to explain how Grammatifix goes about its task of detecting grammar errors. The paper by Arppe (this volume) addresses the more general level design principles in the development of Grammatifix, and provides also a background to the field of Swedish grammar checking in general.Grammatifix has checks on three kinds of phenomena; grammar errors, graphical writing convention errors, and stylistically marked words.' For these phenomena different detection techniques are used; SWECG, matching of regular expressions against character sequences, and lexical tagging, respectively. This paper is concerned with grammar error detection.Prototypical grammar errors can be understood to be norm violations that are to be identified in contexts larger than the word (cf spell-checking) where the contexts are morphosyntactically explainable. Of errors so defined, no computational grammar checker is able to control more than a (more or less) modest part. A realistic grammar checker concentrates on central categories of the language's grammar, and, within those categories, on common, simple patterns that allow precise descriptions. The error categories targeted by Grammatifix are presented in Arppe &amp; al. (1999), for a listing with examples see also Arppe (this volume). A Swedish grammar checker (Grammatifix) has been developed at Lingsoft. In Grammatifix, the Swedish Constraint Grammar (SWECG) framework has been applied to the task of detecting grammar errors. After some introductory notes (chapter 1), this paper explains how the SWECG framework has been put to use in Grammatifix (chapter 2). The different components of the system (section 2.1) and the formalism of the error detection rules (section 2.2) will be overviewed, and the relationship between grammar errors and disambiguation will be discussed (section 2.3). Work on the avoidance of false alarms is also described (chapter 3). Finally, test results are reported (chapter 4).
P ivot alignm ent Parallel texts aligned on the word level have a number of potential uses. Given suitable browsing and search tools, linguists can use aligned parallel corpora in the same way that they already use monolingual corpora, i.e. as a rich source of authentic language data, in this case data on translation equivalence (see, e.g., Olsson &amp; Borin forthcom­ ing). Bilingual lexicography, translator training, and foreign language instruction all stand to benefit from the use of such corpora. In computational linguistics, the applica­ tion which springs to mind first is the automatic or semi-automatic extraction of trans­ lation equivalents for machine translation systems from word-aligned parallel texts, but there are also possible applications in the fields of computer-assisted language learning and cross-lingual information retrieval.The ETAP project is a parallel translation corpus project funded by the Bank of Sweden Tercentenary Foundation. The aim of this project is to create an annotated and word-aligned multilingual translation corpus, which will be used as the basis for the development of methods and tools for automatic extraction of translation equivalents on the word and phrase levels (see Borin forthcoming a). Word alignment of parallel texts is typically carried out using many kinds of knowledge, or information sources, in concert, i.e., it is profitably viewed as a kind of cooperative process, where e.g. distribution, string similarity, cooccurrence statistics, and other in­ formation sources are used together. We investigate a novel such information source in this paper, namely the use of a third language as a &apos;pivot&apos; to increase alignment recall, hence the name pivot alignment. The results of the preliminary experiments reported here indicate that pivot alignment increases word alignment recall, without sacrificing preci­ sion. We conclude that the method is well worth exploring further, by examining more languages and language combinations.
Granska an efficient hybrid system for Swedish grammar checking Grammar checking is one of the most widely used tools within language technology. Spelling, grammar and style checking for English has been an integrated part of common word processors for some years now. For smaller languages, such as Swedish, advanced tools have been lacking. Recently, however, a grammar checker for Swedish has been launched in Word 2000 and also as a stand-alone system called Grammatifix (Arppe 2000, this volume;Bim 2000, this volume).There are many reasons for further research and development of grammar checking for Swedish. First, the need for writing aids has increased, both concerning the need for more efficiency and quality in writing. Secondly, the linguistic analysis in grammar checking needs further development, especially in dealing with special features in Swedish grammar and its grammatical deviations. This is a development that most NLP-systems will benefit from, since they often lack necessary methods for handling ungrammatical input. Thirdly, there is need for more sophisticated methods for evaluating the functionality and usability of grammar checkers and their effect on writing and writing ability.There are two research projects that focus on grammar checking for Swedish. These projects have resulted in two prototype systems: Scarrie (Sagvall-Hein 1998;Scarrie 2000) and Granska (Domeij, Eklundh, Knutsson, Larsson &amp; Rex 1998). In this article we describe how the Granska system is constructed and how grammatical errors are handled by its error rule component. The focus will be on the treatment of agreement and split compound errors, two types of errors that frequently occur in Swedish texts. This article describes how Granska-a surface-oriented system for checking Swedish grammar-is constructed. With the use of special error detection rules, the system can detect and suggest corrections for a number of grammatical errors in Swedish texts. Specifically, we focus on how erroneously split compounds and noun phrase agreement are handled in the rules. The system combines probabilistic and rule-based methods to achieve high efficiency and robustness. This is a necessary prerequisite for a grammar checker that will be used in real lime in direct interaction with users. We hope to show that the Granska system with higher efficiency can achieve the same or better results than systems that use rule-based parsing alone.
A d ap tin g an E n glish In form ation E x tra ctio n S y stem to Sw edish A well-known problem in the area of Information Extraction regards the adaptation of an extraction system to handle a new class of events ( Yangarber and Grishman, 1997). With the increasing interest in multi-lingual and cross-lingual information extraction, it becomes necessary to construct systems that are easily adaptable, not only to new extraction tasks, but also to new languages. This paper presents work on adapting the Proteus Information Extraction system (Grishman, 1995;Yangarber and Grishman, 1998) developed at New York University, to Swedish. The system has previously successfully been adapted to Japanese (Sekine and Nobata, 1998).The topics covered in the following sections are: an introduction to the Information Extraction task, a description of the Proteus Information Extraction system, an account of the adaptations made to the system and some results from evaluating the adapted system. A description of our present work on designing a new information extraction system and the motivations behind it will conclude the paper. This paper presents work on adapting the Proteus Information Extraction sys­ tem to Swedish. It turned out that the cross-lingual adaptation as such was fairly straightforward ; however, the Proteus system design did not render itself that well to reconfiguration at such a low level as needed. To evaluate the adaptation, the system was tested on a Swedish version of the MUC-6 Scenario Template Task. The Swedish version performed excellently on a training corpus, but quite discouragingly on an unseen test corpus. As a consequence of that work, a new Information Extraction system is being designed and the layout of that system is described.
T h e s h o r tc o m in g s o f a ta g g e r The first version of the first ever comprehensive tagger for Norwegian is ready. Both the nynorsk and the bokmål (the two Norwegian language varieties) versions have been used to tag a large number of texts (= the Oslo Corpus of Tagged Norwegian Texts). The corpus has an advanced web-based user interface, which often gives nice results, but it also makes it easy to discover mistakes and shortcomings of the tagger. The present paper will focus on these.The tagger is of a Constraint Grammar-type ( Karlsson et al 1995). The linguistic constraints (rules) were developed by the Text Laboratory, while the software came from Lingsoft, Helsinki. A CG tagger takes as input a multitagged text, where each word form has as many tags as the lexicon allows, and gives as output a text where the tags are disambiguated by the given linguistic constraints according to the context for each word in question. The statistical results are good: The bokmål tagger has a recall of 99,2% and a precision of 96,8%. For nynorsk the results are slightly worse: 98,8 % recall and 95,6 % precision.The tagger, then, makes some mistakes. One kind of shortcoming involves cases where some ambiguity remains (this influences the precision rate) -for a number of reasons, of which structural ambiguity is the most severe one: Sometimes extralinguistic knowledge would be required to disambiguate a certain ambiguity. Another shortcoming has to do with mistaken lexical analysis: We have problems when a text contains words that are unknown to the lexicon or that are analyzed wrongly by our compound analyzer, or if they even contain a wrong language (common in citations, loanwords etc.).Before we go into these mistakes, however, let us give an example which shows that in spite of the errors, the overall impression is that the tagger actually does a good job. In the following example, we have asked the corpus to give us all occurrences of the word stemme ('vote') used as a verb; we therefore do not want any occurrences of the same wordform used as a noun meaning 'vote' or 'voice'. And indeed, the overall impression is that we get what we wanted:(1)Example of an arbitrary selection of hits from a search for the verb stemme 'vote'. as opposed to the noun stemme 'vote' or 'voice'Søkestreng; [word="stemme" &amp; tagg=".* verb.*" &amp; (src="AV.*" | src="SA.*" src="SK.*")] med 30 tegn på venstre side og 40 tegn på høyre side. The tagger used for the Oslo Corpus of Tagged Norwegian Texts has very good statistical results. In spite of this, it makes mistakes. In this paper we take a closer look at some of them. Although some mistakes are of a kind that would disappear if we improved the tagger, many are impossible or very difficult to do anything about. They are due to errors in the corpus (spelling errors, foreign words, non-standard spellings), to elliptic sentences, such as headlines, and to structural ambiguity, which abounds to a surprising extent. Proofreading the corpus would have removed the first kind of problems, but the other two types cannot be resolved in any obvious way.
M e r g in g C la ssifie r s for Im p r o v e d In fo r m a tio n R e tr ie v a l Training several different classifiers and combining their predictions into a single one is a common method for creating a classifier from a set of training data. This approach gen­ erally yields a more accurate result than that from the constituent classifiers, which has been shown by a number of researchers. (For an overview of research and results in this area see for example Merz (1999) and Dietterich (1997).) Similar results have also been shown in the document retrieval domain by Bartell et al. (1994): using different retrieval algorithms and then combining them may significantly improve retrieval performance.When applying a merging strategy, the first thing to decide is what different classifiers to use. In the information retrieval domain the source of information is often documents. As documents consist of words, a feasible approach could be to use linguistic methods for retrieval. If each of the methods captures a different aspect of the documents' content, we could possibly retrieve a larger amount of relevant documents on the whole. When merging the different results, the aim is to produce a final result th at is more accurate, i.e., has a higher average precision, than the output of ciny of the individual results. Obtaining this is, however, not trivial, as we only have recourse to weak clues about text relevance, and since results vary between queries, domains and reader preferences, all of which is based on knowledge th a t is difficult to model reliably. One prospective way to improve information retrieval is to use several indexing methods to retrieve different sets of documents, and then to merge (or combine) these results into one single result. The merging should be done in a way th at produces a final result that is more accurate than the output of any of the individual classifiers. A merging algorithm called S E Q U E L has been applied for this task to data in the field of information retrieval. This article describes the results of these experiments, as well as conceivable future directions.
Extracting Keywords from Digital Document Collections Digital libraries are complex information systems, which augment and extend tradi­ tional libraries by affording users better support for human problem solving and prob­ lem formulation. Digital libraries should be understood to be more than a haphazard col­ lection of electronic resources and associated technical widgets for creating, searching, and using information in various media and over networks. They are, or should be, tai­ lored to the needs and tasks of a group or several groups of users, and their functional capabilities should support the information needs and uses of those individuals and groups.Digital libraries are both an extension and integration of existing information sources, and through the advent of new technology and adjustment of tried and familiar tech­ nologies, a completely new concept. While digital libraries typically improve certain aspects of traditional libraries, most often today they leave other aspects unaddressed, which will decrease their usefulness. Traditional information institutions not only make information resources available to the public, but actively select, collect, organise, and preserve them, engaging in numerous behind-the-scenes tasks seldom addressed, or taken for granted in their digital counterparts.Despite recent advances in both computer technology and computational linguistics, retrieving and extracting useful information in large document collections is still very troublesome. Freetext search is certainly useful and fast, and generates a generous amount of results, but distinguishing the relevant documents from the non-relevant in the abundance of returned documents is a problem. Other systems for structuring infor­ mation to enhance availability has traditionally been by storing information about documents, books, and texts in bibliographic cards; and by indexing the documents by lists of keywords or keyphrases. An indexing tool was built to provide for one of several information seeking tasks. In ac­ cordance with the basic principles of work held by the HUMLE laboratory at SICS, a so­ lution regarding indexing would be a semi-automatic tool. This approach is also relevant as the continuation of the indexing project is conducted in cooperation with the Swedish Parliament, where a staff of professional indexers currently is investigating the utility of automatic and semi-automatic indexing tools to raise productivity.
Ontologically Supported Semantic Matching "A typical information retrieval task is to select documents from a database in re­ sponse to a user's query, and rank these documents according to relevance." Strza- Ikowski et al (1998). The relevance must be defined on the basis of the concepts represented in the text and in the query. Usually information retrieval (IR) systems calculate the relevance of a text with respect to some query according to the num­ ber and the profile of the occurrences in the text of some elements from the query. The meiin stream of research in IR is towards the development of methods for the recognition of more meaning bearing elements of texts which can then be used to evaluate the closeness of the two texts (queries are also texts). Most often a document is converted into a bag of words, stems or other textual ele­ ments which we call atomic text entities (ATEs) (sometimes information associated with them is also used). The hope is that these elements explicate the concepts represented by chunks of text and so define the topics of the document. Similarly, the query is considered to be itself a bag of words, stems, etc. and again the hope is that they explicate the concepts of the query. Although words denote concepts, often they are not sufficient in themselves to explicate these concepts. They can be thought of as names for the concepts in the world. Usually the definition of a concept spells out what are the constraints on its possible representatives or instantiations, it could also give some prototype information and information about this concept's relationship to other concepts. It is our opinion that users of information retrieval systems rarely search simply for words. Rather, they are interested in the concepts that words represent. Thus concepts (including at least some parts of their definitions and relations to other concepts) should be included amongst the atomic text entities. In this way we will capture the intuitive expectation that when one is searching for bird the occurrence of duck is also relevant. This is so because the word duck represents a subconcept (more specific concept) of the concept represented by bird. The problem of the word-to-concept correspondence is well known and intensively studied in a number of areas like linguistics, psychology, artificial intelligence, etc. In order to demonstrate some of its aspects we give here a small example. Let us consider the following top-ontology of particulars (taken from Guarino (1998)):Concrete object Continuant (an apple) Occurrent (a fa ll of an apple) Abstract object ( Pythagoras' theorem) Here, objects are considered to be concrete because of their ability to have some location. Continuants are what is usually considered to be objects, while occurrents correspond to events. Continuants have a location in space. They have spatial parts, but they have neither a temporal location nor temporal peirts. Occurrents are "generated" by continuants, according to the way they behave in time. Occurrents always have other occurrents as parts (continuants take occurrents as parts, but are not part of them). They have a unique temporal location, while their exact spatial location can not be defined in the general case. Abstract objects do not have a location at all. Most of the entities classified as abstract objects can also be thought of as universals. Depending on the definition of a concept and therefore on the objects this concept denotes it can be classified under one or another branch of this ontology. Thus concepts lexicalized via words in a natural language (or lexical concepts) will belong to different branches of any ontology extending on the above minimal ontology. For example, the English word 6oofcdenotes at least the following concepts: "information unit", "physical object" and "commodity" which belong to different branches. As a physical object book is a continuant and as an information unit it is an abstract object.One another important point is that world knowledge, that is our repository of con­ cepts and facts, is considerably more massive than is the set of lexicalized concepts. Therefore, if we use only words as concept denoting entities, we can hope to find only a fraction of the concepts that we have available to us. In this paper we investigate the possibility to use WordNet (see Fellbaum (1998)) as a source for the explication of some concept relations in order to improve the matching of ATEs in documents and queries. More specifically, we exploit the hypernym-hyponym relation. We call this augmented matching of concepts -ontomatching. This improvement can be used in the core of both FTR and IR systems, as well as in other places, like information filtering, dictionary look up, information extraction, etc. The structure of the paper is as follows: the following section gives an overview of WordNet and some of the approaches to using WordNet to enhance the precision in IR systems; afterwards, we discuss different approaches to "concept" search in texts and we introduce the central notion of the paper -onto-matching; the next section is devoted to the application of WordNet and onto-matching to query expansion and document indexing; the last section concludes the paper and lists some problems and directions for future research. A b stra c t Evaluation of the closeness of two texts is a subtask for F T R and I R systems. The basic means used to accomplish it is the matching of a to m ic t e x t e n t i t i e s (ATEs) such as words, stems, simple phrases and/or concepts. We address the question how concepts can be used as ATEs more efficiently in order to match &quot;s m a l l d u c k &quot; with &quot;s m a l l b ir d &quot;. The o n t o-m a tc h in g technique introduced in the paper makes extensive use of lexical ontologies similar to WordNet. We work with two tasks in mind: query expansion and text concept indexing. We outline some arguments showing why onto-matching is useful and how it can be implemented. Also, we conducted some experiments with query expansion for AltaVista.
Automatic Detection of Lexicalised Phrases in Swedish  I wiIrpresent a system under development, called LP-DETECT. The system detects and analyses Swedish lexicalised phrases (LPs) in order to enhance subsequent parsing. LPs are one of a number of stumbling blocks related to word sequences that must be dealt with when parsing unrestricted text. LPs include semantic idioms, syntactic idioms and morphological idioms and so called valency breaking LPs. The system reported on consists of an LP lexicon of some 8000 LPs with analyses, a detection program written in perl and rules for disambiguating between and discarding LP analyses. A small evaluation of the system is also presented. 1. Lexicalised phrases LPs are expressions that belong to the lexicon and consist of more than one word. Included are semantic idioms the meaning of which is not built up compositionally from the individual meanings of the words in the idiom. (An English example is kick the bucket meaning &apos;die&apos;, a Swedish equivalent ta ner skylten (lit. take the sign down) also meaning &apos;die&apos;.) They should be lexically listed because of semantic reasons. Another group consists of syntactic idioms containing &quot;ungrammatical&quot; or non-standard combinations of words, in syntactic terms (inte så värst (lit. not so worst) meaning &apos;not particularly&apos;; English example: by and large). The syntactic idioms do not make up regular grammatical structures and must therefore be listed as wholes in a parsing system. Of course, since syntactic idioms are syntactically irregular, no general function of semantic interpretation can apply over them and therefore a syntactic idiom is also a semantic one. A third group consists of phrases containing words that are unique to the phrases where they occur, often lexical relics, for instance the Swedish LP med nöd och näppe meaning &apos;with difficulty&apos;. The word näppe is not used outside this phrase and should therefore not be individually listed in a word lexicon. Related to the latter group are also phrases containing foreign words (anno domini) or nonce forms (hux flux). These could be called morphological idioms and they of course qualify as semantic idioms too. In addition there are a large number of LPs, partly overlapping with the LP types described above, which I call valency breaking LPs. They simplify the phrase structure of sentences when detected as LPs. They tend to end in function words. An example of these is multi-word prepositions, i.e. in spite o/(Swedish example: på grund av, lit. on ground of, &apos;because of). &quot;Mechanically&quot;, the PP in spite o f the weather consists of a P (in) and a complex NP (spite o f the weather, itself containing a PP). In spite o f here functions as a complex preposition, actually replaceable with the single preposition despite, making the syntactic structure of the PP simpler and more one-to-one with its semantics. Other examples of such LPs are phrases, often described using some kind of subcategorisation, are so called prepositional verbs (tro på, eng: believe in), V-i-NP-i-P
Towards a Finite-State Parser for Swedish In several Natural Language Processing (NLP) tasks, such as information retrieval, information extraction, speech technology, machine translation, etc., full or partial information about phrasal and/or syntactic structures is needed. The main interest in these tasks lies in detecting the constituent structures and sometimes their syntactic functions in a robust and fast way. In this study, our aim is to develop a parser for Swedish part-of-speech tagged texts, based on finite-state techniques using the Xerox Finite-State Tool (Karttunen et al, 1997).Finite-state techniques have been shown to be very useful for parsing unrestricted texts for several languages, such as English, Finnish, French, German, Swedish, etc. Under certain circumstances, these parsers are robust, fast and accurate. There are mainly three approaches that have been applied for the construction of finite-state parsers: constructive, reductionist, and the combinations of these.Briefly, the constructive approach is based on lexical description of large collections of syntactic patterns using subcategorisation frames such as verbs and their arguments, and local grammars (Abney, 1996). The reductionist approach, on the other hand, starts from a large number of alternative analyses that get reduced through the application of constraints where the constraints may be expressed by a set of elimination rules (Voutilainen &amp; Tapanainen, 1993) or by a set of restrictions applied in parallell ( Koskenniemi et al, 1992). The hybrid method merges the constructive and the reductionist approaches. It is developed by Ait- Mokhtar and Chanod (1997) who built an incremental finite-state shallow parser for French in a modular way. The parser makes incremental decisions throughout the parsing process. Syntactic information is added at the sentence level depending on the contextual information. They achieve broad coverage and include richer information than typical chunking systems.A common procedure for building finite-state parsers from part-of-speech tagged texts is to first mark contiguous groups, e.g. noun or verb groups, then mark the heads within the groups and lastly, to extract patterns between non-contiguous group bounderies. However, Grefenstette (1996) points out that several parsers mix non-fmite-state methods with finite-state procedures for different modules. He shows that the entire parser can be built easily within a finite-state framework by using finite-state transducers.Finite-state transducers are finite-state machines that take an input and produce an output with each state transition. They generate or accept regular relations, i.e. sets of pairs of strings where each string has an upper and lower language. They can be written as regular expressions and can be used for introducing extra symbols into an input string, i.e. for labelling entities (groups) in a text. The labels, then, can be used for deriving further information from the text, such as extracting non-contiguous syntactic n-ary dependencies. By composing a sequence of transducers and dividing the parsing task into a sequence of partial tasks, such as contiguous group labelling, head marking, and the detection of non-contiguous group boundaries, Grefenstette presents a robust and fast light parser.In this study, we use the Xerox finite-state tool (XFST), for constructing the parser. The reason for our choise is that the XFST is very convenient to use since it allows powerful and elegant linguistic descriptions by different operators for a high level of abstraction.XFST is a general-purpose Unix application for computing with finite-state networks. Simple automata and transducers can be easily created by a set of operations from text files, binary files, regular expressions and other networks. Thus, XFST can read finitestate networks and compile them from regular expressions and text files. The networks can be simple finite-state automata or finite-state transducers and can be combined by various operations. In addition to the usual operators' (e.g. concatenation, union, optionality, Kleene star, Kleene plus, complement, intersection, relative complement, crossproduct, composition, etc.) XFST also supports some special operators for high level abstraction: restriction, replacement, and left to right longest match replacement. The restriction operator is very useful when writing constraints to exclude unwanted analyses. The rule A =&gt; B _ C expresses that A must appear in the context of B _ C, i.e. between B and C. The replacement operator replaces a string with another string with or without regard taken to context. For example, the rule A -&gt; Bll L_R replaces A by B between a certain left and right context where A and B denote regular languages and the expression as a whole denotes a relation. The longest match operator is a special kind of replacement operator. It imposes a unique factorisation on every input. It can also be constrained by context and generalised for parallel replacement. For instance, the rule A @-&gt; B ... C forces the transducer to locate and pick out maximal instances of the regular language A, leaving the entire string unchanged and inserting B and C around the selected A strings as markers. In this study, we describe a method for parsing part-of-speech tagged unrestricted texts in Swedish using finite-state networks. We use the Xerox Finite-Slate Tool because of its expressiveness and power for writing and compiling regular expressions and relations. The parser is divided into four modules: i) contiguous phrase structure marker, ii) phrasal head marker, iii) syntactic function tagger, and iv) non­ contiguous group boundary recognizer. The aim is to develop a parser that can be used as a light/shallow parser for marking phrase structure and, when needed, to label syntactic functions. We believe that modularity is necessary since different NLP tasks require various levels of analysis. The parser for Swedish is under development, but present-day results are promising.
Sem antic C lustering o f A djectives and Verbs B ased on Syntactic Patterns  In this paper we show that some of the syntactic patterns in an NLP lexicon can be used to identify semantically &quot;similar&quot; adjectives and verbs. We define semantic similarity on the basis of parameters used in the literature to classify adjectives and verbs semantically. The semantic clusters obtained from the syntactic encodings in the lexicon are evaluated by comparing them with semantic groups in existing tax­ onomies. The relation between adjectival syntactic patterns and their meaning is particularly interesting, because it has not been explored in the literature as much as it is the case for the relation between verbal complements and tu-guments. The identification of semantic groups on the basis of the syntactic encodings in the con­ sidered NLP lexicon can also be extended to other word classes and, maybe, to other languages for which the same type of lexicon exists.
A n H PSG A ccount o f D anish Pre-nom inals When investigating empirical data it becomes clear that noun phrases often have multiple specifiers appearing before the noun. An important goal of noun phrase analysis is the specification of selection restrictions for noun phrase specifiers and pre-nominals in general to account for combinations of specifiers. It is this goal that is pursued in this article.In section 1 a set of Danish noun phrases are presented which form the basis of a discussion of what properties determine the restrictions on combinations of pre-nominals. In section 2 a number of previous HPSG analyses of noun phrases and pre-nominals are discussed. In section 3 the proposed analysis is introduced and sample analyses are shown. The proposed analysis has been implemented in the LKB system (Copestake 1999). A test suite consisting of the data in section 1 has been run and the results are presented in section 4. The article is concluded in section 5. This article addresses the issue of selection restrictions for noun phrase specifiers. Danish data is presented which shows that definiteness plays an important role in this respect. It is pointed out that an analysis is required in which the specifier, when present, leaves a mark on the projected phrase. This is achieved by assuming that specifiers are syntactic heads of noun phrase constructions. Further an elaborate classification of specifiers is also needed in terms of which selection restrictions may be formulated, rJong with a cross-categorial definiteness feature. These properties are part of the analysis proposed in this analysis.
  Tonem 1 eller 2 eller 1,5? Arild Noven (arild.noven@gri.no~). Per Arne Larsen (pelarsen@gri.no), Bente Moxness (bente.moxness@gri.no), Kolbjørn Slethei (kolbjom.slethei@gri.no) ~Voss International Language Technology A/S Samandrag Artikkelen presenterer eit eksperiment utført ved Nordisk Språkteknologi A/S der ein vurderte nytten av eil nøytralt tonem som erstatning for tonem 1 eller 2 i ein norsk talesyntetisator. Arbeidshypotesen var at val av dialekt er heilt avgjerande for utfallet, og Ålesunds-dialekten vart vald avdi han har ein minimal skilnad mellom tonem 1 og 2. Eit nøytralt tonem 1,5 vart konstruert som ein interpolasjon mellom tonem 1 og 2, og 2 testsetningar, kvar med ulike kombinasjonar av tonem 1, 1,5 og 2, vart presentert for ei gruppe utvalde forsøkspersonar. Forsøkspersonane vart så bedne om å vurdera kvaliteten på testsetningane langs ein 5 punktsskala. Resultata viste at tonem 1,5 vart akseptert som fullgod erstatning for dei ekte tonema.
S yntactic A nalysis and Error Correction for D anish in th e SC A R R IE project  This paper reports on work carried out at CST in Copenhagen to develop the Danish version of the SCARRIE prototype, addressing in particular the issue of how a form of shallow parsing is combined with error detection and correction to treat context-dependent spelling errors. The paper describes the corpora used to develop the system, and shows some preliminary evaluation results.
Botond Pakucs^&apos;^ Björn Gambäck^&apos;^ The field of information retrieval (IR) has been moving steadily forward for several decades. During the 90's we have seen severed major break-throughs. Until recently, most of the work has been focused on texts; not only has most of the material processed been in text format, but even when other media such as audio and video have been con­ sidered in a system, text has been the primary concern. Consequently, most multimedial retrieval systems are modifications of existing text-based IR systems, disregarding the particular problems caused by the new media types. However, the amount of material in other formats increase all the time, increasing the need for tools that handle this infor­ mation both from a system-oriented and a user-oriented perspective. An important issue for information management is how to represent these objects and collections of objects to best support retrieval. Another issue is to let users search in several modalities. An example would be when a person wants to search a news database: the database contains both news in text format and audio sequences of spoken material. These are stored and indexed in different ways, but the user wants to search both archives at the same time.The term 'Spoken Document Retrieval' (SDR) has, in itself, rendered some confusion. We will use it exclusively for the particular case of information retrieval, when the information is to be retrieved from large volumes of spoken documents. Thus, what media the query itself is formulated in is of no importance to us here; only the format from which the sought information is to be accessed. In Section 2 we review the difficulties caused by trying to access multimedia! documents, in particular audio documents, and some previous attempts to overcome them. We are building a flexible toolkit specifically designed to allow for different approaches to addressing these difficulties and for handling data in different types of media. The toolkit is functional but still open to improvement; however, Section 3 discusses the underlying design philosophy. It is only during the last few years that attention has started to shift from pure text-based retrieval towards other media. Information retrieval from spoken documents is analogous to text-based retrieval; however, accessing audio documents causes some extra problems, in particular with respect to document segmentation, choice of in­ dexing features, and robustness. In addition, retrieval of documents in Swedish, like most non-English languages, adds the extra dimension of morphology; also, when analysing spoken Swedish data, prosodic patterns have to be taken into cu:count. In this paper we introduce SIREN, the Swedish Information Retrieval Engine, a very flexible, modular IR system which has been designed with a specific eye towards these issues.
Statistics and Phonotactical Rules in Finding OCR Errors Optical character recognition (OCR) is a technique for moving text resources from paper medium to electronic form, something that is often needed in our computerised society. Companies and authorities want to make old material machine readable or searchable. Unfortunately, it does not get us all the way. With good paper originals, OCR can achieve 99% of the characters correctly recognised but the result will still contain in average one error word per 20 words which means 5% incorrect words or about one error per sentence (Kukich, 1992). Depending on the application of the optically scanned text, large post processing efforts can be necessary. Since OCR is often used to move large amounts of text to electronic form, the proofreading is a task both demanding and dull. This makes the need for good tools of spell checking and correction large and urgent.Most spell checkers and OCR post processing systems are lexicon based. A lexicon of reasonable size is used to match against the text, and any word token not in the text is presented as a possible error. Probability scores or similarity measures are then used to generate correction suggestions.I will concentrate on the error finding process and not try to generate correction suggestions. I want to find ways of proofreading text without relying on a lexicon. Instead I will try to define rules that identify character sequences that are unlikely to be correct word tokens. I made two experiments: using statistical methods to find unexpectedly frequent character sequences, and using phono-or graphotactical rules to find unlikely character combinations. Obviously these results can be generalised for all kinds of proofreading tasks: e.g. handwriting recognition or dictation tasks.The work described in this report has been done within a Master thesis at the Language Engineering Programme at Uppsala University. The work has been carried out at SICS and was funded by the Digital Library project. This report describes two experiments in finding errors in optically scanned Swedish without lexicon. First, statistics were used to find unexpectedly frequent trigrams and correction rules were created. Second, Bengt Sigurds model of Swedish phonotax was used to detect words with phonotactically illegal beginning or end. The phonotax did not perform as well as the statictic rules did on their training material, but outscored them by far on new text. A correction tool was created with the phonotax as means of error detection. The tool displays every occurrence of an error string at the same time and gives the user the possibility to give different corrections to each occurrence. This work shows that it is possible to find errors in optically scanned text without relying on a lexicon, and that word structure can provide useful information to the correction process.
An Information Retrieval System with Cooperative Behavior In this paper we present a cooperative information retrieval system in the law domain. The information retrieval system is composed by a text knowledge base built from the Portuguese Attorney General documents.As an example, the following dialogue demonstrates the major features of our system (the example will be explained in more detail in the last section).User -Ul: Documents about pensions for relevant services? System -U2; Pensions that were given or refused?User -U3: Both. System -U4; Pensions for militaries or civilians?User -U5: Civilians System -U6 : There are 129 documents, listed in the right frame.User -U7: where there was a drowning? System -U8 : Since there are 45 documents of the 129 above that match the concept.I've listed them in the usual frame. User -U9: No, I mean to look for all documents that match the concept. System -UlO: There are 347 documents listed in the right frame.User -Ul 1; Documents about drugs?System -U12:1 assume that you want a new query about drugs since there are no documents with the concept drugs and pensions for relevant services for civilians. Do you want to see documents about the use of drugs while driving? User: -U13 yes.In order to handle dialogues similar with this one, our system needs to be able to keep the context of the interaction and to cooperatively supply suggestions for further refinement of the user queries. The refinement process is based on the domain knowledge and the ability to compute clusters of documents associating a keyword (from a juridical thesaurus with 6000 expressions) to each cluster.In order to perform a cooperative interaction with the user, the system should be able:• To infer what are the user intentions with the queries. For instance, when a user asks for documents with a particular keyword, he may be interested in documents that do not have that exact keyword and he may not be interested in all documents with that keyword.• To supply pertinent answers or questions as a reply to a user question. The system must supply some information on the set of documents selected by the user query in order to help him in the refinement of his query. As a consequence our system needs:• To record the user interactions with the system. User interactions will provide the context of sentences (questions and answers), allowing the system to solve some discourse phenomena such as anaphoras and ellipses.• To obtain new partitions (clusters) of the set of documents that the user selected with his query(ies). The clustering process should be based on the text knowledge representation. In the next section we will describe the text knowledge base. Then, in section 3 and section 4 the interaction structure and the inference of attitudes will be described. In section 5, the clustering process will also be described. In section 6 , an example of a cooperative session will be presented. Finally, in section 7, conclusions and future work will be presented. In this paper we will present a system that is able to perform cooperative information retrieval actions over a text knowledge base. The knowledge base is composed by four levels: Interaction, Domain, Information Retrieval and Text. The interaction level is responsible for the dialogue management, including the inference of attitudes. The domain level is composed by rules encoding knowledge about the text domain. The information retrieval level includes knowledge about IR actions over sets of documents. The text level has knowledge about the words in each text. Cooperation is achieved through two main strategies: 1) clustering the answer sets of documents accordingly with the domain and IR-level knowledge; 2) keeping the context of the interaction and inferring the user intentions.
An evaluation of the Translation Corpus Aligner, with special reference to the language pair English-Portuguese Two criteria that are often employed in the evaluation of NLP programs are performance and usability. Another criterion, less frequently mentioned, is the adequaey of handling particular languages. The present study describes a set of experiments devised to perform such an evaluation.Although researchers concerned with parallel corpus building and exploration will generally be happy to use a system available for their languages without evaluating it thoroughly, especially when the system is freely distributed -as is the case of the present system, the kind of work reported originates from two relevant concerns. The first one is about methodological aspects related to the development of NLP systems. The second concern is evaluation and comparison of products. In fact, there is a blatant lack of serious evaluation work of products and systems concerning the Portuguese language, which is a situation we have been trying to change in the project Computational Processing of Portuguese at SINTEF. ' The Translation Corpus Aligner (TCA) was developed in connection with the English-Norwegian Parallel Corpus (ENPC) project with the aim of automatically aligning English and Norwegian texts (see e.g. , Hofland &amp; Johansson 1998. Although the program was originally written for the language pair EnglishNorwegian, it has been further developed to handle other language pairs, including English-Portuguese. It includes a language-dependent component in the form of an anchor word list.In the present paper we set out to evaluate the TCA for the language pair English-Portuguese. In particular, we want to• investigate the effect of the anchor word list;• compare the results of the program with and without the anchor word list;• find out how much a proofreader has to check manually after alignment In order to perform the evaluation, we used the English-Portuguese part of the ENPC, which currently includes 16 English texts, about 12,000 words each, that have translations into Portuguese.^ In this paper we describe the evaluation of a language-dependent aligner. We begin by introducing the alignment program, explaining why it would be interesting to evaluate it with particular emphasis on the language pair English-Portuguese. A short presentation of the corpus used to test the aligner is also given. We then describe three experiments that were performed in the evaluation process, presenting the results and di.scussing the methodology. The paper ends with a discussion of more general conclusions relative to an evaluation of this kind.
Automatic proofreading for Norwegian: The challenges of lexical and grammatical variation Among language technology applications, proofreading can be equally challenging as, for instance, machine translation. In a fair number of cases, errors in texts cannot be adequately corrected without understanding the intention of the author in the given context. In practice, however, automatic proofreading systems excel not by their understanding of the text but by their consistency and tirelessness in processing high volumes without becoming 'blind' to relatively simple errors as humans tend to become.But even with limited expectations, the user may may find a proofreading system unacceptable if the number of false alarms is higher than the number of actual errors spotted, or if many suggestions for correction are inappropriate. It is therefore useful to invest in research aimed at improving the coverage of the system as well as the system's ability to propose corrections that are appropriate in the given context, whether grammatical or stylistic.The SCARRIE project is a language technology project aimed at building high-quality proofreading tools for the Scandinavian languages (Danish, Swedish and Norwegian). The project was sponsored by the European Commission through the Telematics programme. The project ran from December 1996 through February 1999. The coordinator was WordFinder Software AB (Växjö, Sweden). The other main partners in the project were the HIT-programme at Universitetet i Bergen, Institutionen för lingvistik at Uppsala Universitet, Center for Sprogteknologi (København) and Svenska Dagbladet (Stockholm). Although the projeet aimed at eventual commercial exploitation, it did involve a great deal of linguistic and computational research.At the end of the project, prototypes and evaluation reports were delivered for these languages. The prototypes correct simple misspellings and mistypings by means of advanced spelling and sound based matching criteria. They also have good coverage in their recognition of new compounds and derivations. Furthermore, they can detect repeated sequences, correct diacritical marks, correct words in the context of idioms and multi-word expressions, correct words based on different styles or norms, and perform limited grammar correction.We will in the remainder of this paper only report on the Norwegian part of the project. Earlier publications (Rosén &amp; De Smedt 1998) have highlighted different aspects of the linguistic and computational methodologies which are at the basis of SCARRIE for Norwegian. In this paper, we concentrate on the problems of proofreading for a language which shows rich variation not only in the lexicon but also in grammar. The specific problems related to grammar correction and style which are discussed below have to our knowledge never before been thoroughly researched with natural language processing methods. In this paper we present some techniques, experiences and results from the SCARRIE project, which has aimed at developing improved proofreading tools for the Scandinavian languages. The focus is on methods which were used for spelling and grammar checking and particularly some novel analyses and treatments dealing with the extensive lexical and grammar variation in Norwegian Bokmål. The major findings are that (1) since in Bokmål, lexical variants may differ with respect to grammatical features, stylistic replacement at the word level causes a need for grammar checking, and (2) the different systems for gender agreement in Bokmål can be handled in an economical way by a single grammar and lexicon if the features in the lexicon are interpreted dynamically depending on the subnorm or style preferred by the author.
Word Alignment Step by Step }Vord alignment aims at the identification of translation equivalents between linguistic units below the sentence level within parallel text (Merkel 1999), mainly bilingual text ibitext). Those units include single-word units (SlVUs) and multi-word units {MWUs) and will be referred to as link units further on. The basic terminology for describing parallel text and word alignment in this paper follows Ahrenberg et al (1999) and Ahrenberg et al (forthcoming). In particular, each word correspondence in the bitext describes a link instance, or simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation.Dep&gt;ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-&gt; word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-&gt; bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan andChurch 1994, van der Eijk 1993) and collocations ( Smadja et al 1996).The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomatic expressions, multi-word names and so on. The difference in compounding between different languages increases the difficulties with the identification of appropriate correspondences further. In addition, the text type is decisive for the word alignment process. Technical text tends to include specific terms and simple structures that are translated directly while e.g. literary texts include many language-specific idioms.Concurrently, Martin Kay's proposal for approaching machine translation can be applied to word alignment as well:"The keynote will be modesty. At each stage, we will do only what we know we can do reliably. Little steps for little feet!" (M.Kay 1980)The alignment of MWUs can be approached in different ways. Smadja et al (1996) propose the compilation of source language collocations using statistical co-occurrence measures {static segmentation). The appropriate correspondent is found by iterative extension of the link unit in the target language segment {dynamic segmentation). Another approach applies collocation lists for both languages, which have been compiled in advance from the bitext ( Ahrenberg et al 1998, Tiedemann 1998). MWUs are then handled like single tokens for both languages. A third possibility is to expand link units iteratively for both languages in order to find the most appropriate link. Melamed (1997) uses iterative processing in order to optimize the underlying translation model. The iteration is alternated in order to cover MWUs in both languages. The word alignment system, which is introduced in this paper, supports all the three approaches to text segmentation as far as contiguous phrases are concerned. The approach to dynamic segmentation differs from Melamed in the usage of ranked candidate lists instead of translation models. Furthermore, classified stop word lists are used for improving the result and reducing the search space. In this paper the current stage o f the Uppsala Word Aligner (UWA) is described. The system is developed within the project on parallel texts, PLUG, which has its focus on the analysis o f bilingual text collections with Swedish either as the source or the target language. UWA comprises a set o f knowledge-lite approaches&apos; for word alignment and lexicon extraction. A distinctive feature is its modularity. In the article, the main principles o f the alignment software are introduced, different configurations and approaches are described, and examples o f alignment results are presented.
On Using the Two-level Model as the Basis of Morphological Analysis and Synthesis of Estonian The module of morphological analysis and/or synthesis is unavoidable in any language engineering tool for Estonian because of its rich morphology. For example, in information retrieval systems, it is usually desirable to make queries using semantic entities, not using special morphological forms of a word. As the word stem often has several shapes in Estonian, the morphological component should belong to any information retrieval system. Example 1. To make a query for all occurrences of the word "rida" ("row"), without a morphological synthesiser, three different queries are needed (we assume the possibility to add star (*) to the end of the stem instead of inflectional suffices):Proceedings of NODALIDA 1999, pages [228][229][230][231][232][233][234][235][236][237][238][239][240][241][242] rida* Cstem in strong grade) rea* Cstein in weak grade) ritta (singular additive ("to the row"), quite often used with this word)The development of EETwoLM is not the first attempt to computerise the morphological analysis and synthesis of Estonian. Ulle Viks (1994) has done important research in the field of morphological classification of Estonian on the basis of pattern recognition theory starting from the 1970s. Viks (1992) has compiled the first morphological dictionary for Estonian as a practical output of the investigations.Further, E. Kuusik and U. Viks (1998) have implemented the rule-based morphological analysis and synthesis for Estonian. Heiki-Jaan Kaalep (1996) has developed the speller for Estonian using the results of Viks (1992).Nevertheless, the growing popularity of the two-level model encourages us to consider its suitability to Estonian morphology. From the practical point of view, the appropriate description of Estonian morphology in the form of lexicons and two-level rules makes the significant move towards the application of Xerox language engineering tools to Estonian language (www.xerox.com/xrce/mltt). The paper deals with the problems of describing the Estonian morphological system in the two-level formalism, developed by Kimmo Koskenniemi, The outlines of Estonian morphology are drawn. The basics of the two-level model are given and illustrated with real examples from the experimental Estonian two-level morphology (EETwoLM) composed by the author. A detailed example of step-by-step morphological synthesis is given referring to all the relevant lexicons and rules. The compilation and testing processes using the XEROX finite-state software tools are described. Examples of morphological analysis and synthesis are demonstrated. The present stage of the system is characterised and the future perspectives are drawn. Finally, the suitability of the two-level model for the description of Estonian morphology is discussed.
LFG-DOT: Combining Constraint-Based and Empirical Methodologies for Robust MT  The Data-Oriented Parsing Model (DOP, [1]; [2]) has been presented as a promising paradigm for NLP. It has also been used as a basis for Machine Translation (MT)-Data-Oriented TVanslation (DOT, [9]). Lexical Functional Grammar (LFG, [5]) has also been used for MT ([6]). LFG has recently been allied to DOP to produce a new LFG-DOP model ([3]) which improves the robustness of LFG. We summarize the DOT model of translation as well as the DOP model on which it is based. We demonstrate that DOT is not guaranteed to produce the correct translation, despite provably deriving the most probable translation. Finally, we propose a novel hybrid model for MT based on LFG-DOP which promises to improve upon DOT, as well as the pure LFG-based translation model. 1 In tr o d u c tio n Neither of the main paradigmatic approaches to MT, namely rule-based and statistical, currently suffice to the standard required. Nevertheless, each contains elements which if properly harnessed should lead to an overall improvement in translation performance. It is in this new hybrid spirit that our search for a better solution to the problems of MT can be
TIPSTER TEXT PROGRAM PHASE I Proceedings of a Workshop held at Sponsored by: Advanced Research Projects Agency A. Data Preparation 1. Tasks, Domains, and Languages for Information Extraction ........ 123  
TIPSTER PROGRAM OVERVIEW  1. TIPSTER PHASE I The task of TIPSTER Phase I was to advance the state of the art in two language technologies, Document Detection and Information Extraction. Document Detection includes two subtasks, routing (run-ning static queries against a stream of new data), and retrieval (running ad hoc queries against archival data). Information Extraction is a technology in which pre-specified types of information are located within free text, extracted, and placed within a database. 2. THE STATE OF THE ART IN DOCUMENT DETECTION BEFORE TIPSTER Before TIPSTER users searching large volumes of data and using many queries had few information retrieval tools to use other than the boolean keyword search systems which had been developed more than a decade earlier. The characteristics of these boolean systems are: • low recall (the user loses an unknown quantity of useful information because the system is unable to retrieve many of the relevant documents) • low precision (the user has to read a very large number of irrelevant documents which the system has mistakenly retrieved) • no ranking or prioritization (the user must scan the entire list of retrieved documents because a good document is just as likely to be at the end of the list of retrieved documents as at the hesinning) • exact matches (the user must generate by hand variant spellings or alternate word choices because there are no built-in rules for adding variants) • hand built queries (the user has to understand how the system works and the syntax of queries in order to use the system) 3. DOCUMENT DETECTION DELIVERABLES IN PHASE H As a result of algorithm development in Phase I, during TIPSTER Phase lI. prototype systems will be built, giving the user Document Detection tools which feature the technology developed in Phase I: • improved recall (comparative evaluation of systems in TIPSTER and TREC [1] has demonstrated higher recall of relevant documents) • improved precisica (the user will read fewer useless documents in order to find the ones he wants) • ranked retrievals (the user reviews documents statistically ranked according to how well they match the query, thus improving the chances that the most useful documents will be near the top of the queue) • query expansion (the system, not the user, automatically expands queries to draw in more relevant documents by using concept based tools such as tbesauri) • automatic query generation (the system uses a natural language description of the subject supplied by the user to generate queries) 4. THE STATE OF THE ART IN INFORMATION EXTRACTION BEFORE TIPSTER Notwithstanding ARPA and commercial support for the development of information extraction technology and the positive impact of the series of Message Understanding Conferences, before TIPSTER, information extraction had been applied to the database update task as largely a manual procedure. Manual extraction is characterized by: • wide variance in the accuracy and consistency of the database content • heavy labor commilanent • continuing cost expenditure and training demand
TIPSTER PROGRAM HISTORY  
THE MESSAGE UNDERSTANDING CONFERENCES  The Message Understanding Conferences (MUCs) provide a forum for assessing and discussing progress in the field of natural language processing. Each conference is preceded by a formal evaluation of text analysis systems that have been developed to perform a shared task, as designed by the Government in consultation with evaluation participants from the research community. The evaluations and conferences bring research groups that are supported by major ARPA contracts together with other research groups, who are attracted by the opportunity to work on a well-defined task with Government-furnished materials, to meet with their peers, and to attract attention to their research and development program. Proceezlings of the last three conferences have been published in the open literature; two of those conferences were held in the period covered by Tipster Phase I [1,2]. A wide variety of system architectures, processing techniques , and tools have been tried, tested, and refined in the context of the evaluations, spurring progress in the robust processing of naturally, occurring text. To a large extent, it was the promise shown by the technology in the period between 1987 and 1991 that led the Government to include information extraction technology in the Tipster program. The task orientation and evaluation format for MUC were adopted by the TIPSTER prokram. Over the years since the first MUC in 1987, the text processing requirements of the evaluation have evolved from analyzing a small number of paragraph-length naval message narratives in a very restricted subject area (MUC-1 in 1987 and MUC-2 in 1989) to analyzing a large number of full-length news articles in much broader areas. MUC-3 (1991) and MUC-4 (1992) required processing of English newswire articles about Latin American terrorism. The MUC-5 evaluation, which ended in July. 1993, was based on the &apos;Iipster articles in ~n~lish and Japanese on joint ventures and microelectrouics. The application which serves as the basis for evaluating the text analysis systems is primarily information extraction, i.e., the identification and formatting of information contained in the news articles. (A secondary task is document detection, since some articles contain no pertinent iaforma-tioga whatsoever.) The MUC-5 application tasks were those that had been defined for the Tipster program. The systems developed by the Tipster-funded contractors (BBN, GEl CMU/NMSU/Brandeis, UMass/Hughes) were evaluated on the basis of their performance in English in both the joint ventures and microelectronics domains; all but the UMass/Hughes system were also evaluated in Japanese in both domains, [1]
THE TEXT RETRIEVAL CONFERENCES  There have been two Text REtrieval conferences frRECs); TREC-1 in November 1992, with 28 participants, and TREC-2 in August 1993, with 31 participants. This conference was inspired by the very successful MUC effort, and by the availability of the new large English test collection built for TIPSTER. Whereas an important goal for ARPA was to investigate a broad range of detection (retrieval) techniques, one of the other goals of the conference was to encourage the use of this collection by many experimenters in the information retrieval community. It was hoped that by providing a very large test collection, and encouraging interaction with other groups in a friendly evaluation forum, a new thrust in information retrieval would occur. It was also hoped that increased interaction between commercial and academic groups would result in a transfer of new retrieval techniques into the commercial setting, enabling better products in the long-term for ARIA cfients. The test design and test collection used for document detection in TIPSTER (and described in a later section of this proceedings) was the same used in the TREC conferences. The test collection consists of over 1 million documents from diverse full-text sources, 150 topics, and the set of relevant documents or &quot;fight answers&quot; to those topics. The participants in TREC ran the same evaluation tasks as in TIPSTER, sent results into NIST for evaluation, and presented the evaluation results at the conferences.
DOCUMENT DETECTION OVERVIEW The goal of the document detection half of the TIPSTER project wasto significantly advance the state of the art in effective document detection from large, real-world document collections. This document detection needed to be used in both the routing environment (static queries against a constant stream of new data) and the adhoc environment (new queries against archival data). An additional requirement was that the algorithms for these tasks be as domain and language independent as possible. To demonstrate language independence, the project was done both in Japanese and English. To demonstrate domain independence, the test collection was selected to cover many different subject areas and different document structures.The document detection task mirrors the general task known as information retrieval. This area of research has seen over 30 years of experimentation [1], leaving a legacy of proven evaluation methodologies. The most prominent of these methodologies is the use of a test collection. A test collection for information retrieval consists of a set of documents, a set of test queries or questions, and a set of relevance judgments that are considered to be the "right" answers to the questions. The first test collections, such as the Cranfield collection, were built in the early 1960's. The Cranfield collection contains 1400 documents (all abstracts), 225 queries (several sentence natural language statements), and 1827 relevance judgments, or an average of about 6 relevant documents per query. Since the early 1960's several other test collections have been built, but none contain the extremely large numbers of documents necessary to reflect the environment to be modeled in TIPSTER.The first step of this project, therefore, was to create a very large test collection and to design the test methodology and evaluation measures needed for TIPSTER. The test design was based on traditional information retrieval models, and is detailed in the next section. Evaluation was done using recall, precision and fallout measures. These measures are discussed in the section on evaluation metrics.The test design and test collection used for TIPSTER was also used for both the TREC conferences [2,3]. The only difference between the evaluation done for the TIPSTER contractors and the TREC participants was in the evaluation schedule and in the number of results submitted for evaluation. The first TREC conference took place 2 months after the 12-month TIPSTER evaluation and the second TREC conference coincided with the 24-month TIPSTER evaluation. The TIPSTER contractors had an additional evaluation at 18 months. TREC participants were limited to submitting only 2 sets of results for adhoc or routing evaluation, whereas the TIPSTER contractors were allowed to submit an unlimited number of runs for evaluation. 
DOCUMENT DETECTION DATA PREPARATION  
DOCUMENT DETECTION SUMMARY OF RESULTS This section presents a summary of the TIPSTER results, including some comparative system performance and some conclusions about the success of the detection half of the TIPSTER phase I project. For more details on the individual experiments, please see the system overviews.Four contractors were involved in the document detection half of TIPSTER. Two of the contractors worked in English only (Syracuse University and HNC Inc.), one contractor worked in Japanese only (TRW Systems Development Division), and one contractor worked in both languages (University of Massachusetts at Amherst). The four contractors had extremely varied approaches to the detection task. TRW transformed an operational English retrieval system (based on pattern matching using a fast hardware approach), into a Japanese version of the same operation, with a special interface designed to facilitate work in Japanese. The University of Massachusetts approach involved taking a relatively small experimental system using a probabilistic inference net methodology, scaling it up to handle the very large amounts of text and long topics in TIPSTER, and modifying the algorithms to handle Japanese. Both Syracuse University and HNC Inc. built completely new systems to handle the English collection. In the case of Syracuse University, their system is based heavily on a natural language approach to retrieval, with many of the techniques traditionally used in document understanding applied to the retrieval task. HNC Inc. took a totally different approach, applying statistical techniques based on robust mathematical models (including the use of neural networks).There were three evaluations of the contractors' work; one at 12 months, one at 18 months, and the final one at 24 months. In each case, the contractors working in English have made multiple experimental runs using the test collection, and turned in the top list of documents found. These results were first used to create the sample pool for assessment, and then were scored against the correct answers based on results from all runs (including TREC-I runs for the 18-month evaluation and TREC-2 runs for the 24-month evaluation). Standard tables using recall/precision and recall/fallout measures were distributed and compared. The evaluation of the Japanese work took place only at the 24-month period. 
INQUERY System Overview  
TIPSTER PHASE I FINAL REPORT  
DR-LINK System: Phase I Summary 1. Description of System 1.1 Approach  The underlying principle of the DR-LINK System is that retrieval must be at the conceptual level, not the word level. That is, a successful retrieval system must retrieve on the basis of what people mean in their query, not just what they say in their query. The same is true of documents-their representation needs to capture the content at the conceptual level of expression. To accomplish this human-like goal, DR-LINK aims to represent and match documents and queries at all of the available levels of linguistic expression at which meaning is conveyed. Accordingly, we have developed a modular system which processes, represents, and matches text at the lexical, syntactic, semantic, and discourse levels of language. In concert, these levels of representation permit DR-LINK to achieve a level of intelligent retrieval beyond more traditional approaches. The DR-LINK system takes an innovative approach to dealing with the specific characteristics of the information retrieval tasks required in TIPSTER, focusing on the development of a retrieval system where documents as well as queries are enriched with multiple levels of annotation, with the final representation being a network of concepts and relations expressed in a conceptual graph (Sowa, 1984), thereby enabling retrieval based on conceptual relations. Relations are extracted and represented throughout the system at many levels, ranging from relations between words, to case relations between arguments of a verb, to discourse level relations involving whole sections of text. The system&apos;s conceptual processing was particularly motivated by various semantic restrictions often found in the TIPSTER topic statements. A retrieval system needs to be able to process natural language sentences and extract key concepts and the implicit relations among them, which cannot be expressed as a set of keywords or phrases. For example, it may be crucial to detect documents that talk about a dispute between Airbus and an aircraft company (i.e. the specific relationship between the two concepts), not just about dispute, Airbus, and aircraft company in isolation. Additional relations, e.g. the discourse level relations of &apos;pending&apos; or &apos;consequence of&apos; are essential requirements of topic statements that need be fulfilled in relevant documents. In order to achieve conceptual level representation, we have implemented a range of methods for detecting concepts and extracting relations from natural language sentences in a large text database, by detecting domain-independent linguistic patterns that reveal relations between concepts, which are contained in the set of knowledge bases. Our efforts at knowledge base construction were geared toward general-purpose use in a variety of text processing applications and were guided by corpus statistics, machine-readable lexical resources, and linguistic theories.
TRW JAPANESE FAST DATA FINDER  The Japanese Fast Data Finder (JFDF) is a system to load electronic Japanese text, allow the user to enter a query in Japanese, and retrieve documents that match the query. Key terms in the browser display are highlighted. The interface is targeted for the non-native Japanese speaker and a variety of tools are provided to help formulate Japanese queries. The system uses TRW/Paracel Fast Data Finder hardware as the underlying search engine.
INFORMATION EXTRACTION OVERVIEW  
TASKS, DOMAINS, AND LANGUAGES FOR INFORMATION EXTRACTION  1. TASKS The information extraction tasks for the ARPA TIP-STER program center on automatically filling object-oriented data structures, called templates, with information extracted from free text in news stories (for discussion of templates and objects, see &quot;Template Design for Information Extraction&quot; in this volume). With text as input, the TIPSTER systems first detect whether the text contains relevant information. If so, the systems extract specific instances of generic types of information that correspond to each slot in the template and output that information by filling the template slots in an appropriate data representation. These slots are then scored by using an automatic scoring program with templates produced by human analysts that serve as answer keys. Human analysts also prepared development set templates for each domain, which served as training models for system developers (for discussion of the data preparation effort, see &quot;Corpora and Data Preparation for Information Extraction&quot; in this volume). With the TIPSTER program goal of demonstrating domain and language-independent algorithms, extraction tasks for two domains (joint ventures and microelectronics chip fabrication) for both English and Japanese were identified. The selection criteria for this pair of languages included linguistic diversity, availability of on-line resources, and availability of computer support resources. The four pairs include EJV, JJV, EME, and JME, abbreviated to reflect the language (E or J) and the domain (JV or ME). The tasks, domains and languages used for the infor-marion extraction portion of the TIPSTER program were also used for the Fifth Message Understanding Conference (MUC-5). In MUC-5, non-TIPSTER participants could choose to perform in one of the domains in Japanese and/or English. Of the TIPSTER participants, three performed in all four pairs, and the fourth in both domains but only in English.
CORPORA AND DATA PREPARATION FOR INFORMATION EXTRACTION  
TEMPLATE DESIGN FOR INFORMATION EXTRACTION  1. ABSTRACT The design of the template for an information extraction application (or exercise) mfieets the nature of the task and therefore crucially affects the success of the attempt to capture information from text. This paper addresses the template design requirement by discussing the general principles or desiderata of template design, object-oriented vs. fiat template design, and template deft-nition notation, all reflecting the results and lessons learned in the TIPSTER/MUC-5 template definition effort which is explicitly discussed in a Case Study in the last section of this paper. 2. GENERAL CONSIDERATIONS The design of the template needs to balance a number of (often conflicting) goals, as reflected by these desiderata, which apply primarily to object-oriented templates (see below), but also have applicability to fiat-structure templates as well. Some of these desiderata reflect well-known, good database design practices, whereas others are particular to Information Extraction. Some of these desiderata are further illusl~ated in the Case Study section below. • DESCRIPTIVE ADEQUACY-the requirement for a template to represent all of the information necessary for the task or application at hand. At times the inclusion of one type of information requires the inclusion of other, supporting, information (for example, measurements require specification of units, and temporally dynamic relations require temporal parametrization). • CLARITY-the ability to represent information in the template unambiguously, and for that information to be manipulable by computer applications without further inference. Depending on the application, any ambiguity in the text may result in either representation of that ambiguity in the template, or representation of default (or inferred) values, or omission of that ambiguous information altogether.
TIPSTER/MUC-5 INFORMATION EXTRACTION SYSTEM EVALUATION Three information extraction system evaluations using Tipster data were conducted in the context of Phase 1 of the Tipster Text program. Interim evaluations were conducted in September, 1992, andFebruary, 1993; the final evaluation was conducted in July, 1993. The final evaluation included not only the Tipster-supported inform~on extraction contractors but thirteen other participants as well. This evaluation was the topic of the Fifth Message Understanding Conference (MUC-5) in August, 1993. With particular respect to the research and development tasks of the Tipster contractors, the goal of these evaluations has been to assess success in terms of the development of systems to work in both English and Japanese (BBN, GE/CMU, and NMSU/Brandeis) and/or in both the joint ventures and microelectronics domains (BBN, GE/CMU, NMSU/Brandeis, and UMass/Hughes).The methodology associated with these evaluations has been under development since 1987, when the series of Message Understanding Conferences began. The evaluations have pushed technology to handle the recurring language problems found in sizeable samples of naturallyoccuring text. Designing the evaluations around an information extraction application of text processing technology has made it possible to discuss NLP techniques at a practical level and to gain insight into the capabilities of complex systems.However, any such evaluation testbed application will undoubtedly differ in important respects from a real-life application. Thus, there is only an indirect connection between the evaluation results for a system and the suitability of applying the system to performance of a task in an operational setting. A fairly large number of metrics have been defined that respond to the variety of subtasks inherent in information extraction and the varying perspectives of evaluation consumers.The evaluations measure coverage, accuracy, and classes of error on each language-domain pair, independently of all other language-domain pairs that the system may be tested on. With its dual language and domain requirements and challenging task definition, Tipster Phase 1 pushed especially hard on issues such as portability tools, languageand domain-independent architectures and algorithms, and system efficiency. These aspects of software were not directly evaluated, although information concerning some or all of them may be found in the papers prepared by the evaluation participants, 
AN ANALYSIS OF THE 3OINT VENTURE JAPANESE TEXT PROTOTYPE AND ITS EFFECT ON SYSTEM PERFORMANCE  
COMPARING HUMAN AND MACHINE PERFORMANCE FOR NATURAL LANGUAGE INFORMATION EXTRACTION: Results from the Tipster Text Evaluation In evaluating the state of technology for extracting information from natural language text by machine, it is valuable to compare the performance of machine extraction systems with that achieved by humans performing the same task. The purpose of this paper is to present some results from a comparative study of human and machine performance for one of the information extraction tasks used in the Tipster/ MUC-5 evaluation that can help assess the maturity and applicability of the technology.The Tipster program, through the Institute for Defense Analyses (IDA) and several collaborating U.S. government agencies, produced a corpus of filled "templates" --StlUCtured information extracted from text. This corpus was used both in the development of machine extraction systems by contractors and in the evaluation of the developed systems. Production of templates was performed by human analysts extracting the data from the text and structuring it, using a set of structuring rules for "filling" the templates and computer software that made it easier for analysts to organize information. Because of this rather extensive effort by analysts to create these templates, it was possible to study the performance of humans for this task in some detail and to develop methods for comparing this performance with that of machines participating in the Tipster/MUC-5 evaluation.The texts that the templates were filled from were newspaper and technical magazine articles concerned either with joint business ventures or microelectronics fabrication technology. Each topic domain used text in two languages, English and Japanese. This paper discusses preparation of templates and presents detailed results for human and machine performance; a shorter paper [1] discusses preparation of templates and basic results.The primary motivation for this study was to provide reliable data that would allow machine extraction performance to be compared with that of humans. The MUC and Tipster programs have included extensive efforts to develop measurements that can objectively evaluate the performance of the different machine systems. However, although these measures are capable of reliably discriminating between the performance of different machine systems, they are not very useful by themselves in evaluating how near the technology is to providing reliable performance and the extent to which it is ready to be used in applications. Sundheim [2] initiated human performance study for extraction by providing estimates of human performance for the task used in the MUC-4 evaluation; the present study provides human data for the Tipster/MUC-5 evaluation that was produced under relatively controlled conditions and with methods and statistical measures that assess the reliability of the data.A second motivation for the study was for its value in helping produce better quality templates so as to allow highquality system development and reliable evaluation. The quality and consistency of the templates being produced were monitored as analysts were trained and gained experience, and particular efforts were made to identify the causes of errors and inconsistency so as to develop strategies for reducing error and increasing consistency.A third motivation for studying human performance was to better understand the nature of the extraction task and the relative performance of humans compared with machines on different aspects of the task. Such an understanding can particularly help in the construction of human-machine integrated systems that are designed to make the best use of what are at the present time rather different abilities of humans and machines [3]. This paper is organized as follows:The paper begins with a discussion of how the templates were prepared, with particular emphasis on the strategies that were used that served to minimize errors and maximize consistency, including detailed fill rules, having more than one analyst code a given template, and the use of software tools with error detection capabilities.The paper next describes the results of an investigation into the extent to which template codings made by analysts that are playing different roles in the production of a particular template influence the resulting key, which provides clues to the effectiveness of the quality control strategies used in the template preparation process.The results of an experimental test of different methods of scoring human performance are then presented, with the goal of selecting a method that is statistically reliable, minimizes bias, and has other desirable characteristics. Data that indicates overall levels of human performance on the task, variability among analysts, and reliability of the data are then presented.The results of an investigation into the development of analyst skill are then presented, with the significant question being the need to understand whether the performance levels being measured truly reflect analysts who have a high level of skill.The performance of humans for information extraction is then compared with that of machine systems, in terms of both errors and metrics that attempt to separate out two different aspects of performance, recall and precision,The results of a study comparing the effect of key preparation on the evaluation of machine performance are then presented. This is particularly relevant to the question of how keys should be future MUC and Tipster evaluations.A study is then presented of the extent to which machines and humans agree on the relative difficulty of particular templates.The results of a pilot study in which the performance of humans and machines is compared for particular kinds of information, to see what information machines are comparatively worse or better than humans in extracting, is then presented.A final section of the paper makes some general conclusions about the results and their implications for assessing the maturity and applicability of extraction technology. This paper presents results from a study comparing human performance on the text of natural language information extraction with that of machine extraction systems that were developed as part of the ARPA Tipster program. Information extraction is shown to be a difficult task for both humans and machines. Evidence for one set of text material, English Microelectronics, indicated that a human analyst produces about half the errors as does machine systems.
BBN&apos;s PLUM Probabilistic Language Understanding System The PLUM System Group*  
THE TIPSTEPdSHOGUN PROJECT *  This paper presents an overview of the TIPSTER/SHOGUN project, the major results, and the SHOGUN data extraction system. TIP-STER/SHOGUN was a joint effort of GE Corporate Research and Development, Carnegie Mellon University, and Martin Marietta Management and Data Systems (formerly GE Aerospace), part of the ARPA TIPSTER Text program. Two of the main technical thrusts of the project were: (1) the development of a model of finite-state approximation, in which the accuracy of more detailed models of language interpretation could be realized in a simple, efficient framework, and (2) (3) experiments in automated knowledge acquisition , to minimize customization and ease the tuning and extension of the system. Innovations in each of these areas allowed the project to meet its goal of achieving advances in coverage and accuracy while showing consistently good performance across languages and domains. 1. SHOGUN SYSTEM DESCRIPTION The GE-CMU TIPSTER/SHOGUN system is the result of a two-year research effort, part of the ARPA-sponsored TIP-STER data extraction program. The project&apos;s main goals were: (1) to develop algorithms that would advance the state of the art in coverage and accuracy in data extraction, and (2) to demonstrate high performance across languages and domains and to develop methods for easing the adaptation of the system to new languages and domains. The system as used in MUC-5 (the final TIPSTER benchmark) represents a considerable shift from those used in earlier stages of the program and in previous MUC&apos;s. The original SHOGUN design integrated several different approaches by combining different knowledge sources, such as syntax, semantics , phrasal rules, and domain knowledge, at run-time. This allowed the system to achieve a good level of performance very quickly, and made it easy to test different modules and methods; however, it proved very difficult to make all the changes necessary to improve the system, especially across languages, when system knowledge was so distributed at run-time. As a result, the team adopted a new approach, relying heavily onfinite-state approximation. This method combines several
CRL/BRANDEIS: THE DIDEROT SYSTEM  1. Description of Final System Diderot is an information extraction system built at CRL and Brandeis University over the past two years. It was produced as part of our efforts in the Tipster project. The same overall system architecture has been used for English and Japanese and for the microelectronics and joint venture domains. The past history of the system is discussed and the operation of its major components described. A summary of scores at the 24 month workshop is given. Because of the emphasis on different languages and different subject areas the research has focused on the development of general purpose, re-usable techniques. The CRL/Brandeis group have implemented statistical methods for focusing on the relevant parts of texts, programs which recognize and mark names of people, places and organizations and also dates. The actual analysis of the critical parts of the texts is carried out by a parser controlled by lexical structures for the &apos;key&apos; words in the text. To extend the system&apos;s coverage of English and Japanese some of the content of these lexical structures was derived from machine readable dictionaries. These were then enhanced with information extracted from corpora. The system has already been evaluated in the 4th Message Understanding Conference (MUC-4) where it was required to extract information from 200 texts on South American terrorism. Considering the very short development time allowed for this additional domain the system performed adequately. The system was then adapted to handle the business domain and also to process Japanese texts. Further extensions to the system allowed it to process texts on microelectronics development. Performance at the 12 and 18 month evaluations was good for Japanese, but less good for English where we have been attempting to automate much of the development process. A more pragmatic approach was adopted for the final 24 month evaluation, using the same hand-crafted techniques for English as had been used for Japanese. We estimate the amount of effort used directly to build the systems described here is around sixty man months.
UMASS/HUGHES: DESCRIPTION OF THE CIRCUS SYSTEM USED FOR TIPSTER TEXT The primary goal of our effort is the development of robust and portable language processing capabilities for information extraction appfications. The system under evaluation here is based on language processing components that have demonstrated strong performance capabilities in previous evaluations [Lehnert et al. 1992a]. Having demonstrated the general viability of these techniques, we are now concentrating on the practicality of our technology by creating trainable system components to replace handcoded d~t~ and manually-engineered software.Our general strategy is to automate the construction of domain-specific dictionaries and other languagerelated resources so that information extraction can be customized for specific application s with a minimal amount of human assistance. We employ a hybrid system architecture that combines selective concept extraction [Lehnert 1991] technologies developed at UMass with trainable classifier technologies developed at Hughes [Dolan et al. 1991]. Our Tipster system incorporates seven trainable language components to handle (1) lexical recognition and partof-speech tagging, (2) knowledge of semantic/syntactic interactions, (3) semantic feature tagging, (4) noun phrase analysis, (5) limited coreference resolution, (6) domain object recognition, and (7) relational link recognition. Our trainable components have been developed so domain experts who have no background in natural language or machine learning can train individual system components in the space of a few hours.Many critical aspects of a complete information extraction are not appropriate for customization or trainable knowledge acquisition. For example, our system uses low-level text specialists designed to recognize dates, locations, revenue objects, and other common constructions that involve knowledge of conventional language. Resources of this type are portable across domains (although not all domains require all specialists) and should be developed as sharable language resources. The UMass/I-Iughes focus has been on other aspects of information extraction that can benefit from corpus-based knowledge acquisition. For example, in any given information extraction application, some sentences are more important than others, and within a single sentence some phrases are more important than others. When a dictionary is customized for a specific application, vocabulary coverage can be sensitive to the fact that a lot of words contribute little or no information to the final extraction task: full dictionary coverage is not needed for information extraction applications.In this paper we will overview our hybrid architecture and trainable system components. We will look at examples taken from our official test runs, discuss the test results obtained in our official and optional test runs, and identify promising opportunities for ~cldjtional research. 
DICTIONARY CONSTRUCTION BY DOMAIN EXPERTS  
Appendix I TIPSTER/MUC-5 INFORMATION EXTRACTION TEST SCORES  This appendix contains the summary score reports for each Tipster system on the Tipster/MUC-5 evaluation. The reports are ordered by language/domain (EJV, JJV, EME, JME) and secondarily by site (in alphabetical order). Deeper investigation into system performance may be done on the basis of template-by-template score reports, which detail the performance of the system on each article in the test set. The template-by-template score reports are not included in this appendix.
TIPSTER TEXT PROGRAM PHASE II Proceedings of a Workshop held at  
WELCOME to TIPSTER  
FOCUS OF TIPSTER PHASES I and II Background of Phase I  
EVALUATION DRIVEN RESEARCH: The Foundation of the TIPSTER Text Program INTRODUCTION  
Some Technology Transfer: Observations from the TIPSTER Text Program Technology Transfer has been an important part of the TIPSTER Text Program from the beginning. Research alone was insufficient as a motivation for the program. What was discovered in the laboratory had also to be transferred as quickly as possible into the workplace. The central role of technology transfer in the initial formation of TIPSTER had several causes. Government sponsors and initiators of the program, in 1990, could see clearly the inadequacies of the tools that analysts were working with, at the time, and could also see that the overload of text which analysts dealt with was only going to get worse, given the proliferation of information sources and the increasing push in the Intelligence Community to tap those sources. Knowing the normally long time it takes research advances to make their way into standard technology, the founders of the program rightly believed a concerted effort to place good technology in the hands of users would be necessary to insure that they received the benefits of the program as soon as possible. There were other pressures: continuing reductions in the numbers of analysts available to do analysis, reductions in budgets, and constant pressures to justify Government sponsored research in terms of demonstrable and practical benefits to the Government. The research fields sponsored by TIPSTER are application oriented in any case. Both Document Detection and Information Extraction imply an eventual user -someone who needs documents or someone who needs particular information about particular kinds of entities or events. Successful research does thus have direct implications for operational applications.But more broadly, applications can be important to much research that deals with human language. Language exists for the communication of meaning.Any automated processing of language cannot be really evaluated outside the context of the actions of expressing and understanding, both actions which are highly situation, or task, dependent. In addition, language is a human construct and, whatever its imperfections, a person is I the only real authority we have on language, the only measure of whether or not meaning has been conveyed. The application of automated language processing to concrete human tasks is itself an important research method in this field.Although the goal of strengthening the science through the application of the technology was not explicitly stated as a reason for the emphasis on technology transfer in the TIPSTER Program, nonetheless I think the emphasis on tasks and on the usefulness of the technology is benefiting the underlying science of computational linguistics.Central to the initial TIPSTER planning, then, was the goal of technology transfer. The magnitude of the difficulty of achieving that goal was perhaps not well understood in the early days of the program; but since then, we have all learned that this transfer does not happen automatically, but requires even more effort, planning, and creativity than the research. The goal of technology transfer is subscribed to by both Government and contractor participants. There are considerable potential rewards which serve as important motivations to people involved in the process -material incentives, but also the satisfaction of building something that works and does useful tasks for people who need it. These rewards are important, because the process of technology transfer is difficult and messy. It requires the persistence and flexibility to tackle many obstacles. It is as much a matter of business and psychology, as it is of technology and engineering.TIPSTER Phase II has made a number of strides forward in transferring the research advances of Phase I into operational use. Collectively the participants in the program have learned a great deal about what works and does not work in transferring technology. The program remains committed to continuing an aggressive technology transfer effort. This paper summarizes what I have learned from the Phase II effort. It can perhaps serve as a basis for others to reflect on the same issues. A record of what was learned in Phase II efforts will be valuable for continued tech transfer in the future. 
ARCHITECTURE OVERVIEW TIPSTER SE/CM tipster@fipster.org THE TIPSTER ARCHITECTURE  
THE MESSAGE UNDERSTANDING CONFERENCES  
The Text REtrieval Conferences (TRECs) A Sample of the TREC-4 Participants  The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text retrieval (see table for some of the participants). The diversity of the participating groups has ensured that TREC represents many different approaches to text retrieval , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks, sent results into NIST for evaluation, presented the results at the TREC conferences , and submitted papers for a proceedings. The test collection consists of over 1 million documents from diverse full-text sources, 250 topics, and the set of relevant documents or &quot;right answers&quot; to those topics. A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics. TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection). The results from TREC-2 showed significant improvements over the TREC-1 results, and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection. TREC-3 therefore provided the first opportunity for more complex experimentation. The major experiments in TREC-3 included the development of automatic query expansion techniques, the use of passages or sub-documents to increase the precision of retrieval results, and the use of the training information to select only the best terms for routing queries. Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector space model), and others tried approaches that were radically different from their original approaches. TREC-4 allowed a continuation of many of these complex experiments. The topics were made much shorter and this change triggered extensive investigations in automatic query expansion. There were also five new tasks, called tracks. These were added to help focus research on certain known problem areas, and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval. The TREC conferences have proven to be very sue-cessful, allowing broad participation in the overall DARPA TIPSTER effort, and causing widespread use of a very large test collection. All conferences have had very open, honest discussions of technical issues, and there have been large amounts of &quot;cross-fertilization&quot; of ideas. This will be a continuing effort, with a TREC-5 conference scheduled in November of 1996.
TIPSTER PHASE III GOALS  
TIPSTER II ACTIVITIES AT HNC  The arrival of the information age has brought with it new challenges for handling the vast quantities of electronically available information. The ARPA and Intelligence Community sponsored TIPSTER program has risen to this challenge. New technologies have been developed for attacking problems in information retrieval, information extraction, and multilingual information processing. As a premier developer of neural network based technologies, HNC has played an important role in successfully bringing innovative approaches to the problem of information retrieval, including multilingual information retrieval. The purpose of this brief paper is to summarize HNC&apos;s research accomplishments and to make recommendations for further study. The HNC approach to information retrieval is based on context vectors, which are unit vectors in a high dimensional vector space. The relative directions of these vectors encode the meaning and context of information that is to be retrieved. Neural network based training laws are used to adjust the components of these vectors in an iterative fashion. The basic context vector methodology has been implemented in a system called MatchPlus, which serves as a stand-alone information retrieval application as well as the technological core upon which additional context vector applications have been built. One problem with the MatchPlus system has been the large computation requirements of the learning law. In order to reduce the severity of this problem, HNC has developed a one step learning law that approximates the behavior of the original learning law at a fraction of the computational cost. Preliminary performance results with this learning law have been quite encouraging. More complete performance results can be generated when additional funding becomes available. HNC has also developed a preliminary prototype of an English-Spanish MatchPlus system. This system uses redundant hash table addressing to store context vectors for a multilingual vocabulary. This system can easily be extended to additional languages, such as Japanese, Russian, Chinese, etc. The multilingual MatchPlus approach is able to perform information retrieval in multiple languages without the necessity of specifying any grammatical information about the language, thereby greatly reducing system development time. HNC&apos;s context vector technology has also been extended to the problem of information routing and filtering, resulting in a COTS product known as CONVECTIS. CONVECTIS is currently being used by DataTimes (a large commercial information provider) as the core technology for routing information to customers based on customer specified &quot;interest profiles.&quot; The use of context vectors results in a natural method for specifying degree of relationship between incoming news feeds and customer interests. It is also a natural mechanism for detecting novel information themes, a fact that could be advantageously used by the Intelligence Community. HNC has also responded to the explosion of information available on the Internet by developing a system of autonomous retrieval agents based upon the MatchPlus technology. These retrieval agents can be scheduled by the user to access information at specified, possibly repeated times, thereby removing the requirement that the user actually be present during the retrieval operation. This system is currently being extended to handle multiple Internet and for-fee database protocols, resulting in a useful system for information retrieval from heterogeneous information sources. Since the context vector methodology requires only that a finite vocabulary of entities be defined for the domain of information, this technology has been extended to other media, images in particular. The ICARS system is being developed to solve the difficult problem of image retrieval and image content characterization. This system attaches context vectors to cluster centroids of feature vectors composed of Gabor features and color information. 45
THE LOCKHEED MARTIN TIPSTER II PROJECT The Lockheed Martin TIPSTER II project focused on several research areas. In addition, three demonstration projects are underway to exhibit the feasibility of transitioning the research into an operational setting.in text together with a set of tools for easily constructing recognizers for new objects. In addition, the group has advanced the state of the art for identifying co-referential noun phrases. This work was evaluated during MUC-6 and performed extremely well. The ability to resolve co-references provides a sound basis for all forms of link analysis [1] 
CERVANTES -A SYSTEM SUPPORTING TEXT ANALYSIS  CRL is engaged in the development of document management software and user interfaces to support government analysts in their information analysis tasks. It is also continuing to develop language technologies to support document detection and information extraction in a variety of languages. It has also been responsible for the integration and delivery of both the six and twelve month Tipster demonstration systems and the development of the first Tipster document manager. Approach CRL has provided general purpose enabling technology for several aspects of the Tipster Phase H program. This work has been carried out in close compliance with the decisions of the Architecture Working Group. The software developed at CRL is based on CRL&apos;s extensive experience of user interface support for government analysts developed during Phase I of Tip-ster. In addition we have developed a large scale document management system, which allows documents used in a Tipster compliant system to be handled in a nniform manner. CRL has investigated the problem of information retrieval against collections of text written in multiple languages. This multilingual information retrieval capability is being designed so that it can be integrated into any statistical information retrieval system. Achievements CRL has been heavily involved in the design of the Tipster Architecture. Prototype document managers supporting the architecture were implemented and used to support the Tipster 6 and 12 month damonstration systems. A mature version of the document manager software has now been developed and distributed. CRL has also developed a Tipster Architecture Validation Suite which allows the testing of Tipster Compliant Document Managers. Both these products will now be subject to an engineering review board. The CRL has provided mulfilingual Human-Computer Interface software, which conforms to the Tipster architecture. This includes a sophisticated editor which allows the display and editing of annotations on documents. A user can work with annotations produced by any Tipster compliant language processing software (e.g nama recognizers, phrase spotters). The editor supports multiple languages, including Arabic, Japanese, Span-ish, Chinese and Russian. All the CRL graphical user interface tools are now available to government agencies and to other research groups (see paper on Graphical User Interfaces). CRL has used the architecture as the foundation for other DoD programs-Oleada and Temple (see separate s-mmaries). CRL has also made significant progress in its research in multilingual query generation. Methods have been developed to &apos;translate&apos; Engfish queries into Spanish (see research papers) CRL has developed systems for recognizing proper names in English, Spanish, Japanese, and Chinese texts (see Multilingual Named Entity Task) Software Packages The following packages are available from CRL-a Tipster compliant document manager and user docu-mentafious, a Tipster document manager validation suite, a graphical user interface toolkit to support development of multilingual Tipster applications, and English name recognition software and data. 49
THE NYU TIPSTER II PROJECT NYU supported the TIPSTER Phase II effort in the development of the TIPSTER Architecture, with enhancements to both their Detection and Extraction systems, and with experiments in the combined use of Extraction and Detection for document retrieval. 
SRA PARTICIPATION IN TIPSTER PHASE II  
THE SRI TIPSTER II PROJECT The SRI TIPSTER Phase II program focused on supporting the development of an integrated architecture by helping to define the TIPSTER architecture and improving the portability of data extraction applications by enabling users to define and tailor their own information needs. 
SPOT: TRW&apos;S MULTI-LINGUAl, TEXT SEARCH TOOL  TRW has developed a text search tool that allows users to enter a query in foreign languages and retrieve documents that match the query. A single query can contain words and phrases in a mix of different languages, with the foreign-language terms entered using the native script. The browser also displays the original document in its native script. Key terms in the browser display are highlighted. The interface is targeted for the non-native speaker and includes a variety of tools to help formulate foreign-language queries. Spot has been designed to interface to multiple search engines through an object-oriented search engine abstraction layer. It currently supports Paracers Fast Data Finder search engine, with support for Excalibur&apos;s RetrievalWare currently being developed.
ADVANCED DATA EXTRACTION AND PREPARATION VIA TIPSTER (ADEPT)  
Cable Abstracting and INdexing S_ystem (CANIS) Prototype  
Management of Free Text for NDIC: An Overview of the FTM Project  
THE HOOKAH INFORMATION EXTRACTION SYSTEM  This paper describes Project HOOKAH, a TIPSTER Implementation Project with the Drug Enforcement Administration to extract information from the DFFA-6 field report. The paper overviews Project HOOKAH, describes the system architecture and modules, and discusses several lessons that have been learned from this application of TIPSTER technology. 1. PROJECT HOOKAH 1.1 Overview Project HOOKAH is a TIPSTER Implementation Project with the Drug Enforcement Administration to extract information from DEA field reports in support of populating a database. Its goal is the partial automation of DEA operations by moving information extraction technology into the DEA fileroom, where these reports are currently manually processed. HOOKAH has been supported by Congressional &quot;Dual Use&quot; funding for transferring TIPSTER technology to civilian agencies. The prototype development effort has been managed by Mary Ellen Okurowski and Boyan Onyshkevych of the Department of Defense. The deployment effort is being jointly managed by DoD and DEA, with DEA responsible for life cycle maintenance. 1.2 Domain: DEA-6s The focus of Project HOOKAH is to improve the processing of the DEA-6 report, a semi-formatted report generated primarily by field agents, as well as legal staff, analysts, and others. DEA-6s are organized into case files, and are composed of multiple sections with varying amounts of formatting. Header fields are normally highly formatted, and indicate the subject, case, date, time, etc. There is a semi-formatted index, which contains references to most subjects to be to the database and some information about them. There is also unformatted text, where much of the useful information is found.
Message Handler  The Message Handler extracts critical intelligence information from the free-text of military messages to merge with the data&apos;parsed from fixed-fields. In the third of four project phases, the current focus is on developing stand-alone capabilities demonstrating the viability of natural language processing in Advance Concept Technology Demonstrations (ACTDs), and to operational users at military commands. E-Systems&apos; engineers address the overall military message traffic processing problem, while SRI Interation~l&apos;s computational linguists adapt their Finite State Automaton Text Understanding System (FASTUS) technology to the domain of military message free-text. In previous phases, the Message Handler processed over 180 types of messages while establishing the feasiblity of trainlng the FASTUS technology to process the free-text of military messages. The previous efforts demonstrated that combining the FASTUS syntactical approach to processing free-text using finite-state automata with conventional parsing of fixed fields can yield results approximating human performance working under similar operational conditions. At the end of Phase II of the project, the Message Handler was able to process 4000 military messages per day. The automatically scored and manually validated results for messages without tables were 75% recall, 31% overgeneration, and 64% precision. The goals of the Message Handler project is to process 10,000 military messages per day at a performance level of 80information extracted from the messages includes facilities, units, equipment, locations, times, coordinates, unit associations , events, event relationships, and parametric measures. The message handling problem involves nuisances such as the structure and organization of both the overall message and the free-text. Another significant message handling issue is the processing of unique ad-hoc tables which commonly appear within the free-text portion of messages. The Message Handler development reflects the evaluation of thousands of real world messages from the military theaters. The voluminous and constantly changing flow of military messages necessitates an ongoing review of the message corpus received by military commands to support the Message Handler development. In analyzing how the Message Handler functions match intelligence production environments, the system engineers established that the Message Handler should produce multiple interpretations of messages which precludes the denial of vital information to downstream systems that would correlate and fuse Message Handler outputs with data from other sources. The Message Handler effort includes initiatives for making natural language processing a viable tool for the military user. Previous phases focused on developing extraction technology allowing minimal human intervention. The current phase assesses the appropriate level of human intervention and the appropriate interface tools. There is an ongoing re-assessment of what military-related language maintenance is amenable to execution by military users, and what maintenance must be reserved for linguistic specialists. The user tools will permit easy modification for lexicon or gazetteer and as much capability as possible for modifying domain-based grammars. The Message Handler program has a collaborative engineering initiative between developer and potential users employing Internet and Intellink capabilities in the design and maintenance of the Message Handler. 83
Oleada: User-Centered TIPSTER Technology for Language Instruction  TIPSTER is an ARPA sponsored program that seeks to develop methods and tools that support analysts in their efforts to filter, process, and analyze ever increasing quantities of text-based information. To this end, government sponsors, contractors, and developers are working to design an architecture specification that makes it possible for natural language processing techniques and tools, from a variety sources, to be integrated , shared, and configured by end-users. The Computing Research Laboratory (CRL) is a longtime contributor to TIPSTER. A significant portion of CRL&apos;s research involves work on a variety of natural language processing problems, human-computer interaction, and problems associated with getting technology into the hands of end-users. CRL is using TIPSTER technology to develop OLEADA, which is an integrated set of computer tools designed to support language learners, and instructors. Further, OLEADA has been developed using a task-oriented user-centered design methodology. This paper describes the methodology used to develop OLEADA and the current system&apos;s capabilities. 2.0 TIPSTER and the Computing Research Laboratory Information extraction is a relatively new application of natural language processing techniques in which basic information and relationships are found and extracted from text. TIPSTER I was an effort to find electronic methods for information retrieval and information extraction.TIPSTER uses texts from a variety of sources including newspaper articles and wire service reports. The information TIPSTER I extracts resembles a completed form. The contents of a form is intended to be used to automatically generate specialized databases for information analysts. The components developed for TIPSTER I enabled it to function in two languages (Jap-anese and English). TIPSTER II is a joint effort among many sites to develop working systems that integrate information retrieval and information extraction. The core of the project is a joint government/contractor committee whose goal is to specify an architecture for TIPSTER II. TIPSTER developers work to provide a variety of specialized software subsystems that support TIPSTER development. These include: • Document managers that provide multi-source document compatibilities. • Translation subsystems that support retrieval of documents in many languages, based on a query in one language. • Libraries of procedures for user interface support with embedded functionality for Information Retrieval and Information Extraction. • Advanced Motif-based multilingual user interface capabilities, supporting Chinese, Japanese, Korean, Arabic and other writing systems.
AN OVERVIEW OF THE PROTOTYPE INFORMATION DISSEMINATION SYSTEM (PRIDES) The Prototype Information Dissemination System (PRIDES) is a TIPSTER technology insertion project sponsored by the Office of Research and Development (ORD). PRIDES applies a portion of the TIPSTER detection architecture and several TIPSTER components to the problem of timely dissemination of Foreign Broadcast Information Service (FBIS) articles. When PRIDES begins operation in July 1996, it will provide one of the first production tests of the TIPSTER architecture. 
SPOT: TRW&apos;S MULTI-LINGUAl, TEXT SEARCH TOOL  TRW has developed a text search tool that allows users to enter a query in foreign languages and retrieve documents that match the query. A single query can contain words and phrases in a mix of different languages, with the foreign-language terms entered using the native script. The browser also displays the original document in its native script. Key terms in the browser display are highlighted. The interface is targeted for the non-native speaker and includes a variety of tools to help formulate foreign-language queries. Spot has been designed to interface to multiple search engines through an object-oriented search engine abstraction layer. It currently supports Paracers Fast Data Finder search engine, with support for Excalibur&apos;s RetrievalWare currently being developed.
The Temple Translator&apos;s Workstation Project  
SRA PROJECT FOR ARPA / USACOM  
CHINESE INFORMATION EXTRACTION AND RETRIEVAL  This paper provides a summary of the following topics: I. what was learned from porting the INQUERY information retrieval engine and the INFINDER term finder to Chinese 2. experiments at the University of Massachusetts evaluating INQUERY performance on Chinese newswire (Xinhua), 3. what was learned from porting selected components of PLUM to Chinese 4. experiments evaluating the POST part of speech tagger and named entity recognition on Chinese. 5. program issues in technology development.
TIPSTER-COMPATIBLE PROJECTS AT SHEFFIELD  Projects currently underway at Sheffield may be more appropriately described by the term Language Engineering than the well-established labels of Natural Language Processing or Computational Linguistics. This reflects an increased focus on viable applications of language technology, promoting a view of the software infrastructure as central to the development process. To this end, Sheffield has produced GATE-a TIPSTER-compatible General Architecture for Text Engineering-providing an environment in which a number of Sheffield projects are currently being developed. GATE GATE is an architecture in the sense that it provides a common infrastructure for building language engineering (LE) systems. It is also a development environment that provides aids for the construction, testing and evaluation of LE systems (and particularly for the reuse of existing components in new systems). GATE presents researchers and developers with an environment in which they can easily use linguistic tools and databases from a user-friendly interface. Different processes, such as tagging or parsing, can be applied to documents or collections and the results compared and analysed. System modules, or combinations of modules which form complete systems , e.g. IE, IR or MT systems, can be configured and evaluated (e.g. using the Parseval tools), then reconfigured and reevaluated, providing a kind of edit/compile/test cycle for LE components. GATE comprises three principal elements: • the GATE Document Manager (GDM)-a TIPSTER-compatible database for storing information about texts; • the GATE Graphical Interface (GGI)-an interface for launching processing tools on data and viewing and evaluating the results; • a Collection of Reusable Objects for Language Engineering (CREOLE)-a collection of wrappers for algorithmic and data resources that in-teroperate with the database and interface. GDM GDM is based on the TIPSTER document manager. It provides a central repository or server that stores all the information an LE system generates about the texts it processes. All communication between the system components goes through GDM, thereby insulating parts from each other and providing a uniform API (applications programmer interface) for manipulating the data produced by the system. 1 Benefits of this approach include the ability to exploit the maturity and efficiency of database technology, easy modelling of blackboard-type distributed control regimes (of the type proposed by [2]), and reduced interdependence of components. GGI GGI is a graphical launchpad for LE subsystems, and provides various facilities for testing and viewing results, and interactively assembling LE components into different system configurations. As we built Sheffield&apos;s MUC-6 entry, LaSIE [6], it was often the case that we were unsure of the implications for system performance of using tagger X instead of tagger Y, or gazetteer A instead of pattern matcher B. In the GGI interface, substitution of components is a point-and-click operation. This facility supports hybrid systems, ease of upgrading and open systems-style module interchangeability. 2 Figure 1 shows the launchpad for a MUC-6 IE system. Colours are used to indicate the status of each component with respect to the current docu-ment/collection: dark (red) components have already been run and their results are available for viewing; light (green) components have all their required inputs available and are ready to run, and grey (amber) components require a currently unavailable input before they can become runnable. IWhere very large data sets need passing between modules, other external databases can be employed if necessary. 2Note that delivered systems can use GDM and CREOLE without GGI. 121
PROGRESS IN INFORMATION EXTRACTION  This paper provides a quick summary of the following topics: enhancements to the PLUM information extraction engine, what we learned from MUC-6 (the Sixth Message Understanding Conference), the results of an experiment on merging templates from two different information extraction engines, a learning technique for named entity recognition, and towards information extraction from speech.
THE ROLE OF SYNTAX IN INFORMATION EXTRACTION Our group at New York University has developed a number of information extraction systems over the past decade. In particular, we have been participants in the Message Understanding Conferences (MUCs) since MUC-1. During this time, while experimenting with many aspects of system design, we have retained a basic approach in which information extraction involves a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure [2]. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead.However, we have recently found ourselves at a disadvantage with respect to groups which performed more local pattern matching, in three regards: 
NATURAL LANGUAGE INFORMATION RETRIEVAL: TIPSTER-2 FINAL REPORT  We report on the joint GE/NYU natural language information retrieval project as related to the Tipster Phase 2 research conducted initially at NYU and subsequently at GE R&amp;D Center and NYU. The evaluation results discussed here were obtained in connection with the 3rd and 4th Text Retrieval Conferences (TREC-3 and TREC-4). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. During the course of the four TREC conferences, we have built a prototype IR system designed around a statistical full-text indexing and search backbone provided by the NIST&apos;s Prise engine. The original Prise has been modified to allow handling of multi-word phrases, differential term weighting schemes, automatic query expansion, index partitioning and rank merging, as well as dealing with complex documents. Natural language processing is used to preprocess the documents in order to extract content-carrying terms, discover inter-term dependencies and build a conceptual hierarchy specific to the database domain, and process user&apos;s natural language requests into effective search queries. The overall architecture of the system is essentially the same for both years, as our efforts were directed at optimizing the performance of all components. A notable exception is the new massive query expansion module used in routing experiments, which replaces a prototype extension used in the TREC-3 system. On the other hand, it has to be noted that the character and the level of difficulty of TREC queries has changed quite significantly since the last year evaluation. TREC-4 new ad-hoc queries are far shorter, less focused, and they have a flavor of information requests (&quot;What is the prognosis of ...&quot;) rather than search directives typical for earlier TRECs (&quot;The relevant document will contain ...&quot;). This makes building of good search queries a more sensitive task than before. We thus decided to introduce only minimum number of changes to our indexing and search processes, and even roll back some of the TREC-3 extensions which dealt with longer and somewhat redundant queries. Overall, our system performed quite well as our position with respect to the best systems improved steadily since the beginning of TREC. We participated in both main evaluation categories: category A ad-hoc and routing, working with approx. 3.3 GBytes of text. We submitted 4 official runs in automatic adhoc, manual ad-hoc, and automatic routing (2), and were ranked 6 or 7 in each category (out of 38 participating teams). It should be noted that the most significant gain in performance seems to have occurred in precision near the top of the ranking, at 5, 10, 15 and 20 documents. Indeed, our unofficial manual runs performed after TREC-4 conference show superior results in these categories, topping by a large margin the best manual scores by any system in the official evaluation. In general, we can note substantial improvement in performance when phrasal terms are used, especially in ad-hoc runs. Looking back at TREC-2 and TREC-3 one may observe that these improvements appear to be tied to the length and specificity of the query: the longer the query, the more improvement from linguistic processes. This can be seen comparing the improvement over baseline for automatic adhoc runs (very short queries), for manual runs (longer queries), and for semi-interactive runs (yet longer queries). In addition, our TREC-3 results (with long and detailed queries) showed 20-25% improvement in precision attributed to NLP, as compared to 10-16% in TREC-4. OVERVIEW A typical (full-text) information retrieval (IR) task is to select documents from a database in response to a user&apos;s query, and rank these documents according to relevance. This has been usually accomplished using statistical methods (often coupled with manual encoding) that (a) select terms (words, 143 phrases, and other units) from documents that are deemed to best represent their content, and (b) create an inverted index file (or files) that provide an easy access to documents containing these terms. A subsequent search process will attempt to match prepro-cessed user queries against term-based representations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms. Although many sophisticated search and matching methods are available, the crucial problem remains to be that of an adequate representation of content for both the documents and the queries.
RECENT ADVANCES IN HNC&apos;S CONTEXT VECTOR INFORMATION RETRIEVAL TECHNOLOGY While the current MatchPlus learning law has proven to be effective in encoding relationships between words, it is computationally intensive and requires multiple passes through the training corpus. The purpose of the one step learning law is to approximate the behavior of the original learning law while performing only a single pass through the training corpus. The one step learning law uses a single pass through the training corpus to obtain desired dot product values for the set of trained context vectors. The desired dot product values are determined on the basis of information theoretic statistical relationships between co-occurring word stems found in the training corpus. Desired dot products are found such that words that tend to cooccur will have context vectors that point in similar directions while words that do not co-occur will have context vectors that tend to be orthogonal. These desired dot products are used to perform a quasi-linear transformation on an initial set of quasi-orthogonal, high dimensional vectors. This vector transformation and subsequent renormalization results in a set of context vectors that represents the relationships between word stems in a near-optimal fashion. The time requirements for training this set of vectors scale as O(Nn) where N is the number of word stems in the vocabulary and n is the average number of word stems found to co-occur (and/or be related to) any given word stem (usually on the order of several hundred). This new learning law reduces the training time by a factor on the order of 100 over the original context vector learning law with little or no degradation in performance. Results can be improved even further by adjusting the dimension of the context vectors.HNC has also developed an approach to learning stem-level relationships across multiple languages and has used this approach to develop a prototype multilingual retrieval system. This technique, called "symmetric learning", is based upon the use of tie words, which provide connectivity between each language's portion of the context vector space.In the symmetric approach, learning is conducted using both languages simultaneously, thus removing any donor language biases. Tie words are used to connect the context vector space for multiple languages through a "unified hash table". The unified hash table provides the mechanism to translate a stem into an associated context vector. In the English-only MatchPlus system, this is a straight forward process. The stem in question is fed to the hashing function and the index is produced. The resulting index is the offset in the stem hash table. The content of that location in the hash table is a pointer to the context vector data structure.Using this approach (hash fimction collisions not withstanding), each unique stem results in a unique entry and thus a unique context vector. In the multilingual system, a tie word list is used to provide multiple references, one word stem ffi'om each language, for common context vectors. Context vector learning is performed in multiple languages simultaneously using multilingual training corpora. HNC has performed preliminary evaluation of an English-Spanish version of this system by examining stem trees for tie words and non-tie words. Results indicate that the English-Spanish MatchPlus prototype is able to learn reasonable word stem interrelationships for tie words and non-tie words, thereby demonstrating the suitability of this concept for further development. Over the past few years, HNC has developed a neural network based, vector space approach to text retrieval. This approach, embodied in a system called MatchPlus, allows the user to retrieve information on the basis of meaning and context of a fi&apos;ee text query. The MatchPlus system uses a neural network based, constrained sel~organization technique to learn word stem interrelationships directly 3~om a training corpus, thereby eliminating the need for hand crafted linguistic knowledge bases and their often substantial maintenance requirements. This paper presents results fi&apos;om recent enhancements to the basic MatchPlus concept. These enhancements include the development of a one step learning law that greatly reduces the amount of time and~or computational resources required to train the system, and the development of a prototype multilingual (English and Spanish) text retrieval system.
A CONTEXT VECTOR-BASED SELF ORGANIZING MAP FOR INFORMATION VISUALIZATION In recent years there has been an explosion in the amount of information available on-line. Much of this explosion has been fueled by the spectacular growth of the Internet and especially the World Wide Web. Along with this spectacular growth has come new challenges for effectively locating on-line information, especially when browsing rather than performing a directed search for a specific piece of information. Key word and linguistically based directed search engines offer some ability to present relevant information to the user, and have resulted in useful Interact products such as Yahoo [1], Lycos [2], and Alta Vista [3]. However, these engines suffer fi'om the fact that they require the user to specify a query of limited length and they offer no visual interface for browsing. To solve the problem of browsing the information space in order to fred information of interest, new techniques for data retrieval and presentation must be developed.HNC has developed an underlying information representation technology and a concept for information visualization that can solve the problem of effectively browsing large textual corpora. As part of HNC's involvement in the ARPA sponsored TIPSTER program, HNC has developed a neural network technique that can learn word level relationships from free text. This capability is based upon an approach called context vectors which encodes the meaning and context of words and documents in the form of unit vectors in a high dimensional vector space. Furthermore, as part of HNC's involvement in the US intelligence communitysponsored P1000 visualization effort, HNC has applied a secondary neural network process, the Self Organizing Map (SOM) [4], which uses the document context vectors to build a visual representation of the information content of the corpus. The combination of these technologies allows users to effectively browse the information space, to locate related documents, and to discover relationships between different themes in the information space.The remainder of this paper is organized as follows. Section 2 presents an overview of both context vectors and the Self Organizing Map. Section 3 presents the DOCUVERSE system and presents the user interface, automatic region fmding and region labeling, information retrieval and document highlighting, and temporal analysis of the information space. Finally, Section 4 presents some concluding remarks and directions for future research. HNC Software, Inc. has developed a system called DOCUVERSE for visualizing the information content of large textual corpora. The system is built around two separate neural network methodologies: context vectors and self organizing maps. Context vectors (CVs) are high dimensional information representations that encode the semantic content of the textual entities they represent. Self organizing maps (SOMs) are capable of transforming an input, high dimensional signal space into a much lower (usually two or three) dimensional output space useful for visualization. Related information themes contained in the corpus, depicted graphically, are presented in spatial proximity to one another. Neither process requires human intervention, nor an external knowledge base. Together, these neural network techniques can be utilized to automatically identi~ the relevant information themes present in a corpus, and present those themes to the user in a intuitive visual form.
A SIMPLE PROBABILISTIC APPROACH TO CLASSIFICATION AND ROUTING  1. ABSTRACT Several classification and routing methods were implemented and compared. The experiments used FBIS documents from four categories, and the measures used were the ff.idf and Cosine similarity measures, and a maximum likelihood estimate based on ass~lming a Multinomial Distribution for the various topics (popula-tions). In addition, the SMART program was run with &apos;lnc.ltc&apos; weighting and compared to the others. Decisions for both our classification scheme (docu-ments are put into any number of disjoint categories) and our routing scheme (documents are assigned a &apos;score&apos; and ranked relative to each category) are based on the highest probability for correct classification or routing. All of the techniques described here are fully automatic, and use a training set of relevant documents to produce lists of distin~i~hin£ terms and weights. All methods (ours and the ones we compared to) gave excellent results for the classification task, while the one based on the Multinomial Distribution produced the best results on the routing task.
AN EVALUATION OF COREFERENCE RESOLUTION STRATEGIES FOR ACQUIRING ASSOCIATED INFORMATION Category -Information Extraction As part of our TIPSTER research program [Contract Number 94-F133200-000], we have developed a variety of strategies to resolve coreferences within a free text document. Coreference is typically defined to mean the identification of noun phrases that refer to the same object. This paper investigates a more general view of coreference in which our automatic system identifies not only coreferenfial phrases, but also phrases which additionally describe an object. Coreference has been found to be an important component of many applications.The following example illustrates a general view of coreference.American Express, the large financial institution, also known as Amex, will open an office in Peking.In this example, we would like to associate the following information about American Express: its name is American Express;an alias for it is Amex; its location is Peking, China; and it can be described as the large financial institution.In the work described in this paper, our goal was to evaluate the contributions of various techniques for associating an entity with three types of information:1. NameV~atious . 
ADVANCES IN MULTILINGUAL TEXT RETRIEVAL Multilingual text retrieval extends the basic monolingual detection task to include retrieving relevant documents in languages other than the query language. The task therefore merges efforts in machine translation with efforts in text retrieval, but the machine translation component may be substantially simplified due to some basic assumptions about the design and implementation of high-performance text retrieval systems. A primary consideration is that most modem text retrieval systems regard queries and documents as unordered "bags" of words. The translation of an unordered set of terms is therefore approximately the translation of the terms themselves. Although a linearity assumption such as this breaks down when considering phrasal elements in most languages, it is reasonably accurate for many terms and becomes increasingly accurate at the sentence level and above.A second consideration in multilingual text retrieval is where the translation is done. It is possible to translate every document at index time, for example, but the resource costs are substantially higher than translating the query at retrieval time. An added benefit of translating only the query is that queries can be prepared with no special weighting scheme applied to the terms. The queries are then available to any natural language text retrieval system.The range of translation techniques that are available to a query translation system is greater than in standard machine translation systems. Previously translated document corpora can be made available for exploiting domain-specific terminology by direct comparison of the retrieval results for the query and target document languages. No special heuristics are needed for using this "example-based" translation approach; the query can be optimized by adding or deleting terms until the target language retrieval results are approximately the same as the source language retrieval results. Lexicaltransfer techniques can also be used in the same context, providing wide coverage of term senses.CRL evaluated five methods for query translation in Tipster II. The results were then evaluated in TREC by hand-translating the TREC Spanish monolingual queries into English and applying the automatic query translation methods to produce new Spanish queries. Ongoing work is focusing on improving the performance of query translation techniques while expanding the techniques to work with new languages and search engines, including WWW search services. 
Integration of Document Detection and Information Extraction  We have conducted a number of experiments to evaluate various modes of building an integrated detection/extraction system. The experiments were performed using SMART system as baseline. The goal was to determine if advanced information extraction methods can improve recall and precision of document detection. We identified the following two modes of integration:
SRI&apos;s Tipster II Project The principal barrier to the widespread use of information extraction technology is the difficulty in defining the patterns that represent one's information requirements. Much of the work that has been done on SRI's Tipster II project has been directed at overconaing this barrier. In this paper, after some background on the basic structure of the FAS-TUS system, we present some of these developments. Specifically, we discuss the declarative pattern specification language FastSpec, compile-time transformations, and adapting rules from examples. In addition, we have developed the basic capabilities of FASTUS. We describe our efforts in one area--coreference resolution. We are now experimenting with the use of FASTUS in improving document retrieval and this is also described. 
OVERVIEW AND ACCOMPLISHMENTS THE SE/CM PERSPECTIVE  
BUILDING AN ARCHITECTURE: A CAWG SAGA  The Tipster Architecture-a standardized interface for providing document management, document retrieval, and information extraction services-is one of the major products of Phase II of the Tip-ster program. The architecture was developed by the Contractors&apos; Architecture Working Group (CAWG) over the past two years. It has been refined through feedback from the demos developed by the CAWG for the 6-month and 12-month Tipster meetings, and from the Tipster-compliant systems now being implemented .
THE ARCHITECTURE DEMONSTRATION SYSTEM  
TUIT: A TOOLKIT FOR CONSTRUCTING MULTILINGUAL TIPSTER USER INTERFACES  
TIPSTER Text Phase II Architecture Concept  Architecture Committee tipster @ fipster.org 1.0 EXECUTIVE SUMMARY 1.1 The TIPSTER Architecture The TIPSTER Architecture is a software architecture for providing Document Detection (i.e. Information Retrieval and Message Routing) and Information Extraction functions to text handling applications. The high level architecture is described in an Architecture Design Document. In May 1996, when the initial architecture design is complete, an Interface Control Document will be provided specifying the form and content of all inputs and outputs to the TIPSTER modules.
TIPSTER Text Phase II Architecture Design and the TIPSTER Phase H Contractors&apos; Architecture Working Group (CA WG): Class Document Reference Type of ObjectReference Class AttributeReference Type of ObjectReference Class AnnotationReference  1.0 GOALS The TIPSTER Program aims to push the technology for access to information in large (multi-GB) text collections, in particular for the analysts in Government agencies. Technology is being developed for document detection (&quot;information retrieval&quot;) and for data extraction from free text. The primary mission of the TIPSTER Common Architecture is to provide a vehicle for efficiently delivering this detection and extraction technology to the Government agencies. The Architecture also has a secondary mission of providing a convenient and efficient environment for research in document detection and data extraction. To accomplish this mission, the TIPSTER Architecture is being designed to: • provide APIs for document detection, data extraction, and the associated document management functions • support monolingual and multilingual applications • allow the interchange of modules from different suppliers (&quot;plug and play&quot;) • apply to a wide range of software and hardware environments • scale to a wide range of volumes of document archives and of document flow • support appropriate application response time • support incorporation of multi-level security • enhance detection and extraction through the exchange of information, and through easier access to linguistic annotations 249 2.0 CONCEPTS The architecture is described by a set of object classes and a set of functions associated with these objects. In addition, there is a &quot;functional&quot; section which indicates how data typically flows between these functions. 2.1 Object Classes An object class is characterized by a class name, a set of named properties, and a set of operations. Unless explicitly noted otherwise, there is an operation (the property accessor function) associated with each property for reading that property&apos;s value. If the property is followed by (R, W), operations are provided both for reading and for writing that property. If the property is followed by (g), no functions are provided for reading or writing the property. Each property has a value, which may be • an object (of one or several classes) • a sequence of objects (ordered), denoted by &quot;sequence of...&quot; • a string (of characters) • an integer • a byte • a Boolean value (true or false) • a member of an enumerated type, denoted by &quot;one of { ... }&quot; • nil The operations will include both procedures (which do not return a value) and functions (which do). The notation is procedure (type of argl, type of arg2 ....) function (type of argl, type of arg2 ....): type of result To indicate the significance of particular arguments, an argument position may contain argument name: argument type If a class C 1 is a subclass of another class C2 (indicated by the notation Type of C2 in the definition of C 1) then C 1 inherits all the properties and operations of C2. The designation of a class as an Abstract Class indicates that the class is not intended to be instantiated but is intended to serve as a superclass for other classes (which will be instantiated). A class C can include operations whose name has the form &quot;class.C&quot;. If D is a type of C (i.e., class D includes the specification Type of C), then the operation as inherited by D has the name &quot;class.D&quot;. This facility is provided to allow for the specialization of operations which create new instances of a class. 2.20ptionality Some objects and functions will be required: they must be implemented by any system conforming to the architecture. Some objects and functions will be optional: they need not be included, but if they are, they must conform to the standard. This allows us to define standards, for example, for some linguistic annotations, without requiring all systems to generate such annotations. 2.3 Correspondence to Interface Specifications This document provides an abstract definition of the architecture in terms of classes and operations. This architecture will be implemented in a number of programming languages; currently implementations are being 251 developed in C, Tcl, and Common Lisp. This section describes the correspondence between the set of operations described in this document and the APIs for implementations of this architecture in these programming languages. Common Lisp: The classes, properties, and operations defined herein correspond to those of a Common Lisp implementation of the TIPSTER Architecture as follows: 1. (because Lisp is normally not case sensitive) each capital letter in the name of a class, property, or operation, except for the first letter in a name, will be preceded by a &quot;-&quot; in Lisp 2. each class, property, and operation corresponds to a Lisp class, property, and function 3. each argument of the form &quot;class&quot; becomes a positional argument; each argument of the form &quot;name: class&quot; becomes a keyword argument with the keyword name 4. sequences are represented as lists In Lisp, the name of the property accessor function is formed from the class name, a hyphen, and the property name (e.g., attribute-name and attribute-value). If the property is writable, the property accessor function acts as a &quot;generalized variable&quot; which can be set by serf; e.g., (serf (collection-owner collectionl) &quot;Mitchell&quot;). C: Each operation defined herein corresponds to a C function, with the same name as in the abstract architecture. All arguments in the C implementation are positional; the argument names (&quot;keywords&quot;) in the abstract architecture are not used. If property Comp of a class is readable, it is accessed by the function Get Comp; if it is also writeable, it is set by the function Set Comp. Note that the abstract architecture occasionally &quot;overloads&quot; operations: the same operation name may apply to different classes of arguments. To support such overloading, the C implementations of the various classes, as well as sets and sequences, should employ a generic container structure which will allow a C function to determine the class of an actual argument. 1 The C-language typing, including the overloading of various functions, is spelled out in Appendix C. Tci: Operation names and argument lists in Tcl shall be the same as in the C implementation. 2.3.1 Optional Arguments In addition to the arguments which are specified for each operation in this document and which are required, an implementation of the Architecture may provide optional keyword or positional arguments for any of the operations. The operation must be able to complete and to perform the specified function even if only the required arguments are given, but use of the optional arguments may provide enhanced performance or a greater range of functionality. 2.3.2 Implementation of Sequences The architecture includes the notion &apos;sequence of X&apos;, where X is a type, as one of the possible values of an argument to an operation or the value of a property. In describing an implementation of the Architecture (an API), it is necessary to specify the representation or set of operations for such sequences. 1 The overloading does not extend to basic C data types (char, int, float), since these could not be differentiated from structures by a called procedure. 252 The C language interface (Appendix C) defines types AttributeSet, AttributeValueSet, DocumentCollectionlndexSet, QueryCollectionlndex, SpanSet, and stringSet, corresponding to &quot;sequence of Attribute&apos;, &quot;sequence of AttributeValue&apos;, &quot;sequence of DocumentCollectionlndex&apos;, &quot;sequence of QueryCollectionlndex&apos;, &quot;sequence of Span&apos;, and &quot;sequence of string&apos; in the Architecture 2. These are referred to collectively as XSets, where X may be Attribute, Span, etc. An empty XSet is created by the operation CreateXSet 0: XSet (i.e., by one of the operations CreateAttributeSet, CreateSpanSet, etc.). The following operations apply to XSets: Nth (XSet, n: integer): X returns the nth element of XSet (where the first element of sequence has index 0) Push (XSet, X) adds X to the end of sequence XSet Pop (XSet): X removes and returns the last element of XSet Length (XSet): integer returns the length of XSet In addition, the operation Free, described just below, applies to all types of objects, including XSets. 2.4 Storage Management A free operation must be provided for every class of object to release the memory associated with that object as well as to perform any necessary implementation specific cleanup operations. 2.5 Error Handling A number of operations in the architecture describe error conditions (generally with the phrase &quot;it is an error if...&quot;). Such errors should be implemented by signaling an error rather than by returning an error value (this could be performed in C by using the longjrnp function and in Common Lisp by the error function). The C implementation provides utility routines which simplify the use of longjmp for this purpose. 2 Annotation sets are treated as a separately defined class in the Architecture, but its Nth and Length operations are designed to parallel those of the other sets. 253 3.0 BASIC CLASSES 3.1 Attributes A number of classes will have &quot;attributes&quot;. This is a list of feature-value pairs, where the feature names are arbitrary strings and the values can be any of a number of types: Class Attribute Properties Name: string Value: AttributeValue Operations CreateAttribute (name: string, value: AttributeValue): Attribute Class AttributeValue Properties Value: string OR ObjectReference OR sequence of AttributeValue Operations CreateAttributeValue(string OR ObjectReference OR sequence of AttributeValue): AttributeValue 3 TypeOf (AttributeValue): one of {string, sequence, CollectionReference, DocumentReference, AnnolationReference, AttributeReference } returns a member of the enumerated type, indicating the type of AttributeValue Note: AttributeValue is made a separate class, with an explicit TypeOf operator, out of deference to languages such as C without dynamic type identification. Because AttributeValue can take on multiple types, including types such as strings which would not use a generic container structure, implementations in such languages must provide an explicit type discriminator here, accessible through the TypeOf operator. The value of an attribute may be (inter alia) a reference to a collection, document, annotation, or attribute: Abstract Class ObjectReference Class CollectionReference Type of ObjectReference Properties CollectionNarne: string Operations CreateCollectionReference (Collection): CollectionReference 3 For implementation in languages which cannot determine the type of the argument at run time, such as C, this operation requires two arguments. The additional argument (the first of the two arguments) is of enumerated type &quot;&apos;one of {string, sequence, CollectionReference, DocumentReference, AnnotationReference, AttributeReference}&quot; and specifies the type of the second argument, which is the value itself.
TIPSTER Text Phase II Architecture Requirements  Architecture Commitee tipster @ tipster.org Compiled Query is the Detection Component specific form of a query generated by a Detection Component from a Statement of Relevance and is understandable only by the Detection Component.
TIPSTER Text Phase II Configuration Management Plan 1.0 EXECUTIVE SUMMARY This document presents the TIPSTER Text Phase II Configuration Management (CM) Plan for identifying, controlling, and auditing the TIPSTER Architecture status and configuration definition  
The Text REtrieval Conferences (TRECs) A Sample of the TREC-4 Participants The Text REtrieval Conferences (TRECs) Phase two of the TIPSTER project included two workshops for evaluating document detection (information retrieval) projects: the third and fourth Text REtrieval Conferences (TRECs). These workshops were held at the National Institute of Standards and Technology (NIST) in November of 1994 and 1995 respectively. The conferences included evaluation not only of the TIPSTER contractors, but also of many information retrieval groups outside of the TIPSTER project. The conferences were run as workshops that provided a forum for participating groups to discuss their system results on the retrieval tasks done using the TIP-STER/TREC collection. As with the first two TRECs, the goals of these workshops were:• To encourage research in text retrieval based on largescale test collections• To increase communication among industry, academia, and government by creating an open forum for exchange of research ideas• To speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems• To increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems• To serve as a showcase for state-of-the-art retrieval systems for DARPA and its clients.The number of participating systems has grown from 25 in TREC-1 to 32 in TREC-3 (see Table 1) and to 36 in TREC-4 (see Table 2). These systems include most of the major text retrieval software companies and most of the universities doing research in text retrieval. Note that whereas the universities tend to participate every year, the companies often skip years because of the amount of effort required to run the TREC tests.By opening the evaluation to all interested groups, TIPSTER has ensured that TREC represents many different approaches to text retrieval. The emphasis on diverse experiments evaluated within a common setting has proven to be a major strength of TREC.The research done by the participating groups in the four TREC conferences has varied, but has followed a general pattern. TREC-1 (1992) required significant system rebuilding by most groups, due to the huge increase in the size of the document collection from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection. The second TREC conference (TREC-2) occurred in August of 1993, less than 10 months after the first conference. The results (using new test topics) showed significant improvements over the TREC-1 results, but should be viewed as an appropriate baseline representing the 1993 state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection.TREC-3 provided an opportunity for complex experimentation. The experiments included the development of automatic query expansion techniques, the use of passages or subdocuments to increase the precision of retrieval results, and the use of training information to help systems select only the best terms for queries. Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector space model), and others tried approaches that were radically different from their original approaches. For example, experiments in manual query expansion were done by the University of California at Berkeley, and experiments in combining information from three very different retrieval techniques were done by the Swiss Federal Institute of Technology (ETH). For more details on the specific system approaches, see the complete overview of the TREC-3 conference, including papers from the participating groups [1].TREC-4 presented a continuation of many of these complex experiments, and also included a set of five focussed tasks, called tracks. Both the main tasks were more difficult --the test topics were much shorter, and the test documents were harder to retrieve. Several groups made major changes in their retrieval algorithms, and all groups had difficulty working with the very short topics. Many interesting experiments were done in the tracks, including 10 groups that worked with Spanish ways being asked, but that new data is being searched. This task is similar to that done by news clipping services or by library profiling systems. In the adhoc task, it is assumed that new questions are being asked against a static set of data. This task is similar to how a researcher might use a library, where the collection is known, but where the questions likely to be asked are unknown.In TREC the routing task is represented by using known topics and known relevant documents for those topics, but new data for testing. The training for this task is shown in the left-hand column of Figure 1. The participants are given a set of known (or training) topics, along with a set of documents, including known relevant documents for those topics. The topics consist of natural language text describing a user's information need (see section 3.3 for details). The topics are used to create a set of queries (the actual input to the retrieval system) which are then used against the training documents. This is represented by Q1 in the diagram. Many sets of Q1 queries might be built to help adjust systems to this task, to create better weighting algorithms, and in general to prepare the system for testing. The results of this training are used to create Q2, the routing queries to be used against the test documents (testing task shown on the middle column of Figure 1).The 50 routing topics for testing are a specific subset of the training topics (selected by NIST). In TREC-3 the routing topics corresponded to the TREC-2 adhoc topics, i.e., topics 100-150. The test documents for TREC-3 were the documents on disk 3 (see section 3.2). Although this disk had been part of the general training data, there were no relevance judgments for topics 100-150 made on this disk of documents. This lessthan-optimal testing was required by the last-minute unavailability of new data.In TREC-4 a slightly different methodology was used to select the routing topics and test data. Because of the difficulty in getting new data, it was decided to select the new data first, and then select topics that matched the data. The ready availability of more Federal Register documents suggested the use of topics that tended to find relevant documents in the Federal Register. Twenty-five of the routing topics were picked using this criteria. This also created a subcoUection of the longer, more structured Federal Register documents for later use in the research community. The second set of 25 routing topics was selected to build a subeollection in the domain of computers. The testing documents for the computer issues were documents from the Intemet, plus part of the Ziff coUection.The adhoc task is represented by new topics for known documents. This task is shown on the right-hand side of Figure 1, where the 50 new test topics are used to create Q3 as the adhoc queries for searching against the training documents. Fifty new topics (numbers 150-200) were generated for TREC-3, with fifty additional new topics created for TREC-4 (numbers 201-250). The known documents used in TREC-3 were on disks 1 and 2, and those used in TREC-4 were on disks 2 and 3. Sections 3.2 and 3. 3 give more details about the documents used and the topics that were created. The results from searches using Q2 and Q3 are the official test results sent to NIST for the routing and adhoc tasks. In addition to clearly defining the tasks, other guidelines are provided in TREC. These guidelines deal with the methods of indexing and knowledgebase construction and with the methods of generating the queries from the supplied topics. In general, they are constructed to reflect an actual operational environment, and to allow as fair as possible separation among the diverse query construction approaches. Three generic categories of query construction were defined, based on the mount and kind of manual intervention used.1. Automatic (completely automatic query construction)2. Manual (manual query construction)3. Interactive (use of interactive techniques to construct the queries)The participants were able to choose between two levels of participation: Category A, full participation, or Category B, full participation using a reduced dataset (1/4 of the full document set). Each participating group was provided the data and asked to turn in either one or two sets of results for each topic. When two sets of results were sent, they could be made using different methods of creating queries, or different methods of searching these queries. Groups could choose to do the routing task, the adhoc task, or both, and were asked to submit the top 1000 documents retrieved for each topic for evaluation. The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text retrieval (see table for some of the participants). The diversity of the participating groups has ensured that TREC represents many different approaches to text retrieval , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks, sent results into NIST for evaluation, presented the results at the TREC conferences , and submitted papers for a proceedings. The test collection consists of over 1 million documents from diverse full-text sources, 250 topics, and the set of relevant documents or &quot;right answers&quot; to those topics. A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics. TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection). The results from TREC-2 showed significant improvements over the TREC-1 results, and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection. TREC-3 therefore provided the first opportunity for more complex experimentation. The major experiments in TREC-3 included the development of automatic query expansion techniques, the use of passages or sub-documents to increase the precision of retrieval results, and the use of the training information to select only the best terms for routing queries. Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector space model), and others tried approaches that were radically different from their original approaches. TREC-4 allowed a continuation of many of these complex experiments. The topics were made much shorter and this change triggered extensive investigations in automatic query expansion. There were also five new tasks, called tracks. These were added to help focus research on certain known problem areas, and in-eluded such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval. The TREC conferences have proven to be very successful , allowing broad participation in the overall DARPA TIPSTER effort, and causing widespread use of a very large test collection. All conferences have had very open, honest discussions of technical issues, and there have been large amounts of &quot;cross-fertilization&quot; of ideas. This will be a continuing effort, with a TREC-5 conference scheduled in November of 1996.
DESIGN OF THE MUC-6 EVALUATION THE MUC EVALUATIONS  The sixth in a series of &quot;Message Understanding Con-ferences&quot;, which are designed to promote and evaluate research in information extraction, was held last fall. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe the development of the &quot;message understanding&quot; task over the course of the prior MUCs, some of the motivations for the new format, and the steps which led up to the formal evaluation.1 The Message Understanding Conferences were initiated by NOSC to assess and to foster research on the automated analysis of military messages containing textual information. Although called &quot;conferences&quot;, the distinguishing characteristic of the MUCs are not the conferences themselves, but the evaluations to which participants must submit in order to be permitted to attend the conference. For each MUC, participating groups have been given sample messages and instructions on the type of information to be extracted , and have developed a system to process such messages. Then, shortly before the conference, participants are given a set of test messages to be run through their system (without making any changes to the system); the output of each participant&apos;s system is then evaluated against a manually-prepared answer key. The MUCs have helped to define a program of research and development. DARPA has a number of information science and technology programs which are driven in large part by regular evaluations. The MUCs are notable, however, in that they have substantially shaped the research program in information extraction and brought it to its current state. 3 from Morgan Kaufmann. 3There were, however, a number of individual research efforts in information extraction underway before the first MUC, including the work on information formatting of medical narrative by Sager at New York University [3]; the formatting of naval equipment failure reports at the Naval Research Laboratory [1]; and the DBG work by Montgomery for RADC (now 413
OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC-6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC-6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.All except the Scenario Template task are defined independently of any particular domain.This paper surveys the results of the evaluation on each task and, to a more limited extent, across tasks. Discussion of the results for each task is organized generally under the following topics:• Results on task as whole; • Results on some aspects of task; • Performance on "walkthrough article." The walkthrough article is an article selected from the test set. Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers. 
THE MULTILINGUAL ENTITY TASK (MET) OVERVIEW  
MULTILINGUAL ENTITY TASK (MET): JAPANESE RESULTS Introduction*  
AN INTERPRETATIVE DATA ANALYSIS OF CHINESE NAMED ENTITY SUBTYPES  
THE MULTILINGUAL ENTITY TASK A DESCRIPTIVE ANALYSIS OF ENAMEX IN SPANISH  
MITRE: DESCRIPTION OF THE ALEMBIC SYSTEM AS USED IN MET  Alembic is a comprehensive information extraction system that has been applied to a range of tasks. These include the now-standard components of the formal MOC evaluations: name tagging (NE in MUC-6), name normalization (WE), and template generation (ST). The system has also been exploited to help segment and index broadcast video and was used for early experiments on variants of the co-reference identification task. (For details, see [1].) For MET, we were of course primarily concerned with the foundational name-tagging task; many downstream modules of the system were left unused. The punchline, as we see it, is that Alembic performed exceptionally well at all three of the MET languages despite having no native speakers for any of them among its development team. We were one of only two sites that attempted all three languages, and were the only group that exploited essentially the same body of code for all three tasks. RULE SEQUENCES The crux of our approach is the use of rule sequences, a processing strategy that was recently popularized by Eric Brill for part-of-speech tagging [2]. In a rule sequence processor, the object is to sequentially relabel a body of text according to an ordered rule set. The rules are evaluated in order, and each rule is allowed to run to completion only once in the course of processing. The result is an iteratively-improved labelling of the source text. In the name-tagging task, for example, the process begins with an approximate initial labelling, whose purpose is simply to find the rough boundaries of names and other MET-relevant forms, such as money. This rough labelling is then improved by applying a rule sequence. Individual rules then refine the initial rough boundaries, determine the type of a phrase (person, location, etc.), or merge fragmented phrases into larger units. See Figure 1 below. The rules themselves are simple. The two below come from the actual sequence for Spanish MET. (def-phraeer label NONE
NameTag TM Japanese and Spanish Systems as Used for MET We have participated in the Multilingual Entity Task (MET) for Japanese and Spanish using SRA's multilingual text-indexing software called NameTag TM.Its English version was used for the Named Entity Task (NE) in MUC-6 [2]. The NameTag Japanese and Spanish systems were customized to accommodate the MET-specific requirements and were able to achieve high performance in both recall and precision. 
APPROACHES IN MET (MULTI-LINGUAL ENTITY TASK)  Financial Crimes Enforcement Network (FinCEN) 2070 Chain Bridge Road Vienna, VA 22182 703-905-3648 1. TWO APPROACHES BBN and FinCEN participated jointly in the Spanish language task for MET. BBN also participated in Chinese. We also fielded two approaches. The first approach is pattern based and has an architecture as shown in Figure 1. This approach was applied to both Chinese and Spanish. The algorithms (rectangles in the Figure) were used in the two languages; the only component difference was the New Mexico State University segmenter, used to find the word boundaries in Chinese. The components common to both languages are the message reader, which dealt with the input format and SGML conventions via a declarative format description; the part-of-speech tagger (BBN POST); a lexical pattern matcher driven by knowledge bases of patterns and lexicons specific to each language; and the SGML annotation generator. While not shown in Figure 1, an alias prediction algorithm was shared by both languages, using patterns unique to each language. A second approach based on statistical learning was used to create a learned Spanish namefinder. One component is a training module that learns to recognize the MET categories from examples. The understanding module uses the model developed from training to predict the MET categories in new input sentences. Data annotated with the correct answers was provided by the government in its training materials. In addition, we annotated some additional data. The current probability model is a hidden Markov model (HMM) which is more complex than is typically used in part-of-speech tagging and is therefore more general. 2. CHALLENGES AND STRENGTHS IN OUR APPROACH TO CHINESE One of the challenges in processing Chinese is the difficulty of word segmentation. Segmentation in Chinese seems more difficult than in Japanese. With Japanese, changes in the character sets used in running text can be used to detect many of the word boundaries. The use of the part-of-speech tagger was both a strength and a weakness in Chinese. The part-of-speech labels proved useful in finding boundaries such as those between organization names and text which is not one of the MET categories. However, part-of-speech labeling in Chinese is more of a challenge than in the other languages because of two factors: • Chinese has very little inflection and no capitalization, thereby offering less evidence to predict the category of an unknown word. Ap~tion Feat~es~
CRL&apos;s APPROACH TO MET A Pattern Based Approach  From February to April CRL carried out investigations into the modification of our l~n~Hsh name recognition software developed for MUC-6 [1] to Chinese and Spanish. In addition a Japanese system, developed under Tipster Phase I [2], was modified to comply with the MET task. Finally learning methods developed for MUC-6 were adapted to handle Chinese. All systems performed with good levels of accuracy and it is clear that further Dining and refinement, for which there was no time or resources, would lead to even higher levels of pe~forma.nc¢.
NTT DATA: DESCRIPTION OF THE ERIE SYSTEM USED FOR MUC-6 Erie is a name recognition system developed for the Multilingual Entity Task (MET) in MUC-6. The pattern matching engine recognizes organization, person, and place names along with time and numeric expressions in Japanese text. Although our previous information extraction system Textract performed well in MUC-5, the pattern matching engine, which was written in AWK language, was slow [2]. System maintenance was also difficult, since the patterns were defined in both the matching engine and the pattern files. Erie solves these problems by generating a pattern matching engine in C language directly from the defined patterns. Figure 1 shows Erie's system architecture, which includes the following functions: 
MET Name Recognition with Japanese FASTUS* SRI's Japanese FASTUS used in the Multilingual Entity Task (MET) evaluation is the initial Japanese system based on the FastSpec pattern specification language. We describe its system architecture, strengths, weaknesses, and its contribution to the prospects of a full information extraction system. 
&quot;Description of NEC/Sheffleld System Used For MET Japanese&quot;  
  
MUC-5) Order number ISBN 1-5860-336-0 from  
  
The TIPSTER Text Program Overview These TIPSTER Phase III Proceedings bring to a close a program that had significant impact on information technology. Since 1991, the TIPSTER Text program has fostered the advancement of stateof-the-art technologies for text handling through the efforts of researchers and developers in the U.S. Government, industry and academia. The resulting capabilities are being deployed throughout the intelligence community to provide analysts with improved information processing tools.The TIPSTER Program focused on research and development in three technology areas: Document detection, information extraction and text summarization. In addition, the development of multilingual or cross-lingual capabilities in each of these areas became a vital component of the program. Metrics-based evaluations also constituted a critical program element. For example, the Text Retrieval Conference (TREC), TIPSTER's metrics-based evaluation for document detection is now recognized as the premier source of ground-truth data upon which information retrieval developers can test their systems. The TREC collection is a sizable one and provides the foundation for developers to test scalability of retrieval systems.The Message Understanding Conference (MUC), the Multilingual Entity Task (MET) and the Summarization Analysis Conference (SUMAC) also are TIPSTER evaluation mechanisms to help the Government gauge technology progress in relevant areas.A summary of the accomplishments for the first two phases of the TIPSTER program is provided in the sections below. For more details, please see the proceedings for TIPSTER Phase I [1] and TIPSTER Phase II [2], respectively. The next article in this volume [3] provides the highlights from Phase III, the last phase of the TIPSTER program. 
TIPSTER Phase III Accomplishments The TIPSTER Text Program Phase III continued the sponsorship of research and development to advance state-of-the-art technologies for text handling and facilitation of cooperation among research and development components from industrial, academic and the U.S. Government.The TIPSTER Phase III formally started with a Kick-off Workshop held in October 1996. Specific Phase III goals were to:• Sustain the successes of Phase I and II in detection, information extraction, architecture, and formal evaluation; • Push research in text processing technologies; • Expand the architecture; • Increase participation with Government agencies, researchers, developers, and the academic community in general; • Expand multilingual and summarization efforts.The overall purpose was to field a system for use within the operational elements of the Intelligence community and other Government agencies. Phase III Government participants included the Defense Advanced Research Projects Agency (DARPA), the Central Intelligence Agency (CIA), the National Security Agency (NSA), the National Institute of Standards and Technology (NIST), the Naval Research Lab (NRL), the Air Force Research Lab (AFRL), the Space and Naval Warfare Systems Command (SPAWAR) and the Defense Intelligence Agency (DIA) 
TIPSTER LESSONS LEARNED: THE SE/CM PERSPECTIVE The TIPSTER architecture is a domain-specific software architecture (DSSA) for the text processing domain. The primary goal of the architecture was to allow the use of standardize text processing components, enabling "plug and play" capabilities of the various tools being developed. This would permit the sharing of software among the various research efforts and operational prototype applications.PRC, Inc., was the systems engineering and configuration management (SE/CM) contractor for the TIPSTER program, phases II and III (1994- 1998). During this time we had close association with all the TIPSTER participants: Government, research contractors, and project contractors. Our primary role was to support the TIPSTER Architecture Committee and its co-chairpersons. We were able to observe the entire process of creating the TIPSTER architecture and applying it to research, projects, and the Architecture Capabilities Platform (ACP).The architecture was successful in many ways, though it also fell short of expectations in most areas. This paper describes our perspective on the architecture, to help subsequent efforts understand how to benefit from, and efficiently extend, the architecture. 
THE COMMON PATTERN SPECIFICATION LANGUAGE  This paper describes the Common Pattern Specification Language (CPSL) that was developed during the TIPSTER program by a committee of researchers from the TIPSTER research sites. Many information extraction systems work by matching regular expressions over the lexical features of input symbols. CPSL was designed as a language for specifying such finite-state grammars for the purpose of specifying information extraction rules in a relatively system-independent way. The adoption of such a common language would enable the creation of shareable resources for the development of rule-based information extraction systems.
Project Penlight -A Government Perspective  
Project Underline -A Government Perspective  
The Cornell TIPSTER Phase III Project  
THE SRI TIPSTER III PROJECT Introduction&apos;  
Reflections of Accomplishments in Natural Language Based Detection and Summarization  
COREFERENCE RESOLUTION STRATEGIES FROM AN APPLICATION PERSPECTIVE As part of our TIPSTER III research program, we have continued our research into strategies to resolve coreferences within a free text document; this research was begun during our TIPSTER II research program. In the TIPSTER II Proceedings paper, "An Evaluation of Coreference Resolution Strategies for Acquiring Associated Information," the goal was to evaluate the contributions of various techniques for associating an entity with three types of information: 1) name variations, 2) descriptive phrases, and 3) location information.This paper discusses the evolution of the coreference resolution techniques of the NLToolset ~, as they have been applied to an information extraction application, similar to the MUC Scenario Template task. Development of this application motivated new coreference resolution algorithms which were specific to the type of entity being handled. It also has raised the importance of understanding the structure of a document in order to guide the coreference resolution process.In the following paper, Section 2 discusses entity related coreference resolution techniques and Section 3, the relevance of document zoning. Section 4 concludes with a discussion of future work, which will include location merging, event coreference resolution, and event merging.The NLToolset is a proprietary text processing product, owned by Lockheed Martin Corporation. 
EXTRACTING AND NORMALIZING TEMPORAL EXPRESSIONS As part of our TIPSTER III research program, we have enhanced the NLToolset's ~ capability to extract temporal expressions from free text and convert them into canonical form for accurate comparison, sorting, and retrieval within a database management system.The date or time that an event occurs is often a critical piece of information. Unfortunately, natural language expressions that contain this information are so numerous and varied that the interpretation of temporal expressions within free text becomes a challenging task for automatic text processing systems. This paper will look at the nature of the problem, the extraction and computation tasks, the use of a learning program, and the normalization strategy. The concluding section will discuss possible future endeavors related to time extraction. 
Definitions and Goals  Information extraction involves picking out specified types of information from natural language text. Recent Message Understanding Conferences [1,2,3] have developed a spectrum of such tasks, and we have worked on two of them, at opposite ends of the spectrum: the named entity task, which involves identifying and classifying names, and the scenario template task, which involves extracting critical information (participants, location, date, etc.) about specified classes of events.
INFORMATION EXTRACTION RESEARCH AND APPLICATIONS: CURRENT PROGRESS AND FUTURE DIRECTIONS Analysts face a daunting task: they must accurately analyze, categorize, and assimilate a large body of information from a variety of sources and for a variety of domains of interest. The complexity of the task necessitates a variety of information access and extraction tools which technology up to this point has not been able to provide. SRI's TIPSTER Phase III project has focused on two major obstacles to the development of such tools: inadequate degrees of accuracy and portability. We begin by providing an overview of SRI's information extraction (IE) system, FASTUS, and then describe our efforts in these two areas in turn. We then conclude with some thoughts concerning future directions. 
ALGORITHMS THAT LEARN TO EXTRACT INFORMATION m BBN: TIPSTER PHASE III We believe that trained statistical models offer significant advantages for information extraction tasks. In this report on BBN's research under the TIPSTER III program, we describe a number of research efforts that developed fully-trained systems whose extraction performance was close to the highest levels achieved by carefully optimized systems based on hand-written rules.SIFT, the first system described, extracts entities and relations from text. On the sentence level, it combines syntactic and semantic knowledge in a novel way, thus taking advantage of the significant recent progress in statistical parsing and leveraging those techniques for information extraction. Knowledge of English syntax extracted from the Penn Treebank is automatically combined with semantically annotated training material in the target domain that identifies how the entities and relations of interest in the domain are signaled in text. At the message level, the local entities and relations identified within each sentence are then merged, and cross-sentence relations are identified using an additional trained model. The resulting system achieved the second-best score of those participating in the MUC-7 evaluation.The second system described here is the IdentiFinder TM system for locating named entities. This system is a fully-trained, HMMbased model that learns from examples the contextual clues that help to identify names in the text. All of BBN&apos;s research under the TIPSTER III program has focused on doing extraction by applying statistical models trained on annotated data, rather than by using programs that execute handwritten rules. Within the context of MUC-7, the SIFT system for extraction of template entities (TE) and template relations (TR) used a novel, integrated syntactic/semantic language model to extract sentence level information, and then synthesized information across sentences using in part a trained model for cross-sentence relations. At the named entity (NE) level as well, in both MET-1 and MUC-7, BBN employed a trained, HMM-based model.
Japanese IE System and Customization Tool  
Transforming Examples into Patterns for Information Extraction The task of Information Extraction (IE) as understood in this paper is the selective extraction of meaning from free natural language text. 1 This kind of text analysis is distinguished from others in Natural Language Processing in that "meaning" is understood in a narrow sense -in terms of a fixed set of semantic objects, namely, entities, relationships among these entities, and events in which these entities participate. These objects belong to a small number of types, all having fixed regular structure, within a fixed and closely circumscribed subject domain, which permits the objects to be stored in a database (e.g., a relational data base). These characteristics make the IE task both simpler and more tractable than the more ambitious problem of general text understanding. They allow us to define the notion of a "correct answer", and, therefore, to conduct quantitative evaluation of performance of an IE system. A series of formal evaluations --the Message Understanding Con1For a review of a range of extraction systems, the reader is referred to [9]. ferences (MUCs), 2 conducted over the last decade --are described in [8,6].The MUCs have yielded some widely (if not universally) accepted wisdom regarding IE:• Customization and portability is an important problem: to be considered a useful tool, an IE system must be able to perform in a variety of domains.• Systems have modular design: Control is encapsulated in immutable core engines, which draw upon domain-or scenario-specific information stored in knowledge bases (KB) which are customized for each new domain and scenario.• Text analysis is based on pattern matching: regular expression pattern matching is a widely used strategy in the IE community. Pattern matching is a form of deterministic bottom-up partial parsing. This approach has gained considerable popularity due to limitations on the accuracy of full syntactic parsers, and the adequacy of partial, semantically-constrained, parsing for this task [2,1,5].• Building effective patterns for a new domain is the most complex and time-consuming part of the customization process; it is highly errorprone, and requires detailed knowledge of system internals.In view of these findings, it becomes evident that having a disciplined method of customizing the pattern 2In this paper we will use IE terminology accepted in the MUC literature. A subject domain will denote the class of textual documents to be processed, such as "business news," while scenario will refer to the set of facts to be extracted, i.e., the specific extraction task that is applied to documents within the domain. One example of a scenario is "executive succession", the task tested in MUC-6, where the system seeks to identify events in which corporate managers left their posts or assumed new ones. Information Extraction (IE) systems today are commonly based on pattern matching. The patterns are regular expressions stored in a customizable knowledge base. Adapting an IE system to a new subject domain entails the construction of a new pattern base-a time-consuming and expensive task. We describe a strategy for building patterns from examples. To adapt the IE system to a new domain quickly, the user chooses a set of examples in a training text, and for each example gives the logical form entries which the example induces. The system transforms these examples into patterns and then applies metarules to generalize these patterns.
The Smart/Empire TIPSTER IR System The primary goal of the Cornell/Sabir TIPSTER Phase III project is to develop techniques to improve the end-user efficiency of information retrieval (IR) systems. We have focused our investigations in four related research areas:1. High Precision Information Retrieval. The goal of our research in this area is to increase the accuracy of the set of documents given to the user.2. Near-Duplicate Detection. The goal of our work in near-duplicate detection is to develop methods for delineating or removing from the set of retrieved documents any information that the user has already seen. 
DYNAMIC DATA FUSION Information retrieval researchers have long appreciated the value of combining, or ffilsing, multiple retrieval systems' relevance scores for a set of documents to improve retrieval performance. However, it is only recently that researchers have begun to consider adjusting the score fusion method to the user's topic and initial results. This study explores the value of fusing multiple retrieval systems' scores in a manner that adjusts to: the semantic and syntactic features of the user's natural language query, the various systems' biases toward long or short documents, and the extent to which the scores produced by the multiple systems are statistically independent. 
IMPROVING ENGLISH AND CHINESE AD-HOC RETRIEVAL: TIPSTER TEXT PHASE 3 FINAL REPORT As increasing amounts of computer-readable texts are becoming available on the web or on CDROMs, text searching and detection has become an indispensable tool for information users and analysts of all walks of life. Up till the late 1980's, research in text retrieval has been mainly with small collections of a few thousand items. Since 1990, with the foresight of the TIPSTER and TREC programs, substantial progress has been made to advance the state-of-the-art in text detection and ad-hoc information retrieval (IR) methodologies.Examples include: availability, experimentation and uniform evaluation of gigabytesize collections, term weighting improvements, 2-stage 'pseudo-feedback' retrieval strategy, recognition of difficulties of short queries versus long, use of phrases, treatment of foreign languages for multilingual retrieval, among others. This investigation builds upon previous findings to bring further advances in this field using our PIRCS system.We have participated in all past TREC experiments with consistently superior results. Since 1996, we have also participated in the TIPSTER Text Phase 3 program. This report serves to summarize work that has been done, and some of the important findings for both English and Chinese IR. Section 2 and 3 gives an overview of our PIRCS system and the 2-stage retrieval strategy. Section 4 presents our work for English ad-hoc retrieval employing term, phrasal and topical concept levels of evidence. Section 5 describes various Chinese retrieval experiments. Section 6 has the conclusions. We investigated both English and Chinese ad-hoc information retrieval (IR). Part of our objectives is to study the use of term, phrasal and topical concept level evidence, either individually or in combination, to improve retrieval accuracy. For short queries, we studied five term level techniques that together lead to improvements over standard ad-hoc 2-stage retrieval some 20% to 40% for TREC5 &amp; 6 experiments. For long queries, we studied linguistic phrases as evidence to re-rank outputs of term level retrieval. It brings small improvements in both TREC5 &amp; 6 experiments, but needs further confirmation. We also investigated clustering of output documents from term level retrieval. Our aim is to separate relevant and irrelevant documents into different clusters, and to re-rank the output list by groups based on query and cluster-profile matching. Investigation is still ongoing. For Chinese IR, many results were confirmed or discovered. For example, accurate word segmentation is not as important as first thought, but short-word segmentation is preferable to long-word (phrase). Simple bigram representation can give very good retrieval. A stopword list is not necessary; and presence of non-content terms does not hurt evaluation results much. One only needs screening out statistical stopwords of high frequency. Character indexing by itself is not competitive, but is useful for augmenting short-words or bigrams. Best results were obtained by combining retrievals of bigram and short-word with character representation. Chinese IR retums better precision than English, and it is not clear if this is a language-related, or collection-related phenomenon.
Enhancing Detection through Linguistic Indexing and Topic Expansion  Natural language processing techniques may hold a tremendous potential for overcoming the inadequacies of purely quantitative methods of text information retrieval. Under the Tipster contracts in phases I through III, GE group has set out to explore this potential through development and evaluation of new text processing techniques. This work resulted in some significant advances and in a better understanding on how NLP may benefit IR. Tipster research has laid a critical groundwork for future work. In this paper we summarize GE work on document detection in Tipster Phase III. Our summarization research is described in a separate paper appearing in this volume.
 CAMP software has been used in a variety of areas, but at the end of TIPSTER it finishes as it startedas a coreference annotation system. The coreference output has been used to participate in MUC-6 and MUC-7, served as the foundation for three types of summarization engines and been input to a cross-document coreference system for names and events. This document focuses on the most successful of these application, a query sensitive summarization system and a cross-document coreference system. 
AN NTU-APPROACH TO AUTOMATIC SENTENCE EXTRACTION FOR SUMMARY GENERATION Towards the end of the 20 th century, the Internet has become a part of life style. People enjoy Internet services from various providers and these ISPs (Internet Services Providers) do their best to fulfill users' information need. However, if we investigate the techniques used in these services, we will find out that they are not different from those used in traditional Information Retrieval or Natural Language Processing. However, the cyberspace provides us an environment to utilize these techniques to serve more persons than ever before. Automatic summarization and information extraction are two important Internet services. MUC and SUMMAC play their appropriate roles in the next generation Internet. This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1. For categorization task, positive feature vectors and negative feature vectors are used cooperatively to construct generic, indicative summaries. For adhoc task, a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the user-directed summaries. The result shows that the NormF of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0.447. The NormF of the best summary and that of the fixed summary for categorization task are 0.4090 and 0.4023. Our system outperforms the average system in categorization task but does a common job in adhoc task.
Improving Robust Domain Independent Summarization Summarization is the problem of presenting the most important information contained in one or more documents. The research described here focuses on multi-lingual summarization (MLS). Summaries of documents are produced in Spanish, Japanese, English and Russian using the same basic summarization engine.The core summarization problem is taking a single text and producing a shorter text in the same language that contains all the main points in the input text. We are using a robust, graded approach to building the core engine by incorporating statistical, syntactic and document structure analyses among other techniques. We have developed a system design which allows the parameterization both of the summarization process and of necessary information about the languages being processed.Document structure analysis (Salton &amp; Singal 94, Salton et al. 95) is important for extracting the topic of a text. In a statistical analysis for example (Paice 90, Paice &amp; Jones 93), titles and sub-titles would be given a more important weight than the body of the text. Similarly, the introduction and conclusion for the text itself and for each section are more important than other paragraphs, and the first and last sentences in each paragraph are more important than others. The applicability of these depends on the style adopted in a particular domain, and on the language: the stylistic structure and the presentation of arguments vary significantly across genres and languages. Structure analysis must be tailored to a particular type of text in a particular language. In the MINDS system document structure analysis involves the following subtasks:• Language Identification We discuss those techniques which, in the opinion of the authors, are needed to support robust automatic summarization. Many of these methods are already incorporated in a multilingual summarization system, MINDS, developed at CRL. The approach is sentence selection, but includes techniques to improve coherence and also to perform sentence reduction. Our methods are in distinct contrast to those approaches to summarization by deep analysis of a document followed by text generation .
Automatic Text Summarization in TIPSTER  
SUMMARIZATION: (1) USING MMR FOR DIVERSITY-BASED RERANKING AND (2) EVALUATING SUMMARIES With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find information quickly. Conventional IR systems rank and assimilate documents based on maximizing relevance to the user query [1,8,6,12,13]. In cases where relevant documents are few, or cases where very-high recall is necessary, pure relevance ranking is very appropriate. But in cases where there is a vast sea of potentially relevant documents, highly redundant with each other or (in the extreme) containing partially or fully duplicative information we must utilize means beyond pure relevance for document ranking.In order to better illustrate the need to combine relevance and anti-redundancy, consider a reporter or a This research was performed as part of Carnegie Group Inc.'s Tipster III Summarization Project under the direction of Mark Borger and Alex Kott. student, using a newswire archive collection to research accounts of airline disasters. He composes a wellthough-out query including "airline crash", "FAA investigation", "passenger deaths", "fire", "airplane accidents", and so on. The IR engine returns a ranked list of the top 100 documents (more if requested), and the user examines the top-ranked document. It's about the suspicious TWA-800 crash near Long Island. Very relevant and useful. The next document is also about "TWA-800", so is the next, and so are the following 30 documents. Relevant? Yes. Useful? Decreasingly so. Most "new" documents merely repeat information already contained in previously offered ones, and the user could have tired long before reaching the first non-TWA-800 air disaster document. Perfect precision, therefore, may prove insufficient in meeting user needs.A better document ranking method for this user is one where each document in the ranked list is selected according to a combined criterion of query relevance and novelty of information. The latter measures the degree of dissimilarity between the document being considered and previously selected ones already in the ranked list. Of course, some users may prefer to drill down on a narrow topic, and others a panoramic sampling bearing relevance to the query. Best is a usertunable method that focuses the search from a narrow beam to a floodlight. Maximal Marginal Relevance (MMR) provides precisely such functionality, as discussed below.If we consider document summarization by relevantpassage extraction, we must again consider antiredundancy as well as relevance. Both query-free summaries and query-relevant summaries need to avoid redundancy, as it defeats the purpose of summarization. For instance, scholarly articles often state their thesis in the introduction, elaborate upon it in the body, and" reiterate it in the conclusion. Including all three in versions in the summary, however, leaves little room for other useful information. If we move beyond single document summarization to document cluster summarization, where the summary must pool passages from different but possibly overlapping documents, reducing redundancy becomes an even more significant problem.Automated document summarization dates back to Luhn's work at IBM in the 1950's [12], and evolved through several efforts including Tait [24] and Paice in the 1980s [17,18]. Much early work focused on the structure of the document to select information. In the 1990's several approaches to summarization blossomed, include trainable methods [10], linguistic approaches [8,15] and our information-centric method [2], the first to focus on query-relevant summaries and anti-redundancy measures. As part of the TIPSTER program [25], new investigations have started into summary creation using a variety of strategies. These new efforts address query relevant as well as "generic" summaries and utilize a variety of approaches including using co-reference chains (from the University of Pennsylvania) [25], the combination of statistical and linguistic approaches (Smart and Empire) from SaBir Research, Cornell University and GE R&amp;D Labs, topic identification and interpretation from the ISI, and template based summarization from New Mexico State University [25].In this paper, we discuss the Maximal Marginal Relevance method (Section 2), its use for document reranking (Section 3), our approach to query-based single document summarization (Section 4), and our approach to long documents (Section 6) and multidocument summarization (Section 6). We also discuss our evaluation efforts of single document summarization (Section 7-8) and our preliminary results (Section 9). This paper 1 develops a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in ad-hoc query and in single document summarization. The latter are borne out by the trial-run (unofficial) TREC-style evaluation of summarization systems. However, the clearest advantage is demonstrated in the automated construction of large document and non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. This paper also discusses our preliminary evaluation of summarization methods for single documents.
AUTOMATED TEXT SUMMARIZATION AND THE SUMMARIST SYSTEM  This paper consists of three parts: a preliminary typology of summaries in general; a description of the current and planned modules and performance of the SUMMARIST automated multilingual text summarization system being built sat ISI, and a discussion of three methods to evaluate summaries.
MULTIPLE &amp; SINGLE DOCUMENT SUMMARIZATION USING DR-LINK  
A Text-Extraction Based Summarizer A good summarization tool can be of enormous help for those who have to process large amounts of documents. In information retrieval one would benefit greatly from having content-indicative quick-read summaries supplied along with the titles returned from search. Similarly, application areas like routing, news on demand, market intelligence and topic tracking would benefit from a good summarization tool.Perhaps the most difficult problem in designing an automatic text summarization is to define what a summary is, and how to tell a summary from a non-summary, or a good summary from a bad one. The answer depends in part upon who the summary is intended for, and in part upon what it is meant to achieve, which in large measure precludes any objective evaluation. A good summary should at least be a good reflection of the original document while being considerably shorter than the original thus saving the reader valuable reading time.In this paper we describe an automatic way to generate summaries from text-only documents. The summarizer we developed can create general and topical indicative summaries, and also topical informative summaries.Our approach is domainindependent and takes advantage of certain organization regularities that were observed in news-type documents. The system participated in a thirdparty evaluation program and turned out to be one of the top-performing summarizers. Especially the quality/length ratio was very good since our summaries tend to be very short (10% of the original length).The summarizer is still undergoing improvement and expansion in order to be able to summarize a wide variety of documents. It is also used successfully as a tool to solve different problems, like information retrieval and topic tracking. We present an automated method of generating human-readable summaries from a variety of text documents including newspaper articles, business reports, government documents, even broadcast news transcripts. Our approach exploits an empirical observation that much of the written text display certain regularities of organization and style, which we call the Discourse Macro Structure (DMS). A summary is therefore created to reflect the components of a given DMS. In order to produce a coherent and readable summary we select continuous, well-formed passages from the source document and assemble them into a mini-document within a DMS template. In this paper we describe an automated summarizer that can generate both short indicative abstracts, useful for quick scanning of a list of documents, as well as longer informative digests that can serve as surrogates for the full text. The summarizer can assist the users of an information retrieval system in assessing the quality of the results returned from a search, preparing reports and memos for their customers, and even building more effective search queries.
TIPSTER Information Extraction Evaluation: The MUC-7 Workshop The last of the "Message Understanding Conferences", which were designed to evaluate text extraction systems, was held in April 1998 in Fairfax, Virginia. The workshop was co-chaired by Elaine Marsh and Ralph Grishman. A group of 18 organizations, both from the United States and abroad, participated in the evaluation.MUC-7 introduced a wider set of tasks with larger sets of training and formal data than previous MUCs. Results showed that while performance on the named entity and template elements task remains relatively high, additional research is still necessary for improved performance on more difficult tasks such as coreference resolution and domain-specific template generation from textual sources. 
MUC/MET Evaluation Trends During the course of the Tipster Program, evaluation methodology for information extraction developed as the technology progressed. Multiple task levels and multiple languages were successful targets of information extraction. Automated scoring and statistical significance algorithms were developed for use in scoring systems and for interannotator agreement measures. The scoring interface allowed both system developers and annotators to analyze errors and improve their work. This software and the marked datasets are now in the public domain. Future projects are being carried out based on simplifications indicated by the data, downstream applications, and tractability of scoring algorithms. 
THE TEXT RETRIEVAL CONFERENCES (TRECS) Phase III of the TIPSTER project included three workshops for evaluating document detection (information retrieval) projects: the fifth, sixth and seventh Text REtrieval Conferences (TRECs). This work was co-sponsored by the National Institute of Standards and Technology (NIST), and included evaluation not only of the TIPSTER contractors, but also of many information retrieval groups outside of the TIPSTER project. The conferences were run as workshops that provided a forum for participating groups to discuss their system results on the retrieval tasks done using the TIPSTER/TREC collection. As with the first four TRECs, the goals of these workshops were:• to encourage research in text retrieval based on large test collections;• to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas;• to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems;• to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems; and• to serve as a showcase for state-of-the-art retrieval systems for DARPA and its clients.For each TREC, NIST provides a test set of documents and questions. Participants run their retrieval systems on the data, and return to NIST a list of the retrieved top-ranked documents. NIST pools the individual results, judges the retrieved documents for correctness, and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. The most recent workshop in the series, TREC-7, was held at NIST in November 1998.The number of participating systems has grown from 25 in TREC-1 to 38 in TREC-5 (Table 1), 51 in TREC-6 (Table 1), and 56 in TREC-7 (Table 1). The groups include representatives from 16 different countries and 32 companies.TREC provides a common test set to focus research on a particular retrieval task, yet actively encourages participants to do their own experiments within the umbrella task. The individual experiments broaden the scope of the research that is done within TREC and make TREC more attractive to individual participants. This marshaling of research efforts has succeeded in improving the state of the art in retrieval technology, both in the level of basic performance (see Figure 1) and in the ability of these systems to function well in diverse environments, such as retrieval in a filtering operation or retrieval against multiple languages.Each of the TREC conferences has centered around two main tasks: the routing task (not run in TREC-7) and the ad hoc task (these tasks are described in more detail in Section 2.3). In addition, starting in TREC-4 a set of "tracks" or tasks that focus on particular subproblems of text retrieval was introduced. These tracks include tasks that concentrate on a specific part of the retrieval process (such as the interactive track which focuses on user-related issues), or tasks that tackle research in related areas, such as the retrieval of spoken "documents" from news broadcasts.The graph in Figure i shows that retrieval effectiveness has approximately doubled since the beginning of TREC. This means, for example, that retrieval engines that could retrieve three good documents within the top ten documents retrieved in 1992 are now likely to retrieve six good documents in the top ten documents retrieved for the same search. The figure plots retrieval effectiveness for one well-known retrieval engine, the SMART system of Cornell University. The SMART system has consistently been one of the more effective systems in TREC, but other systems are Table 3:TREC-7 participants comparable with it, so the graph is representative of the increase in effectiveness for the field as a whole. Researchers at Cornell ran the version of SMART used in each of the seven TREC conferences against each of the seven ad hoc test sets (Buckley, Mitra, Walz, &amp; Cardie, 1999). Each line in the graph connects the mean average precision scores produced by each version of the system for a single test. For each test, the TREC-7 system has a markedly higher mean average precision than the TREC-1 system. The recent decline in the absolute scores reflects the evolution towards more realistic, and difficult, test questions, and also possibly a dilution of effort because of the many tracks being run in TRECs 5, 6, and 7.The seven TREC conferences represent hundreds of retrieval experiments. The Proceedings of each conference captures the details of the individual experiments, and the Overview paper in each Proceedings summarizes the main findings of each conference. A special issue on TREC-6 will be published in Information Processing and Management (Voorhees, in press), which includes an Overview of TREC-6 (Voorhees &amp; Harman, in press) as well as an analysis of the TREC effort by Sparck Jones (in press). 
  
Language Typology and the Comparison of Languages  While comparison of languages presupposes a common framework neutral to the structures of the languages compared and contrasted, the contemporary grammatical terms and concepts have been developed primarily on the basis of Accusative-type European languages. This has created a great deal of confusion in the universals research. Typological studies in recent years have in part been concerned with the correction of this unfortunate situation. This presentation looks at the syntactic organizations of three distinct types of languages (the Accusative type, the Ergative type, and the Philippine-type) and examines the nature of grammatical relations, Subject in particular, the voice patterns, and the ways in which semantic interpretation interacts with the typological differences in the organization of the argument structure.
Verb Alternations and Japanese How, What and Where? This is an attempt to document the full range of verb alternations in Japanese from analysis of structural regularities between entries in a valency dictionary, and determine the degree of permeation of each individual alternation type. Various techniques are analysed for extracting alternations, focusing on full or partial preservation of the selectional restrictions describing each case slot, and the degree of restriction of the selectional restrictions. These methods operate around the assumption that selectional restrictions are essentially unchanged under alternation. ) Verb alternations have been the target of considerable research within linguistic circles, in terms of describing the range of alternations for a particular language (e.g. (Levin 1993) for English and ( Fukui et al. 1985) for Japanese), applying alternations to model verbal semantics (e.g. ( Levin 1993;Hale &amp; Keyser 1987)), or analysing alternating potential and interpretational differences between verbs in different clause contexts (e.g. (Goldberg 1995;Wierzbicka 1988)). More recently, alternations have been the object of interest within the natural language processing community, for use in lexical selection in natural language generation (Dorr &amp; Olsen 1996;Jing &amp; McKeown 1998), and as a means of expanding dictionary coverage ( Baldwin et al. 1999b). There has also been some work on automatically extracting verbs which undergo particular alternations from corpora (McCarthy Korhonen 1998). Our work represents a variation on this same theme, whereby we compare the different senses 2 of a given verb within the Goi-Taikei pattern-based valency dictionary ( Ikehara et al. 1997;Shirai et al. 1997), and exhaustively determine all possible mappings between each valency frame pair. Naturally, not all such mappings are going to constitute true alternations, but by scoring each mapping and combining the scores for each mapping paradigm over all dictionary entries, it is hoped that alternations will find themselves into the top-ranking analyses. One area in which this research attempts to break new ground is, therefore, the fully-unsupervised extraction of alternations.Our purpose in extracting alternations is twofold. Firstly, we are interested in determining the possibilities for collapsing dictionary entries together through dynamic generation of regular alternations. Preliminary research on exactly this subject indicates that the use of prominent alternation types produces a reduction of almost 9% in the number of senses ( Baldwin et al. 1999a), suggesting that a fuller set of alternations could produce even greater reduction. Secondly, we are interested in deriving the inventory of Japanese alternations through automatic means, for comparison against previous analyses of alternations in Japanese. The main point of reference is the work of Jacobsen (1992) in given a relatively complete coverage of "event structure" and particularly structural alternation in Japanese.We test out three main methods to extract alternations, the first being based on full coincidence of selectional restrictions (method 1-(m i )), the second incorporating semantic backing-off to capture slight disagreements in selectional restrictions (method 2 -(m2 )), and the third scoring each alternation according to the quality of the match, in terms of the strength of the selectional restrictions and degree of overlap (method 3 -(m,3 )) . For each, we test the effects of exhaustive vs. highestranking analysis between case frames, and apply Zipf's law in weighting each case frame for expected frequency.In the next section (Section 2), we define what we mean by alternation and outline the assumptions underlying this research. We then go on to describe the basic extraction methodology in Section 3, and detail the three proposed methods in Sections 4 to 6, describing the results obtained for each approach as we go. Section 7 concludes the paper with a discussion of the future direction of this research. We set out to empirically identify the range and frequency of basic verb alternation types in Japanese, through analysis of the Goi-Taikei Japanese pattern-based valency dictionary. This is achieved through comparison of the selectional preference annotation on corresponding case slots, based on the assumption that selectional preferences are preserved under alternation. Three separate extraction methods are considered, founded around: (1) simple match of selectional restrictions; (2) selectional restriction matching, with recourse to penalised backing-off; and (3) semantic density, again with recourse to backing-off.
DETECTION AND CORRECTION OF PHONETIC ERRORS WITH A NEW ORTHOGRAPHIC DICTIONARY  The Phonetic and the Homophone Error problem in a language have been characterized as a symbol substitution problem. Phonetically equivalent symbols or symbol combinations in the language are grouped together. Each group or a number of related groups give(s) rise to a dictionary or a number of dictionaries. A new design methodology for Orthographic Dictionaries in alphabetic languages has been described. The dictionaries include the root words. The meanings are stored only in case of Homophone words. Words are sorted on the basis of a Phonetic Ordering Scheme. The dictionaries are being used to detect and correct the Phonetic Error and the Homophone Error in isolated words of Bengali. The methodologies described in this paper can be used in developing spell-checkers not only for other Indian languages which have evolved from the ancient Brahmi script and have a common phonetic structure but also for other alphabetic languages with suitable modifications. 1. The Phonetic Error Problem in NLP-an Introduction The practical NLP systems must deal with erroneous or ill-formed inputs. The nature of errors basically depends on the source of input as well as on the user&apos;s exceeding the systems limited grammatical, conceptual or functional coverage. Automatic word correction research may be viewed [7] as focusing on three increasingly broader problems: (1) nonword error detection; (2) isolated word error correction; and (3)context dependent word correction. Work on the isolated-word error correction spanned from as early as the 1960s into the present. Distinctions are sometimes made among three different types of nonword misspellings : (1) Typographic Errors-the writer or the typist knows the correct spelling but simply makes a motor coordination slip, (2) Cognitive Errors-due to misconception or lack of knowledge on the part of the writer or the typist and (3) Phonetic Errors-a special class of cognitive errors in which the writer substitutes a phonetically correct but orthographically incorrect sequence of letters for the intended words. Moreover, there exists a class of real-word errors in which the misspellings result in a valid word. It occurs due to the presence of words in the language having similar pronunciation but different meaning. This is also termed as Homophone Errors. With an increase in Natural Language applications using Indian Languages, it has become necessary to study the reasons and the nature of errors that can occur in these languages and how these errors can be
THE EFFECT OF AGE ON THE STYLE OF DISCOURSE AMONG JAPANESE WOMEN Over the past three decades, attention from a variety of disciplines has increasingly focused on the area of gender research, largely as a response to the activities of the feminist movement. Linguists have been no exception to this trend, showing an increase in interest in the influence of gender on language use.As the number of studies in the area grew, inconsistency arose over the use of the terms 'gender' and 'sex', with both being used by linguists to refer to the biological categories 'male' and 'female'. Recently, however, the general opinion appears to be that 'gender' should refer to the culturally designated characteristics corresponding to the concepts of 'masculinity' and 'femininity', while 'sex' should refer to the biological distinctions of 'male' and 'female'. This is in recognition of the fact that masculine and feminine characteristics do not necessarily correspond to a person's biological sex. While 'male' and 'female' are regarded as being two fairly distinct categories within any population, 'masculinity' and 'femininity' are not so easily divided. It is possible for a single individual, regardless of their physical features, to show a mixture of both traits. Being characteristics that are culturally defined, the meanings of 'masculine' and 'feminine' can vary widely between different societies.To date, most studies in the area language and gender research have been concerned with contrasting language use based on the biological divisions of the two sexes, whether they purport to be looking at gender and differences in language use or sex and differences in language use. It is my belief that the focus of attention will move away from this dichotomic treatment of language, and shift towards a multifaceted approach. Two paths I think likely to be taken are investigations into: 1) the relations between gender characteristics displayed by the speaker (regardless of their biological sex) and language use; and 2) the interaction of gender/sex with other social factors such as age and socio-economic class. This presentation is a step in the direction of the latter. It investigates the differences in style within the bounds of the speech of Japanese women, with particular emphasis on the effect of age on women's speech patterns. This study looks at language differences found in the conversations of two groups of Japanese women differentiated by their age. In particular it investigates differences in the strategic use of such aspects of speech as personal pronouns, adverbs, sentence-final particles, and fillers and words of habit. The use of repetition and the lengthening and shortening of sounds are also taken into account. It is found that the younger Japanese women tend to use various techniques in conversation to express their loyalty and support to other members of their group, whereas older women are more likely to adopt strategies which appear to aid them in seeking support and holding the floor during the conversation.
COMPUTER ESTIMATION OF SPOKEN LANGUAGE SKILLS  Skillful performance in a human language often involves a composite of elementary skills, such that language skills, cognitive skills, and social skills can be conflated in the judgement of a human listener. A new, computer-based method (PhonePass testing) provides an estimate of spoken language skills, relatively independent of the speaker&apos;s other social and cognitive skills. The PhonePass system has been implemented using interactive voice response to administer a standardized spoken English test that measures speaking and listening during a 10-minute telephone call. The system calculates scores on five performance subscales from a set of more basic measures that are produced by automatic speech recognition of examinee responses. Item response theory is used to analyze and scale aspects of examinee performance. The scores are also related to performance rubrics used in criterion-based human scoring of similar responses. The test construct is outlined, and the scaling methods and validation process are described with reference to experimental procedures that were used in the development of the SET-10 test, a standardized instrument. 1. BACKGROUND Computational and applied linguists have developed various methods for evaluating the performance of different elements and agents in speech communication. These include the spoken message source, the transmission medium or system, and the spoken message receiver. The spoken message source and receiver are most often human beings, but can be speech synthesizers or speech recognizers. The testing methods used to evaluate message sources and message receivers differ by tradition (e.g. applied linguistics or spoken language engineering) and depend also on the population being tested (e.g. automatic systems, or second language learners, or deaf children). This paper presents an automatic method for evaluating the spoken language skills of second language learners. In particular, we describe a proficiency test called SET-10, developed by Ordinate Corporation, that measures a person&apos;s facility in spoken English. A proficiency test, as such, assumes a domain of knowledge or skill that is independent of any particular instructional curriculum, but that measures a construct. The construct of a test is a hypothesis about the attribute (or trait) that the test is designed to measure, for example, &quot;mechanical aptitude&quot; or &quot;language proficiency&quot;.
TEXTUAL INFORMATION SEGMENTATION BY COHESIVE TIES Recent research in textual information science has increasingly turned to text processing beyond sentence level, partly because text analysis is manifestly necessary, and partly through implicit or explicit endorsement that negotiation of meaning in verbal transactions is achieved within the framework of text. Text has a rich structure in which sentences are grouped and related to one another in a variety of ways. A text is usually taken to be a piece of connected and meaningful sentences, rather than just a collection or sequence of sentences, connected only by contiguity. It is clear that text cannot simply be defined as language above the sentence (Winograd, 1972:65). Nor can we assume that stretches of language use which are isomorphic with sentences are the sole business of the grammarian, for these too are realizations of the text process. To understand a text, one must understand the relations between its parts and determine how the various pieces fit together. Clearly, these parts are not just the individual sentences: rather, sentences are joined together to form larger units, which in their turn may be the building blocks of yet larger units.In information science, there is much ongoing research in finding textual regularities on the basis of empirical investigations. Analyzing text structure often calls for more than understanding individual sentences; it usually involves a process of making connections between sentences and sustaining that process as the text progresses (Grosz &amp; Sidner, 1986). Current work also shows that centrality of text should no longer be defined in terms of any simplistic linguistic rules, but rather in terms of linguistic ties which exist among text segments (Stoddard, 1991). Among other kinds of interrelationship which a text may exhibit, cohesion has a prominent role in the understanding of its structure. As advocated by Halliday and Hasan (1976), cohesion can be a unity-creating device in texts. Lexical cohesion, which is a special kind of cohesion, investigates the repetitive sequences of lexically linked (co-articulated) items and their relations to the core sentences. Lexical cohesion has been identified as an important feature which underlines the structure of a wholesome text, distinguishing it from a non-text. Lexical cohesion, including lexical repetition as well as rhetorical continuity in conjunction with other related overt and covert linguistic markers, contributes to textual coherence by creating cohesive ties within the text. If the lexical items in a text can be related to the preceding or to following items, obviously, the text is seen more closely knit together than a text where such relationships do not exist. It has been ascertained that sentences with the greatest degree of lexical cohesion could be used to reflect the textual structure (Morris &amp; Hirst, 1991). Moreover, it is commonly believed that the recurrence of semantically related lexical items across the sentences could be used as an aid to identifying the core sentences which are characterized by their centrality and their expressive power. At the same time, while it is important for readers to be able to trace continuities in the entities under discussion, it is equally important to locate and understand the breaks in continuity. However, little research has demonstrated the functions of lexical cohesion in text segmentation and no computational theory or objective measure has as yet emerged in analyzing text structure on this basis. Given the increasing recognition of text structure in the fields of information retrieval in unpartitioned text, lexical cohesion also reveals the textual segmentability which means how texts are seen not as a continuous whole but as a complex grouping of larger pieces. There is a mounting demand for the indepth study of an implementable quantitative model on lexical cohesion in text segmentation.This research is to devise a quantitative model of text segmentation based on the study of lexical cohesion as well as other related linguistic features. What distinguishes it from previous studies is that attention is not primarily focused on itemizing cohesive features across a text but on investigating how they combine with other linguistic features to organize text into a coherent whole. We propose a novel approach in textual information science. The approach will identify discoursally salient text segment boundaries. The main objectives of this research are (i) to investigate patterns of cohesion in expository texts in order to test hypothesis about the textual continuity; (ii) to devise a measure in order to analyze the interrelations between each segment; (iii) to formulate a computational model and an objective measure in analyzing textual continuity; (iv) to propose and implement a method for the segmentation of texts into thematically coherent units. Demonstrations will be focused on Chinese expository and argumentative texts which usually consist of long sentences with little punctuation and textual demarcation. Section 2 describes the bonding analysis among the text. Our algorithm in textual segmentation identification is described in Section 3. The experimental results are presented in Section 4, followed by a conclusion. This paper proposes a novel approach in clustering texts automatically into coherent segments. A set of mutual linguistic constraints that largely determines the similarity of meaning among lexical items is used and a weight function is devised to incorporate the diversity of linguistic bonds among the text. A computational method of extracting the gist from a higher order structure representing the tremendous diversity of interrelationship among items is presented. Topic boundaries between segments in a text are identified. Our text segmentation is regarded as a process of identifying the shifts from one segment cluster to another. The experimental results show that the combination of these constraints is capable to address the topic shifts of texts.
Chomsky and Lasnik 1993, etc.), gapping constructions  111 Wh-less wh-questions are possible in Korean Korean has a special form of wh-construction in which wh-question readings are produced even without a wh-phrase. For example, the sentences in (1) and (2) below convey the wh-question readings given in the English translations (with or) without the shaded parts overtly realized. (1) ameyrikha taylyuk-ul palkyenha-n salam-un WAVO.1 ni ►ka)? America continent-Acc discover-Adn man-Top who-be-QE`Who QE`Who is the person who discovered America?&apos; (2) hankwuk cencayng ttay mikwun-i mayncheum sanglyukha-n hangkwu-nun Korean war during US;force-Nom the;first land-Adn port-Top (eti-i-pnikka)? (where-be-QE) `(Which place is) the port that the US force first landed during the Korean War?&apos; Such WH-less WH-Questions (let us call them WLWQs) are very often used as questions in written tests or uttered by quizmasters in quizzes, though not often in daily conversations. 2 I assume that, despite their restriction in the range of registers, WLWQs are also constrained by the knowledge of human language. To my knowledge, there has been no previous research on WLWQs in the field. This paper discusses some syntactic issues as to WLWQs. In the course of discussions, I try to argue among others: (i) WLWQs differ in several syntactic aspects from other reduced expressions like VP ellipsis, gapping, pseudogapping, and other shortened wh-questions (section 2); (ii) they also differ from the cleft construction despite apparent similarities in interpreation (section 3); the overtly realized parts in WLWQs are topics (section 4); they read as subjects but not as other grammatical functions like objects or adjuncts due to the principle of deletion up to recoverability, which applies to elliptical structures in general (section 5); they do not have a non-interrogative reading due to a general principle constraining information structures (section 6); and nominative (as opposed to topic) WLWQs do not exist because of the restriction on phases (Chomsky 1998) (section 6). In the appendix, I discuss a potential conceptual problem that WLWQs face with respect to the feature copying mechanism and suggest possible solutions. [21 WLWQs and Other Reduced Expressions In this section, WLWQs and other reduced expressions will be compared. Among the reduced expressions are VP. I thank the participants for their comments and suggestions. I also thank an anonymous Paclic paper reviewer for her valuable comments, suggestions, and corrections. I regret that I could not incorporate all of them into the paper, though. Of course, all shortcomings are mine. WLWQs are not entirely excluded in spoken languages, as the following example is acceptable: (i) kewul-a, kewul-a, seysang-eyse kacang yeppwun yeca-un (rmuk mirro-Voc mirror-Voc, world-in most beautiful woman-Top who-be-QE`Mirror QE`Mirror, mirror, who is the most beautiful woman in the world?&apos;
A Computational Study of the Ba Resultative Construction: Parsing Mandarin Ba Sentences in HPSG Probably no other lexeme in Mandarin evokes as much controversy as ba in regard to the lexical category. The category of ba in Modem Mandarin has been claimed to be various types of verbs (including auxiliary verb) as well as a `deverbalized' preposition (cf. Wang 1947, Chao 1968, Hashimoto 1971, Huang 1974, Li &amp; Thompson 1981, Sun 1996, and Sybesma 1999. What counts towards the so-called`bacalled`ba-construction' is also controversial (for details, see Ding 1993). Mangione (1982) presents an investigation of ba sentences most comparable to the one undertaken here in terms of the types of ba sentences and the explicit formal approach. The largest divergence lies in his treatment of ba as a preposition, heading a sort of 'adverbial prepositional phrase' (Mangione 1982: 158). Since Montague Grammar allows prepositions to head predicates semantically (cf. Dowty et al 1981: 243), the crux of the ba problem is dissolved in this framework. Taking advantage of this conflation of verb and preposition, Mangione is able to offer a formal analysis of the predicate headed by ba while arguing for treating ba as a preposition. This convenience, however, is intrinsically built upon the premises of Montague Grammar.The perennial debate about the syntactic category of ba endures such a lengthy period partly due to neglect of its resultative meaning, and in part because of the discrete-category classification that hinges on a clear-cut distinction. This study aims at demonstrating the advantage of treating ba as a resultative verb, proposed in Ding (1993), through a rigid computational implementation of the analysis of the Ba Resultative Construction within the framework of Head-driven Phrase Structure Grammar (HPSG). It also includes a small parsing experiment for processing the Ba Resultative Construction, showing the crucial interplay between syntax and semantics.The major parts of the paper are organized as follows: A working definition of ba sentences is proffered in §2. Discussing the option of considering ba as a resultative verb, §3 analyzes the general syntactic structure of the resultative construction. Following the HPSG analysis presented in §4, §5 is devoted to the parsing experiment that implements the proposed analysis. Recognizing Mandarin ba sentences as the Ba Resultative Construction, this study implements a rigid test of Ding&apos;s (1993) treatment of ba as a matrix verb in the periphrastic resultative construction. The computational experiment is facilitated with a formal rendition of the analysis of ba in Head-driven Phrase Structure Grammar. Moreover, the small parsing experiment also highlights the important role of semantics in natural language processing, including common-sense knowledge.
CATEGORIAL GRAMMAR WITH FEATURES AND THE PARSER ON WEB PAGES  In this paper, I present a version of Categorial Grammar reinforced with subcategorizing and operational features. Employing the features allows the further specification of combinatory restrictions in natural languages. I show also that by assigning higher-order categories to words, such irregular expressions as &quot;ago&quot; and &quot;last&quot; and controversial constructions such as the formal subject &quot;it,&quot; &quot;tom&quot; constniction and subject raising can be analyzed only through the rule of functional application. Flat categories are assigned to Japanese verbs for their &quot;scrambling&apos; nature. These are demonstrated on a parser working on Web pages.
A CONVERSATIONAL LOGIC: WA AND GA*  This paper (a) presents a fragment of a logic of conversation with some philosophical basis (b) attempts to model and explain differences and properties of wa and go, notably the so called Unagi-Bun, the comparative (contrastive) readings of wa and uniqueness (complete-list) readings of go (c) brings inner piece to those who wisely do not react it.
ACCESSIBILITY AMONG SITUATIONS: PRAGMATICS OF DISJUNCTION IN JAPANESE The long-term goal behind our current study is to give an integrated account of how variables in formal languages and indeterminate expressions in natural languages should be treated in formal pragmatics. These expressions are typically used not in connection with particular entity or objects but to express quantification, query or co-variation among entities or objects within the domain of discourse. In Japanese, we have indeterminate expressions such as "dare" (who), "dore" (which one), "doko" (where) and "dono" (which)'. Combined with appropriate particles such as "mo" and "ka", these indeterminate expressions are used not only for question but also for quantification. It is interesting to note that these two particles are used also in coordination: "mo" for conjunctive coordination and "ka" for disjunctive coordination.' For disjunction with "ka", we observe that so-called "or"-reading and "and"-reading are available depending on the situations involved. Further, this disjunctive particle "ka" is also used as 'question marker'. Reflections upon these linguistic facts in Japanese will suggest how disjunction relates to existential quantification, question, and "and"-reading of conjunction. In this paper, we will mainly focus our attention on interpretation of disjunction with "ka", discussing how the differences between "or"-reading and "and"-reading emerge, touching upon existential and universal quantification and question.Before we start our discussion on disjunction, it may be worthwhile to see how coordination is expressed in Japanese. In Japanese, conjunctive coordination is expressed with conjunctive particle "mo" while disjunctive coordination is expressed with disjunctive particle "ka". These two particles, with indeterminate expressions such as "dare" (who), "dore" (which one), "doko" (where) and "dono" (which), are used to express existential quantification and universal quantification, respectively. Interestingly, disjunctive particle "ka" is also used as 'question marker'. Disjunctive expressions in Japanese coordinated with &quot;ka&quot; get either &quot;or&quot;-reading or &quot;and&quot;-reading depending on the situations in which the utterance takes place. This is comparable to some extent to the English counterparts with &quot;or&quot;. In this paper, we will examine some cases where the same string of words gets different readings depending on the situations involved. We argue that the differences in interpretation emerge from the differences in accessibility among situations associated with the utterance. Pragmatically, our observation that a given disjunctive expression gets either &quot;or&quot;-reading or &quot;and&quot;-reading in a given situation reflects that only one of those readings can make the expression under consideration informative enough in that particular utterance situation.
THE MODULE-ATTRIBUTE REPRESENTATION OF VERBAL SEMANTICS  In this paper, we set forth a theory of lexical knowledge. we propose two types of modules: event structure modules and role modules, as well as two attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences. Our data is drawn from a comprehensive corpus-based study of Mandarin Chinese verbal semantics.
USING BILINGUAL SEMANTIC INFORMATION IN CHINESE- KOREAN WORD ALIGNMENT  This pape clarifies the definition of alignment from the viewpoint of linguistic similarity. We propose new method for the alignment betwee the languages that do not belong to the same language family. On the contrary to most of the previously proposed methods that rely heavily on statistics, our method attempts to use linguistic knowledge to overcome the problems of statistical model.
MULTIPLE REPRESENTATION OF JAPANESE COMPLEX PREDICATES: A LEXICALIST ANALYSIS OF SUBJECT AND OBJECT HONORIFIC FORMS Lexicalist approaches to syntax can benefit from multidimensionality of representation as exhibited by Mohanan(1997). By multidimensionality of representation(MR) is meant a representational scheme in which more than one representation can be associated with a liguistic unit in the lexicon at a particular point of its generation. The lexicon as the generator of wordforms is rather like a knowledge base whose function as a repository of lexical information Comes not only in its static capacity of serving relevant, information at the demand of syntax but also in its dynamic capacity of modifying and updating the overall structure of the lexicon in terms of the relationships between the items of information. It is uncontroversial that lexical information has to reflect various levels of representation in syntax as well as those of semantics and pragmatics. Thus the idea of MR in the lexicon is germane to the nature of the lexicon.Mohanan(1997)'s treatment of NV complex predicates in Hindi is an instance of the MR approach to the lexicon. In the paper, complex predicates are defined as follows:A COMPLEX PREDICATE construction is one in which two semantically predicative elements jointly determine the structure of a single syntactic clause.(p.432)Mohanan uses the following example to illustrate how complex predicates are formally represented using two levels of representation: the semantic structure (Sem Str) and the argument structure (Arg Str).  Semantically, the complex verb (CP) bharosaa kar 'trust do' has at least two arguments, the truster and the trusted. It has also to be shown that the truster, which is an experiences, takes ergative case, just as the doer does. Moreover the nominal bharosaa behaves as an argument ill the syntactic level. These properties of the CP is captured by the following representation.With a relevant algorithm for ordering and merging the two predicates, it is clear that the representation can account for how the predicates are combined to form a CP which has the above stated characteristics. Lexicalist approaches to syntax can benefit from multidimensionality of representation as exhibited by Ailohanan(1997). Subject and object honorific forms in Japanese are shown to be complex predicates with its second component grammaticalized and triggering argument-sharing of the two components of the predicate. Multidimensional representation makes it possible to account for the Gricean inference mechanism working during the argument-sharing process of these honorific forms.
GENERATION OF ADAPTIVE VOCABULARY LEXICON FOR JAPANESE LVCSR No sooner had we created our first lexicon for Japanese large vocabulary continuous speech recognition than we realized the challenge of out-of-vocabulary problems involved. Although 220 thousand words have been acquired from traditional dictionaries to build our lexicon, we can still find ten thousands of 00V words and one million occurrences of them in our corpora. The high OOV rate, ranging from 4.3% to 9.8%, may cause an impact on recognition system and account for a major share of recognition errors.Inspecting to the text segmentation errors, several types of vocabulary problems can be identified. As an agglutinative language, the inflection of Japanese is very complicated and hard to handle. For example the gerund form`natteform`natte' is one of the difficult problems for segmentation'. About compound and derived words, it's difficult to make new words recognizable without introducing new composing units (subwords). We need to acquire prefixes and a suffixes from the derived words like Ifi-Atl-'(new food law) to help identify new words composed in the similar way. Again we sometimes have to combine words to make a new entry in lexicon because the pronunciation of the combination can not be predicted by individual components, such as ' -'(one minute). We also found that Arabic numbers, alphabetic acronyms, and katakana2 words are other prevailing types of OOV words appearing in news context like the Nikkei News3 . However, not all of the OOV words should be added into the lexicon. We have to consider certain selection strategy to keep the lexicon efficient. An LVCSR system will have difficulty to perform well if it fails to cope with the vocabulary problems in early stages.We consider an integrated approach to deal with the above issues. Motivated by Geutner's work [1] on similar problems and based on our needs, we propose a lexicon development model to easily and quickly To keep consistent with the segmentation of its plain form`naform`na-ru'(become), it should be segmented as`naas`na-tte'(teform of naru). But it leads to a weird`smallweird`small-tsu'-initialized segment -tte.Katakana is one of Japanese writing symbol types. It's used to represent words of foreign origin. 3 For example, the numbers of the three types of OOV words in half a year of Nikkei news are about 7000, 5000, and 44000, respectively.  Fig.2 Samples of Lexicons generate the lexicon we want. It allows us to adapt the vocabulary of our lexicon from domain to domain, which may share only a quarter out of 55000 OOV words. It also enables us to freely reduce the lexicon size with minimum coverage loss. One of the thorniest problems of large vocabulary continuous speech recognition systems is the large number of out-of-vocabulary (OOV) words. This is especially the case for the languages like Japanese, which has many inflections, compound words and loanwords. The OOV words vary with the application domains. It&apos;s not realistic to have a big general-purpose lexicon including any possible 00V words. Furthermore, embedded speech recognition systems become more and more popular recently. They strongly demand an economical and effective exploitation of lexicon space. In this paper, we introduce a lexicon development model dealing with different kinds of OOV words with the help of linguistic morphological knowledge. It provides unsupervised, fast vocabulary adaptation among different application domains. In the experiments that adapt a lexicon of typical vocabulary, we were able to reduce the OOV rate by 40% and improve the word segmentation error rate by 27%. And the smaller the lexicon is, the more we benefit from the vocabulary adaptation.
QUALIA STRUCTURE AND THE ACCESSIBILITY OF ARGUMENTS: JAPANESE INTERNALLY-HEADED RELATIVE CLAUSES WITH IMPLICIT TARGET Japanese IHRCs present several puzzling behaviors that challenge most syntactic approaches to them. One of such behaviors concerns the identification of the antecedent (henceforth, the (semantic) target) of the IHRC. Namely, the target is not limited to an argument of the head verb of an IHRC; in some cases it can be a set of arguments, while in others it can even be some entity merely implicit in the sentence. In order to accommodate such cases, most analyses posit an empty category as the relative head, whose antecedent is determined by some pragmatic considerations ( [1], [2], [6]). The recourse to contextual information is shared by other approaches including the ones in Cognitive Grammar ( [3], [4]). The analyses in Cognitive Grammar state that the target is selected from among salient participants of the situation, where the saliency is related to but is independent of syntactic argumenthood One question that emerges is what the "pragmatic" considerations licensing the implicit target exactly are. Obviously, only a certain set of implicit arguments can stand as the target. Without due restrictions, the theory would become too powerful, overgenerating the IHRCs with illicit implicit targets. Besides, the complete recourse to pragmatics predicts that the availability of the implicit target is totally independent of the lexical information; it would be sensitive only to the contextual information. However, as we shall see below, the distribution of the IHRC with an implicit target is actually very limited, and the restrictions reveal regularities related to lexical information.If a syntactic analysis assumes a lexicon which accommodates only what is structurally realized, then the IHRC with an implicit target falls outside the scope of analysis. But if one assumes a lexiconbased framework which includes in its lexicon even the non-linguistic information of some kind, the implicit target could be dealt with in a formally restricted way. As such framework, I will employ the model of GL ( [5]), and explore at the same time what kind of revision it needs to fully accommodate the data. My goal is to demonstrate that the availability of the implicit target of the IHRC is fairly constrained, and that the data can be accommodated without calling for the poorly-defined "pragmatics."The organization of this paper is as follows : Section 2 presents data of IHRC including the ones with implicit target, indicating the direction of approach this paper takes. The data of the IHRC with implicit target are put to detailed examination in Section 3. It shows that the availability of the implicit target is more restricted than previous analyses tacitly assume, giving the first approximate delineation of the implicit targethood. Section 4 introduces the model of GL by Pustejovsky, and explores how it can accommodate the primary data. Section 5 discusses how the model of GL should be modified to fully account for the data of IHRC. Section 6 concludes the discussion. The relationship between lexicon and pragmatics has been one of the most controversial issues in recent studies in linguistics. Lexicon in the conservative definition encodes only the information responsible for the regular syntactic mapping of lexical items. Any information that is not directly reflected in the syntactic structure is attributed to pragmatics. A more recent trend, in contrast, considers that lexicon should cover even apparently non-linguistic information if it leads to certain syntactic and/or semantic regularities. Generative Lexicon (GL) by Pustejovsky is a representative of such a trend. This paper applies GL to analyze the Japanese internally-headed relative clause (henceforth, IHRC) with implicit target, which has presented a serious challenge to formal approaches. The IHRC whose target is neither a syntactic argument nor an adjunct has strongly motivated a pragmatic approach to the identification of the target. This paper argues that IHRCs with implicit target can be formally accommodated without drawing on the poorly-defined pragmatics, if the lexicon is sufficiently elaborated.
A Constraint-Based Approach to Some Multiple Nominative Constructions in Korean Much attention has been paid to the generation and structure of the so-called multiple nominative constructions (MNC). Some of the constructions I will discuss in this paper are given in (1).(1) a. Mary-ka son-i yepputa. (Whole-Part Pattern: WPP) Mary-NOM hands-NOM pretty`Mary pretty`Mary's hands are pretty.' b. Mary-ka emeni-ka yepputa. (Focus Pattern: FP) Mary-NOM mother-NOM pretty`It pretty`It is Mary whose mother is pretty.' c. Mary-ka John-i twulyepta (Psych-Pattern: PP) Mary-NOM John-NOM fearsomèfearsomèMary fears John.'These three patterns seem to be identical types. However, a careful examination of these apparently similar constructions reveals that each of these patterns has its own syntactic, semantic, and even pragmatic properties. These differences have not been recognized by much literature. This paper will first examine their properties and then suggest the syntactic structures of each type.2 Basic Properties 2.1 Properties of the First Nominative NP Let us begin with some basic properties of the first nominative NP in these three patterns. As shown by O' Grady (1991) and others, the three types behave differently with respect to honorification, binding, and equi-phenomena.For example, in the whole-part pattern (or often called inalienable pattern) and psych pattern, it is the first NP that triggers honorific agreement, whereas in the focus pattern it is the second NP that determines the agreement. This is illustrated in the examples (2).(2) a. WPP: sensayng-nim-i elkwul-i yeyppu-si-ta. teacher-HON-NOM face-NOM pretty-HON-DECL`The DECL`The teacher's face is pretty.' b. FP: *Sensayng-nim-i sonca-ka yeyppu-si-ta. teacher-HON-NOM grandchild-NOM pretty-HON-DECL`It DECL`It is the teacher whose grandchild is pretty.' c. PP: sensayng-nim-iJohn-i twulyep-usi-ta. teacher-HON-NOM John-NOM fearsome-HON-DECL`The DECL`The teacher fears John.'Another difference comes from binding. In the whole-part pattern and in the psych, the first NP can serve as the antecedent of the anaphor caki. In the focus pattern, the antecedent is not the first but the second NP in a normal context as shown in (3) Equi constructions illustrate another difference: In the whole-part pattern (4)a and the psych pattern (4)c, the first NP can serve as the controller of the unexpressed subject (PRO) of the embedded clause. The relation is different in the focus pattern. In example (4)b, it is not the first NP but the second NP that serves as the controller (cf. Yoon 1986). In addition to these differences, we can also observe a difference in plural copying. The plural copying allows the plural marker to be attached to elements other than the plural subject within a clause. As shown in (5)a, the first plural nominative NP in the whole-part pattern can trigger the plural copying on the adverbial element acwu. But when the first NP is singular as in (5)c, no plural copying is possible on the adverbial. The focus pattern contrasts with the whole-part pattern; even though the first NP in (5)b is plural, we cannot have the plural marker on the adverbial acwu.(5) a. WPP: haksayng-tul-i maum-tul-i acwu-tul kop-ta. student-PL-NOM heart-PL-NOM very-PL pretty`Students pretty`Students are really good-hearted.'b. haksayng-tul-i maum-i acwu-tul kop-ta.(6) a. FP: John-i chinku-tul-i acwu-tul chakha-ta. John-NOM friends-PL-NOM very-PL honest-DELL`It DELL`It is John whose friends are honest.' b. *haksayng-tul-i chinkwu-ka acwu-tul chakhata.(7) a. PP: sensayngnim-tul-i haksayng-i acwu-tul mwusepta. teacher-PL-NOM student-NOM very-PL fearsomèfearsomèTeachers fear students.' b. *John-i haksayng-tul-i acwu-tul mwusepta.We also observe a difference in the possibility of having an indefinite NP. As shown in (8), the first NP in the focus pattern cannot be an indefinite NP, unlike the one in the whole-part or psych-pattern.' (8) a. WPP: nwukwunka-ka elkwul-i yepputa someone-NOM face-NOM pretty`Someone pretty`Someone's face is pretty.'b. FP: ??Nwukwunka-ka emeni-ka yeppusita someone-NOM mother-NOM pretty`It pretty`It is somebody whose mother is pretty.' c. PP: nwukwunka-ka John-i mwusepta. someone-NOM John-NOM fearsomèfearsomèSomeone fears John.'In sum, we can observe that the first nominative NP in the focus pattern is basically different from the first NP in the whole-part and in the psych pattern. It is only the latter two patterns whose first nominative NP bears the canonical properties of subject. The so-called multiple nominative constructions (MNC) have been one of the puzzling phenomena in topic-prominent languages like Korean, Japanese, and Chinese. This paper first investigates the basic syntactic properties of three MNCs and then provides a constraint-based analysis within the theory of HPSG (Head-driven Phrase Structure Grammar). The analysis presented here is lead-driven&apos; and &apos;constraint-based&apos; in the sense that properties of lexical heads and tight interactions among declarative constraints play crucial role in the formation of puzzling MNCs. Some appealing consequence of this analysis is a straightforward account of long standing problems, such as getting the semantics right and generating multiple nominative constructions without resorting to a mechanism that allows unlimited valence increase.
Dynamics of Information Packaging in Korean It is generally known that Korean is a highly context-dependent language, and said that any arguments recoverable from the context are freely dropped. This paper starts from a doubt on the second assumption, and attempts to characterize the deletion of arguments in Korean as a systematic process motivated and restricted by information packaging strategy in the sense of Vallduvi(1992) and Vallduvi &amp; Engdahl(1996). The following examples illustrate diverse deletion of arguments in Korean:( 1 ) A: nuku-ka Minsu-eykey chaek-ul who-nom Minsu-dat book-acc 'Who threw a book to Minsu?' B 1: Suni-ka 0 0 Suni-nom 'Suni threw it.' B2: Minsu-eykey-nuni Suni-ka ti Minsu-dat-top Suni-nom 'As for Minsu, Suni threw it to him.' B3: chaek-un, Suni-ka 0 ti book-top Suni-nom 'As for a book, Suni threw it.' B4: # Suni-ka Minsu-eykey chaek-ul Suni-nom Minsu-dat book-acc ' Suni threw a book to Minsu.' tenchi-ess-ni? throw-pst-qus tenchi-ess-e. throw-pst-dcl 0 tenchi-ess-e. throw-pst-dcl.tenchi-ess-e. throw-pst-dcl.tenchi-ess-e. throw-pst-dcl.The responses in (1B1)-(1B3) are felicitous, and this gives the superficial impression that deletion of old arguments is optional.' Close observation, however, makes us notice several important points. First, the answers in (1B1)-(1B3) denote slightly different interpretations, though they express the same propositional content. Secondly, the undeleted old arguments in (1B2) and(1 B3) undergo some kind of structural changes. That is, Minsu in (1B2) and chaek 'book' in (1 B3) are moved from their canonical position to the sentence initial position, and the topic marker nun is attached to them. Thirdly, the sentence (1B4) shows that the repetition of an old argument without any structural difference makes the answer unnatural. The examples in (1) imply that dynamic changes (including deletion) of old arguments are obligatory in Korean, and that these changes are closely related with some kind of functional structure which induces the different interpretations. The object of this paper is twofold. One is to show that deletion as well as movement of old arguments is a systematic process of information packaging to generate the most optimized information structure. This paper argues that there is neither free word order nor free deletion in Korean from the viewpoint of information packaging. The other is to suggest a constraint on information packaging which covers both deletion and movement operations. The constraint gives an indirect account for the motivation of both processes. This paper examines diverse information structures in question-answer dialogue from the perspective of information packaging and shows that old arguments always undergo deletion or some kinds of structural change to denote their functional change in information structure. This paper discusses deletion and movement operations in Korean from the perspective of information packaging, and claims that each operation is a systematic process of information packaging to generate the most optimized information structure. This paper argues that there is neither free word order nor free deletion in Korean. This paper explicitly illustrates that old arguments cannot stand still in information structure, but must undergo some kind of structural changes or deletion. This paper proposes a constraint on information packaging that covers both operations and gives an explanation of underlying motivation for them. It is also suggested to decompose each component of information structure into a feature complex.
PROBLEMS IN DEFINING A PROTOTYPICAL TRANSITIVE SENTENCE TYPOLOGICALLY Transitivity has been defined in many different ways during the history of linguistics. Traditionally transitivity was understood to be a semantic phenomenon: Transitive sentences were sentences which describe events that involve a transfer of energy from subject to object (e.g. 'he killed the man'). Structurally these are mostly sentences with a grammatical subject and a direct (accusative) object (at least in the languages that were the targets of description). In the golden era of formal grammars transitivity became a purely formal concept: Verbs (or more rarely sentences) with a direct object were classified as transitive and those without one as intransitive without taking their semantics into account (see e.g. Jacobsen 1985:89 [4]). Formal definitions very rarely allow any intermediate forms, i.e. every transitive sentence is equally prototypical (because there are no adequate criteria by means of which the prototypicality could be measured). Therefore, the concept of a prototypical transitive sentence is irrelevant in the formal descriptions. Lately, because of the growing interest in "exotic" languages this purely structural definition (that was mainly based on English or other Indo-European languages) has become insufficient. It is, for example, not capable of describing transitivity in languages in which subject and object cannot be defined so easily as in the Indo-European languages. As a result of this, a new kind of "typological" definition (which is more or less directly based on the traditional semantic definitions) has been developed. This definition is a kind of combination of the two previously presented ones: It defines the prototypical transitive sentence as one which describes a semantically highly transitive event. This definition has the advantage that it does not restrict the structure of the prototypical transitive sentence to some specific structure, which is the case in the formal definitions. It is, as such, however, not so suitable for describing transitivity as one might assume, but it has to be made more accurate. In this paper I present some issues (some of which are very basic in nature (and may even seem naive), but should in any case be explicitly mentioned in a valid definition) that one has to take into account when defining a prototypical transitive sentence. The goal of this paper is to propose a new (specified) prototype that could possibly be more suitable for typological studies. I hope to be able to show that the presented prototype is not so unproblematic as has often been assumed. This paper is trying to show that the concept of the prototypical transitive sentence is very useful in the study of transitivity, but is as such not so unproblematic as has often been assumed. This is achieved by presenting examples from different languages that show the weaknesses of the &quot;traditional&quot; prototype. The prototype proposed here remains, however, a tentative one, because there are so many properties that cannot be described typologically because of their language-specific nature.
4-VALUED REASONING WITH STRATIFIED BILATTICES  Since [5], 4-valued logic is known to be a useful tool to capture the human reasoning: it is paraconsistent, can treat incompleteness and inconsistency of information etc. In this paper, I propose a 4-valued reasoning system with stratified bilattices of [12]. It inherits desirable formal properties of 4-valued logic, and further realizes a certain kind of default reasoning and truth maintenance system with a simple, lucid LK-style calculus without esoteric, exotic 4-valued operations in [6, 7, 2, 3] etc.
On the Discourse Analysis in Korean Dialogues In Korean, the zero anaphora is very common in a domain restricted dialogue such as the one found in the situation of hotel reservation as follows':(1) U 1 : iss e-yo? exist (Is there a room free?) U2:nalcca encey-sip-nikka? date when (For what date are you going to make a reservation?) U3:onul cenyek-ey. today night (I'd like to make a reservation for tonight)[U = Utterance, = zero pro-form]In the above example, a long discussed issue is how to establish the antecedent of the zero anaphors. In this study we propose a reasonable and reliable solution to the problem.The following five types of information structures are assumed in CIPT:(2) a. Link -Tail -Focus structure (L-T-F structure) b. Link-Focus structure (L-F structure) c. Tail -Focus structure (T-F structure) d. Focus structure (F structure) e. Slot Link -Focus structure (SL-F structure)The SL-F structure is the one defined by Lee &amp; Lee(1998), in addition to the original information packaging theory of Vallduvi (1994). We adopt the concept of the frame theory devised in the Artificial Intelligence community.We thank Professor Jungyun Seo of Sogang University and Professor Hyunho Lee of Tongyang Technical College for allowing us to use the corpus they constructed for the Soft Science Project.In this paper we claim that the sentences with zero anaphors tend to exhibit the SL-F structure, on the basis of empirical evidence from actual dialogue corpora found in situations such as hotel reservation, theater talk, etc. As a next step we propose a revised ranking of the forward-looking centers in the sense of centering theory. It is claimed that the componential status of the information structure of the relevant utterance is revealed in the form of a hierarchy as follows:(3) SL-component &gt; Speaker, Hearer &gt; Subject &gt; Indirect Object &gt; Direct Object &gt; Others With this hierarchy, we can calculate the reference of zero anaphora in any form of domain restricted dialogues.As for the overt anaphor, H. Lee(1998) postulates a constraint for the recovery of its antecedent at the moment when a sentence is uttered after returning from a sub-dialogue. He observes that an overt pronoun must have its antecedent in the sub-dialogue when it appears in the first utterance immediately after the sub-dialogue. Look at the example in (4).(4) Ul: Seoul ollawa-se-nun meyn cheum-ey came-after at first ince ku Naksan kkokteyki-ey ku acu ku chenmakchon well, the top at well the tent kathun tey Inca. like place well (When I arrived in Seoul, I (went) to the top of the Naksan mountain, well, to the poor village ) &lt;Sub-dialogue&gt; U2(S 1): naksan-imyen ce Tongtaymunccok ? direction (Do do you mean the Nagsan mountain near East Gate? ) U3(S2): yey Tongtaymun-ye i-ss-upni-ta. yes at exist (Yes, it is. It is located near Tongtaymun.) U4(S3): yey, yey.yes, yes (I see. I see.) &lt;/Sub-dialogue&gt; U5: kuri kass-ess-nun-tey, there went... (I went there, ... )In H. Lee's(1998) analysis, the overt anaphor kuri 'there' in the utterance U5 has its antecedent Naksan in the previous sub-dialogue(namely, U2(S1)). We, however, claim that the proposed analysis is not convincing because the same antecedent can also be found in the utterance Ul, which is in the main dialogue.In this paper we show that H. Lee's hypothesis is not correct and we propose a general constraint on the interpretation of the overt anaphor, on the basis of the analysis of the realistic corpus. The constraint is stated as follows:(5) The overt anaphor has its antecedent in the discourse segment of the same or higher level. The purpose of the paper is twofold. First, we revise the well-known Centering Theory of anaphora resolution and propose the Controlled Information Packaging Theory (CIPT, for short). Second, we suggest a solution to the resolution of the antecedents of pronouns within the framework of CIPT. For this purpose, we select a dialogue of hotel reservation as a domain-restricted discourse, and discuss the characteristics of the distribution of pronouns. We suggest that we need to place the Slot-Link element on the top of the forward centering list. We claim that we need to establish a constraint on conceptual compatibility. As for the pronouns in the main dialogue, we propose a constraint of discourse command (d-command).
A CCG FRAGMENT OF KOREAN* We briefly introduce CCG (Combinatory Categorial Grammar) and Set-CCG (Set Combinatory Categorial Grammar), a direct descendent of the former. Combinatory Categorial Grammar is a lexicalised formalism which is mildly context-sensitive. Recently Set Combinatory Categorial Grammar, a direct descendent of Combinatory Categorial Grammar was proposed to treat local scrambling adequately. In this paper, we briefly sketch Set Combinatory Categorial Grammar analyses of various Korean syntactic phenomena including coordination, extraction and multiple nominative construction.
DEVELOPING DATABASE SEMANTICS AS A COMPUTATIONAL MODEL* For business transactions or academic administration, a commercial database management system (DBMS) like DB2, Oracle or Informix is widely used. Database Semantics is an attempt to adopt such a system for doing semantics for ordinary language. Since an interpretation model or background is necessary for processing linguistic information, Database Semantics can use as its model a database in a DBMS that provides both lexical meaning and world knowledge information. Furthermore, a DBMS constantly updates its database with new data, as fragments of a natural language like Korean or English are processed through a linguistic processing system LIPS. Hence, Database Semantics can consistently process even a larger fragment of discourse in natural language. Both Hausser [1] and Lee [2][3] proposed Database Semantics as a computational model for natural language semantics that makes use of a database management system, DBMS. As an extension of these efforts, this paper aims at dealing with ways of representing linguistic descriptions in table forms because all the data in a relational model of DBMS is conceived of being stored in table forms. It is claimed here that, if an algorithm can be developed for converting linguistic representations like trees, logical formulas, and attribute-value matrices into table forms, many available tools for natural language processing can be efficiently utilized as part of interface or application programs for a relational database management system, RDBMS.
NOMINAL SCOPE IN SITUATION SEMANTICS  This paper s introduces a semantical storage approach for representing nominal quantifi-cation in situation semantics. Quantificational determiners are treated as denoting binary relations, and their domains and ranges are defined. The linguistic meaning of an expression is given as a pair of its quantificational storage and basis. The storage contains the meanings of quantified NPs occurring in (/), while the basis represents the semantical structure of the result of the substitution of those NPs with parameters. Scope ambiguity is available when more than one quantifier is in the storage. A generalized quantificational rule moves some of the quantifiers out of the storage into the basis. There is a restriction that prohibites relevant free parameters from being left out of the binding scope. The storage is empty when there are no quantified NPs occurring in 0, or when there is enough linguistic or extra-linguistic information for resolving scope ambiguities.
How Computer Selects Antecedent? This study provides a computer-simulated optimal selection of antecedent of reflexive pronouns based on the Visual C++ computer programming language. The algorithm that I use for this implementation is based on the optimal theoretic approach to the proper interpretation of reflexive pronoun roughly adopted in Moon (1999). I show that the complex behavior of reflexive pronouns can nicely be explained in the Optimality Theory (henceforth, OT) and the optimal selection of reflexive pronoun can be implemented on computer simulation.Following the ideas of Hendriks &amp; Hoop (1999) and Moon (1995Moon ( , 1999), I claim that the seemingly intricate anaphoric interpretation better be explained by the interaction of the six ranked violable constraints-Thematic Hierarchy Constraint (THC), Larger Domain Preference Constraint (LPC), Subject-Oriented Constraint (SOC), C-Command Constraint (CCC), Locality Condition (LOC), and Discourse Binding Constraint (DBC).The organization of this study is as follows: in section two I briefly introduce the core of optimality theory developed in Prince and Smolensky (1993); in section three I explore the major characteristics of unbounded reflexives; in section four I explain how OT is applied to the proper interpretation of reflexive pronouns; and finally in section five I show the computer implementation algorithm that I used for the optimal antecedent selection of reflexive pronoun. 
ON CASE ALTERNATION PHENOMENA: A CATEGORIAL APPROACH  In Japanese potential and tough constructions, arguments standing in various semantic relations to a base verb can be marked with nominative case. Such subjects also may show up with genitive case in nominalizations irrespective of their semantic relations. This paper, addressing a variety of case alternations which have not hetherto attracted much attention from theoretical linguistics, proposes within the categorial framework that case alternations can be accounted for in terms of the applicability of a type shift rule introducing gaps into predicatephrases and the composition rule which combines base verbs with higher stative predicates passing the information about gaps up to the result categories. 0. INTRODUCTION In this paper, we propose a completely new analysis concerning nominative and genitive marking in Japanese stative clauses in the categorial framework. Its aim is twofold: to examine a wider range of data than has done in the past, and to provide a unified account of the phenomena called case alternations in Japanese. Let us begin with an overview of data.&apos; I add the semantic roles of the arguments showing case alternations to each of the examples: (1) a. Tanaka-ga sakana-ga/-o tabe-rare-nai-koto (theme) Tanaka-NOM fish-NOM/-ACC eat-can-Neg-fact &apos;the fact that Tanaka cannot eat fish. b. kono boorupen-ga/-de hagaki-o kaki-nikui-koto (instrumental) this boll-point-pen-NOM/with postcard-ACC write-difficult-fact &apos;the fact that it is difficult to write a postcard with this boll-point pen&apos; c. Yamada-no ie-gat-kart daigaku-ni tuugaku-si-yasui-koto (starting point) Yamada-GEN house-NOM/-from college-to go-easy-fact &apos;the fact that it is easy to go to college from Yamada&apos;s house&apos; d. tihoo-gakkai-ga/-notameni jugyoo-ga yasum-e-nai-koto (reason) local meeting-NOM/because-of class-NOM skip-can-Neg-fact &apos;the fact that we cannot skip a class because of meetings of the local society&apos; e. kono jitensya-gai-no taia-ga kookan-si-yasui-koto (possessor of theme) this bicycle-NOM/GEN tire-NOM change-easy-fact add the formal noun koto &apos;fact&apos; to example sentences throughout this paper. When the stative predicate expresses the property of an individual denoted by the subject that are permanent or relatively stable, the subject should be marked with the topic marker-wa, as in kono boorupen-wa kaki-nikui &apos;it is difficult to write with this ball-point pen&apos; and if it is marked with nominative case, it receives the exhaustive-listing reading (see Shirai 1985). When it is embedded in the nominalization context, as in (1), the interpretation becomes ambiguous between the exhaustive-listing and neutral-description (Kuno 1973).
A MOVE TOWARDS A GENERAL SEMANTIC THEORY  In this paper, I would like to propose an idea of semantics for discourse and pictorial representations. The central notion of this paper is function. A representation is a representation of something. A function determines the connection between representation and what is represented. This is true for both discourse and pictorial representations. To properly interpret a representation system means to find an intended function that connects representation and what is represented. By combining different semantic functions, we can express complex semantic relations. In the first part of this paper, I will present a general semantic theory; in the second part, this theory will be applied to several examples.
Collocation Deficiency in a Learner Corpus of English: from an overuse perspective  Collocational deficiency is a pervasive phenomenon in learner English. Language learners often fail to choose the correct combination of two or more words due to their unawareness of collocational properties in vocabulary. They are apt to adopt lexical simplification strategies such as using a synonymous or Ll-influenced expression. This paper presents a corpus-based study on the collocational deficiency of Taiwanese learners of English. The work utilizes two pre-tagged corpora, Taiwanese Learner Corpus of English and British National Corpus, to examine the learner&apos;s use of collocations over a set of synonymous words: big, large, great. The experimental findings indicate that among the three words the collocations with big are significantly overused by the learners when it is used to refer to abstract concepts. This overuse phenomenon is further investigated and it is found that the collocations of high frequency in the learner English tend to be used to express vague ideas when more specific meanings should be conveyed. It is also found that the learners are apt to apply those collocations to the cases where more concise expressions are preferred. Another finding shows that problematic collocations, pertaining to big, large and great, are produced as the result of learner&apos;s application of the Ll-transfer and synonym strategies, which the Taiwanese learners commonly adopt for lexical simplification.
JAPANESE HONORIFICATION IN AN HPSG FRAMEWORK The Verbmobil system is a machine translation system for German, English and Japanese dialogues. The szenario is a special sort of task-oriented dialogues: appointment scheduling' .Spoken language encodes references to the social relation of the dialogue partners. The utterances can express social distance between addressee and speaker and third persons, who are mentioned. Honorifics can even express respect concerning entities of the world. Consider the following examples from Japanese, German and French:( Information about honorification is -on the one hand -necessary for the description of syntactic phenomena like honorific agreement or relative sentences and -on the other hand -necessary for correct translation. In order to understand the whole meaning of the Japanese utterances it is important to represent the different honorific attributes in the analysis structure. The information can be used to resolve zero pronominalization and topicalized structures. It is even more important for the adequate generation of the Japanese utterances. In other investigations on zero pronoun resolution in task-oriented dialogues (cf. [Siege11997]) we calculated that 23.9% of the zero pronominals can be solved using lexical pragmatic restrictions about honorification. We present a solution for the representation of Japanese honorificational information in the HPSG framework. Basically, there are three dimensions of honorification. We show that a treatment is necessary that involves both the syntactic and the contextual level of information. The Japanese grammar is part of a machine translation system.
A STUDY OF METAPHORICAL MAPPING INVOLVING SOCIO-CULTURAL VALUES: HOW WOMAN IS CONCEPTUALIZED IN JAPANESE Since Lakoff and Johnson [1], the study of metaphor has made great progress in cognitive linguistics. It has been well established that there are inter-domain mappings of concepts in two different domains, and that the set of correspondences between them are called "conceptual metaphors." Research has also shown that in the most basic conceptual metaphors, the image-schematic structure of the source domain is preserved in the target domain. Up till now, the focus has been on the conceptual metaphors based on the image-schematic correlation. While such metaphors are important for the research in cognitive semantics, other kinds of conventional metaphors deserve more attention. Recent development in this field of study, however, has seen other conventional metaphors identified and classified in a more elegant way. One such example is the work of Grady [2] who has distinguished motivations for two kinds of metaphors, "correlation metaphor" and "resemblance metaphor." The present study makes a further distinction within resemblance metaphors, and proposes what we may call "socio-cultural metaphor." This is exemplified by Japanese metaphors that conceptualize women in terms of animals or plants. We present a model of socio-cultural metaphors illustrating how they are formed as a result of socio-cultural interpretations of the source and target concepts. The present study investigates a type of metaphor involving socio-cultural values in their mapping and interpretation. The linguistic data are Japanese metaphorical expressions that conceptualize women as plants or animals. First, the typology of metaphor based on previous research is discussed, focusing on conceptual, correlation, and resemblance metaphors, followed by our proposal to distinguish &quot;socio-cultural metaphors&quot; within resemblance metaphors. The main part analyzes the data to illustrate various characteristics of socio-cultural metaphors, which is divided into the following sections: 1) Women as Animals or Plants, 2) Some Mapping Gaps and Asymmetry, 3) Properties Involved in the Mapping, 4) Socio-cultural Codes, and 5) Social Structures and Interpretation of Woman Metaphors. The result of the questionnaire survey reported in 4) seems to suggest that interpretations and usage of some metaphorical expressions have undergone certain changes over the years. The study concludes by suggesting further research in socio-cultural metaphor in Japanese and other languages.
Automatic Conversion from Phonetic to Textual Representation of Cantonese: The Case of Hong Kong Court Proceedings With the implementation of legal bilingualism in Hong Kong, Cantonese Chinese is increasingly used in the legal domain, particularly in court proceedings. Previously, when court proceedings were conducted exclusively in English, verbatim records were kept by court stenographers using the Pitman method and, more recently, the shorthand machine. The shorthand codes recorded by the machine were transcribed into English words via a Computer Aided Transcription (CAT) system. However, it is incapable of processing Cantonese, the dialect spoken by the predominant majority of Chinese litigants in Hong Kong. In the absence of an efficient and reliable device, the Judiciary of Hong Kong is confronted with the urgent problem of finding a way to maintain legally tenable records of court proceedings conducted in Cantonese Chinese. [1,2,3] Currently, the Judiciary has to resort to a primitive solution, i.e., transcribing the audio records of court proceedings into Chinese characters by means of audio-typing. This process is not only time-consuming but also error-prone. To remedy the situation, two supporting facilities are required, namely, a computer-compatible Cantonese shorthand method that allows court stenographers to make verbatim records of Cantonese speech, and a Cantonese CAT system that facilitates the transcription of Cantonese shorthand codes into Chinese characters. Both English-and Mandarin-based CAT systems have been available for some years. [4,5] However, neither a computer-compatible Cantonese shorthand method nor a Cantonese CAT is currently available. This paper discusses (1) a phonetically-based Cantonese shorthand method for use on stenograph machines; and (2) the design and initial implementation of a Cantonese CAT system capable of converting the phonetically-based shorthand codes into Chinese characters.The rest of this paper is organized as follows. The next section briefly reviews the conversion from phonetic to textual representation in CAT. Section 3 gives a detailed account of the design of our Cantonese shorthand method and Cantonese CAT. Section 4 discusses the statistical techniques employed. Section 5 reports the evaluation results of the system. Section 6 discusses further linguistic analysis and enhancement features. Finally, Section 7 provides a summary and considers ways in which the system can be refined and expanded. The resumption of sovereignty over Hong Kong by China and the implementation of legal bilingualism there have given rise to an urgent need for producing verbatim court records of proceedings conducted in Cantonese, the predominant Chinese dialect spoken by the majority of the population. This has created a challenge to build up the jurilinguistic infrastructure vital for the full implementation of bilingualism and the retention of the Common Law system in Hong Kong. While there are Computer-Aided Transcription (CAT) systems for processing English and Mandarin (Putonghua), none exists for processing Cantonese. This paper discusses the design of a Cantonese CAT system based on the special features of Cantonese speech sounds. The CAT system works on the conventional English-based keyboard to process Cantonese and meets the bilingual requirements of the Hong Kong courts. By utilizing primarily statistical techniques, the system is highly successful in handling the ambiguity resolution of homophonous Chinese characters, a tantalizing problem in the conversion from phonetic to textual representation of Chinese. Additional linguistic analysis and related processing are discussed which could further improve the performance of the system from about 92% to over 94% accuracy.
Domain Unconstrained Language Understanding Based on How-net  In this paper, we propose a method for domain unconstrained language understanding based on the How-net knowledge base. The goal is to construct a system that reads in an article and answers some related questions. For each sentence in the article, word segmentation is first applied. Then, the major components such as agent, theme, event, time, and place are extracted to construct a semantic-slot table and a semantic network. Answers of the questions are derived using two approaches, which are based on the relational and hierarchical relation among the major components. Our method is applied to the understanding of the primary-school textbook, and it is able to answer questions in the exercise of the textbook.
OBJECT TOPICALIZATION, PASSIVE, AND INFORMATION STRUCTURE IN JAPANESE Functionally, the passive is generally understood to be a syntactic construction that topicalizes the patient, de-topicalizes the agent and stativizes the event (Givon [1] cited by Forrest [2, p.149]). If one takes the view that constructions can be related by a function, as Givon [3] and Shibatani [4] do, object topicalization in a topic-prominent language like Japanese should be related to the passive because it can perform the first two functions listed above. Indeed, object topicalization appears occasionally in place of a passive in English-toJapanese translation. However, an examination of the properties of object topicalization reveals that the resemblance is only partial. This paper presents a study of object topicalization in relation to the passive in Japanese. First, I argue that object topicalization is semantically more restricted and information-structurally marked than the passive. Then, I propose that the difference in information structure presents different effects on the continuing discourse. The rest of the paper discusses the two constructions in written register only, because it is recognized (e.g. by Biber et al. [5] and Heo [6]) that generalization across registers/genres is difficult. Object topicalization shares two functional properties with the passive: foregrounding of the patient and de-topicalization of the agent. This fact makes one wonder why the former occurs far less frequently in written text. The present paper is an attempt to illuminate some properties of object topicalization not shared by the passive. It is argued that while object topicalization may de-topicalize the agent, informational focus is brought upon it. It is then proposed that the information structure imposed by object topicalization has consequences different from the passive on the continuing discourse.
 Articulatory synthesis is the production of speech sounds using a model of the vocal tract, which directly or indirectly simulates the movements of the speech articulators. It provides a means for gaining an understanding of speech production and for studying phonetics. In such a model coarticulation effects arise naturally, and in principle it should be possible to deal correctly with glottal source properties, interaction between the vocal tract and the vocal folds, the contribution of the subglottal system, and the effects of the nasal tract and sinus cavities.Articulatory synthesis usually consists of two separate components. In the articulatory model, the vocal tract is divided into many small sections and the corresponding cross-sectional areas are used as parameters to represent the vocal tract characteristics. In the acoustic model, each cross-sectional area is approximated by an electrical analog transmission line. To simulate the movement of the vocal tract, the area functions must change with time. Each sound is designated in terms of a target configuration and the movement of the vocal tract is specified by a separate fast or slow motion of the articulators.A properly constructed articulatory synthesizer is capable of reproducing all the naturally relevant effects for the generation of fricatives and plosives, modeling coarticulation transitions as well as source-tract interaction in a manner that resembles the physical process that occurs in real speech production. Articulatory synthesizers will continue to be of great importance for research purposes, and to provide insights into various acoustic features of human speech. The aim of this research was to develop a flexible, high quality articulatory speech synthesis tool. One feature of this research tool is the simulated annealing optimization procedure that is used to optimize the vocal tract parameters to match a specified set of formant characteristics. Another aspect of this study is the derivation of a new form of the acoustic equations. A transmission-line circuit model of the vocal system, which includes the vocal tract, the nasal tract with sinus cavities, the glottal impedance, the subglottal tract, the excitation source, and the turbulence noise source, was constructed. The acoustic equations of the vocal system were rederived for the proposed articulatory synthesizer. A digital time-domain approach was used to simulate the dynamic properties of the vocal system as well as to improve the quality of the synthesized speech.
SYMMETRIC PROJECTION IN JAPANESE AND ENGLISH: A MODIFICATION OF STABLER&apos;S PARSING/GENERATION MODEL FOR MINIMALIST GRAMMAR  ABSTRCAT The essence of standard X-bar theory is that structure building is asymmetric in the sense that a complex structure inherits properties from only one of its constituents. There are some structures, however, that are best analyzed as reflecting the properties of all their constituents. This kind of symmetric projection should in principle be allowed within the minimalist program if the union of the features of all the constituents contains no incompatible features. This claim is supported by the fact that Japanese wh-phrases marked with ka can function as indefinites as well as interrogatives. Under the assumptions that a wh-phrase with ka has the same internal structure regardless of its interpretations and that ka has no category feature, merging a wh-phrase with ka is a case of symmetric projection. The properties of both ka and its sister wh-phrase interact with those of the predicate taking the ka-phrase as its argument or adjunct, which ensures that an appropriate interpretation will be picked up from the two possible interpretations of the ka-phrase. 1. INDEFINITES AND PHRASAL INTERROGATIVES IN JAPANESE It is well-known that wh-phrases in Japanese, unlike those in English, can be used as indefinite as well as interrogative expressions. For example, each sentence below contains exactly the same strings of words; the first one is interpreted as an indefinite and the second, as an interrogative: (1) John-ga [ dare ka nagutta rasii ga [ dare ka ] sitteiru hito wa i nai. NOM someone beat it-seems but who Q know person TOP exist not &apos;John seems to have beaten someone but nobody knows who.&apos; (2) John-ga [ dare kara ka ] henna tegami-o moratta rasii ga, NOM someone from strange letter ACC received it-seems but boku-wa [ dare kara ka sir-anai. I TOP who from Q know not &apos;John seems to have received a strange letter from someone but I don&apos;t know from whom.&apos; (3) John-wa [ naze ka naiteita ga [ naze ka ]-wa daremo siranai. TOP some reason was-crying but why Q TOP nobody know &apos;John was crying for some reason but nobody knows why.&apos; The same point can be illustrated with (4), which can be interpreted as (5a) or (5b) depending on its context: (4) John-wa [ naze ka rikai dekinakatta TOP why understand couldn&apos;t
Temporal and conditional clauses in Chinese spoken discourse: A function-based study  It is generally agreed that different categories of adverbial clauses-temporal,
ALTERNATIVE QUESTIONS IN THE SYNTAX-SEMANTICS INTERFACE Questions can be classified into three types according to the kind of reply they elicit, polar questions (or yes-no questions), wh-questions, and alternative questions. Compared to other types of questions, alternative questions have not received as much attention. It is perhaps because the relationship between alternative questions and the other two types of questions is often not very clear; furthermore, an analysis of alternative questions usually requires dealing with various syntactic and semantic issues, such as disjunction, coordination, syntax and semantics of interrogatives, and treatment of whether and if.In this paper, we deal with English alternative questions such as (1) within the framework of Head-Driven Phrase Structure Grammar.(1) Did Mary buy books or video tapes?There are two issues to focus on regarding the analysis of (1). First, it should be explained how the alternative question interpretation of (1) is obtained. Some previous semantic analyses assume that the interpretation of (1) involves a semantic version of conjunction reduction, with its denotation corresponding to a set of propositions (Karttunen [5], Groenendijk &amp; Stokhof [4], Roberts [10]). On the other hand, there can be other semantic mechanisms that yield wide scope of disjunction in ( I ). In this paper, I will pursue an approach that posits a disjunction operator associated with or. Second, the syntactic representation of (1) should be determined in conjunction with its semantic interpretation. After examining two possible syntactic analyses, one involving movement of disjunction scope indicator, and the other employing syntactic reduction process, I will claim that (1) can be analyzed without assuming either movement or a reduction process. In the analysis to be proposed, (1) simply involves coordination of noun phrases. With the base-generated disjunction in syntax, I will present an analysis where alternative question interpretation is obtained by scoping of a disjunction operator originating from or.In section 2, I will discuss some basic properties of alternative questions. In particular, it is pointed out that an analysis of alternative questions should explain the fact that questions like (1) may receive a polar question interpretation as well as an alternative question interpretation. Section 3 deals with the semantic and syntactic behaviors of disjoined phrases in alternative questions. Based on Rooth &amp; Partee [11], I will show that the alternative question interpretation is related to the wide scope reading of or. Moreover, as mentioned above, I will argue that alternative questions like (1) can be simply analyzed as involving coordinated NPs. In section 4, I propose an analysis of alternative questions within HPSG. I will show that in the type hierarchy, alternative questions can be represented as a clause type, whose semantic representation is distinguished from that of other clause types by an alternative operator. The ambiguity of alternative questions noted above is accounted for by the optional nature of the disjunction operator. When the operator does not arise in or, the question at hand will receive a polar question interpretation. Section 5 concludes the paper. This paper deals with alternative questions in English, and proposes an analysis employing an alternative operator. I claim that the alternative question interpretation is not obtained by such syntactic processes as movement or gapping, but by scoping of an alternative operator that originates from the conjunction or. Accordingly, while the syntactic forms of alternative questions contain nothing more than a coordinate structure, their semantic component will be analyzed as including an alternative operator. Furthermore, using a &apos;multiple inheritance&apos; type hierarchy of clauses, I will show that how the relationship between alternative questions and other types of questions can be represented.
A UNIFIED APPROACH TO TENSE IN JAPANESE Descriptive studies on tenses in Japanese complex sentences have revealed that tenses in the subordinate clause need a different interpretation from those in the matrix sentence (see Suzuki 1976, Teramura 1984, and KudO 1995. This paper proposes an HPSG/DRT version of the traditionally known hypothesis (`relative tense theory') that the criterion for interpreting a syntactically lower tense is established by the immediately higher tense. On the basis of this, we will argue that each of the two tense-bearing forms is given a single temporal meaning, irrespective of its position in the syntactic structure. Tenses in subordinate clauses which this specification seemingly cannot capture are explained away by difference in manners they are adjoined to the matrix sentence. Relative clauses, which call for idiosyncratic tense interpretations, are accounted for semantically by means of ambiguous scopings. In this paper we propose a method to compositionally interpret tenses of Japanese complex sentences on the basis of Head-Driven Phrase Structure Grammar and Discourse Representation Theory. In this approach, each of the tense-bearing forms such as main verbs and the past auxiliary is given a single temporal meaning independent of its position in the syntactic structure. ThèThèrelative tense theory&apos; according to which a tense in a subordinate clause is interpreted in relation to a syntactically higher tense lays a foundation for this formalization.
Exclusion phrases and criticisms of semantic compositionality Any reasonable version of the principle of semantic compositionality uses in its formulation three conceptually non-trivial and theoretically loaded notions: function, meaning and (syntactic) part. (cf. Janssen 1996 and Pelletier 1994a for the review of various problems related to the principle of compositionality). This means that the principle can be relativized to any of these notions, none of which can be arbitrary, otherwise the principle would be formally void. In particular, groupings of constituants, simple or complex, into more complex constituants not only cannot be arbitrary but can also be tested relative to their compatibility with the principle of compositionality.Obviously the notion of function should also be properly understood when testing the validity of the principle. Various criticisms of the principle ignore the fact that values of functions can be given by a finite enumeration of cases to which various conditions on values of arguments can give rise. In this way it can be easily seen that for instance complex idioms which are often given as supposed counter-examples to the compositionality principle in fact are not genuine counter-examples. Indeed, since the number of idioms with a given syntactic structure is finite (and in addition the structure is usually "frozen") in order to get the computing function it is sufficient to enumerate separately all these finite "idiomatic" cases as special cases associating semantic values with the meanings the corresponding idioms have.The purpose of this paper is to discuss some more recent and more sophistiocated criticisms of the principle of compositionality. I will consider a specific version of it in the context of exclusion phrases (EXCL phrases), i.e. phrases of the type No/every student except Leo/Albanian(s). The version of the principle I am interested is as follows: let E be a complex expression and SA its syntactic analysis. According to SA, for 0 &lt; i &lt; n, each Ei is an immediate part of E and for 0 &lt; k &lt; 7n, each Ei k is an immediate part of Ei . We will say that analysis SA is compatible with the principle of semantic compositionality if the meaning of E is a function of the meanings of Ei and the meaning of every Ei is a function of the meanings of Eli,. I will show in particular that, contrary to what one could claim, there are two natural syntactic analyses of EXCL phrases which are both compatible with the principle of compositionality in the above version. Furthermore, I will relate the discussion of this case to some arguments against compositionality which were given in the context of complex sentences with unless.There are many reasons to do this. First, EXCL phrases are formed syntactically from very specific syntactic elements, namely nominal determiners, and consequently their denotations are also specific since they are higher order objects. Since in general the principle of compositionality has been discussed in connection with major categories such as sentences, noun phrases or verb phrases, the discussion of the principle in relation to "minor" categories may be enlightening. This is even more obvious if it appears that some results obtained in connection with one category are easily generalisable to other categories: I show that the connective except occurring in EXCL phrases is in fact categorially polyvalent. Consequently any discussion of the validity of the principle of compositionality at sentential level appears directly relevant for other levels. At the background of this paper are two discussions of the semantics of natural languages, one in Higginbotham 1986 evoking semantic compositionality and the other, a reply to it, in Pelletier (1994b). In order to show that natural languages cannot be compositional in general, Higginbotham considers sentences with the connective unless like those in (1):(la) John will eat steak unless he eats lobster (lb) Every person will eat steak unless he eats lobster (lc) No person will eat steak unless he eats lobster Higginbotham notices that unless in (la) and (1b) corresponds to the (exclusive) disjunction whereas in (lc) it corresponds to the connective "and not". Thus, unless "means" different things in different contexts. From this observation Higginbotham draws the conclusion that a semantic principle which he calls the Principle of Indifference and which is related to the principle of compositionality, is false, and consequently that the facts like those in (1) show that the principle of compositionality is false for natural languages.Pelletier (1994b) discusses Higginbotham's argument and proposes two solutions to the problem it raises. According to the first solution, unless is "vague", and its meaning is neither the disjunction nor the connective "and not" but rather some connective or other from a given set of possible connectives. The second solution makes unless "ambiguous" in the sense that this connective could be replaced by two different words corresponding to different "meanings" one finds in (la) and (1b) on the one hand and in (1c) on the other hand. Since the notion of ambiguity seems to play an important role in this argument, I will first make some related comments.It is well-known that Boolean connectives in specific contexts tend to have different meanings than the one they have in isolation. There may be various reasons for this. One of them is the scopal influence of other operators present in the context. Consider for instance (2a) which is naturally interpreted by (2b) and not by (2c): (2a) No student or teacher (2b) No student and no teacher (2c) No person who is a student or a teacherNow the fact that the connective or in (2a) is interpreted by and (in conjunction with no) in no way indicates that or is ambigous or vague or that expressions containing it do not have a compositional semantics. This is just a manifestation of the well-known fact that many Boolean connectives are logically dependent and some of them can be used to define others. As for the logical status of (2a) Keenan and Moss (1985) provide a simple semantics for expressions of this type. Another case, which leads to a similar "ambiguity" of logical connectives is a phenomenon which may be called local equivalence, i.e. the fact that two globally different connectives can take the same value when the value of their arguments is restricted to a particular domain or when their arguments are logically related. For instance, if p is equivalent to q then p or q is equivalent to p and q. Similar examples can be given for many other pairs, and the "local equivalences" to which they give rise look less trivial when one considers functions taking their arguments in more complex Boolean algebras. Take, for instance, the binary function * corresponding to so-called symmetric difference) : A *B = (A -B) A (B -A). One can easily show that if A &lt; B then A*B = B-A ("B and not-A") and when A nB = 0 then A * B = AV B ("A or B"). So in some contexts the symmetric difference corresponds to the exclusive disjunction and in others to and not. Whatever the complexity of arguments of Boolean functions, however, the existence of such local equivalences in no way indicates that Boolean functions are vague or ambiguous, and even less that they are evidence for non-compositionality.As for the connective unless various difficulties concerning its analysis are well known. It is important to realize, however, that this variety of proposed analyses, even if many of the proposed solutions are truth-functionally equivalent, does not address the problem of compositionality but rather the question of whether there is a unique (binary) truth-functional connective corresponding to unless. 
The Semantics of amwu-N-to/-irato/-ina in Korean -Arbitrary Choice and Concession  This paper reports the syntactic distribution of amwu-N-to/-irato/-ina phrases, which are representative polarity sensitive items (PSIs) in Korean, and accounts for their semantic characteristics in terms of &quot;arbitrary choice quantification&quot; and &quot;concession.&quot; In the first section, we extensively illustrate the distributional behaviour of the PSIs in various constructions and roughly generalizes the distribution in terms of (anti/non-) verdicality.&quot; Section 2 interprets amwu as an arbitrary choice quantifier, and the particles-to/-irato/-ina as &quot;concessive&quot; markers, so the compounds denote a special element in a pragmatic scale determined by context/situation. Section 3, based on the pragmatics of scalar implicature, accounts for the apparent ambiguity of PSIs between &quot;universal&quot; and &quot;existential&quot; readings, and further characterizes the difference among the concessive markers-to/-irato/-ina in terms of &quot;quantity/quality scale.&quot;
External Argument and English Psychological Verbs Peculiar properties of the Experiencer-object (EO) verbs have been observed by several linguists ( Belletti andRizzi 1988, Grimshaw 1990, Pesetsky 1995, among others). The following sentence illustrates some of the properties:(1) Pictures of each other depress the politicians.In (1) Theme appears as subject Pictures of each other and Experiencer appears as object the politicians. The thematic role, Theme, in the subject position, is thematically lower than that, Experiencer, in the object position: Thus, the syntactic structure in (1) does not reflect the thematic structure directly. Furthermore, the backward reflexive each other is not bound by its antecedent the politicians, violating Binding Theory, but the sentence is grammatical.To account for the peculiar properties of the EO verb construction, Belletti and Rizzi 1988 presents the D-structure in (3) for the EO verb construction in (2):(2) This worries John.One version of the Thematic Hierarchy is given below: Agent &gt; Experiencer &gt; Goal/Source/Location &gt; Theme ( Grimshaw 1990:24) -425- In the structure in (3), Experiencer John is higher than Theme this, and this enables us to account for the backward reflexive since the Experiencer position c-commands the Theme position.For the structure in (3), Belletti and Rizzi 1988 have to assume that the verb is unaccusative; it does not have external argument. This assumption is critical for them because the movement of Theme argument to subject position is motivated by the absence of external argument; the verb without external argument cannot assign Case to its complement NP, following Burzio's Generalization.Grimshaw 1990 also claims that the EO verbs do not have external argument. According to her, external argument is the most prominent argument in both tiers, thematic and aspectual. But the EO verbs do not have any argument which is most prominent in the two tiers.On the other hand, Chung 1998 presents evidence that the EO verbs do have external argument. Chung shows that many of the EO verbs take the -er nominal suffix which only attaches to the verb with external argument. The verbs with external argument in (4a) can take -er whereas the unaccusative verbs without external argument in (4b) cannot. The data in (5) show that many EO verbs take the -er suffix: (4) a. teacher(teach), actor(act), maker(make), baker(bake), driver(drive), striker(strike) b. *appearer(appear), *collapser(collapse), *dier(die), *disappearer(disappear) (Hovav and Levin 1992) (5) amuser, annoyer, appeaser, astonisher, attractor, comforter, delighter, disappointer, discourager, disenchanter, disgracer, distracter, distruster, disturber, enchanter, encourager, entertainer, exciter, frightener, hurter, impresser, insulter, irritator, offender, pleaser, provoker, puzzler, reliever, satisfier, scarerThe data in (5) clearly show that the EO verbs have external argument contra Belletti and Rizzi's 1988 and Grimsahw's 1990 claim.NP On the other hand, Pesetsky 1995, Bouchard 1995, Iwata 1995 suggest that the EO verbs do have external argument. Pesetsky's claim is based on the zero morpheme, but his argument cannot account for the -er nominals in (5) (See Chung 1998 for the detailed discussion).2 Bouchard 1995 just assumes that the E0 verbs have external argument without presenting evidence for the external argument of the EO verbs. Iwata 1995 presents some pieces of evidence (i.e., middle formation, -er nominals, -able adjectives) that the EO verbs are simple transitive verbs: That is, the verbs have external and direct internal argument. But Iwata's data are very limited and Iwata does not account for the problem of Pesetsky 1995.The present study is concerned about the further evidence that the EO verbs have external argument. The evidence is passives derived from the EO verbs. Most of the EO verbs may form either verbal or adjectival passives and both types of passives are evidence for the external argument of the EO verbs.In Section 2 I will examine the verbal passives from the EO verbs and Section 3 deals with the adjectival passives from the EO verbs, in Section 4 I will examine adjectival passives from unaccusative verbs, and Section 5 is a concluding part. 
Language, Information and Computation Edited by Language, Information and Computation Proceedings of The 15th Pacific Asia Conference  
Building a Large Lexical Databank Which Provides Deep Semantics* It is generally assumed that the goals of computational linguistics and those of ordinary linguistic analysis are unlikely to coincide in the area of lexicon building. The full range of the knowledge speakers have about the words in their language appears to be out of reach of practical efforts to acquire and organize such knowledge for efficient use in NLP applications.FrameNet, as a computational linguistics project, aims at a compromise between two extremes. At the one end it is clearly possible to do careful and subtle conceptual analysis of the items in a small lexicon designed for restricted purposes in a narrow domain, where the goal of achieving reliable language understanding and accurate inference generation seems attainable. At the other end, one can aim at fairly superficial information retrieval or other NLP applications by using statistical methods for acquiring very large lexicons or or by creating clever programs for making maximal use of the information available in machine-tractable dictionaries. The latter contain large amounts of data but are limited by what their compilers believed would be relevant to human users ( Wilks et al., 1996).The FrameNet team are convinced that at a certain level, the depth and subtlety of careful linguistic analysis can be attained using the wisely exploited judgments of linguistically trained researchers who annotate syntactically sorted sentences 1 taken from a large natural language corpus. Some of the information made available by such research will be recorded directly by the annotators, and some will be derivable-partly manually, partly automatically-from the results of such annotations. The labor-intensive nature of such research will be eased in several ways: by computational facilitation of the annotation process itself; by computing maximal likelihood estimates for certain levels of analysis, so that researchers can simply accept or reject these judgments; and by allowing the annotation to proceed automatically in some cases, at least for patterns that appear to be clear. All such automatically created data will be vetted by human reviewers. This paper reports on the design of a lexical database for English which is currently under construction (&quot;FrameNet-2&quot;), and describes the kinds of linguistic facts that the database is intended to make available, for both human and computer consumers. Building on a recently completed pilot study (&quot;FrameNet-1&quot;), it is centered on the nature of the relation between lexical meanings and the conceptual structures which underlie them (semantic frames). The database will show the semantic and syntactic combinatorial possibilities (based on frame membership) of the lexical items it includes, as these are documented through grammatical and semantic annotations of sentences extracted from a large corpus of contemporary written English. The notions of profiling within a frame, frame inheritance including multiple inheritance, frame blending, and frame composition will be explained and illustrated, and the manner of storing information about them in the database will be outlined. The building of the database, with its necessary labor-intensive manual component, will be explained.
A Comparative Study of English and Chinese Synonym Pairs: An Approach based on The Module-Attribute Representation of Verbal Semantics The Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of the representation of verbal semantics that is developed based on Mandarin Chinese data ( Huang et al., 2000). This theory proposes two types of modules: event structure modules and role modules, as well as two sets of attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences.Studies in MARVS (e.g. Biq 2000) found that the composition of an event modules and its attested lexical semantic attribute(s) could be generalized to a natural semantic class. For example, the contrast between bai3 ( `set') and fang4 ( `pue) in Mandarin Chinese to do with the fact that bai3 has a roleinternal feature of [design] attached to the location role, while fang4 does not. Moreover, this contrast can be generalized across the semantic class of verbs that involve design on the focussed location role (i.e. hua4 'to paint') and those that do not (i.e. tu2 'to cover with paint, to doodle'). What we would like to determine in this paper is if similar contrasts be found in near synonyms of other languages. In particular, are event modules in other languages (such as English) organized along similar conceptual lines?In this paper, we examine this theory in light of the English data. In particular, we will look at the near synonym contrast of the verbs 'put' and 'set' based on data from the two million word sampler of the British National Corpus (BNC). To preview our results, we find that the event structure in English is slightly different from that of Chinese. In Chinese, bai3 has a role-internal feature of [design] attached to the location role, while fang4 does not. However, in English, 'put' has the roles of Agent, Theme and Location in its event structure, while 'set' has the three roles of Agent, Theme, and Proposition. Thus, conceptualizations of 'set' and 'put' in English and Mandarin have different semantic and syntactic entailments. The Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of the representation of verbal semantics that is developed based on Mandarin Chinese data (Huang et al., 2000). This theory proposes two types of modules: event structure modules and role modules, as well as two sets of attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences. Studies in MARVS (e.g. Biq 2000) found that the composition of an event modules and its attested lexical semantic attribute(s) could be generalized to a natural semantic class. For example, the contrast between bai3 (`set&apos;) and fang4 (`pue) in Mandarin Chinese to do with the fact that bai3 has a role-internal feature of [design] attached to the location role, while fang4 does not. Moreover, this contrast can be generalized across the semantic class of verbs that involve design on the focussed location role (i.e. hua4 &apos;to paint&apos;) and those that do not (i.e. tu2 &apos;to cover with paint, to doodle&apos;). What we would like to determine in this paper is if similar contrasts be found in near synonyms of other languages. In particular, are event modules in other languages (such as English) organized along similar conceptual lines? In this paper, we examine this theory in light of the English data. In particular, we will look at the near synonym contrast of the verbs &apos;put&apos; and &apos;set&apos; based on data from the two million word sampler of the British National Corpus (BNC). To preview our results, we find that the event structure in English is slightly different from that of Chinese. In Chinese, bai3 has a role-internal feature of [design] attached to the location role, while fang4 does not. However, in English, &apos;put&apos; has the roles of Agent, Theme and Location in its event structure, while &apos;set&apos; has the three roles of Agent, Theme, and Proposition. Thus, conceptualizations of &apos;set&apos; and &apos;put&apos; in English and Mandarin have different semantic and syntactic entailments. 2 Methodology Our methodology in analyzing near synonym pairs contrasts with that of Levin (1993), who concentrates on analyzing the range of possible syntactic alternations of a single verb (or a single verb class), and extracting semantic information from syntactic behavior. However, there are two reasons 27
Inferring Semantics from Collocation Clusters to Represent Verbs and Nouns  Current lexical semantic theories provide representations at a coarse grained level. In this paper, I will provide motivations for a fine grained representation for verbs and. nouns. An initial case study is done to serve as evidence that a more detailed representation is needed for tasks that require high accuracy rates, such as machine translation. An automatic approach to gather fine grained information via corpus extraction is described. Lastly, issues of lexical representation and cross-lingual translation are discussed.
A Parallel Interpretation of Floated Quantifiers and Adverbials As a relatively free word order language, Japanese often exhibits quantifier floating phenomena. The most common analyses of these are syntactic ones; for example, that by Miyagawa (1989) has been widely accepted within GB theory. In this paper, however, we point out that the transformational analysis is on the wrong track, and propose a way to interpret the meanings of floated quantifiers and to account for their syntactic behaviour. Our assumption is that quantifier floating should be accounted for not just by syntactic operations but rather by the interaction of syntactic and semantic conditions. Our theory also intends to interpret 'pragmatic implication' and semantic differences caused by quantifier floating. In this paper we propose a method to compositionally interpret the meanings of floated quantifiers on the basis of Categorial Grammar (Carpenter 1997) and Davidsonian semantics (Davidson 1980). In this approach, floated quantifiers are accounted for semantically as a kind of adverbial phrase so that we do not need any special syntactic operations, such as quantifier raising, in order to interpret their meanings. The approach also explains &apos;pragmatic implication&apos; introduced by floated quantifiers on the basis of the whole-part relationship between sets.
What else to quantify? Several researchers have recurrently taken up a construction so called an "exceptive construction" at least within the last decade. Among those in the literature are Moltmann (1995Moltmann ( , 1996 or von Fintel (1993). Representative examples are the following:(1) a. Every student except John b. No student but John and Bill c. Except for John and Bill, Mary knows every student.( Moltmann: 1996: 139) A very naïve characterization of this construction may be that the referent of the noun phrase governed by the preposition, except (for), but or other than, is subtracted from the domain of quantification denoted by the quantified noun phrase to which the prepositional phrase attaches. And in this paper I take up an English word, else, which may count as appearing in an exceptive construction. But I observe else has some peculiar properties compared to other exceptive prepositional phrases. I argue for the above naïve view of "exception" is at least correct in the case of else; rather than a more elaborate view on the exceptive construction. Then we see that another property of else necessitates a more enriched domain of discourse, which contains pairs of individuals in addition to ordinary individuals. This paper concentrates on peculiar behaviors of an English word, else, among various lexical items which have to do with the exceptive constructions. Reviewing its intriguing properties, it is argued that a simple way of reducing an &quot;exception&quot; from the domain of quantification and a more enriched domain of quantification, containing at least pairs of individuals, are preferable.
Asymmetry, Zero Morphology and Tractability We assume a principled-based theory of morphology, (Di Sciullo, 1995), where morphological expressions include Specifier-Head-Complement structures in the course of derivation, and where the Spec, the Head, as well as the Compl can be overt or covert. We show that optimal recovery of morpho-syntactic structure requires the separation of overt and non-overt processing. We also assume that asymmetry inverting operations affect Spec-Head-Compl relations in the derivation of syntactic expressions. Evidence from computational tractability suggests that this is also the case in the derivation of morphological expressions, though in a specific way.The organization of this paper is as follows. The first section presents basic properties of Strict Asymmetry Theory, (Di Sciullo, 1999), and show that argument structure restrictions on morphological composition follow in a principled way. The second section presents different PAPPI, (Fong, 1991), implementations of the theory, and provides evidence to the effect that the positions of the Specifier and the Complement have an effect on tractability, as well as the separation of overt and covert processing. The last section considers consequences for the properties of the interfaces between grammar and the performance systems. We present a theory of grammar based on asymmetrical relations, the Strict Asymmetry Theory, and we provide evidence form Indo-European and non Indo-European languages to show that argument structure restrictions on morphological composition follow in a principled way from the theory. We describe and implement a bottom-up parser for morphological selection in the Strict Asymmetry framework. Core lexical properties including argument structure and derivational affix selection are encoded in a uniform mechanism. We consider the computational implications of three different implementations. In particular, we examine the effect on bottom-up parsing from varying the Specifier-Head-Complement order. We provide computational motivation for the logical separation of overt and covert affixation.
Emphatic Particles and their Scopal Interactions in Japanese  Japanese emphatic particles such as mo, wa, and sae are known to present exceedingly recalcitrant problems for grammarians. Since they are clearly concerned with connecting discourse presuppositions with the assertive content of the current utterance , their nature has to be pragmatic as well as semantic. Their syntactic, or rather morphological, behaviour also seems highly unmanageable since they interact not only with themselves but also with other types of particles, especially case particles. In this paper, we try to present a basic scheme for treating emphatic particles based on four features: type, self, edge and polarity. We also try to place emphatic particles in their proper place within overall grammar of the Japanese language.
Japanese Negative Polarity Items and Negative Concord Negative polarity items (NPIs) are a class of expressions whose distribution is restricted to affective contexts, especially to the negative context (Kato (1985)). Examples of the expressions traditionally counted as Japanese negative polarity items are: kessite ('ever'), nannimo ('anything'), daremo (`anyone'), dokomo (`anywhere'), tittomo (`not at all'), and sika ('only') ( Ikeya and Kawamori (1998)These items have conventionally been treated as belonging to one category, namely that of NPI, and explained in terms of same conceptual apparatuses. We will show, however, that these items do not constitute a monolithic, single category, but rather are to be grouped into two distinct categories -one comprising sika, the other composed of other NPIs.We specifically show that Japanese NPIs other than sika is like negative phrases in negative concord sentences in such languages as non-standard English and Romance languages. We also show that sika is unlike other NPI items in several important respects. A feature-based account for syntax-semantics interface sika is presented that attempts to account for these points. 
Towards a Conceptual Representation of Lexical Meaning in WordNet Knowledge representation has traditionally been thought of as the heart of various applications of natural language processing. Anyone who has built a natural language processing system has had to tackle the problem of representing its knowledge of the world. One of the applications for knowledge representation is word sense disambiguation (WSD) for unrestricted text, which is one of the major problems in analyzing sentences. In the past, the substantial literature on WSD has concentrated on statistical approaches to analyzing and extracting information from corpora ( Gale et al. 1992;Yarowsky 1992Yarowsky , 1995Dagan and Itai 1994;Luk 1995;Ng and Lee 1996). However, WSD knowledge acquired from specialized corpora such as various encyclopaedia, book collections or newspaper archive may be biased or incomplete. Words in knowledge are simply partitioned, depending on their use through out the corpus. Those sense partitions do not correspond with those provided in dictionaries. For instance, this type of knowledge would fail to acquire the fish-sense of bass from the Wall Street Journal, in the situation where such corpora was employed as a learning resource. Although statistical knowledge acquired from a very large corpus has shown effectiveness in disambiguating text in the same domain, no corpus provides sufficient information to disambiguate unrestricted texts.Dictionaries provide a ready source of knowledge about senses such as morphology, syntax, definition, example sentences, and collocation. Many references have shown that information in machine-readable dictionary (MRD) is an unbiased knowledge source for WSD ( Guthrie et al. 1991;Slator 1991; Li et a1.1995;Chen and Chang 1998b). When a dictionary is directly used as semantic knowledge for any applications of natural language processing, it will lead to immense parameter space. In addition, it is also difficult to master the related semantics for each word sense in a dictionary.A thesaurus provides conceptual classifications for word senses. It seems to reduce the semantic parameter space, but its lexical and semantic gaps cause another problem, one of incomplete knowledge. For instance, there are ten distinct nominal senses for the word bank in WordNet, but only six categories, such as Obliquity(217), Land(342), Store(636), Bare(663), Defence(717) and Treasure(802), are listed in the Roget's 1911 thesaurus. The Roget's Thesaurus arranges words in a 3-layer hierarchy and organizes over 30,000 distinct words into some 1,000 categories on the bottom layer. These categories are divided into 39 middle-layer sections that are further organized as 6 toplayer classes. Each category is given a 3-digit reference code. However, there is no appropriate Roget's category for the PILE-sense of word bank in WordNet. Chen and Chang (1998a) exploited a two-stage approach to fill the sense gap of LLOCE and generate conceptual topics in LLOCE from each sense definition of nominal words in LDOCE. Also, Chen and Chang (1998b) applied these conceptual semantics to disambiguate word senses in the Brown corpus and the Wall Street Journal. They reported conceptual representation acquired from MRD for a word sense did improve the precision of sense tagging on ambiguous words when compared with the corpus-based approach. However, the number of possible senses allowed by thesaurus senses seems very small (only 129 topics). Also, the coverage of their experiment is less impressive since there are over 23,000 dictionary senses for over 16,000 words.WordNet (Miller 1995;Fellbaum 1998) is a popular on-line lexical reference system for English, organized as a semantic net. Its design is inspired by current psycholinguistic theories of human lexical memory. WordNet (Version 1.6) contains some 118,000 words that are divided into four categories including English nouns, verbs, adjectives and adverbs. Word meanings for each of these categories are organized into sets of synonyms (synset), each representing one underlying lexical concept, and are logically grouped such that words in the same synonym set are interchangeable in some contexts. WordNet contains both individual words and collocations (such as "fountain pen" and "take in"). Different semantic relations, such as hypernymy, hyponymy, meronymy, holonymy, antonymy etc., link the synonym sets. Although it has good coverage (Farreres et a1.1998, Kwong 1998) and its synset is much like a thesaurus, the synset fails to provide an explicit classification.The objective of this paper is to present an automated mapping of dictionary-defined word senses in WordNet into the coarse-grained thesaurus classes in Roget's. The characteristic of this representation is to reduce the lexical dimension and enrich its conceptual information for a lexical word sense. The technique uses an information retrieval approach for extracting conceptual information from available semantic relations for each sense definition in WordNet, such as synsets, hypernym, hyponym etc. To this end, category information in the Roget's is exploited to represent conceptual semantics of word sense in order to characterize the typical context of the sense in question. We are interested in the semantics of four distinctive puts-of-speech, i.e. noun, verb, adjective, and adverb. Applications of this knowledge feature include word sense disambiguation and its related tasks such as information retrieval, machine translation, document classification, and text summarization.2 Linking WordNet to Roget's In this section, we apply an information retrieval technique to link MRD senses to thesaurus categories. The current implementation of this approach uses the category information in Roget's to represent conceptual knowledge for WordNet senses. In the following subsections we describe how that is done. Knowledge acquisition is an essential and intractable task for almost every natural language processing study. To date, corpus approach is a primary means for acquiring lexical-level semantic knowledge, but it has the problem of knowledge insufficiency when training on various kinds of corpora. This paper is concerned with the issue of how to acquire and represent conceptual knowledge explicitly from lexical definitions and their semantic network within a machine-readable dictionary. Some information retrieval techniques are applied to link between lexical senses in WordNet and conceptual ones in Roget&apos;s categories. Our experimental results report an overall accuracy of 85.25% (87, 78, 89, and 87% for nouns, verbs, adjectives and adverbs, respectively) when evaluating on polysemous words discussed in previous literature.
Forming an Integrated Lexical Resource for Word Sense Disambiguation As a repository of lexical information, lexical resources are indispensable for almost every natural language processing (NLP) task. Some NLP tasks, notably word sense disambiguation (WSD), especially require a lot of different types of lexical information to perform well. There are basically three ways of forming comprehensive resources. One is to start from scratch and encode as much information as desired into a lexical database manually (e.g. Wilks, 1975;Small &amp; Rieger, 1982;Miller et al., 1990). However, this is heavy work, and the time and effort often limit the coverage. Some people have therefore taken an alternative route, by acquiring lexical semantic information semi-automatically from existing resources such as machine-readable dictionaries (e.g. Amsler, 1981;Calzolari, 1988;Chodorow et al., 1985;Vossen et al., 1989) and corpora (e.g. Resnik, 1993;Riloff &amp; Jones, 1999). Nevertheless, this approach may not be entirely satisfactory. A single resource may not contain all types of information. Even if it does (e.g. a comprehensive dictionary might contain thesaural information implicitly in its meaning definitions), the extraction of some information is not always straightforward, rendering the acquired information incomplete or unreliable.Thus a third and possibly better approach is to combine various existing resources, especially different types of them which are exemplars for representing different types of information, into an integrated repository. By integration, we mean that the various resources are linked in some way but the different types of information are preserved in their original structures in individual resources. Such a linkage thus enables us to access the different types of information simultaneously, compatibly, and flexibly.Although such resource integration has been suggested (e.g. Yarowsky, 1992) or implemented in some way (e.g. Sanfilippo &amp; Poznariski, 1992;Knight &amp; Luk, 1994;McHale &amp; Crowter, 1994;Klavans &amp; Tzoukermann, 1995;Chen &amp; Chang, 1998), surprisingly few studies have actually made use of multiple existing lexical resources simultaneously in WSD. Rather, most studies to date which use multiple knowledge sources for WSD are only maximising the exploitation of a single (type of) resource. One study which actually used information distributed across various resources is perhaps McRoy (1992), but the resources were tailor-made in her study and the linkage between them was manually imposed. We shall explore how an automatically integrated resource could be put into practical use.In this study, we made use of a simple but effective structurally-based sense-mapping algorithm to link all noun senses shared by two existing lexical resources, namely WordNet and Roget's Thesaurus, to form an Integrated Lexical Resource (ILR). The algorithm will be presented in Section 2, followed by a description of the linking process in Section 3. The ILR will be evaluated and its properties discussed in Section 4. Some potential use of the ILR in WSD will be suggested in Section 5, before we conclude in Section 6. This paper reports a full-scale linkage of noun senses between two existing lexical resources, namely WordNet and Roget&apos;s Thesaurus, to form an Integrated Lexical Resource (ILR) for use in natural language processing (NLP). The linkage was founded on a structurally-based sense-mapping algorithm. About 18,000 nouns with over 30,000 senses were mapped. Although exhaustive verification is impractical, we show that it is reasonable to expect some 70-80% accuracy of the resultant mappings. More importantly, the ILR, which contains enriched lexical information, is readily usable in many NLP tasks. We shall explore some practical use of the ILR in word sense disambiguation (WSD), as WSD notably requires a wide range of lexical information.
Some Principles of Automated Natural Language Information Extraction Here is presented and discussed some principles for extracting the semantic or informational content of texts formulated in natural language. More precisely, as a study of computational semantics and information science we describe a couple of methods of logical translation that may be considered a kind of information extraction. We discuss the translation from dataflow structures partly to parser programs or logic grammars and partly to informational content. The methods are independent of any particular semantic theory and seem to fit nicely with a variety of available semantic theories.Information is a concept of crucial importance in any conceivable scientific endeavour. Also the modeling of information or semantic representation is becoming more and more important, not least in information systems. Databases, knowledge bases as well as knowledge management systems are continually growing. Modeling helps to understand, explain, predict, and reason on information manipulated in the systems, and to understand the role and function of components of the systems. Modeling can be made with many different purposes in mind and at different levels. It can be made by emphasising the users' conceptual understanding. It can be made on a domain level on which the application domain is described, on an algorithmic level, or on a representational level. Here the interest is focused on modeling of information on a representational level to obtain sensible semantic representations and in particular on the flow of information between the vertices of a structure describing a natural language utterance.We are in the habit of considering the syntactic phenomena, and especially those concerning parsing, as essentially well understood and highly computational. Quite the opposite seems to be the case with semantics. We shall argue that certain central semantic phenomena (here termed logico-semantic) can be equally well understood and computational. So, in this rather limited sense we may claim that the semantic problem has been solved (meaning that there exists a computational solution). This paper contains a brief discussion and sketches a solution. A more comprehensive discussion may be found elsewhere.The method presented will produce one single logico-semantic formula for each textual input. In case more solutions are required (and hence ambiguity is present) it is certainly possible to build together the resulting individual logic grammars.Here we are exclusively concerned with parsing or textual analysis. Analogous considerations can be made concerning textual synthesis or generation (Kawaguchi, 1997).We shall discuss a new method for extracting the informational content of (brief) texts formulated in natural language (NL). It makes sense to consider information extraction from NL texts to be essentially the same task as building simple kinds of information models when parsing the texts. Here we present a method that is distinguished by extreme simplicity and robustness. The simplicity makes programming of the method feasible, and so a kind of automatic program synthesis is obtained. The robustness causes wide applicability, and so the method has a high degree of generality (Koch, 1991), (Koch, 1994a), (Koch, 1994b), (Koch, 1997), (Koch, 2000a), (Koch, 2000b), (Koch, 2000c). Here is presented and discussed some principles for extracting the semantic or infor-mational content of texts formulated in natural language. More precisely, as a study of computational semantics and information science we describe a method of logical translation that seems to constitute a kind of semantic analysis, but it may also be considered a kind of information extraction. We discuss the translation from Dataflow Structures partly to parser programs and partly to informational content. The methods are independent of any particular semantic theory and seem to fit nicely with a variety of available semantic theories.
A Structure-Shared Trie Compression Method Trie structure is a data structure frequently used for searching a datum from sources of data, such as natural language dictionaries, databases, tables in compilers and so forth. One of the merits of trie structure is its searching efficiency. The searching time in trie structure depends only on the length of keyword, while that of other structures involves the number of words (or entries). However, a problem of an original trie is that it wastes a lot of space. Some memories in the trie is reserved even though they are not used. According to trie uses an array for fast accessing to the needed data, the size of array in trie is fixed to the maximum possible numbers of alphabets (or cardinality of alphabets). This introduces a lot of explicit useless space for a large cardinality of alphabets through small average branching factor. In order to solve this problem, the cardinality of alphabets is reduced to two for binary trie, ten for 10-ary trie, and so forth. Although the problem of explicit useless space is reduced, the number of nodes and transitions are increased, which leads to slow searching speed.During the past few decades, a number of ideas were proposed to reduce the size of trie structure such as PAT (Morrison 1968), Double Array Trie (Aoe 1989), and LC-trie ( Karlsson 1999, Zandieh 1999). However, it is still in a question whether there is more efficient compression method than these approaches. The idea of reducing branching factor of trie may have other ways to solve the space usage problem. In this paper, we propose a new idea called structure-shared trie (SStrie) to reduce the size of trie structure and preserve quality of retrieval speed.The SS-trie is an overhead-reducing version for a 256-ary trie. It uses the combination of three techniques: (1) path compression, (2) structure sharing, and (3) node compression to solve trie problem. SS-tie uses the path compression technique to discard the node with only one child. Then the second technique, Structure sharing, is applied to solve explicitly useless space by sharing frequently used node structure. For unshared structure, SS-trie uses the node compression to compress explicitly useless space into bit vectors. With the combination of these techniques, it can reduce size of trie structure more than naïve trie structure but still preserving the quality of searching speed.For the rest of this paper, section 2 explores some previous works related with trie compression. Section 3 illustrates the main idea of SS-trie. In section 4, the effectiveness of SS-trie is shown through a set of experiments. Finally conclusion and discussion are given. Trie, a well-known searching algorithm, is a basic and important part of various computer applications such as information retrieval, natural language processing, database system, compiler, and computer network. Although a merit of trie structure is its searching speed, the naïve version of trie structure size may not be acceptable when the key set is very large. To reduce its size, several methods were proposed. This paper proposes an alternative approach using a new tie structure called, structure-shared trie (SS-trie). The main idea is to reduce unused space using shared common structure and bit compression. Three techniques are used: (1) path compression, (2) structure sharing, and (3) node compression. A number of experiments are conducted to investigate trie size, sharing rate, and average depth of the proposed method in comparison with binary search, naïve trie, PAT, and LC-trie. The result shows that our method outperforms other methods. 1 Introduction Trie structure is a data structure frequently used for searching a datum from sources of data, such as natural language dictionaries, databases, tables in compilers and so forth. One of the merits of trie structure is its searching efficiency. The searching time in trie structure depends only on the length of keyword, while that of other structures involves the number of words (or entries). However, a problem of an original trie is that it wastes a lot of space. Some memories in the trie is reserved even though they are not used. According to trie uses an array for fast accessing to the needed data, the size of array in trie is fixed to the maximum possible numbers of alphabets (or cardinality of alphabets). This introduces a lot of explicit useless space for a large cardinality of alphabets through small average branching factor. In order to solve this problem, the cardinality of alphabets is reduced to two for binary trie, ten for 10-ary trie, and so forth. Although the problem of explicit useless space is reduced, the number of nodes and transitions are increased, which leads to slow searching speed. During the past few decades, a number of ideas were proposed to reduce the size of trie structure such as PAT (Morrison 1968), Double Array Trie (Aoe 1989), and LC-trie (Nilsson and Karlsson 1999, Zandieh 1999). However, it is still in a question whether there is more efficient compression method than these approaches. The idea of reducing branching factor of trie may have other ways to solve the space usage problem. In this paper, we propose a new idea called structure-shared trie (SS-trie) to reduce the size of trie structure and preserve quality of retrieval speed. The SS-trie is an overhead-reducing version for a 256-ary trie. It uses the combination of three techniques: (1) path compression, (2) structure sharing, and (3) node compression to solve trie problem. SS-tie uses the path compression technique to discard the node with only one child. Then the second technique, Structure sharing, is applied to solve explicitly useless space by sharing frequently used node structure. For unshared structure, SS-trie uses the node compression to compress explicitly useless space into bit vectors. With the combination of these techniques, it can reduce size of trie structure more than naïve trie structure but still preserving the quality of searching speed. 129
Processing Local Coherence of Discourse in Centering Theory  In this paper we will investigate the centering theory proposed by Grosz, Joshi, and Weinstein (1995) (henceforth GJW) and revised in Walker, Joshi, and Prince (1998) (henceforth WJP), and argue that their theory needs to be further extended and revised. We show that the centering theory proposed by GJW and WJP would make wrong predictions about the preferred transition states in discourse processing since their backward-looking center (Cb) is not primitive and thus cannot be used to adequately model the local coherence of discourse. We propose that Cb should be distinguished from the discourse segment topic (DST), and provide a more restrictive definition of Cb, which restricts Cb(U) to be the element in Cf(U 11) that is realized by the subject pronoun or the pronoun contained in the subject in U.
Temporal Structure on Discourse Level within the Controlled Information Packaging Theory] This paper aims to show how to represent temporal structures in English within the framework of the Controlled Information Packaging Theory (for short, CIPT). The CIPT has been developed as a new approach to describe semantic relations on the discourse level. Recent works on the tense structure of discourse have tried to determine the temporal relationship between events described in successive sentences in narrative discourse (Thompson 1999:123). This study explores temporal relations on discourse level, too.In natural language, the temporal information is conveyed in various ways; namely, by means of grammatical devices or categories like tense, temporal adverbs, nouns, adjectives, and conjunctions. In actual utterances, the use of tense is supposed to be closely related to the discourse structure. However, it has not been fully understood how this interaction takes place. In this paper we will explore how the discourse structure influences the interpretation of tense on the level of global discourse like the following (Webber 1988: 69):(1) a. John went into the florist shop.b. He had promised Mary some flowers. c. She said she wouldn't forgive him if he forgot. d. So he picked out three red roses, two white ones, and one pale pink.This work was partly supported by Korea Research Foundation Grant. (KRF -2000-A00379) In the discourse, sentences (1b)-(1c) constitute a sub-discourse, but here the main discourse, started at ( I a), is continued at (1d).On the base of this study, we will show how to represent temporal relations of events within the CIPT. This paper extends the previous work, which used d-command constraint proposed as a means for resolving anaphora, and proposes the principle of temporal d-command in order to explain the behaviour of tense on the global discourse level.2 Related Works 2.1 Kamp and Reyle (1993) Within DRT, Kamp and Reyle try to represent the temporal structures of a natural language discourse. Their approach is based on the assumption that events and states are ontological primitives. Sentences are supposed to describe either events or states. Every initial sentence of a discourse introduces a reference point in the sense of Reichenbach (1947). If a sentence follows another one, then the temporal relation between the two eventualities depends on whether it describes an event or a state. In case the second sentence describes an event, its eventuality follows the reference point of the preceding sentence. On the other hand, in case the second sentence describes a state, its eventuality always includes the reference point of the preceding sentence. The following example shows how this approach works (Kamp and Reyle 1993:521):(2) a. A man entered the White Hart.b. He was wearing a black jacket. c. Bill served him a beer.In the first sentence, a reference point is introduced. It precedes the utterance time, because the past tense is used. The second sentence describes a state. Therefore, its eventuality includes the reference point of the first one and it doesn't revise the reference point. The third sentence describes an event again and it's eventuality follows the reference point of the first sentence. The new event described here revises the reference point. The serving event becomes now the new reference point.To sum up, non-initial sentences without temporal adverbs, need a contextually informed reference point to determine the location of the eventualities they describe ( Kamp and Reyle 1993:529). However, when a non-initial sentence does contain a temporal adverb, then it is the adverb which will supply the location. The adverbs can override the effect of the antecedent context, as the following discourse exemplifies.(3) a. Fred arrived on the first of January.b. It was raining continuously. c. But the next day the sun was shining.The first sentence introduces Fred's arrival as the reference point that is located at the first of January. The second sentence describes a state and it doesn't revise the reference point. The state described by the third sentence is considered to be located at the time designated by the adverb "the next day".Kamp and Reyle maintain that their basic scheme need to be revised to deal with the so-called extended flashbacks such as example (4) (Kamp/Reyle 1993: 594): (4) a. Fred arrived at 10. b. He had got up at 5; he had taken a long shower, had got dressed and had eaten a leisurely breakfast. c. He had left the house at 6:30.According to Kamp and Reyle, all the past perfect clauses of (4) use the arrival time as their "point of reference" in the sense of Reichenbach. However, they form a narrative progression much like the sequences such as (3). It means that the "point of reference" of the second past perfect clause in (4b) is related not only to the arrival time described in clause (4a) but also to the getting-up time described in the first past perfect clause. In this context, they believe that there are two distinct notions of "point of reference".For the purpose of describing the temporal structures of a natural language discourse, Kamp and Reyle introduce a new term "temporal perspective point (TPpt)" that has a distinct function from the "reference point (Rpt)". The former corresponds to the original notion of "point of reference" of Reichenbach. The latter is used to describe the narrative progression. Based on this observation, Kamp and Reyle propose a revised theory on the tense interpretation. A described situation is first related to a time of location, this time of location is related to a temporal perspective point, and it is this perspective point which is related to the utterance situation (van Eynde 1998: 243). Along this line of description, the present tense is characterized by the relation pair &lt;TPpt coincides with utterance time; described eventuality overlaps with TPpt&gt; and the past tense corresponds to the pair &lt;TPpt before utterance time; described eventuality overlaps with TPpt&gt;.In our opinion, Kamp and Reyle's approach is complicated and does not seem to be very convincing, because they assume an additional notion of the "reference point" besides the three notions corresponding to the Reichenbach's framework. Furthermore, they don't explain, how this notion would possibly be related to the others. As an alternative, we present a reasonable account in section 4 within the framework of CIPT. The temporal structure of events on the discourse level has long been of great interest in both theoretical and computational linguistics. In this paper, we offer a unified approach to the temporal relationships related to a hierarchical discourse structure. We apply the method of pronoun resolution to the interpretation of tense. It is based on an analysis within the framework of the controlled information packaging theory. A unique aspect of our account is that temporal interpretation across discourse segments in global discourse is subject to the same principles as the interpretation of global anaphora, and that there is thus no need to postulate independent principles to account for the discourse behaviour of tense. In this way, we can neatly explain the general view that tense parallels the anaphoric nature of pronouns.
Highlighting Utterances in Chinese Spoken Discourse In the field of discourse analysis, many researchers such as Sacks, Schegloff and Jefferson (Sacks et al. 1974;Schegloff et al. 1977), Taylor and Cameron (Taylor &amp; Cameron 1987), Hovy and Scott (Hovy &amp; Scott 1991) and the others have extensively explored how conversation is organised. Various topics have been investigated, ranging from turn taking, self-corrections, functionality of discourse components, intention and information delivery to coherence and segmentation of spoken utterances. The methodology has also accordingly changed from commenting on fragments of conversations, to theoretical considerations based on more materials, then to statistical and computational modelling of conversation. Apparently, we have experienced the emerging interests and importance on the structure of spoken discourse.Recently, Clark and Brennan (Clark &amp; Brennan 1991) proposed that production-related as well as perception-related activities have to be coordinated in conversation. They should serve the purpose of a speaker to get an addressee's attention, to plan and produce utterances, to recognise when the addressee does not understand, to initiate and manage repairs and to display or acknowledge understanding. Since interlocutors in conversation usually communicate freely and spontaneously, utterances may be interrupted without being completed and turn taking may take place unpredictably. Besides, there can be erroneous or unknown words and phrases, when speakers are not able to express their thoughts properly or when they spontaneously create new words or compound words for their thoughts. While monitoring and correcting their speech, speakers make repairs and they may need time for re-planning and editing their speech. Discourse particles and pauses (silent or filled) are often used for this purpose. Especially discourse particles are usually located in related positions in a given discourse. They may possibly signal turn-initial positions or they may express special pragmatic functions such as surprise or hesitation.With regard to the internal structure of utterances, speech disfluency no doubt results in serious problems for natural language processing systems. But, not for humans! Why? One of the reasons could be that speakers of a common language share similar knowledge and competence on how to express emotion pragmatically and how to perform monitoring and repairing in a reasonable way, whenever there is a need. Therefore, the communication partners can easily decode the meaning and the function of speech disfluency in conversation, because they would encode it by using similar sequence combinations. Moreover, speech disfluency also contributes to the segmentation of discourse by pointing out problematic speech sequences.In the production of speech, all of the phenomena mentioned above are at the same time accompanied by prosody. No doubt, prosody is the most powerful highlighting means in spoken language. A grammatically ill-formed sentence may just sound like a perfectly correct sentence, when it is produced with a professional and fluent intonation. However, it is not the intention of interlocutors in conversation to impress each other in this way. In contrast, the speaker would rather let the addressee know that he/she made a mistake previously and what he/she is now saying is correct. Thus, it is very likely that discourse particles and speech disfluency are highlighted by prosodic means to emphasise the semantic and syntactic inadequacy.This paper mainly deals with means emphasising particular speech sequences and four possibilities of observing these activities are 1) frequently used words, 2) discourse particles and markers, 3) speech disfluency and 4) prosodic marking. In the coming sections, they will be investigated on the basis of results of a corpus analysis. Before the results are presented, speech data used for the analyses are introduced first. This paper presents results of an empirical analysis on the structuring of spoken discourse focusing upon how some particular utterance components in Chinese spoken dialogues are highlighted. A restricted number of words frequently and regularly found within utterances structure the dialogues by marking certain significant locations. Furthermore, a variety of signals of monitoring and repairing in conversation are also analysed and discussed. This includes discourse particles, speech disfluency as well as their prosodic representation. In this paper, they are considered a kind of &quot;highlighting-means&quot; in spoken language, because their function is to strengthen the structuring of discourse as well as to emphasise important functions and positions within utterances in order to support the coordination and the communication between interlocutors in conversation.
A Decidable Linear Logic for Speech Translation The recent progress in the theories of natural language semantics enables us to capture a great deal of subtleties in the meanings conveyed by utterances. Even several decades ago, when Montague initiated the program of research, we could already handle beliefs, quantification, tenses and aspects and so on. The field of study has since then been greatly expanded to include attitudes, mental states, communication states among many others, all pointing to the real life rather than the unreal such as unicorn or the nonexistent French King. Through inference we can find something more interesting items of information, not limited to a simple answer, "yes" or "no."The progress was however not achieved without paying a price. The objects to represent the meaning of utterances have got complicated and look sometimes very exotic to outsiders who have not seen anything more curious than q or O. Even insiders suffer from the complexity when they try to build, based on a theory, something useful such as a speech translation system as the definition of each semantic object becomes too long to view on the computer display at a glance. The definition may sometimes not be formatted in one page.One of the important lessons we have learnt through implementing linguistic theories, is that theories should be as simple as possible so that we can check whether an algorithm implementing an idea terminates successfully or unsuccessfully. Theories should also require the least computational power to run so that we will not be frustrated in front of the computer display, waiting for a return.How far can we reasonably get into details of the meaning, without requiring too much computational power? This is the issue we address in this paper. We take a logical approach to study the computational aspects of the theories on meanings and show how we can specify the semantic objects employed in the contemporary linguistic theories in linear logic. Linear logic particularly suits us because its rich set of connectives allows us to specify varieties of objects and its close connection to computation brings us a clear insight into what class of computational complexity is expected when we encode and operate on those objects on computers.The minimum requirement for our project is to ensure that there is a decidable method whether or not a translation or inference is valid. Because we specify semantic objects as formulae and translation rules as axioms, translating a semantic object into that of the target language is seen naturally as an inference in linear logic. The decidability is particularly important because it ensures that we can test whether our translation rules work properly as we expect provided enough time to run. Once we are assured that there is a decidable method, then we are concerned with the computational complexity. We discuss the issue briefly in this paper, too.The body of this paper is divided into two parts. In the first part we show how we can specify semantic objects in a monadic, multiplicative linear logic, which is decidable. In the second part we extend the fragment with the exponential so that we can handle contexts that relate the meaning of utterances to the extra-linguistic factors and propose a way to keep the fragment decidable by imposing some restriction on the use of the exponential. The body parts are followed by discussions on logical and computational issues and we conclude the paper by claiming our contributions to computational linguistics. The structure of objects employed in the study of Natural Language Semantics has been increasingly being complicated to represent the items of information conveyed by utterances. The complexity becomes a source of troubles when we employ those theories in building linguistic applications such as speech translation system. To understand better how programs operating on semantic representations work, we adopt a logical approach and present a monadic and multiplicative linear logic. In designing the fragment, we refine on the multiplicative conjunction to employ both the commutative and non-commutative connectives. The commutative connective is used to glue up a set of formulae representing a semantic object conjointly. The non-commutative connective is used to glue up a list of formulae representing the argument structure of an object, where the order matters. We also introduce to our fragment a Lambek slash, the directional implication, to concatenate the formula representing the predicative part of the object and the list of formulae representing the argument part. The monadic formulae encode each element of the argument part by representing its sort with the predicate and the element as the place-holder. The fragment enjoys the nice property of being decidable. To encode contextual information involved in utterances, however, we extend the fragment with the exponential operator. The context is regarded as a resource available as many as required, but not infinitely many. We encode the items of context with the exponential operator, but ensure that the operator should appear only in the antecedent. The extention keeps the fragment decidable because the proof search will not fall into an endless search caused by the coupling of unlimited supply and consumption. We show that the fragment is rich enough to encode and transform semantic objects employed in the contemporary linguistic theories. The result guarantees that the theories on natural language semantics can be implemented reasonably and safely on computers.
Building domain-independent text generation system Natural Language Generation (NLG) is the automatic generation of Natural Language by computer in order to meet communicative goals (Smith, 1995). Until now, almost all of the existing generation systems are domain-dependent, ie., they were built for some specific applications. For example, IDAS "produces on-line hypertext help messages for users of complex machinery" (Reiter and Dale, 2000) (p.12); STOP "is a natural language generation system which produces personalised smoking-cessation letters" (p.16). However, how to build a domain-independent generation system is still regarded to be a difficult problem.We think that the main difference between domain-dependent and domain-independent generation system is due to the problem of text structuring. Generally speaking, before building a domain-dependent text generation system, designers should make a domain model. In the model, the characteristics of the structure of text to be generated are described. For domaindependent systems, text structure varies with application domains. In fact, no matter what domain a text belongs to, its structure reflects the common features of English text. If we make a text structure model representing the common features of English texts, the texts generated by this model are domain-independent.( Ozaki et al., 1997) introduced how to generate coherent Japanese text from semantic network. Based on their research, we built an English Generation System which is domain-independent. Our reserach has two stages. At the first stage, we focus on sentence generation and text generation, but we do not consider discourse structure (cue phrases, "but", "similarly", for example). At the second stage, we will explore the problem of discourse generation. In this paper, we describe the first stage of our research in detail. The rest of the paper is organized as follows: section 2 introduces semantic network of the English Generation System; section 3 introduces generating coherent text flexibly on sentence level and text level; section 4 introduces experiment and generation results; section 5 introduces future directions. This paper investigates an effective method of building domain-independent text generation system. In our English text generation system, Semantic Network is used as the internal Knowledge Representation. Nodes and links of the semantic network are classified according to word class and grammatical relations respectively. Generation results prove that the system works well and can generate coherent text flexibly.
Vowel Shortening and Surface Ternary Feet -Ternary Rhythm Through Strictly Binary Footing Vowel shortening occurs generally in moraic trochee languages. In Fijian, for instance, there is a phonological rule called Trochaic Shortening that shortens long vowels in penultimate syllables when the final vowel is short. Representative examples are given in (1).( In Japanese, there is a phonological tendency for word-final long vowels to undergo shortening after a heavy syllable. ( Sukegawa et a1.1990 In recent Japanese loanwords, word-final long vowels are also apt to shorten after a heavy syllable. Some examples are in (3). The last high-pitched mora (Accented Mora) is specified by an acute accent mark v Furthermore, word-final long vowels even after a light syllable tend to become short in the following recent Japanese loanwords. Consider the following recent loanwords. The vowel shortening in (3) is different from (4) in that while the word-final long vowel after a heavy syllable shortens in (3), the word-final long vowel after a light syllable becomes short in (4). But for both cases, the word-final long vowel in an unaccented syllable tends to become short. On the other hand, Trochaic Shortening in Fijian is very different from the shortenings in Japanese since the long vowel in a stressed syllable shortens in Fijian.In this paper, I will focus on the shortenings observed in recent loanwords in Japanese, as in (3) and (4) and on the trochaic shortening in Fijian. First, I will analyze accent patters of Japanese loanwords in (3) and (4) under Metrical Phonology and show that the right peripheral bimoraic foot is extrametrical. While the analysis predicts the correct accent position, the strictly binary foot parsing creates a stray mora between two bimoraic feet. In order to confirm how stray moras are organized to achieve surface exhaustivity, I have conducted a phonetic experiment. Based on the result of the experiment, I will propose two parametrically distinct cognitive surface ternary foot structures for the two languages and argue that Fijian and Japanese shortenings are triggered by two distinctive types of extrametricality mora and foot extrametricality respectively. The focus of this paper is on two types of shortening observed in recent Japanese loanwords and on the trochaic shortening in Fijian. I will argue that the Japanese shortenings take place when a metrically unstable binary foot (extrametrical foot) becomes a stray mora so as to be accommodated into one of the two proposed surface ternary foot structures whereas Fijian Trochaic Shortening occurs when a metrically unstable mora (extrametrical mora) is obligatorily accommodated into the other proposed surface ternary foot structure. I will conclude that the distinct shortening phenomena observed in the two languages can be accounted for simply on the ground of parametrical differences.
Mappings From the Source Domain of Plant in Mandarin Chinese The Contemporary Theory of Metaphor (CTM) postulates that metaphors are not just figurative parts of speech, but are instead fundamental to our understanding of how we view the world (Lakoff 1993). The CTM hypothesizes that information about a source domain (SD) maps to a target domain (TD). For example, in LOVE IS PLANT metaphor, one conceptual domain is mapped (i.e. love) onto another (i.e. plant). The former domain is target domain, which is comparatively more abstract. The latter domain is source domain, which is comparatively more concrete. People understand the abstract target domain of love through the concrete source domain of plant. This is demonstrated in example (1).(1) women de ai jianjian chengzhang we MOD love gradually grow Our love grows gradually.The target domain is specified as LOVE, and the source domain is specified as PLANT. People understand love by understanding how plant grows. Thus, LOVE IS PLANT metaphor is a conceptual metaphor in English. Lakoff (1993) proposes the Invariance Principle to account for the mapping relationships between the target and the source domains.Metaphorical mappings preserve the cognitive topology of the source domain, in a way that is consistent with the inherent structure of the target domain.Thus, growth is an image-schema in the source domain of plant that is preserved when mapped onto the target domain of love. Conceptual domains used in conceptual metaphors are in fact quite complex domain matrices, which include many image-schematic domain mappings (Clausner and Croft, 1999). Lakoff (1993) stated that the general principles governing the patterns of inferences in metaphorical correspondences could not be precisely formulated because they are part of the entire conceptual system. We disagree with this point of view and instead propose that it is only once the governing patterns of inference in the metaphorical correspondences (what we call mapping principles) have been analyzed that one can say whether or not a concept can be mapped between the source and target domain. In this paper, we first discuss our methodology in section 2 and then examine the four target domains that use the source domain of PLANT in section 3. In section 4, we discuss the mapping principles involved. In section 5, we discuss further areas of study. This paper investigates what words map from the source domain of PLANT in Mandarin Chinese. In particular, we examine how different aspects of the source domain of PLANT are mapped onto the different target domains of LOVE, MARRIAGE, HAPPINESS, and BELIEFS. We found that mapping principles can account for the different mappings (Ahrens 2000).
Floating Quantifiers and Lexical Specification of Quantifier Retrieval This paper deals with floating quantifiers (FQ) in English and investigates how the syntactic and semantic characteristics of floating quantifiers can be explained within the framework of the HeadDriven Phrase Structure Grammar. In contrast to Maling (1976), Sportiche (1988), McCawley (1998), and Haegemann &amp; Gudron (1999), who posit the movement of either the quantifier or the host noun phrase, this paper presents an analysis in which a floating quantifier is base-generated as a VP modifier. The view that floating quantifiers are VP modifiers is well motivated in Dowty &amp; Brodie (1984). However, unlike Dowty &amp; Brodie, who provide a "rule-to-rule" model-theoretic interpretation for each phrase structure rule employed in floating quantifier constructions, this paper presents an analysis of floating quantifiers within a theory where "syntactic" and "semantic" components are contained as subparts of structural representations that are defined by recursive feature constraints. The proposed analysis employs a lexical approach in the sense that the syntactic and semantic characteristics of the construction are encoded as part of the lexical information that floating quantifiers have. Furthermore, in order to account for the fact that the floating quantifier all takes scope over the VP that it modifies, the retrieval of the quantifier is lexically specified. I will show that lexical specification of quantifier retrieval is necessary in accounting for the scope facts in other constructions as well. Floating quantifiers (FQs) in English exhibit both universal and language-specific properties, and this paper shows that such syntactic and semantic characteristics can be explained in terms of a constraint-based, lexical approach to the construction within the framework of Head-Driven Phrase Structure Grammar (HPSG). Based on the assumption that FQs are base-generated VP modifiers, this paper proposes an account in which the semantic contribution of FQs consists of a &quot;lexically retrieved&quot; universal quantifier taking scope over the VP meaning.
Pragmatic Inference with Conditionals and Concessives in. Japanese  Expressions in Japanese with indeterminates such as &quot;nani (what)&quot; and &quot;dare (who)&quot; get interpretations very much like universal quantification or existential quantification when followed by conjunctive particle &quot;mo&quot; or disjunctive particle &quot;ka&quot;. Focusing our attention on sequences of the form &lt;indeterminate + ka&gt; such as &quot;nani-ka (something)&quot;, they generally get interpretations of &quot;something exists which&quot; in affirmative sentences, whereas they generally get interpretations of &quot;something exists which does not&quot; in negative sentences. Interestingly, the latter with negation changes its figure in conditional sentences and in concessive conditional sentences. In both of these, negative sentences with sequences of the form &lt;indeterminate + ka&gt; get not only interpretations of &quot;something exists which does not&quot; but also interpretations of &quot;nothing exists which&quot;, depending on the situation in which utterances take place. Similar phenomena can be seen with numerals, where negation can take either a narrower or a wider scope with respect to numerical quantification in conditionals and concessives. In this paper, we would argue that those phenomena could be accounted for by postulating pragmatic inferences with conditionals and concessives, and by assuming that Japanese focus particles &quot;wa&quot; and &quot;mo&quot; in these cases function as a conditional operator and a concessive operator respectively.
On the Semantics of Japanese Particles wa and mo and Their Interaction with Quantifiers  In this paper, two Japanese particles wa and mo are taken up. Wa is known as a topic marker and mo is a correspondent of English particles too and also. After reviewing several studies that claim topics and particles like too and also evoke alternatives, a formal semantic system with structured meanings is constructed for wa and mo, incorporating the insights of the previous studies. The proposed semantics of wa and mo, then, is applied to adverbial quantifier constructions and an attempt is made to derive the special implicatures added by the particles by means of their semantic properties and pragmatic inferences.
Nominal Markers and Word Order in Korean  The purpose of this paper is to explore the relationship between case and free word order in Korean by providing proper LP constraints which solve the problems of
Thai Classifiers and the Structure of Complex Thai Nominals A comparison of the word orders of Thai and English simple and complex nominals reveals that Thai is a mirror image of English as shown in 1). This paper aims to posit a functional category for Thai classifiers and demonstrate the analysis of Thai complex nominals adopting the antisymmetry framework (Kayne 1994). It proposes that Thai classifiers have an independently functional status and project the Classifier Phrase (ClassP) basically because they work in the same way as agreement. Evidence supporting their functional status includes properties of classifiers in forming their own word class distinct from the category of nouns, their non-modificational property by adjectives, and multiple occurrences. The underlying structure of Thai nominals is constructed in terms of the DP analysis. To derive a Thai nominal word order, it is argued that classifiers features are strong and there exist a combination of raising operations regulated by asymmetrical c-command relation (Kayne 1994) as well as feature checking (Chomsky 1995). The analysis suggests that Thai nominals possess a commonly underlying head-initial structure in which movement plays a key role in deriving the surface word order.
Negation,. VP Ellipsis, and VP Fronting in English: A Construction-HP S G Analysis The English auxiliaries are sensitive to the so called NICE (Negation, Inversion, Contraction, and Ellipsis) phenomena, as seen from the contrast with main verbs in (1) and (2):(1) a. John may not leave Seoul.b. Will John leave Seoul? c. John can't leave Seoul. d. Mary will leave Seoul, and John will, too.(2) a. *John not left Seoul. b. *Left John Seoul? c. *John leftn't Seoul. d. *Mary wants to leave Seoul, and John wants.Based on these empirical properties of auxiliaries, this paper argues for the existence of the construction aux-head-phrase) whose subtypes include negation-ph, inversion-ph, ellipsis-ph, and vp-filler-ph. Each of this subtype has its own construction-specific constraint as well as those inherited from its supertypes. The paper ]. shows that if we accept this view of the English auxiliary system in terms of construction types and declarative type constraints on them, we can offer a more straightforward and explicit explanation for English negation, VP ellipsis, and VP fronting phenomena. This paper starts with an analysis for English negation. Following Kim and Sag (1995), Kim (2000), and Warner (2000), this paper assume that the English negative marker not leads two lives -one as an adverbial modifier (as constituent negation) and the other as the complement of a finite auxiliary verb (as sentential negation): 'The theory this paper assumes is Construction-HPSG, roughly Head-driven Phrase Structure Grammar, augmented with a theory of constructions as in Ginzburg and Sag to appear.[adv [ It is well-known that the English auxiliaries are sensitive to the so called NICE (Nega-tion, Inversion, Contraction, and Ellipsis) phenomena. Based on these empirical properties of auxiliaries, this paper argues for the existence of the construction aux-head-ph(rase) whose subtypes include negation-ph, inversion-ph, ellipsis-ph, vp-filler-ph, and the like. Each of this subtype has its own construction-specific constraints as well as those inherited from its supertypes. The present analysis uses grammatical constructions with declarative constraints and posits a rich network of inheritance relations among them. This enables us to provide a clean analysis for some of the puzzling phenomena in English such as negation, VP ellipsis, and VP fronting, while capturing new levels of generalizations among these seemlingly unrelated phenomena.
Feature Percolation, Movement and Cross-Linguistic Variation in Pied-Piping  This paper deals with cross-linguistic variation in WH-pied-piping. In this paper, I claim that pied-piping is the result of the intermediate stages of WH-movement (i.e., the indirect feature checking movement of Chomsky (1998)) to the Spec of XP and the subsequent percolation of the WH-feature to the XP and that cross-linguistic variation in pied-piping can be explained in terms of various universal and language-specific syntactic constraints and properties. Crucially, I claim that the overt/covert nature of indirect feature checking movement could vary independently from that of direct feature checking movement. This means that in principle, there can be four different types of languages in terms of WH-pied-piping. Specifically, I show that Basque and Imbabura Quechua are languages where both the direct and indirect WH-feature checking movement are overt while WH-in-situ languages like Sinhala and Korean are those where both the direct and indirect WH-feature checking movement are covert. As for the languages where the indirect WH-feature checking movement is covert while the direct WH-feature checking movement, is overt, I claim that English is such a language and show how various facts of pied-piping in English can be explained in a principled manner if we assume this. Finally, concerning the languages where the indirect WH-feature checking movement is overt while the direct WH-feature checking movement is covert, I speculate that a potential candidate for this type of language is Slave, where it was claimed that partial WH-movement is possible without an overt scope marker.
Gerundive Complements in English: A Constraint-Based Analysis A gerundive complement is a gerund phrase used in verb complementation constructions. The underlined part in a sentence like (1) below is a typical example:(1) I hate Pat's/Pat smoking cigars.Depending on the case of the NP preceding the verbal gerund, it has been called a POSS-ing (Pat's smoking cigars) or an ACC-ing gerund (Pat smoking cigars). The verbal gerunds are quite an old issue in English syntax, and there has long been a considerable amount of discussion of them in generative grammar. But Malouf (2000), a pioneering Constraint-Based lexicalist approach to verbal and nominal gerunds, has reminded us that there are still many interesting questions to be explored in this area.Gerunds constitute a very broad issue in English syntax, but we will focus on gerundive complements in this paper. Before presenting our analyses in Section 3, we discuss categorial status of the verbal gerund and some formal mechanisms to treat them in Section 2, highlighting the theoretical significance of Malouf (2000)'s study of 'mixed categories.' I consider a tentative alternative in Section 4 before I conclude in Section 5. Much attention has been paid to English verbal gerunds in generative grammar, but most of the studies dealt with the abstract &apos;functional&apos; category empty categories, derivational deep structure analyses, and things like that, which all ignored the Lexical Integrity Prniciple in the sense has presented a nonderivational, constraint-based lexicalist approach to verbal gerunds, in which he has provided a theory of mixed categories, proposing a new lexical category gerund in a multiple inheritance type hierarchy. And yet there are still some big gaps to be filled in the area of verb complementation involving verbal gerunds, what I call verbal complements. This paper is primarily an attempt to fill the gaps, and also an examination of the new category gerund.
An HPSG Account of the Hierarchical Clause Formation in Japanese -HPSG-Based Japanese Grammar for Practical Parsing Hierarchical clause formation (HCF) in Japanese is a phenomenon that has reflexes in syntax and semantics. Although there is a large body of descriptive work on this topic (see Minami (1974) and references cited there), there has been little attempt to give a formal analysis of the phenomenon.This study concentrates on two types of relations for HCF: head-head relation and head-modifier relation headed by sentence-final clusters of auxiliary verbs and conjunctive particles, which are exemplified in some kind of complex predicate and subordinate clause, respectively. These hierarchical complexities make sentence to be long and not simple, so many linguists and engineers try to capture in mainly syntactic terms how these relations should be described. Especially, ambiguity in dependency analysis of subordinate clauses has been one of the major causes of the failure of long sentence analyses.As an information-theoretic and constraint-based framework, Head-driven Phrase Structure Grammar ( Pollard and Sag, 1994) is well-suited for a formal treatment of HCF that simultaneously captures generalization in syntax and semantics as well as interactions between them. Building on the work by Gunji (1994) and Ohtani et al. (2000), we propose a head-driven account of HCF, which introduces non-trivial extensions to HPSG and makes it possible to construct a practical parser.The organization of this paper is as follows: Sec. 2 explains our extension of HPSG. Sec. 3 concerns adjacency of predicates referring to causatives. Sec. 4 treats the adjunction of subordinate clauses and proposes a formalization of HCF in HPSG framework. Sec. 5 states concluding remarks. The most perspicuous phenomenon that demonstrates the head-final property of Japanese is the sentence-final clusters of auxiliary verbs such as (s)ase and conjunctive particles nagara. Although they are related to the hierarchical clause structure that has been discussed in the literature of Japanese linguistics, the hierarchical complexities have also been one of the major causes of the failure of long sentence parsing in the field of natural language processing. To overcome this parsing problem, we develop Japanese Phrase Structure Grammar, NAIST JPSG, which is a partial implementation of ideas from not only recent developments in Head-driven Phrase Structure Grammar but also Minami&apos;s four-level syntactic/semantic hierarchy. We devote our discussion to the analysis of hierarchical clause formation, pseudo-lexical treatment of causatives and regulation of co-occurrence of subordinate clauses, with main focus on their specific lexical information of the sentence-final clusters. Causative constructions exhibit properties of simple/complex thematic relation in spite of their morphologically simple status. A clause can be embedded as subordinate clause if and only if the clause is a member of the same or lower level in Minami&apos;s sense than the main clause belongs to. These hierarchical clause phenomena can be explained by introducing lexical description and general mechanism assumed in JPSG.
The Japanese Internally-Headed Relative Clause as a Marked Head-Complement Structure This paper claims that IHRC should be defined as a special type of non-head daughter of a headcomplement structure. To be more exact, it is claimed that IHRC is defined in the feature specification of the predicate that takes the complement. I deny that IHRC in isolation designates a certain entity (or a target). Several motivations invite this claim. The structure of IHRC in Japanese is synchronically and diachronically in close relation with other constructions which are all characterizable in terms of different argument structures of the main verb.' Besides, the IHRC has no syntactic marker in itself indicating as such; IHRC in isolation can be interpreted only as denoting an event. IHRC interpretation obtains only when it occurs as an argument of a certain main predicate.Previous analyses of IHRC have preferred defining IHRC in its own terms, i.e., that it denotes an entity internal to the IHRC, for the sake of maintaining the locality of information.' A simple main predicate is not supposed to refer to an entity embedded in the complement clause. I draw on Yoon's (1993) concept of R-relation (which of course needs revision for IHRC) to circumvent the problem. That is, the main predicate does not directly refer to a specific entity of the complement clause; it refers to the event denoted by the clause, which is indirectly associated with the entity.3Undeniably, IHRC has a peculiar structure. It is characterized by the apparent syntacticsemantic discrepancy; IHRC apparently involves a clausal complement, which, however, designates not the event denoted by the clause but a certain entity participating in the event. Besides, IHRC in Japanese is almost notorious for its elusiveness as a grammatical construction. Although Kuroda's 1 Here and throughout this paper I use the term construction in a non-technical sense. It is paraphrasable as "a pattern or class of structure" 2 Kim's (1999) analysis successfully accommodates the indeterminacy of entity and event readings of IHRC(/clausal complement), but the analysis has two drawbacks. First of all, the analysis fails to explain the cases of implicit target; secondly, it implies the semantic equivalence between IHRC (of entity reading) and EHRC, which actually is not the case. The semantic difference between IHRC and EHRC is particularly evident when the target is modified by, quantifiers and certain types of other modifiers (Shimoyama 1999). In other words, Kim's (1999) analysis licenses IHRC (of entity reading) as a full-fledged relative clause, and therefore fails to capture any peculiarity of IHRC as a relative clause. 3 This actually makes IHRC a misnomer.   seminal work called attention to IHRC as a special class of relative clauses nearly a quarter of a century ago, there still is a controversy whether IHRC is an independent construction or not. In Modern Japanese, IHRC is superficially non-distinct from clausal complement structures, and some of the IHRC are indistinguishable from certain adverbial clauses. Some maintain that IHRC is just an adverbial clause with empty categories as shown below. The data of IHRC provide no solution to the controversy. The data are documented in novels and in other publications, and yet they are far from being unanimously accepted. Some find them awkward at best, some regard them as cases of sloppy wording, and some reject the data across the board.A constraint-based phrase structure grammar is a system to generate (or license) all and only grammatical structures of a given language. A structure is licensed if it satisfies all the relevant constraints. A licensed structure is a legitimate structure of the language. There is no difference, conceptual, functional, or whatever, among the licensed structures. The question is: if IHRC is a legitimate structure, then why is it so hard to obtain?Generally, the scarcity of acceptable data is ascribed to the amount of constraints. Passives are generally difficult to obtain in Japanese, for example. This is considered due to semantic constraints (such as animacy) on subjects and on verbs to be passivized, as well as to pragmatic constraints involving point of view and other concepts. Notice, however, that such interplay of constraints does not necessarily lead to the negation of the construction per se. If some passives are not readily available, nobody questions the legitimacy of passives as a class of structure in Japanese. Admittedly, IHRC is under many semantic and pragmatic constraints, and it is possible to explain the low acceptability in terms of such constraints. However, IHRC is peculiar in that the scarcity of acceptable data is interpreted as a reason to doubt IHRC as a construction. In this paper, I would also like to explain the elusiveness of IHRC without having recourse to semantic or pragmatic constraints. I do not claim that the elusiveness indicates the fuzziness of the boundaries between IHRC, clausal complement and adverbial clause; the three are distinct classes of structure. I assume instead that the elusiveness reflects two fundamental traits of IHRC: (1) The structure of IHRC is literally close to that of the other two, and (2) IHRC is an extremely marked structure. What follows explores how these two traits really lead to the elusiveness. This paper proceeds in the following way. Section 2 overviews the problems of IHRC from synchronic and diachronic points of view. It will be shown that IHRC is synchronically an elusive and diachronically an unstable construction. Section 3 proposes a new analysis of IHRC as a transient and marked structure, and considers the consequences of the proposal. Section 4 concludes the discussion. This paper proposes a new analysis of Internally-Headed Relative Clauses (IHRC) in Japanese in Head-driven Phrase Structure Grammar (HPSG) which defines IHRC in terms of the feature specification of the main predicate. IHRC is characterized as a special type of head-complement structure, in which the predicate has highly marked ARG-ST and VAL features corresponding to IHRC. Motivated by diachronic and synchronic data, the proposed analysis also suggests a way to accommodate the constructional elusiveness of IHRC. 1 Introduction This paper claims that IHRC should be defined as a special type of non-head daughter of a head-complement structure. To be more exact, it is claimed that IHRC is defined in the feature specification of the predicate that takes the complement. I deny that IHRC in isolation designates a certain entity (or a target). Several motivations invite this claim. The structure of IHRC in Japanese is synchronically and diachronically in close relation with other constructions which are all characterizable in terms of different argument structures of the main verb.&apos; Besides, the IHRC has no syntactic marker in itself indicating as such; IHRC in isolation can be interpreted only as denoting an event. IHRC interpretation obtains only when it occurs as an argument of a certain main predicate. Previous analyses of IHRC have preferred defining IHRC in its own terms, i.e., that it denotes an entity internal to the IHRC, for the sake of maintaining the locality of information.&apos; A simple main predicate is not supposed to refer to an entity embedded in the complement clause. I draw on Yoon&apos;s (1993) concept of R-relation (which of course needs revision for IHRC) to circumvent the problem. That is, the main predicate does not directly refer to a specific entity of the complement clause; it refers to the event denoted by the clause, which is indirectly associated with the entity.3 Undeniably, IHRC has a peculiar structure. It is characterized by the apparent syntactic-semantic discrepancy; IHRC apparently involves a clausal complement, which, however, designates not the event denoted by the clause but a certain entity participating in the event. Besides, IHRC in Japanese is almost notorious for its elusiveness as a grammatical construction. Although Kuroda&apos;s 1 Here and throughout this paper I use the term construction in a non-technical sense. It is paraphrasable as &quot;a pattern or class of structure&quot; 2 Kim&apos;s (1999) analysis successfully accommodates the indeterminacy of entity and event readings of IHRC(/clausal complement), but the analysis has two drawbacks. First of all, the analysis fails to explain the cases of implicit target; secondly, it implies the semantic equivalence between IHRC (of entity reading) and EHRC, which actually is not the case. The semantic difference between IHRC and EHRC is particularly evident when the target is modified by, quantifiers and certain types of other modifiers (Shimoyama 1999). In other words, Kim&apos;s (1999) analysis licenses IHRC (of entity reading) as a full-fledged relative clause, and therefore fails to capture any peculiarity of IHRC as a relative clause. 3 This actually makes IHRC a misnomer. 317
Distributional Properties and Endocentricity of English Gerunds English gerunds generally manifest nominal properties in their external distribution, but they have verbal characteristics construction-internally. Theses dual properties of the gerund phrases (GPs, hereafter) pose recalcitrant problems with traditionally entertained basic linguistic assumptions such as endocentricity and possible syntactic or lexical categories. The noun-like distribution of gerunds within a sentence necessitates the top node of the phrase to be labeled as a nominal category while the construction-internal aspects of the phrases call for setting up a kind of verbal categories inside. In what follows I will address the above issues based on the taxonomy presented in (1).(1) Nominal GP a. Brown's deft painting of his wife is impressive. b. The tapping on the floor was very irritating. c. John's continuous tapping on the floor annoyed everyone. d. There is no difference any more, no checking of passports, Verbal GP e. Brown's deftly painting his wife is very impressive. f. Mary is well known for singing songs (gracefully). g. John dislikes Linda trying to tell him a lie. h. John was angry at Linda's trying to lie to his brother. i. His not having left yet could be a sign of his reluctance.Many researchers concentrate on verbal type gerunds that are exemplified by ( 1 e-i) and tend to exclude the nominal type listed in (la) through (1d). This paper will attempt to provide a comprehensive analysis to the both types of gerunds observing endocnetricity, and will account for the distribution problem without creating a separate syntactic category'.As is pointed out by Pullum (1991) and many others, the two types of gerunds have some of the characteristics as shown in (2) (2) Properties unique to verbal GPs• can take an ordinary NP objects as complements -(le), (10, (1g)• can contain adverbial modifiers --(le), (10, (1i) • can contain negating particle not --(li) (cf. (1d)) • allow some auxiliaries like 'have' --(1i) • The logical subject can be a genitive or an accusative NP Properties unique to nominal GPs • can take PP objects as complements • can contain adjectival modifiers • contain determiners including no • cannot contains any auxiliaries2• the logical subject can be a genitive NP --(la), (lb), (lc) -(1c) --(lb), (1d) This paper attempts to resolve problems involving distributional properties and endocentricity of English genrund constructions. This paper identifies two syntactic types in English gerunds: nominal gerunds and verbal gerunds. This distinction is based on syntactic and semantic characteristics of each type and is intended to account for the external distribution and endocentricity of the construction. Treating verbal gerunds syntactically as verb categories, this paper proposes that there are cases of control involving gerunds as in infinitives. Specifically, this paper claims that control is not limited to gerundive complements of verbs but can be extended to prepositional complements. This proposal not only resolves the distributional properties of the gerund construction but also captures syntactic parallelism observable between gerunds and to-infinitives in English.
Robust N-gram Based Syntactic Analysis Using Segmentation Words Almost all stochastic syntactic parsers are based on context free grammars (especially, probabilistic context free grammar) ( Bell et al. (1999); Charniak (1997); Liu and Soo (1994)) or its extensions, e.g. HPSG ( Kanayama et al. (1999)) or LFG ( Bod and Kaplan (1998)). Several researchers have tried to acquire syntactic rules from corpus automatically ( Zhou and Ren (1999); Chelba and Jelinek (1998); Shirai et al. (1997); Pereira and Shabes (1992)). There are several issues in automatic rule acquisition:(1) Erroneous inputs, (2) The size of the training corpus, (3) The number of syntactic rules.A morphological analyzer normally passes words to a syntactic parser together with morphological information, like parts of speech, inflection and so on. Though morphological parsers have achieved high performance, using statistical information gathered from large corpora, syntactic parsers must allow for erroneous information about words, especially word segmentation error and tagging error. The available corpora with syntactic information are usually smaller than the corpora with morphological information and are not sufficient to acquire syntactic rules directly. In addition, many syntactic rules consisting of parts of speech and words are generated. It is necessary to generalize rules to process various sentences by sacrificing the performance. Human-made syntactic rules are only a portion of the rules needed in such a grammar. We think that information in the morphological corpus should be used for syntactic analysis.Introducing language-dependent rules is a key to solving the above issues. But if we add such ad hoc rules, the performance of parser would improve only for the sentences based on which the rules were derived. We think that a parser should guarantee that it would process all sentences. To achieve this, the grammar model should not be constructed from a priori knowledge alone.This paper proposes a method of handling these issues by using N-gram information of word or part of speech:(1) N-gram information is collected from the corpus, which is generated by a morphological parser. Tagging errors and word segmentation errors are accounted in the N-gram information. (2) The corpus is large enough for N-gram information to be very reliable. (3) N-gram information calculated using a linear interpolation method which estimates N-gram information from under N-gram information approximates occurrence probabilities of various sentences.To apply N-gram information to syntactic analysis, we describe here a model of syntactic analysis, some approximations of real world information and experimental results. We describe an N-gram based syntactic analysis using a dependency grammar. Instead of generalizing syntactic rules, N-gram information of parts of speech is used to segment a sequence of words into two clauses. A special part of speech, called segmentation word, which corresponds to the beginning or end symbol of clauses is introduced to express a sentence structure. Segmentation words for each clause were learned using the hill climbing method and a small bracketed corpus. Experimental results for Japanese sentences showed that N-gram based syntactic parser achieved 72.2% recall, which is about the same level of performance as a probabilistic context-free grammar based parser with human-made language-dependent information.
Recursive Top-down Fuzzy Match: New Perspectives on Memory-based Parsing With more and more computerized corpora becoming available, the development of techniques which allow to compile a corpus automatically into a running NLP-tool has been established, together with the creation of such corpora, as a central issue in NLP. Corpora with syntac= tic annotations are commonly referred to as treebanks and are developed for more and more languages. One way to benefit from these treebanks is to compile them into parsers for that language, which, as first attempts have shown, tend to outperform parsers working on hand-written rules. Nevertheless, these grammar-derived parsers share with their hand-crafted counterparts a number of drawbacks such as their inefficiency and unrecoverable generalizations. Trying to avoid these drawbacks we investigate an alternative parsing strategy, using for training and testing the Chinese CKIP-treebank ( Chen et al., 1999). With treebanks becoming available for more and more languages, their usage for the development of natural language parser has become a topical issue in NLP. This paper tries to give a new spin to this this stream of research, proposing a new direction in corpus-based parsing. Contrary to competitive approaches, this approach does not involve a chart parser which reassembles phrases extracted form a treebank. Instead, parsing proceeds via the extraction of example trees from the treebank using fuzzy pattern matching techniques. A set of adaptation rules modify the extracted example trees so as to produce the best possible parse given the current set of examples.
  
Robust Syntactic Annotation of Corpora and Memory-Based Parsing  This talk provides an overview of current work in my research group on the syntactic annotation of the Tubingen corpus of spoken German and of the German Reference Corpus (Deutsches Referenzkorpus: DEREKO) of written texts. Morpho-syntactic and syntactic annotation as well as annotation of function-argument structure for these corpora is performed automatically by a hybrid architecture that combines robust symbolic parsing with finite-state methods (&quot;chunk parsing&quot; in the sense Abney) with memory-based parsing (in the sense of Daelemans). The resulting robust annotations can be used by theoretical linguists, who are interested in large-scale, empirical data, and by computational linguists, who are in need of training material for a wide range of language technology applications. To aid retrieval of annotated trees from the treebank, a query tool VIQTORYA with a graphical user interface and a logic-based query language has been developed. VIQTORYA allows users to query the treebanks for linguistic structures at the word level, at the level of individual phrases, and at the clausal level.
A Simple Syntax for Complex Semantics The main purpose of this work is to lay a syntactic basis for computational semantics. Despite the complexity of computational semantics, it is assumed that its underlying syntax must be simple. A simple syntax is advocated for complex semantics.The task of computational semantics cannot be but complex because its ultimate aim is to model how human-machine communications are carried out by means of natural language. That of syntax can, however, be made simple at both the structural and the procedural level. The proposed syntactic module KoSyn, for instance, is implemented to be structurally simple, only consisting of a set of concatenation rules with some constraining subrules like valency check or parameter binding. It is also made procedurally simple because it processes linguistic input in a time-linear manner and builds up semantic data structures incrementally. For this reason, the present talk will aim at showing how to minimalize the task of syntax by constructing the Korean syntactic module called KoSyn for testing and demonstration purposes. It is designed to generate a network of semantic data structures without deriving intermediate syntactic trees and also to represent them in a format that database-theoretic semantics can recognize and manipulate for the purpose of interpretation, production, and inference. This presentation will, however, be restricted to the discussion of representational issues for semantics.By analyzing a fragment of Korean, this presentation will show in detail how KoSyn generates semantic structures for simple and conjoined sentences and also for adnominal clauses. It will also show how valency checking works for building argument structures, how nominal indexes are distributed over conjoined sentences, how events are temporally anchored, and how parameters are bound in adnominal constructions. All these processes are shown to be carried out linearly without backtracking. As part of a long-ranged project that aims at establishing database-theoretic semantics as a model of computational semantics, this presentation focuses on the development of a syntactic component for processeing strings of words or sentences to construct semantic data structures. For design and modeling purposes, the present treatment will be restricted to the analysis of some problematic constructions of Korean involving semi-free word order, conjunction and temporal anchoring, and adnominal modification and antecedent binding. The present work heavily relies on Hausser&apos;s (1999, 2000) SLIM theory for language that is based on surface compositionality, time-linearity and two other conditions on natural language processing. Time-linear syntax for natural language has been shown to be conceptually simple and computationally efficient. The associated semantics is complex, however, because it must deal with situated language involving interactive multi-agents.. Nevertheless, by processing input word strings in a time-linear mode, the syntax can incrementally construct the necessary semantic structures for relevant queries and valid inferences. The fragment of Korean syntax will be implemented in Malaga, a C-type implementation language that was enriched for both programming and debugging purposes and that was particluarly made suitable for implementing in Left-Associative Grammar. This presentation will show how the system of syntactic rules with constraining subrules processes Korean sentences in a step-by-step time-linear manner to incrementally construct semantic data structures that mainly specify relations with their argument, temporal, and binding structures. 2
Identification of Chinese Personal Names in Unrestricted Texts Automatic identification of Chinese personal names in unrestricted texts plays a key role in Chinese NLP tasks such as word segmentation. Personal name identification in Chinese involves many different issues that are not found in similar tasks in English. Despite extensive research in the issue in recent years ( Wang et al. 1992, Song et al. 1993, Sun et al. 1995), many NLP applications still suffer from the weakness of current available performance. The paper has four sections: Section 1 illustrates some IT applications in which Chinese personal name identification plays important roles, to promote an appreciation of the significance of the research. Section 2 introduces the structure of Chinese personal names, and Section 3 discusses the relevant processing strategies. Lastly, we will highlight the significant differences of Chinese personal names between Beijing and Hong Kong in Section 4, showing the added difficulty caused by variations in different Chinese communities. 2 The IR systems, including search engine on WWW, retrieval system of digital library, text mining etc., search and retrieve the requested information from large bodies of data stored in electronic form. With the advent of the Internet, search engine techniques have become most widely influential. However; the effectiveness of current systems is not satisfactory especially for retrieving Chinese web sites/pages. We have examined how personal names interfere with the correct retrieval of relevant results in Sinai, a popular Chinese search engine, and Google [Big5 Chinese] 2. For instance, suppose we want to get some information about "EN:" (lieutenant general) from WWW, and enter that word as a query to. Sina, then some of the "relevant" web pages will be returned, as in (1)-(4):(1)Mitigt4111 (2)AiktiStl i. .fr 1A-11-SAMPI TIXti (0.10These webpages are actually irrelevant to "lieutenant general" although the string "414" is embedded.The string "4:1" in the two sentences cannot be analyzed as the word meaning "lieutenant general". In (1) and (2), the character "4" is the given name, and 1:" an adverb by itself meaning "will" for referring to the future. In (3) and (4), "11:1:" forms the two-character given name. The misinterpretation would have been avoided if the search engine could detect the presence of proper names and segment the words accordingly. Automatic identification of Chinese personal names in unrestricted texts is a key task in Chinese word segmentation, and can affect other NLP tasks such as word segmentation and information retrieval, if it is not properly addressed. This paper (1) demonstrates the problems of Chinese personal name identification in some IT applications, (2) analyzes the structure of Chinese personal names, and (3) further presents the relevant processing strategies. The geographical differences of Chinese personal names between Beijing and Hong Kong are highlighted at the end. It shows that variation in names across different Chinese communities constitutes a critical factor in designing Chinese personal name identification algorithm.
Mismatches in Korean Copula Constructions and Linearization Effects  One main complexity of the copula constructions concerns a mismatch between morphology and syntactic constituency: the copula seems to form a morphological unit with the immediately preceding element, whereas in terms of syntax the copula appears to take this as its syntactic complement. In capturing such mismatches, we show that the copula is treated as an independent verb at the level of tectogrammatical structure (or syntax tree), whereas as a bound morpheme at the level of phenogram-matical structure (or domain tree), in terms of Dowty 1992 (or Reape 1994). This paper, adopting the notion of DOMAIN in HPSG, shows that copula constructions are a subtype of compacting-constructions. These constructions compact the domain value of the copula and that of its preceding element together into one domain unit, eventually making it inert to syntactic phenomena such as scrambling, deletion and pro-form substitution. This construction-based approach provides a clean analysis for the formation of the copula construction and related phenomena.
Heuristic-based Korean Coreference Resolution for Information Extraction Information extraction(IE) systems take texts containing natural language as input and produce database templates relevant to a particular application. IE system must create templates describing the relevant entities that are reported on. This requires determining when two or more templates describe the same entity, as templates created from conferencing words to be merged. Thus, it is difficult to resolve coference in order to extract more reliable information. Results of coreference resolution are used for the clue of template generation. In coreference resolution, there are two kinds of problems such as anaphora resolution and name aliases recognition. The name aliases could be resolved by lexical pattern matching or synonym dictionary (Huyck, C. (1998). Fukumoto, J., Masui (1998)). However, the anaphora resolution has more complexities of natural language. It has been studied conservatively in the discourse part of natural language processing. Recently, several proposals addressed that using limited knowledge is better than using heavy linguistic and domain knowledge (Lappin, S. and Leass, H. (1994). Baldwin, F. B. (1995). Mitmov, R. (1998)).This paper presents the heuristic-based approach with limited knowledge such as pattern rules, preference rules, and conditional rules. The resolution procedure is to find antecedents and then to evaluate the weight of antecedents with the heuristics. In this paper, we focus on the anaphora resolution. In Korean, an anaphora consists of 'pronoun' and 'demonstrative pronoun + noun phrases' .Gaizauskas, R. Azzam, S., Huyck, C., Mitchell, B., Cunningham, H. and Wilks, Y. (1998). Lin, D. (1998). Fukumoto, J., Masui, F., Shimohata, M., and Sasaki, M. (1998). Aone, C., Halverson, L., Hampton, T., and Ramos-Santacruz, M. (1998)). With a view of the methodology, it could be divided with rule-based approaches using limited knowledges such as lexical patterns (Huyck, C. (1998). Fukumoto, J., Masui, F., Shimohata, M., and Sasaki, M. (1998)) and heuristics (Lappin, S. and Leass, H. (1994). Baldwin, F. B. (1995). Mitmov, R. (1998)), knowledge based approach using semantic network (Yangarber, R. and Grishman, R. (1998). Humphreys,K., Gaizauskas, R. Azzam, S., Huyck, C., Mitchell, B., Cunningham, H. and Wilks, Y. (1998)), and Hybrid approaches which integrate knowledge based and machine learning approaches (Urbanowicz, R. A. and Nettleton, D. J. (1998). Lin, D. (1998)). Another is statistical approach( Kehler, A. (1997)). In-the case of coreference resolution, the statistical approach is rare since the phenomenon of coreference is generally inter-sentential problem than intra-sentential thing. Thus, it makes the statistical modeling of coreference very difficult.The approach of using limited rules does not depend on the massive linguistic knowledge or domain knowledge but it only depends on the simple heuristics. The general resolution procedure is the selection of antecedent candidates, the ranking of the candidates, and the decision of the candidate of an anaphoric word. In each step, it uses the empirical heuristics. The typical heuristic-based approach is the dynamic coreference model generation such as (Lappin, S. and Leass, H. (1994)). It divides the antecedents into the intra and inter sentential types, and evaluates the salience factors of the antecedents. The approach shows 86% precision in the computer manual domain.Another heuristic-based research is the file card approach. It makes first discourse model of the anaphora and then resolves the model by the file card operation such as antecedents grouping, deleting and weighting with the morphological and syntactic analysis. It shows the 73% precision in the Wall Street Journal (Baldwin, F. B. (1995)). The best report, 89.7% precision, is the approach that uses the antecedent indicator which is antecedents weighting types( Kehler, A. (1997)). It depends on the heuristics and syntactic pattern of the anaphora contexts. However, in MUC, the heuristic approaches did not report affirmative result (Huyck, C. (1998). Fukumoto, J., Masui, F., Shimohata, M., and Sasaki, M. (1998)).According to the previous researches, the heuristic-based approaches presented good results. Therefore, we use the approach. Furthermore, since a coreference resolution is the part of information extraction and the independence and conciseness of the module is mostly important, the approach of the using limited knowledge is appropriate for the information extraction system. anaphora words detection using lexical information using lexical pattern and name entity type(semantic information) antecedents candidates detection comparing anaphora and antecedents each other with salience factors, then give weight to each antecedent select the most weighted antecedent not the antecedent. Only the half of the cases have the antecedent. Therefore, we should divide it into the formal and informal phenomena. The coreference resolution depends on the name entity since the resolution procedure is the part of the information extraction system and is following the name entity recognition step. Therefore, the antecedents of the coreference are the name entities. In table 1, the formal coreference is the ordinary case, but the informal coreference could not be resolved with the formal approach. Thus, the informal coreference resolution approach is different with the formal approach. In this paper, we focus on the formal phenomenon, but we consider the informal coreference resolution, too. The information extraction is to delimit in advance, as part of the specification of the task, the semantic range of the output and to filter information from large volumes of texts. The most representative word of the document is composed of named entities and pronouns. Therefore, it is important to resolve coreference in order to extract the meaningful information in information extraction. Coreference resolution is to find name entities co-referencing real-world entities in the documents. Results of coreference resolution are used for name entity detection and template generation. This paper presents the heuristic-based approach for coreference resolution in Korean. We constructed the heuristics expanded gradually by using the corpus and derived the salience factors of antecedents as the importance measure in Korean. Our approach consists of antecedents selection and antecedents weighting. We used three kinds of salience factors that are used to weight each antecedent of the anaphor. The experiment result shows 80% precision.
On Negative Imperatives in Korean  In this paper, we address two questions concerning negative imperatives in Korean:. (i) what is the morpho-syntactic nature of mal in negative imperatives?; and (ii) why is it impossible to form negative imperatives with short negation an? We will argue that the clause structure of imperatives include a projection of deontic modality and a projection of imperative operator encoding illocutionary force, and that mal is a lex-icalization of long negation and deontic modality. We then propose that a negative imperative with short negation is ruled out because such construction maps onto incoherent interpretation which can be spelled out as I direct you to bring about a negative state or a negative event.
Penn Korean Treebank: Development and Evaluation With growing interest in Korean language processing, numerous natural languages processing (NLP) tools for Korean, such as part-of-speech (POs) taggers, morphological analyzers , parsers, have been developed. This progress was possible through the availability of large-scale raw text corpora and POS tagged corpora (ETRI, 1999;Yoon and Choi, 1999a;Yoon and Choi, 1999b). However, no large-scale bracketed corpora are currently available to the public, although efforts have been made to develop guidelines for syntactic annotation ( Lee et al., 1996; Lee et al., 1997). As a first step towards addressing this issue, we built a 54-thousand-word s Korean Treebank using a phrase structure annotation. At the same time, we also developed annotation guidelines based on the morpho-syntactic phenomena represented in the corpus, over the period of Jan. 2000 andApril 2001. The corpus that we used for the Korean Treebank consists of texts from military language training manuals. These texts contain information about various aspects of the military, such as troop movement, intelligence gathering, and equipment supplies, among others. This corpus is part of a Korean/English bilingual corpora that was used for domain specific Korean/English machine translation project at the University of Pennsylvania. One of the main reasons for annotating this corpus was to train taggers and parsers that can be used for the MT project.In this paper, we first discuss some issues in developing the annotation guidelines for POS tagging and syntactic bracketing. We then detail the annotation process in §3, including various methods we used to detect and correct annotation errors. §4 presents some statistics on the size of the corpus, and §5 discusses the results of the evaluation on the Treebank. This paper discusses issues in building a 54-thousand-word Korean Treebank using a phrase structure annotation, along with developing annotation guidelines based on the morpho-syntactic phenomena represented in the corpus. Various methods that were employed for quality control and the evaluation on the Treebank are also presented.
A Deterministic Method for Structural Analysis of Compound Words in Japanese A sequence of words may assume a single syntactic function in English and the same is true in Japanese. Machine translation system, for instance, consists of three words machine, translation and system. Its equivalent in Japanese, MA F IR 3 5-1A, is a compound word with three constituent words, OW (machine), (translation), and (system). It is a single concept and functions as a noun.Both English and Japanese face problems in processing such a sequence of words. A problem involved in English is in identifying the sequence of words as a syntactic as well as semantic unit. It is easy to find a compound word in Japanese but we have difficulties in segmenting it into constituent words as well as determining its syntactic or semantic structure among the constituents.Finding its constituents and determining the structure of a compound word is an important process in constructing practical natural language systems for machine translation, information retrieval, text summarization, etc. For the compound word fc H gait (the former president of Bank of Japan), for instance, there are six lexically possible segmentations: 7-E/ H I iei g , 7GI 19 /, /en, H /*tit&amp; H /WA, 7G H /e/It and 5---C H / ;F /glia, the correct segmentation here being 7c(former)/ H (Bank of Japan)/a ot (president).Standing alone, the word segmentation is useful in many areas of natural language processing. However, we should know how the constituents in a compound word are combined to make the meaning, say, for its translation into foreign languages. 7G/19 "d /km has two probable dependency structures: (CAM H )(a4 ))) and (((A)( H ))(M)). The former leads to the meaning of the former president of Bank of Japan and the latter the president of former Bank of Japan. The problem is how can we arrive at the structure, ((i-E)(( H 1A1F)(atit))).In this paper we offer a method for analyzing the structure of compound words in Japanese. In Section 2 describes some previous work done for a compound word on the identification of constituents and determination of its structure. We propose a method and subsequently an algorithm in Section 3 for analyzing the structure of compound words using the strength of association measures among the constituent words. In Section 4 conducts experiments for our method and others, and compares the results. Finally, we conclude the paper with some remarks in Section 5. Structural analysis of compound words is necessary and an important process in natural language processing. Proposed here is a corpus-and statistics-based method for the structural analysis of compound words in Japanese. We determine the structure of a compound word by using Internet corpus and calculating the strength of word association among its constituent words. Experiments with 5, 6, 7, and 8 kanji compound words show that our method works well and its performance is better than those of other comparable studies.
Implicit Adjuncts The Cases of Degree Modifiers in Japanese and English  
Type Construction of Nouns with the Verb ha-&apos;do&apos; Much of the current research on sure 'do' in Japanese and ha-'do' in Korean has been focused on its light verb function when it combines with verbal nouns since Grimshaw &amp; Mester (1988). Although many papers have tried to uncover the nature of the Korean verb ha-'do', they still have difficulty in defining the lexical semantic property of this verb. Recent studies of the verb ha-including Jun (2001), Im &amp; Lee (2001) touch on the issues of substantival nouns with ha-'do'. In this paper, we account for how the verb ha-' do' is directly combined with substnatival nouns or complex type nouns.Specifically, we first list the nouns used together with the verb ha-'do' in chapter 2. The nouns are classified into verbal nouns, substantival nouns, and dot objects. In chapter 4, we accounted for the combination of the verb ha-`do' with substantival nouns or complex type nouns. A substantival noun undergoes a type shifting operation by type coercion of the governing verb ha-'do'. Type pumping operation makes possible that ha-selects a proper type out of multiple types of a dot object.Our explanation is based on the Generative Lexicon (GL) Theory (Pustejovsky, 1995(Pustejovsky, , 2001). This paper aims to give an explanation of the combination of certain nouns and the verb ha-&apos;do&apos;. Although the verb ha-&apos;do&apos; normally takes an event type argument, it takes some substantival nouns such as paiolin &apos;violin&apos;, umsikcem &apos;restaurant&apos;, and so on. A substantival noun undergoes type shifting, because the governing verb ha-&apos;do&apos; coerces an entity type noun to an event reading, taking missing information from the qualia of the entity type noun. In addition, some nouns like ppallay &apos;launch-3e are dot objects. The verb taking a dot object selects a proper type between multiple subtypes of the dot object. Type pumping operation makes that selection possible.
An Event-Based Semantics for Japanese Emphatic Particles  Following Herburger (2000), I will develop an event-based semantics for Japanese emphatic particles which can address the issue of the mechanism of association with focus involving the emphatic particles. The proposed semantics makes use of Herburger&apos;s three key ideas: events as basic entities, decomposition of predicates into subatomic formulas, and separation of backgrounded and foregrounded information.
Small Clauses and Default Case The purpose of this paper is to compare secondary predication constructions such as small clause complements, resultatives, and depictives in English and Korean and argue that these two typologically different languages employ different modes of satisfying the Case Filter (Chomsky 1981) in case of the subjects of small clauses. More specifically, we argue that the subject of a small clause in English is .Accusative Case-marked by the higher governing verb, while that in Korean is Nominative Case-marked by default. This paper compares secondary predication constructions such as small clause complements, resultatives, and depictives in English and Korean. It argues that these two typologically different languages employ different modes of satisfying the Case Filter with regard to the Case of the subjects of small clauses. More specifically, it is argued that the subject of a small clause in English is Accusative Case-marked by the higher governing verb, while that in Korean is Nominative Case-marked by default.
Edge-Integrity and the Syllable Structure in Korean Syllable structure is regulated by several different constraints. One of them is a purely prosodic requirement, and the asymmetry between onsets and codas in the syllable is well known in that respect: syllables require onsets while they permit codas at most (Ito 1986(Ito , 1989, McCarthy and Prince 1993a, Blevins 1995 and references therein). All else being equal, the best syllable structure for a given sequence N i CV2/, is (la) with the intervocalic consonant in the onset of 62. Another constraint regulating syllable structure is the integrity of morphological structure: given a morphologically complex sequence N iC+V2/, where + is a morphological boundary, the structure in (lb) is chosen over the one in (la): prosodic structure respects morphological structure. Everything else being equal, no evaluation yields (lc) as optimal. If syllable structure conditions dominate the morphological integrity, (la) is optimal structure. If the ranking is reversed such that the morphological structure takes a priority over the syllable well-formedness conditions, (lb) is optimal. The representation itself is more complex than the other two, too. However, (1c) satisfies both sets of constraints to a certain degree: the intervocalic consonant serves as an onset of a2 : (lc) fares better than (lb) with regard to ONSET requirement. It also respects morphological structure and its integrity by being linked with the morpheme with which it is affiliated: the structure (lc) satisfies morphological integrity better than (la) in this sense.In this paper I present a case study in which the ambisyllabic structure in (lc) is forcefully chosen at the cost of simple representation to satisfy both sets of requirements: the so-called overapplication of Coda Neutralization in Korean. I will show that the conflicting constraints on syllable structure from prosody and morphology are resolved by finding a compromise. In the optimality theoretic analysis, it will be shown that the ambisyllabic structure in (1c) also provides us with a means to avoid the problem of phonological opacity, which arises from the interaction of phonological processes and the so-called resyllabification. I will propose a constraint, EDGE-INTEGRITY as a critical constraint regulating morphological requirements of integrity at the edges of morphological constituents. Its crucial role will be further supported by the analysis of Consonant Copy in the Jeju dialect in Korean. The so-called overapplication of Coda Neutralization in Korean, the occurrence of a neutralized consonant in a non-neutralizing environment, is often considered as evidence for serial derivation. In this paper I propose that the neutralization effect at surface is not a result of a phonological process at an intermediate level in serial derivation, but due to a constraint requiring the integrity of the morphological constituent: EDGE-INTEGRITY. It is argued that this is not reducible to an alignment constraint, but a genuine faithfulness constraint on the edge of a morphological constituent. The putative opacity related with the coda neutralization is shown to be an epiphenomenon arising from the ambisyllabic representation of a consonant at a morphological juncture, satisfying both EDGE-INTEGRITY and syllabic conditions. Consonant Copy in the Jeju dialect provides further evidence for EDGE-INTEGRITY, the only difference being that the conflict between syllabic conditions and EDGE-INTEGRITY is resolved by insertion of a copied consonant.
An Alignment based technique for Text Translation between Traditional Chinese and Simplified Chinese Aligned parallel corpora have proved very useful in many tasks, including statistical machine translation ( Brown et al., 1993;Wu andNg, 1995, Chen et al., 1997;Moore, 2001) and word sense disambiguation ( Chang et al., 1996;Chen and Chang, 1998).Traditional and Simplified Chinese are two Chinese writing systems that used by Chinese-speaking communities. Since their typefaces are different, foreigners always view these two languages as a family of cognate languages. Aside from differences in typeface, their encoding schemas are also different. For the conversion of text, special utilities or tools are required for mapping the correspondence between the two schemas. At present, the methods used to undertake this mapping are far from perfect. In general, a table-conversion method is used to translate between Traditional and Simplified Chinese text. There are several problems with this method. First, correspondences between Big5 (Traditional Chinese) and GB2312 (Simplified Chinese) code schemas are not one-to-one. Thus, this method can cause mismatches in character translation. Second, the unit of processing should be words instead of morphemes since the meaning of a morpheme can be very ambiguous. Third, conventional language usage is also quite different between Traditional and Simplified Chinese. To tackle these difficulties, it thus seems wise to acquire a set of word mappings between Traditional Chinese and Simplified Chinese from parallel corpora automatically.This paper discusses the issues mentioned above, and especially focuses on Traditional and Simplified Chinese text lexical translation and a method for building a synonym thesaurus meaning-translation. The character set standards used in this paper are Big5 code for Traditional Chinese and GB2312 code for Simplified Chinese; nevertheless, these issues are code-independent. Simply categorized, there are three methods for Traditional Chinese and Simplified Chinese conversion and translation, each of which satisfies different purposes. These include code schema conversion, word translation, and semantic translation, all of which are described in the present paper. Aligned parallel corpora have proved very useful in many natural language processing tasks, including statistical machine translation and word sense disambiguation. In this paper, we describe an alignment technique for extracting transfer mapping from the parallel corpus. During building our system and data collection, we observe that there are three types of translation approaches can be used. We especially focuses on Traditional Chinese and Simplified Chinese text lexical translation and a method for extracting transfer mappings for machine translation.
Verb Pattern Based Korean-Chinese Machine Translation System Machine translation requires correct analysis of source languages, appropriate generation into target languages and a large amount of knowledge such as rules, statistics or patterns. Especially persistent and consistent knowledge acquisition and management, monotonic improvement of performance according to knowledge accumulation are the keys to machine translation. Rule based methods suffer from knowledge acquisition and consistent management. Statistical methods show no connections between the previous statistical knowledge and the new statistical knowledge and have difficulty in reflecting linguistic phenomena and peculiarities directly into knowledge. Patterns have several formats such as sentence-based patterns (Kaji Hiroyuki(1992)), phrase-based patterns (Watanabe Hideo(1993)) and collocation-based pafterns (Smadja(1996), Kevin McTait(1999)). Sentence-based patterns uses a whole sentence as a pattern and transfer the input sentence in one time. It suffers mainly from data sparseness. Phrase-based patterns can be used for both syntactic analysis and transfer. The transfer is done phrase by phrase. Collocation-based patterns are used for lexical transfer, that is, the transfer unit is a word. ETRI performed a verb-pattern based Korean-English machine translation from 1999 to 2001 and experienced strong points on the side of knowledge acquisition, consistent management of linguistic peculiarities between two languages and monotonic increase in system performance according to the number of patterns (Kim, Y.K. et al.(2001); Seo,Y.A. et al(2000) analysis directly leads to correct generation, which is very effective in machine translation between languages with quite different linguistic structures. With respect to the reusability of knowledge, verb patterns for Korean-English machine translation can be reused after just modifying the English pattern part into Chinese, thus saving the cost of knowledge construction. In section 2 we show the system overview and a simulation example translating a Korean sentence into Chinese. Verb patterns are explained in more detail in section 3 and a parser using verb patterns are described in section 4. A generation module is explained in section 5 and an evaluation is made in section 6. In section 7 we conclude our system with some remarks. This paper describes our ongoing Korean-Chinese machine translation system, which is based on verb patterns. A verb pattern consists of a source language pattern part for analysis and a target language pattern part for generation. Knowledge description on lexical level makes it easy to achieve accurate analyses and natural, correct generation. These features are very important and effective in machine translation between languages with quite different linguistic structures including Korean and Chinese. We performed a preliminary evaluation of our current system and reported the result in the paper.
A Korean Homonym Disambiguation System Based on Statistical Model Using weights The ambiguity, which is the most difficult problem in natural language processing (NLP), occurs inevitably in every analysis process including morphological analysis and syntactic analysis. Ambiguity problems occurring at some parts have been resolved to some degree. As the studies on semantic and discourse analysis are becoming active, more effort is being made to research schemes for word sense disambiguation (WSD). WSD refers to disambiguating the sense of a word contextually suitable for the sentence when it is used with two or more different meanings in sentences. [1,3,4,5,8] Studies for solving ambiguity are largely grouped into methods using dictionaries according to the pattern of learning data and ones using a corpus. In terms of methodology, there are largely methods using rules, ones using probability statistics, and ones using semantic hierarchy structure. A method using dictionaries is disadvantageous in that it is difficult to reflect the dynamic characteristic of a language [10,11], while advantageous in that it can abstract the detailed information of word senses [2,3]. To disambiguate using a corpus, a large-sized semantic tagged corpus is required. However, a high-quality corpus is hard to find, and costly and time-consuming to build. But the method reflects the dynamic characteristic of a language.In the thesis, we abstract semantic information from a dictionary definition corpus based on the method suggested in J. Huh (2000) [3] research. We will study a plan to utilize the semantic information in the homonym disambiguation model based on Bayes' theorem. [yuk-ji-reul pa-seo gang-eul nae-go bae-ga da-ni-ge han su-ro (Canal A homonym could be disambiguated by another words in the context as nouns, predicates used with the homonym. This paper using semantic information (co-occurrence data) obtained from definitions of part of speech (POS) tagged UMRD-S 1). In this research, we have analyzed the result of an experiment on a homonym disambiguation system based on statistical model, to which Bayes&apos; theorem is applied, and suggested a model established of the weight of sense rate and the weight of distance to the adjacent words to improve the accuracy. The result of applying the homonym disambiguation system using semantic information to disambiguating homonyms appearing on the dictionary definition sentences showed average accuracy of 98.32% with regard to the most frequent 200 homonyms. We selected 49 (31 substantives and 18 predicates) out of the 200 homonyms that were used in the experiment, and performed an experiment on 50,703 sentences extracted from Sejong Project tagged corpus (i.e. a corpus of morphologically analyzed words) of 3.5 million words that includes one of the 49 homonyms. The result of experimenting by assigning the weight of sense rate(prior probability) and the weight of distance concerning the 5 words at the front/behind the homonym to be disambiguated showed better accuracy than disambiguation systems based on existing statistical models by 2.93%. 1 Introduction The ambiguity, which is the most difficult problem in natural language processing (NLP), occurs inevitably in every analysis process including morphological analysis and syntactic analysis. Ambiguity problems occurring at some parts have been resolved to some degree. As the studies on semantic and discourse analysis are becoming active, more effort is being made to research schemes for word sense disambiguation (WSD). WSD refers to disambiguating the sense of a word contextually suitable for the sentence when it is used with two or more different meanings in sentences. [1, 3, 4, 5, 8] Studies for solving ambiguity are largely grouped into methods using dictionaries according to the pattern of learning data and ones using a corpus. In terms of methodology, there are largely methods using rules, ones using probability statistics, and ones using semantic hierarchy structure. A method using dictionaries is disadvantageous in that it is difficult to reflect the dynamic characteristic of a language [10, 11], while advantageous in that it can abstract the detailed information of word senses [2, 3]. To disambiguate using a corpus, a large-sized semantic tagged corpus is required. However, a high-quality corpus is hard to find, and costly and time-consuming to build. But the method reflects the dynamic characteristic of a language. In the thesis, we abstract semantic information from a dictionary definition corpus based on the method suggested in J. Huh (2000)[3] research. We will study a plan to utilize the semantic information in the homonym disambiguation model based on Bayes&apos; theorem.
Analysis of Korean Predicative Verb Forms in LAG Framework  Korean predicative verb forms obligatorily denote the three categories speech level, mood and sentence type which are not handled by most of the automatic word form recognition systems for this language. These categories are marked by special endings. This paper examines predicative verb forms concentrating on the lexical description of these endings&apos; in the framework of Left-Associative Grammar (LAG). Additionally this paper suggests a system to analyse verb forms in these aspects. The results of this study have been implemented using Malaga2 and integrated into an automatic word form recognition system for Korean called KMM (Korean Malaga Morphology). 1 Theoretical Background 1.1 LA-Grammar The aspects of natural language signs are strictly &apos;time linear&apos;, that is, reflect processing in real time3. The time-linear structure of natural language is so fundamental that a speaker cannot but utter a text sentence by sentence, and a sentence word form by word form. Thereby the time-linear principle suffuses the process of utterance to such a degree that the speaker may decide in the middle of a sentence on how to continue. Correspondingly, the hearer need not wait until the utterance of a text or sentence has been finished before her or his interpretation can begin. Instaed the hearer interprets the beginning of the sentence without knonwing how it will continue. The time-linear nature of language can be very well captured by an LA-grammar. The LA-Grammar is a grammar formalism proposed by Hausser (1989a). The same way as speakers and hearers utter and understand sentences and word forms as a linear sequence, one element at a time, LAG produces and analyses sentences or word forms step by step based on the principle of possible continuations: the parts of a sentence or a word form are concatenated from the left to the right, hence the name &quot;Left-Associative Grammar&quot;. LAGs, based on the principle of successive concatenation of categorised surfaces result in the time-linear derivation order, which is cognitively adquate. The time-liearity is inherent to
Concession and Linguistic Inference This paper attempts to explain linguistic inference processes involving concession and unexpectedness. Concession and unexpectedness are claimed to be important semantic contents of English even and Korean -(la)to. In this paper, the notion concession will be approached from discourse or cognitive perspectives as well as from formal perspectives. I will review some of the claims made by a few authors involving concession in section 2. The intuitive discussions of different semantic levels and semantic relations involving concession will be the background of the formal treatment of the topic in the ensuing sections. This paper will adopt Crevels' (2000) categorization of different levels of concessive meanings, but will reject Konig's (1998) claim that concessive constructions implicates a presuppositional contents.Many formal proposals regarding even in English and -(la)to in Korean posits the scalar implicature approach following Fauconnier (1975) as in Bennet (1982) and Kim (2001). This paper will benefit form the discussions in these papers. In this paper it has been proposed that concession should be analysed as involving scalar implicatures and that an alternative set of situations have to be assumed to account for the the relative nature of likelihood of event occurrence. This paper also claims that the notion of likelihood is the basis of the corresponding pragmatic inference and a universal quantification effect. Unexpectedness, which is conceptually tied to concession, on the other hand involves the same kind of pragmatic inference but presuppose the existence of an alternative set of individuals instead of an alternative set of situations.
Morphological Passivization and the Change of Lexical-Semantic Structures in Korean Many previous studies treated passivization as a part of syntactic phenomena. But in Korean, many passive verbs are morphologically derived from root stems by affixing '4-, -hi-, -1i-, -gi-' 1 , such as`yel as`yel-ta' (to open) -`yel-li-ta' (to be opened), or ' cap-ta' (to catch) -cap-hi-ta' (to be caught), and behave as independent lexical items. I call this process the morphological passivization. But passive verbs and their roots are very closely related to each other in their semantics and morphology. There are also argument changes and aspectual changes resulting form morphologicalIn Korean, causative morphemes have the same forms as passive ones. Thus some passive verbs are similar to causative ones. But this paper will not mention about that similarity to just focus on derived passive.There is also another device for passivization, `-e cita' (to become). But this device is not morphological but syntactical. Therefore, this paper will not treat the syntactic passivization by`-by`-e cita' (to become).passivization. It is noticed that these changes have some tendency. Therefore, the argument realizations of the passive verbs can be predicted according to verbal forms.Passivization is usually treated as a syntactic process in many studies. But at least in Korean, a passive form of a verb is an independent word, but in English and many other European languages, a passive form is realized just as a past participle of a active verbs. Thus, passivization can be analyzed as a morphological process in some languages such as Korean, Japanese, while it is a syntactic process in other languages such as English. I also assume that morphologically derived passive verbs must be examined as lexical items and morphological passivization is related to their lexical-semantic structures.This paper proposes the lexical-semantic structures of the morphologically derived passive verbs in Korean, based on Pustejovsky (1995)'s Generative Lexicon Theory (GL). As well, the basic lexical-relationship between derived passive verbs and their roots will be suggested through observing and the change of the lexical-semantic structure resulting from passivization. In the following, I begin with the outline of the Lexical-Semantic Structure of GL. In this section 2, I also mention that GL's Lexical-Semantic Structure is a very useful device to represent the relationship between morphologically related lexical items. Then, in section 3 and 4, I will suggest the change of Argument Structure and Event Structure by passivization and the Lexical-Semantic Structure of the derived passive verbs and their roots.2 The Lexical-Semantic Structure of GL Pustejovsky (1995) criticizes that the Lexicon of the previous lexical semantics, such as Generative Semantics, is not enough to explain the creative use or words, the permeability of word senses, and the expression of multiple syntactic forms because it is a Sense Enumeration Lexicon and uses a lexical decomposition into a specific number of primitive. Thus, Pustejovsky (1995) suggests Generative Lexicon (GL) to represent lexical meaning by means of generative devices and to proposes a new way of viewing lexical decomposition, focusing on the compositionality of lexical semantics.Pustejovsky (1995) also represents the lexical-semantic structure with greater internal structure in order to show the compositionality and generative aspect of lexical meaning. GL's lexical-semantic structure has three substructures: Event Structure, Argument Structure and QualiaStructure. Event Structure defines the event type of a lexical item and does not characterizes only the basic event type, such as process or state, but also the subeventual structure composed of at least two subevents. Argument Structure specifies the number and type of the logical and syntactic arguments. In GL, Argument Structure has four types of arguments: true argument, default argument, shadow argument and true adjunct. From this Argument Structure, we can know that GL assumes the broad range of arguments, including even true adjunct. As Pustejovsky (1995), QualiaStructure represents the set of properties or event associated with a lexical item. Event Structure and Argument Structure are related to each other in this sub structure, Qualia.The headedness, which represents the prominency of a subevent in the Event Structure, is a generative mechanism to realize arguments syntactically and show the verbal polysemy. The enemy sank the boat.The boat sank.As we know in (1) and (2) 'sink' has only one lexical-semantic structure but two meanings and syntactic realization. If the process event, e l , has the headedness, a causative sentence (2a) is realized, whereas if the state event, e 2, is headed, an inchoative sentence (2b) is the surface sentence of 'sink'.GL also suggests the Default Causative Paradigm (DCP) in order to explain causative/inchoative alternation verbs like in (1). I think that if the verbs belong neither to direct causation nor to indirect causation, Pustejovsky (1995) classifies those verbs as DCP. The typical DCP verbs are unaccusative verbs, whether they show causative/inchoative alternation or not. So it can be inferred that the lexical-semantic structure of complex event verbs is based on DCP.Now let me think in what point GL is useful to characterize the relationship between morphologically related lexical items. First, GL's lexical-semantic structure has systematic substructures and offers an analytic tool. Thus GL's lexical-semantic representation captures similarities and differences between lexical items more concretely and more clearly than other structure. We can infer that morphologically related lexical items share some parts of the lexicalsemantic structures.Second, GL's representation indicates the relationship between form and meaning. We can assume that in Korean, root active verbs are corresponding to causative verbs in (2a), while morphological derived passive verbs are similar to inchoative verbs in (2b). Their forms, however, are different from each other, whereas English counterparts have the same forms. GL provides one representation for causative/inchoative alternation verbs in English, but two related representations for active verbs and derived passive verbs.3 The Change of Argument Structure: deletion of the agent argumentGenerally, the object argument of the active is realized as the subject of the passive and the subject of the active is mapping in to the oblique argument of the passive. That is, the agent argument is obligatory in the active, while it is optional in the passive. So passive verbs has one less true argument than active ones. Lit. "An apple was eaten by Mary."Korean passive verbs such as 'yel-li-ta' (to be opened), `inek-hi-ta' (to be eaten) do not take the oblique agent argument. (4b) and (5b) are less natural and acceptable than (4a) and (5a). In addition, it is anomalous that the oblique agent argument occurs in the question like (6a), while it is very natural in English like (4b). pungtay-ka John-eyuyhay Mary-hante kam-ki-ess-ta the band-NOM John.by Mary-DAT wind-Passive-Past-DecLit. "The bandage was wound on Mary by John."As we see in (8), if the agent argument is not a goal in the active, the dative argument with -eygey'or -hante' is just a goal, not an optional agent. So if another dative argument occurs like in (9), the sentence is ungrammatical. However, the adjunct with -ey uyhae' , which marks the agent, can be added like in (10). From these facts, I suggest that the dative argument in the passive is not an agent but a goal and that this argument is a default argument because it is syntactically optional but logically necessary.As well, sentences are ungrammatical when the dative arguments occur as we see in (11). If these sentences can be interpreted, they means that somebody (agent) does something for the dative argument. Therefore, the dative argument is not an agent. As Comrie(1981) mentioned, the passive expresses the perfective event and has a complex event structure. In the passive, the event is described from the end point. Y-S. Kim, et al (2000) and Y-S. Kim (2001) suggest that the result state is a prominent subevent and has the headedness in the passive. That is, the event of the passive verb is the right-headed event, achievement. The examples in (13) support this suggestion.(13)(a)??Mary-ka 10 pun tong-an John-eygey cap-hi-ess-taMary-NOM for 10 minutes John-DAT catch-Passive-Past-DecLit. "Mary was caught by John for 10 minutes."(b) Mary-ka 10 pun-maney John-eygey cap-hi-ess-taMary-NOM in 10 minutes John-DAT catch-Passive-Past-DecLit. "Mary was caught by John in 10 minutes."(c) *Mary-ka cemcem John-eygey cap-hi-ess-taMary-NOM gradually John-DAT catch-Passive-Past-DecLit. "Mary was caught gradually by Mary."Mary-ka keuy John-eygey cap-hi-ess-taMary-NOM almost John-DAT catch-Passive-Past-DecLit. "Mary was almost caught by John."Achievement verbs cannot occur with adverbial phrases, which modify the process of the complex event. With a durative adverbial, (13a) can mean that the result state continue "for 10 minutes".Except that cast, it is somewhat unacceptable and unnatural. `cemcem'(gadually) in (13c) is a manner modifier which express the manner of the process, and it cannot occur with achievement verbs. Whereas (13b) has a frame adverbial, '10 pun-maney'. This is the typical example that support the telicity of the event. And`keuyAnd`keuy' (almost) in (13d) also modify the telicity.The event of the active is, however, focused on the process subevent. Therefore, it indicates the left-headed event (accomplishment verb). The following example proves this fact. The purpose of this paper is to analyze the lexical-semantic structure of morphologically derived passive verbs in Korean based on Pustejovsky (1995)&apos;s Generative Lexicon Theory (GL) and to explain the change of the root verb&apos;s lexical-semantic structure by means of passivization. Passivization in this paper is defined as the unaccusaztivization. In Argument Structure of derived passive verbs, the agent argument is deleted and the theme argument is realized as a syntactic subject. As for Event Structure, derived passives express left-headed event (achievement), whereas their roots denote right-headed event (accomplishment). In Qualia Structure, passive verbs and root ones have the same Fomal Role, but in Agentive Role of passive verbs, an act weakens to a process. Both Formal and Agentive Roles have the same theme argument.
A Phase-Based Approach to ECM across CP in Korean It has been widely held that A-movement/agreement is strictly clause-bounded. A classical case which illustrates the locality of A-movement/agreement is the English ECM (Exceptional Case Marking) construction: ECM is possible across a TP (or IP) but it is impossible across a CP, a clause boundary, as illustrated in (1).(1) a. John believes [TP her to be pretty]. b. *John believes [cp that [ Tp her was pretty]].Apparently in contradiction to this standard view, however, a number of languages have been documented as possessing ECM (or`raisingor`raising-to-object') and/or Passivization (or 'raising-to-subject') across a CP boundary (Massam 1985, Rivero 1987, J.-M.Yoon 1991, Ura 1994. Korean is an example of such languages. As illustrated in (2), Korean allows optional ECM across a CP (K.-H. Lee 1988, J.-M. Yoon 1991, J.-S. Lee 1992, among many others).(2) John-i [CP Mary-ka/lul yepp-ess-tako] mitnunta -Nom -Nom/Acc be.pretty-Past-Comp believèbelievèJohn believes that Mary was pretty.'In (2), the embedded subject can be optionally assigned accusative Case by the matrix ECM verb (or more correctly, by the matrix v under the Minimalist Case theory). An immediate question raised by this Korean long distance ECM (hereafter LD ECM) construction can be stated as in (3). ECM across a CP in Korean poses difficulties from the standpoint of the locality of A-movement/agreement. A phase-based analysis is developed which requires two steps: (i) in the embedded CP, VP/vP containing its VP-internal subject first moves to Spec-CP, which renders the subject accessible to the matrix v, in accordance with Chomsky&apos;s Phase Impenetrability Condition; (ii) ECM takes place in a local relation between the matrix v and the embedded subject. It is shown that the otherwise puzzling fact that ECM across a CP, but not Passivization across a CP, is affected by the type of the embedded verb in Korean is accounted for in a principled way, based on the assumption that CP and vP, but not TP and VP, are phases.
Generating a Category Set of Words Using a Hierarchical Part-of-Speech System and Tagged Corpus In natural language processing, it is important to categorize words or morphemes properly. A proper categorization depends on, among other things, the kind of processing task, the domain of target corpus, and the size of the corpus. When categorization is too general, we can not use characters of individual categories. Because the characters of categories hide each other. On the contrary, when categorization is too specific, we can not use the characters of categories also. Because a low frequency of a category decreases the reliability of characters of the category.Past researches have proposed categorizations and tagsets for differnt purposes: morphological analysis, syntactic analysis or information extraction, etc. While some of the categorizations are made by hand using linguistic knowledge, others are created from annotated corpora automatically or semiautomatically. Several researches have forcused on methods of modifing the existing categorization in order to improve the accuracy of their task with respect to their purpose (Brants, 1995). Criteria of categorizations in these researches are the accuracies of their tasks. Other researches have proposed an criteria of categorizations of words based on linguistic quality and not processing quality (Dejean, 2000).We propose here a method to decide a proper categorization of morphemes, giving a hierarchical part-of-speech system and a corpus tagged using this part-of-speech system. In other words, our method forcuses on reducing an existing category set using hierarchical information of part-of-speech system and statistical information of the corpus. We recursively subdivide the categories using topdown approach with subdivision score. The subdivision score, which indicates how significant it is to subivide a category, is based on difference between the context of the category and that of its parent category.We explain structure of given information, which consist of a part-of-speech system and a tagged corpus, and a generating category set in Section 2. Section 3 explains the method of generating a category set. Section 4 shows experiments that are performed in order to test the generating algorithm. In the next section, we disscuss the result of the experiments. In this paper, we propose a method of generating a proper categorization of morphemes by giving a hierarchical part-of-speech system and a corpus tagged using this part-of-speech system. Our method use hierarchical information in the part-of-speech system and statistical information in the corpus to generate a category set. The statistical information is based on the context of occurrence of categories. First, we specify the format of given information. Then, we describe an algorithm to generate a proper categorization. Finally, we present the results of our experiments in applying this method. We obtaind a moderately proper categorization and found several candidates for improvement.
Disambiguation of Coordinate Expressions in Japanese by Extracting Mutual Case Relation  Japanese coordinate noun phrases by the particle TO are often ambiguous on whether it means two parallel propositions (and) ,or a mutual case relation (with or against) , as deep case structure. It was a hard problem to determine it, though they are widely used. We propose a method of solving the ambiguity by analyzing mutualness of verbs and adjectives. The mutualness is determined by three features of each verb or adjective. The first feature indicates permission of mutual expression in the subject, and the second in the object. The last shows if a verb is voluntary. Using this method we design a parsing mechanism, where matching of features is represented as neutralization between predicate arguments.
Finite Small Clauses in Japanese and Their Theoretical Implications Japanese has small clauses as shown in (1a). 1 In this example, the small clause subject has its Case feature checked against the matrix verb, resulting in an accusative Case-marking. This is indeed the only way for the small clause subject to check its Case feature. Since there is no finite T in the small clause, as is indicated by the non-inflected form of the predicate, the small clause subject cannot appear in the nominative as in (lb):(1) a. John-ga [sc Mary-0 [ However, it is often said that Japanese allows another type of small clause, in which a small clause predicate shows up in a finite form as in (2a). I will refer to this type of small clause as (mite small clause (FSC). That there exists a finite T in the FSC can be made clear by the availability of the nominative Case marking on the embedded subject as in (2b):(2) a. John-ga Mary-o kasikoi to omotta (koto) nom -acc intelligent C considered (fact) b. John-ga Mary-ga kasikoi to omotta (koto) -nom -nom intelligent C considered (fact)The string (2b) can be analyzed as a complex sentence that embeds a finite clause, in which the subject Mary-ga "M-nom" is moved from the predicate-internal subject position to Spec-TP as is shown in (3):(3) John-go [cp [Tp Mary-go; [PRED kasikoi]] to] omotta (koto) -nom -nom intelligent C considered (fact) "John considered that Mary is intelligent."Then, what is the internal structure of FSCs like (2a)? I will answer this quetion by considering the two questions (4) and (5):(4) Where is the FSC subject base-generated? a. At predicate-internal subject position b. At Spec-CP of the embedded clause (5) Where is the FSC subject moved to before Spell-Out?a. Spec-TP of the embedded clause b. Outer Spec-vP of the matrix clause c. Spec-CP of the embedded clauseThere are two conceivable answers for the former question as in (4a-b) and three answers for the latter as in (5a-c). Therefore, there are logically six combinations. However, the combination of (4b) and (5a) can be precluded in the first place because it requires lowering operation in overt syntax, which runs counter to the extension condition in the sense of Chomsky (1993). The other five possible combinations are displayed in (6a-e). 2 In (6a-c), it is assumed that the FSC subject is base-generated at the predicate-internal subject position. 3 On the other hand, in (6d-e), it is assumed to be base-geneated at Spec-CP of the embedded clause. In addition to these, we will consider another possiblity illustrated in (60 that the apparent FSC subject is in fact an argument of the matrix verb, called Major Object, which binds a pro that is base-generated at the predicate-internal subject position.(6) a. John-ga; [ This paper aims to clarify the internal structure of FSCs within the Minimalist flamework developed by Chomsky (2000 ;. This paper is organized in the following manner. In sections 2, I will deny the possibilities (6a-c) on the basis of the fact that a FSC subject, when it appears as a WH-phrase, cannot take embedded scope. In section 3, I will discard the possibility (6d) and (60, resting on the fact that the predicate plus complementizer part of a FSC cannot be moved to a position higher than the FSC subject. hi section 4, I will propose the internal structure FSCs, which involves a null operator. In section 5, I will show four consequences of the proposal: (i) a complex sentence (i.e., a sentence that embeds another sentence) can be a FSC predicate (5.1); (ii) a simplex sentence whose predicate is a transitive verb can be a FSC predicate (5.2); (iii) the proposal paves the way towad eliminating Superraising (5.3); (iv) Hyperraising may also be dispesend with (5.4). Section 6 concludes the paper, followed by an appendix. 2. Against (6a-c) In this section, I will eliminate the possiblities (6a-c), through examination of FSCs headed by an interrogative complementizer ka, instead of a declarative complementizer to. Before considering the crucial data, let us see how WH-phrases are lisenced in Japanese. The lisencing conditon can roughly be stated as in (7). Consider the examples in (8) for illustration of (7).(7) A WH-phrase must be c-commanded by the interrogative complementizer of a clause in which it takes scope at some stage of the derivation. The WH-phrase in (8a) is licensed because it is c-commanded by the interrogative complementizer ka of the embedded clause, in which it takes scope. In (8b), the WH-phrase is moved out of the c-command domain of the interrogative complementizer via scrambling. Nevertheless, it can take embedded scope because it was c-commanded by the embedded complementizer before it is scrambled, hence meeting the condition (7). In contrast, (8c) is ungrammatical because the WH-phrase is outside of the c-command domain of the interrogative complementizer of the embedded clause throughout the derivation. However, this example can be rescued as a matrix WH-question by adding an interrogative particle no at the sentence-final position as in (8d).With this background in mind, let us consider cases in which an embedded subject appears as a WHphrase as in (9) and (10) -nom who-nom class-in most intelligent Q discussed "John and Mary discussed who is most intelligent in the class." b. *John to Maiy-ga dare-o kurasu-de ichiban kashikoi ka gironshita -nom who-acc class-in most intelligent Q discussed "*John and Mary discussed whom to be most intelligent in the class c. John to Mary-ga dare-o kurasu-de ichiban kashikoi ka gironshita no -nom who-acc class-in most intelligent Q discussed Q "*Whom did John and Mary discuss to be most intelligent in the class?" shiri-tai I-top who-nom class-in what position-in intelligent Q know-want "I want to know who stands where in the class." b. *Boku-wa dare-o kurasu-de nan banme-ni kashikoi ka shiri-tai I-top who-acc class-in what position-in intelligent Q know-want "*I want to know whom to stand where in the class." c. Kimi-wa dare-o kurasu-de nan banme-ni kashikoi ka shiri-tai no You-top who-acc class-in what position-in intelligent Q know-want Q "*Whom do you want to know to stand where in the class."In (9a) and (10a), the embedded WH-subject shows up in the nominative and can take embedded scope because it occupies Spec-TP of the embedded clause, being c-commanded by the interrogative complementizer ka. However, the minimal pairs of these examples are ungrammatical, where the WHsubject manifests itself in the accusative as a FSC subject as in (9b) and (10b). Considering that the nominative subject can alternate with the accusative one as a FSC subject in examples like (2), the ungrammaticality of (9b) and (10b) is unexpected. There are two possibilities to rule them out. One is simply to say that there is no FSC headed by an interrogative complementizer whatever the reason may be. The other is to say that the FCS subject is base-generated at a position high enough not to be ccommanded by the embedded complementizer throughout the derivation. Notice that (9b) and (10b) can be rescued as a matrix WH-question by adding an interrogative particle at the sentence-final position as in (9c) and (10c), just like (8c) is rescued as in (8d). This is compatible only with the latter possibility because if the former one were correct, (9c) and (10c) would also be ungrammatical.Let us now consider which of the six hypotheses in (6) should be adopted to explain the ungrammaticality of (9b) and (10b) by attributing it to the failure to license the WH-subject. We cannot adopt (6a-c), because on these hypotheses, the WH-subject in (9b) and (10b) would be c-commanded by the interrogative complementizer throughout or at some stage of the derivation, and they would be wrongly predicted to be grammatical. On the other hand, if we adopt (6c1-0, we can elucidate their ungrammaticality. On these hypotheses, the WH-phrase in question can be analyzed as being outside of the c-command domain of the interrogative complementizer throughout the derivation. More specially, it is base-generated at Spec-CP of the embedded clause under (6d-f) whereas it is base-generated as an internal argument of the matrix verb under (60.Consequently, we can discard the possibilities (6a-c) based on the fact that a FSC subject, when it appears as a WH-phrase, cannot take embedded scope, assuming that the internal structure of FSCs does not differ when they are headed by a different complementizer. This paper investigates the internal structure of finite small clauses (FSC). I will propose that a FSC is base-generated at Spec-CP and a null operator is involved to check the formal features of the embedded T and turn a sentence into a predicate.
Issues in Chinese prosody: conceptual foundations of a linguistically-motivated text-to-speech system for Mandarin My main starting point is fairly obvious to many, though controversial in some circles: that we need to incorporate some notion of hierarchical prosodic structure. For the motivation for this decision, I refer readers to work by Anthony Fox dating from the mid-eighties (Fox, 1985(Fox, , 1986, and summarized in his more recent book (Fox, 2000), which harks back to work done in the 1950's and 60's. Fox looks at prosodic hierarchies proposed by Hockett (1955), Pike (1967), Togeby (1965) and Halliday (1967): it is his claim that, by factoring out differences in terminology and diagramming conventions and by recognizing the difference between primary and secondary hierarchies and between units and features of units, we can say that they were all proposing essentially the same hierarchy. Further, relating known features of Chinese prosody to their domains of operation we can draw up a  Table : Units and features of units This is not especially novel, nor is it entirely uncontroversial, but it will serve as a starting point.What I propose to do is go through these levels and features in turn and survey some of the main proposals that have been made. This will of course preclude any deep discussion of individual controversies. But I believe it is appropriate to do this as, by and large, researchers are concerned with either intonation or other lower-level prosodic features, not both: this means that, though there are fine works available on, e.g., tone structure, syllable structure, intonational endings, etc, not enough has been done to draw all this together into a clear picture of Chinese prosodic structure as a whole. I examine various controversial aspects of Chinese prosody-tone structure, syllable structure, stress, and intonation-and stress the need to view all of these as interacting systems, aspects of a hierarchical prosodic structure. I examine various proposals at these various levels of the hierarchy and suggest which are most appropriate. Specifically, I suggest the adoption of Bao&apos;s version of syllable and tone, and Chen&apos;s account of stress. As for intonation, it is still not possible to make any definitive claims regarding an optimal model, but I examine work done by Kratochvil, Shih, and Garding et al, and suggest promising directions for future work.
An Operator Assisted Call Routing System We proposed an end-to-end system for operator assisted call routing. The system was designed based on the idea that a telephone operator is capable to capture the intent of caller's inquiry but the operators may not be familiar or kept up to date with all the business of the organization. Therefore, he or she can then operate the call router via a speech interface. The system transforms the natural speech, matches the information related to directory, and finally determines the routing destination. The output is then displayed on the screen for the operator to carry out the actual routing action. The goal is to assist the operator in selecting the desired destination that matches caller' s intension. This will expedite operator's response since there is no need to search a printed directory or enter the query in Chinese text. Numerous previous works on call routing has been reported (Riccardi et. al., 1997;Chu-Carroll and Carpenter, 1999). Most approaches in the literature require a corpus of routed calls to train a routing matrix or language model. Such corpora of dialog are sometimes difficult to obtain due to the concerns over invasion of privacy right. In addition to the acoustic module, a call router also needs to classify caller' s request according to routing destination. This task of classification is similar to text categorization ( Lewis et al., 1996) in IR or topic identification (McDonough et al., 1994) in speech research. When a corpus of calls is available, researchers tend to adopt a vector space model, under which terms in the corpus is weighted statistically and treated as independent. Many systems adopted a form-based approach that is commonly used in spoken understanding system (Seneff, Glass, and Goddeau, 1991;Lamel, 1998;Chu-Carroll, 1999;Papineni, Roukos, and Ward, 1999). A form consists of a set of keywords all (or most) of which need to be present in the caller' s request in order for the destination to be activated. This idea amounts to formulating an AND query of many independent keywords.There are problems with adopting a form-based approach. Firstly, the acoustic module produces incorrect output, which may cause the IR module to fail. If we set the precision of the acoustic output too high, there could be insufficient information for the IR module to produce a destination. On the other hand, if the precision is set low for better recall, there could be too much noise causing the IR module to produce more than one destination. The second problem is manual effect needed to construct forms for each destination each time a new call router is built. Furthermore, one needs to provide synonyms to these independent keywords in the form. Otherwise, the system will be very fragile when faced with alternative ways of expressing the same routing request.To cope with the problems mentioned above, we design the proposed call router with the following considerations:• Tighter integration of acoustic and IR module.• Automatic construction of forms.• Automatic discovery of similar words.By integrating acoustic and IR module, the system achieves satisfactory performance and provides a promising approach to call routing. To easy the burden of developing forms for destination and ensure consistency and coverage, we develop an automatic algorithm to extract keywords based on the description of destinations as well as a corpus of news. A collection of forms can be generated semi-automatically for a specific routing task. An application developer is allowed to fine tune them in this framework. A system to assist call routing task for telephone operators at the Directorate General of Telecommunications (DGT) in Taiwan is reported in this paper. The system was developed based on DGT organization profile with description of its six divisions instead of a corpus of recorded and transcribed call-routing dialogs. An acoustic module and an information retrieval module were built specifically for this task. The construction of IR module was based on term extraction and thesaurus discovery processes. By integrating acoustic and IR module, the system achieves satisfactory performance and provides a promising approach to call routing. Simulation results indicated that the proposed algorithm outperforms standard classification methods. A working system based on the proposed approach has been implemented and experimental results are presented. 1 Introduction We proposed an end-to-end system for operator assisted call routing. The system was designed based on the idea that a telephone operator is capable to capture the intent of caller&apos;s inquiry but the operators may not be familiar or kept up to date with all the business of the organization. Therefore, he or she can then operate the call router via a speech interface. The system transforms the natural speech, matches the information related to directory, and finally determines the routing destination. The output is then displayed on the screen for the operator to carry out the actual routing action. The goal is to assist the operator in selecting the desired destination that matches caller&apos; s intension. This will expedite operator&apos;s response since there is no need to search a printed directory or enter the query in Chinese text. Numerous previous works on call routing has been reported (Riccardi et. al., 1997; Lee et al., 1998; Chu-Carroll and Carpenter, 1999). Most approaches in the literature require a corpus of routed calls to train a routing matrix or language model. Such corpora of dialog are sometimes difficult to obtain due to the concerns over invasion of privacy right. In addition to the acoustic module, a call router also needs to classify caller&apos; s request according to routing destination. This task of classification is similar to text categorization (Lewis et al., 1996) in IR or topic identification (McDonough et al., 1994) in speech research. When a corpus of calls is available, researchers tend to adopt a vector space model, under which terms in the corpus is weighted statistically and treated as independent. Many systems adopted a form-based approach that is commonly used in spoken understanding system (Seneff, Glass, and Goddeau, 1991; Lamel, 1998; Chu-Carroll, 1999; Papineni, Roukos, and Ward, 1999). A form consists of a set of keywords all (or most) of which need to be present in the caller&apos; s request in order for the destination to be activated. This idea amounts to formulating an AND query of many independent keywords. There are problems with adopting a form-based approach. Firstly, the acoustic module produces incorrect output, which may cause the IR module to fail. If we set the precision of the acoustic output too high, there could be insufficient information for the IR module to produce a destination. On the other hand, if the precision is set low for better recall, there could be too much noise causing the IR module to produce more than one destination. The second problem is manual effect needed to construct forms for each destination each time a new call router is built. Furthermore, one needs to provide 271
Backward Anaphora The binding theory has been discussed as a dependency relation between NPs. It attempted to clarify how an anaphor is related with its antecedent. The different binding conditions were proposed by Chomsky (1981) to explain the different phenomena of the binding facts. This has been developed into the LF-movement theory that accounts for both the long-distance anaphors and locally bound anaphors in a unified way (Chomsky 1986, Battistella 1989, Cole, Hermon, and Sung 1990, Pica 1991and Cole and Wang 1996. Along the same line, a covert anaphor movement is changed into feature checking with minimalism (Chomsky 1992, Yang 1994, 1996, Lee 1997, Kim 1999). Another proposal was made by Reinhart and Reuland (1993) based on the semantic properties of predicates. Their work is meaningful in that not only NPs but also predicates come into play in the binding phenomena. These approaches could not provide an explanation for the residue of the anaphoric phenomena, i.e. backward anaphora, but only partially account for core constructions. In this paper, I will focus on the backward anaphors that have been a problem in the area of the binding theory. To deal with such cases, I claim that the dependency relation between NPs is determined by the thematic prominency. 2 Previous StudiesThe c-command requirements in the binding relations are essential to form the binding theory. In Chomsky (1981), all the binding conditions were defined by the word "bound" and the word "bound" was defined by the notion of c-command. It has thus been claimed that the NPs that are not bound within the local domain or do not satisfy the c-command requirements are not anaphors but something else. McCawley (1972) and Katada (1991) argue that they are reflexive pronouns. Clement (1975), Maling (1984), Sells (1987) and Reinhart and Reuland (1989;1993) argue that they are emphatic pronouns or logophors which make reference to the individual whose speech, thought, or feelings are reported to other individuals. According to this line of research, the relaxation of the strict c-command and locality might be due to the fact that no syntactic binding is involved.If they are not anaphors, it should be accounted for why there is a referential dependency at all. If they are actually pronouns, the question is why they have a reflexive form different from a pronoun. Is the structural and formal explanation not possible at all for such anaphors? It is undeniable that there are anaphors that should be accounted for by the rules of discourse. However, It is also required to find a unified syntactic rule, because there is an obvious syntactic dependency between the anaphor and the antecedent from large amounts of core data. Furthermore, if we could find a syntactic account for non-core anaphors, which do not seem to belong to the syntactic binding, better generalization on the binging facts could be obtained.Anaphors contained in psych-verb and causative constructions are in an arguable boundary between logophors and anaphors. Belletti and Rizzi (1988), instead of putting them aside as involving logophors, claimed that the principle A applies at D-structure for those constructions.(1) Questi pettegolessi su dei se; preoccupano Gianni; piudi ogni altra cosa.`These gossips about himself worry Gianni; more than anything else ( Belletti and Rizzi (1988)Belletti and Rizzi said that both the subject and object are in the VP complement position at D-structure as in (2). It was claimed that the experiencer is in the higher position than the theme at D-structure, and thus it could properly bind the theme subject, satisfying the c-command requirements. With the elimination of D-structure in minimalism, their analysis should be reanalyzed, adopting the concept of a thematic hierarchy only.Other linguists such as Giorgi (1991), Reinhart and Reuland (1993), Reuland and Koster(1991), Hellan(1991), Everaert(1991Everaert( , 1996, Katalin (1991), and Pollard and Sag (1992) argued that the binding theory should make crucial reference to the thematic structure in an effort to explain the different binging phenomena across languages. Grimshaw (1990) proposed that the relative prominence of an argument is determined in both the thematic and the aspectual dimension. The psych-verb constructions were claimed to involve mismatch in those dimensions, which made the most prominent experiencer object antecede the theme subject, yielding backward anaphora.The previous analyses that argue for the need of a thematic hierarchy are not against Chomsky (2001) in that he proposes to adopt a pure configurational theta-theory, eliminating s-selectional features or theta-grids distinct from the semantic properties of head. (H.B.Lee (2001: Presentation at the Korean Generative Grammar Circle. Chomsky (2001) said that the C-I system requires that SEM express a variety of semantic properties, which include at least argument structure (Chomsky 2001:7). He also said that theta-theoretic properties depend in part on configuration and the semantic properties SEM (H) of the head (label) (Chomsky 2001:8). By H.B.Lee's interpretation, a theta-role is determined by the structure, so that s-selectional features or theta-grids are dispensable. I will adopt the configurational theta theory from Chomsky (2001) to analyze the backward anaphora. What follows are the detailed proposals for this paper. This paper aims to account for the backward anaphora that seem to be against the c-command requirements in the anaphor-antecedent relations. It was claimed that the binding conditions should apply at LF for the backward binding cases involving phych-verbs and causatives. Under the recent development of minimalism where the concept of levels disappears to adopt a cyclic derivation, the data that show the backward binding phenomena have not been discussed in the area of the binding theory. In this paper, I argue that the backward binding cases can be incorporated into the core binding phenomena with the general assumptions on the thematic prominency. It is discussed how the dependency between NPs involving backward anaphora is determined by the thematic prominency. The Agree operation takes place between the probe T and the goal with the uninterpretable u[a] and [prominent] feature, by which an anaphor is valued, producing a proper interpretation.
A Korean Noun Semantic Hierarchy (Wordnet) Construction Thesaurus 1 or wordnet takes too much time and effort to construct them manually. In this paper, we introduce a semi-automatic method for the construction of Korean noun semantic hierarchy with lexical mapping to each noun's sense. In this method, an MRD (Hangeul Society, ed. 1997) and an existing translated thesaurus (Ikehara, et al. 1997) are used. By assigning the semantic category of the existing thesaurus to each sense 2 of the nouns in MRD, we combine these two resources andproduce an expanded l exicalized n oun s emantic h ierarchy. T he s emantic c ategory i s assigned first and manual correction is performed in post-processing, so semantic hierarchy is constructed with relatively high accuracy and small effort. Since thesaurus is used as a knowledge resource in many natural language processing systems, it is very useful and necessary for the high quality systems, especially for dealing with semantics. In this paper, we introduce a semi-automatic method for the construction of Korean noun semantic hierarchy by utilizing a monolingual MRD and an existing thesaurus.
Implementation of Long-distance Reflexives in Korean A Categorial Grammar Approach Anaphora has been one of the hottest topics in Linguistics. Various theories have been developed under the name of Binding. There have been roughly two types of Binding Theories. The first group uses tree-theoretic notions to search an antecedent for the anaphora. Chomskyan traditions, especially GB framework, belong to this type of approach. The other utilizes argument-functor relations, instead of structural configurations, for anaphora resolution. This strategy is taken by GPSG (Generalized Phrase Structural Grammar), PLG (Phrase Linking Grammar), and CG (Categorial Grammar). Chierchia (1988)'s theory in Categorial Grammar is a good example.There have been lots of observations that the distributions of reflexives are different from those of pronominals. That is, reflexives have some more restrictions that pronominals do not obey. In order to implement this intuition, many theories adopted two kinds of different mechanisms, one for reflexives and the other for pronominals. The conditions by which reflexives are licensed are different from those for pronominals.Korean reflexives have characteristics that English counterparts do not exhibit. In Korean, long-distance reflexives are possible in addition to ordinary sentence-bound ones. The distributions of caki in (1), (2), and (3) illuminate this fact"2.(1) Chelsoo1-ka caki1-lul salangha-n-ta. In (1), caki is sentence-bound, i.e., the antecedent of caki is within the sentence boundary. In (2), Chelsoo, an antecedent, is outside of the sentence boundary; and it shows a case of long-distance reflexive. In (3), both sentence-bound reading and long-distance interpretation are possible for caki. Sentences in (4) and (5) correspond to English counterparts of (2) and (3), where long-distance interpretations are impossible.(4) a. *Chelsoo thinks that himself is smart.b. Chelsoo thinks that he is smart.(5) a. *Chelsoo thinks that Younghee loves himself. b. Chelsoo thinks that Younghee loves herself.The goal of this paper is to provide computational algorithms for the interpretations of Korean reflexives, for which both sentence-bound and long-distance readings are possible. For the analyses, a CCG-like system will be introduced, which is slightly modified from Steedman (1996,2000)'s Combinatory Categorial Grammar (CCG). Through the analyses, this paper will show how two different interpretations of Korean reflexives are calculated and how they are implemented. This paper provides computational algorithms for a Korean reflexive caki, for which both sentence-bound and long-distance readings are possible. Its analyses are based on Chierichia&apos;s theory in Categorial Grammar, and a CCG-like system is introduced for the implementation. In this system, we can get both readings of caki with the same resolution mechanisms, while the difference is where the reflexive is resolved. These algorithms enable us to account for the distributions and characteristics of a long-distance reflexive caki with a more unified way.
The Interface between Syntax and Morphology: Taiwanese Verbal Complexes* Whether word formation occurs solely in the lexicon has been a frequent issue of serious debates. In the strong lexicalist hypothesis, it is assumed that all types of word formation occur in the lexicon as advocated by Di Sciullo &amp; Williams (1987), for example. An opposite extreme approach is adopted by Lieber (1988,1992), who proposes that phrasal compounds in English are syntactically derived and extends the syntactic analysis to other types of word formation. Baker (1988) also applies a syntactic analysis for various types of complex predicate formation. Borer (1988), however, provides a different view on word formation. She argues that morphology is parallel to the other three components of grammar: lexicon, syntax, and phonology. Morphology is not linearly ordered before or after syntax, and it is also distinct from the lexicon where morphemes, idiosyncratic words, idioms, * Research for this paper was supported by a grant from the National Science Council of Taiwan (NSC90-2411-H-194-029). and lexically formed words are stored. Yoon (1989) also proposes that morphology as a rule system is distinct from the grammatical components, such as the lexicon and syntax, and that the rules of morphology can occur in different components. Compounding at the post-syntactic level is also proposed by Shibatani and Kageyama (1988) for Japanese post-syntactic compounds. This paper supports the view on parallel morphology through the discussion of the formation of verbal complexes in Taiwanese. This paper aims to argue that words (or compounds in this case) can be formed in syntax as well as in the lexicon.Taiwanese, the dialect spoken by more than 80% of the population of Taiwan, abounds with verbal complexes. Among the various types of verbal complexes, phasal complexes (V-Phasal Particle), resultative complexes (V-Resultative Verb), and directional complexes (V-Directional Particle) are alike in that the second component denotes some sort of result, as illustrated in (1-3). The difference is that the second element of phasal complexes indicates the phase which the activity denoted by the first verb is in, while that of resultative and directional complexes describes the state of the subject or object as a result of the activity denoted by the first verb.(1) gua hit pun cheq khuaN-liau a. Phasal Moreover, these three types of verbal complexes behave similarly. For example, they can occur in V-ho-V, V-e/be-V, and V-bo-V forms as in (4) gua png cia-be-liau. Taiwanese abounds with verbal complexes. Among them, phasal complexes, resultative complexes, and directional complexes are alike in that their second component denotes some sort of result. Moreover, they behave similarly in that they can occur in V-ho-V, V-e/be-V, and V-bo-V forms. Despite the similarities, they still differ from one another in several aspects, such as whether objects are allowed inside or after the verbal complex, whether infixing changes their basic meaning, etc. This paper examines their individual properties carefully and proposes that these three types of complexes are all different from one another in their formation and thus the difference in their syntactic behavior. Directional complexes are syntactic phrases, resultative complexes are compounds derived in syntax, and while some phasal complexes are also syntactically derived compounds, others are compounds formed in the lexicon. This paper aims to argue that words (or compounds in this case) can be formed in syntax as well as in the lexicon.
The Structure of Polysemy: A study of multi-sense words based on WordNet WordNet (WN), which is a large scale, domain-dependent semantic network of English words, provides a broad-coverage of lexical information. It represents a system of semantic relations among words, between words and synsets' , and between synsets themselves (Miller, 1990(Miller, , 1995. Two features of the system are concept definitions and an inheritance hierarchy of concept types. Rather than using lexical entries only, the design-is based on linguistic theories about cognitive organization of natural languages. English nouns, verbs, adjectives, and adverbs are arranged to synsets that are in turn linked through semantic relations such as antonymy, hypernymy, etc.Like other conventional dictionaries and thesauri, WordNet also provides different meanings for one word. The lexicographic database represents a complex linguistic structure in which a word form may carry multiple senses. These word senses that are related in systematic ways build different synsets for each sense of a word. A word meaning then, is the pairing of a word form with a synset. However, WordNet's sense distinctions are more fine-grained than other machine-readable dictionaries, resulting in abundant polysemy and difficulty of computation (Kilgarriff 1997).In this paper, we will pay particular attention to the issues in polysemy with respect to the verbs in WordNet 1.7 and attempt to find a typical hypernymy/hyponymy structure for the multiple senses of a word form. In the following sections we will briefly overview the verb hierarchies in WN and illustrate the patterns of sense clusters. The types of autotroponymy will be discussed as well.1 A set of synonyms refering to the same concept is called a synset. Members of synsets may be simple words or compounds. The issues in polysemy with respect to the verbs in WordNet will be discussed in this paper. The hypernymy/hyponymy structure of the multiple senses is observed when we try to build a bilingual network for Chinese and English. There are several types of polysemic patterns and a co-hypernym may have the same word form as its subordinates. Fellbaum (2000) dubbed autotroponymy that the verbs linked by manner relation share the same verb form. However, her syntactic criteria seem not compatible to the hierarchies in WN. Either the criteria or the network should be reconducted. For most verbs in WN 1.7, polysemous relations are unlikely to extend over 3 levels of IS-A relation. Highly polysemous verbs are more complicated and may be involved in certain semantic structures. Semi-automatic sense grouping may be helpful for multimlinguital information retrieveal.
Complexity of Presuppositions in Local Domain In this paper we would like to deal with interaction of presupposition triggers and show that the binding theory of presupposition comes up with it better than the satisfaction theory. In section 2, the newly developed presupposition conception is introduced. Section 3 explains the Binding Theory in contrast with the Satisfaction Theory. In sections 4 and 5, Kamp's new idea about the treatment of complex interaction of presuppositions is introduced, and it is suggested that his theory can be extended to two other phenomena; linguistic contexts in which presupposition triggers only have narrow scope reading and specificity interpretation of some Japanese NPs. Based on the recent development of DRT approaches on presuppositions, we will deal with floating quantifiers as a special case of specificity phenomena in sections 6 to 8. In section 6, three different configurations of a quantifier and its restriction NP are introduced: the genitive construction, the compound construction and the floating construction. As a preliminary research, we concentrate in section 7 and 8 on the compound construction of floating quantifiers to the effect that comparison with Semantic Incorporation is made to understand non-specificity of the construction more in detail. This will be done first in GQT terms (section 7) and finally as an application of de Swart &amp; Farkas' theory on weak NPs (section 8). 
Grammaticalization and Semantic Typology: Time-relationship Adverbs in Grammaticalization is an important process in the framework of Cognitive Linguistics because it highlights the flexible nature of boundaries between lexical and grammatical categories, and this flexibility, in turn, reflects interdependence of structure and use. However, relatively little attention has been paid to how this process is constrained. Although some authors (e.g. Hopper 1991) point out general characteristics of this process, some of which can obviously be regarded as constraints, they fail to address the issue of how such constraints operate to yield different systems in various languages. This paper argues that time-relationship adverbs such as mada("still, (not) yet") and moo("already, (not) any longer") in Japanese or their (partial) counterparts acik or pelsse in Korean are constrained by the same principles of grammaticalization, layering and persistence, although individual languages manifest apparently different distributional patterns. ' The organization of this paper is as follows. In section 2, we will look into basic distributional patterns of time-relationship adverbs in Japanese and Korean and present a preliminary semantic analysis of them. In section 3, the above-mentioned two basic principles of grammaticalization are introduced and in section 4, we will observe how these principles operate to produce different distributions between the two languages. In section 5, it will be argued that the same principles can also explain different distributions in English and German time-relationship adverbs. In section 6, we will suggest that the distributional difference between Japanese and Korean is a reflection of the overall semantic typological differences between the two languages in the sense of Hawkins (1986). Section 7 will summarize the major findings of this study and present the conclusion.2 Time Relationship Adverbs in Japanese and Korean This paper discusses constraints on grammaticalization, a primarily diachronic process through which lexical elements take on grammatical functions. In particular, it will argue that two constraints on this process, namely Persistence and Layering, explain the different distributional patterns of time-relationship adverbs in Japanese, Korean, English and German. Furthermore, it will suggest that the distributional difference between Japanese and Korean time-relationship adverbs is not an isolated phenomenon but is a reflection of the overall semantic typological differences between the two languages in the sense of Hawkins (1986).
Double Subject, Double Nominative Object and Double Accusative Object Constructions in Japanese and Korean Much attention has centered on the so-called double (multiple) subject construction (hereafter, DSC) in Japanese and Korean,' as illustrated in (1) and (2), in the literature of empirical-and theoretical linguistics:  (1) and (2), we insert the adverb saikin and choykun 'recently' between the two nominative NPs to show that these two nominative NPs do not form a constituent (if the 'possessor' NP shows up with genitive case, adverbs may not intervene between these NPs, as in *syusyoo-no saikin byoojoo 'lit. *Prime Minister's recently condition'). In the generative grammar framework, it has been argued that the sentence-initial nominative NP (hereafter, we refer to it as 'major subject') raises to a higher position (the targets vary depending on the movement theories the authors adopt) from the possessor position of the following subject by NP-movement in DSC. For example, Hasegawa (1999) proposes the derivation like (3): soft-PRES DSC has an IP-adjunction structure generated by NP-movement in (3) 2 . Notice here that the movement in (3) violates the limitation of 'one case per chain' since it involves two case (i.e., genitive and nominative case) positions in the chain formed by the movement of the possessor. Besides, this kind of explanation has ignored a lot of important characteristics of this construction, concentrating only on the multiple assignment/licensing of nominative case to the major and regular subjects.On the other hand, some Japanese traditional grammarians have proposed a kind of complex structure for DSC. (4) is cited from Sugimoto (1986): Sugimoto assumes that DSC has a complement sentence structure, in which the S1 is embedded under the matrix sentence S2. This argumentation immediately conflicts with the standard notion of a 'clausal complement', which is assumed to be a clause subcategorized for by a higher predicate. In (4), there is no matrix predicate which would take the clausal complement S1 as argument. He also dubs S1 BUN-JUTUGO 'Sentence-Predicate', but he does not give any explanation for the reason WHY this sentence, in which the argument structure of the predicate kirei-da 'is beautiful' is fully satisfied, can be predicated of the major subject. I believe that the availability of layers of predication in DSC like (4) is the most important feature of this construction which requires an explanation.Japanese and Korean also have interesting constructions related to DSC. Consider sentence (5b), where the object and its possessor shows up with nominative case:3 (5) a. Hanako-ni(wa) kono hon-no naiyoo-ga yoku wakar-u. Hanako-DAT(TOP) this book-GEN content-NOM well understand-PRES 'Hanako understands the content of this book well.' b. Hanako-ni(wa) kono hon-ga yoku naiyoo-ga wakar-u. Hanako-DAT(TOP) this book-NOM well content-NOM understand-PRES.It is well-known that both the possessor and possessed NP's can be marked with accusative Case in 2 hl fact, some generative grammarians do not admit the IP-adjunction structure like (3) and suggest that a major subject move to a specifier position of some functional category. Mihara (1990) and Takezawa (2000), while assuming that DSC have the IP-adjunction structure, argue that a major subject is not raised to the IP adjoined position but base-generated there. 3 Note here that, in the sentence in (5), the nominative NP kono hon-ga 'this book' is NOT a subject as can be seen from the pair in (i): (i) a. Taroo-ni jibuni-no koto-ga wakar-u. Taroo-DAT SELF-GEN thing-NOM understand-PRES Taro() understands himself.' b. * Jibun-ni Taroo-no koto-ga wakar-u. SELF-DAT Taroo-GEN thing-NOM understand-PRES In (ia), the nominative jibun 'self can be corefer with the dative NP Taroo, whereas in (ib), Taroo cannot be the antecedent of the reflexive zibun (Notice that the word order is irrelevant, as can be seen in Jibun no koto-ga Tarooni wakaru vs *Taroo-no koto-ga jibun-ni wakar-u). Also, the dative NP, but not the nominative objsect, induces subject-honorification, a kind of subject-agreement in Japanese, as illustrated in (ii): (ii) Senseirni(wa) imadoki-no seitoi-ga o-wakari-ninaraipi-nai. Teacher-DAT(-TOP) these-days-GEN students-NOM HON-understand-HON-NEG The teacher doesn't understand students of today.' All the tests for subjecthood clearly show that dative (experiencer) NPs are subjects, whereas nominative objects have much in common with normal accusative objects in transitive sentences. This paper presents a unified account of three kinds of constructions in which more than one NP can show up with the same case in simple sentences in Japanese and Korean: double subject, double nominative object and double accusative constructions. Noting that the second NPs in these constructions are functional or relational, this paper proposes to assign them the category and type different from the first NPs. We show the derivations of these three constructions in a parallel manner, and explain the asymmetries in extractability between possessor and possessed NPs in relativization.
A Focus-Based Approach to Scope Ambiguity in Japanese  This paper puts forward an analysis of scope interactions between Japanese adverbial quantifiers like mainichi &apos;everyday&apos; and tokidoki &apos;sometimes&apos; and a negative morpheme nai &apos;not&apos; on the basis of Aocus)-structures. In this analysis, three f-structures are assigned to a sentence with an adverbial quantifier and a negative morpheme. One of them represents a negation-wide reading, and the other two represent quantifier-wide readings. Some f-structures, however, are unacceptable due to semantic or pragmatic factors. Different scope behaviors of the two quantifiers mentioned above can then be ascribed to acceptability of f-strucures. 1 The Objective This paper mainly concerns with the following pairs of Japanese negative sentences. (1) a. Taro-wa mainichi gakko-ni ika-nai Taro-TOP everyday school-TO go-not b. Taro-wa mainichi-wa gakko-ni ika-nai Taro-TOP everyday-TOP school-TO go-not`Taro not`Taro does not go to school everyday&apos; (2). a. Taro-wa tokidoki gakko-ni ika-nai Taro-TOP sometimes school-TO go-not b. Taro-wa tokidoki-wa, gakko-ni ika-nai Taro-TOP sometimes-TOP school-TO go-not`Taro not`Taro sometimes does not go to school&apos; All of these four sentences contain a frequency quantifier (mainich &apos;everyday&apos; tokidoki &apos;some-times&apos;) and a negative morpheme (naìnot&apos;). (b) sentences in these pairs differ from (a) sentences in that the adverbial quantifiers that appear in them are marked by a topic marker wa. There are two potential interpretations for these sentences: one in which the adverbial quantifier takes the wider scope than the negative morpheme, and one in which the scope relationship is reversed. I will henceforth call the former reading QN reading and the latter NQ reading. What is interesting concerning these sentences is the following contrast. Both sentences in (2) are invariably interpreted as QN, while the (lb) is invariably NQ. Judgments on (la) are controversial. The QN reading is the dominant reading and, in addition, it is even the only reading available for some speakers. Nonetheless, other speakers do admit the NQ reading for (la). These facts can be summarized as in the following table. 370
Representing Topic-Comment Structures in Chinese Shi (2000) claims that topic-comment constructions are derived from basic sentence structures via syntactic operations, and a topic must be related to a syntactic position inside the comment which can be filled either by an empty category or a resumptive pronoun, thus denying the existence of dangling topics in Chinese. In this paper we will argue that topics in Chinese can be licensed not only by a syntactic gap or resumptive pronoun, but also by a semantic variable which does not have a corresponding syntactic position. We first show that a topic in Chinese need not be related to a syntactic position in the comment, and then show that, besides being syntactically licensed, a topic in Chinese can also be semantically licensed. In this case, it is licensed by a semantic variable which does not have any syntactic realization, and this semantic variable can turn the relevant comment clause into a semantic open predicate, thus deriving a well-formed topic-comment construction. Under our analysis, the dangling topics discussed in Shi (2000) are just topics licensed by a semantic open predicate, though not by a syntactic one. Shi (2000) claims that topics must be related to a syntactic position in the comment, thus denying the existence of dangling topics in Chinese. Under Shi&apos;s analysis, the dangling topic sentences in Chinese are not topic-comment but subject-predicate sentences. However, Shi&apos;s arguments are not without problems. In this paper we argue that topics in Chinese can be licensed not only by a syntactic gap but also by a semantic gap/variable without syntactic realization. Under our analysis, all the dangling topics discussed in Shi (2000) are, in fact, not subjects but topics licensed by a semantic gap/variable that can turn the relevant comment into an open predicate, thus licensing-dangling topics and deriving well-formed topic-comment constructions. Our analysis fares better than Shi&apos;s in not only unifying the licensing mechanism of a topic to an open predicate without considering how the open predicate is derived, but also unifying the treatment of normal and dangling topics in Chinese. 1 Introduction Shi (2000) claims that topic-comment constructions are derived from basic sentence structures via syntactic operations, and a topic must be related to a syntactic position inside the comment which can be filled either by an empty category or a resumptive pronoun, thus denying the existence of dangling topics in Chinese. In this paper we will argue that topics in Chinese can be licensed not only by a syntactic gap or resumptive pronoun, but also by a semantic variable which does not have a corresponding syntactic position. We first show that a topic in Chinese need not be related to a syntactic position in the comment, and then show that, besides being syntactically licensed, a topic in Chinese can also be semantically licensed. In this case, it is licensed by a semantic variable which does not have any syntactic realization, and this semantic variable can turn the relevant comment clause into a semantic open predicate, thus deriving a well-formed topic-comment construction. Under our analysis, the dangling topics discussed in Shi (2000) are just topics licensed by a semantic open predicate, though not by a syntactic one. 2 Dangling Topics In English topics are derived via syntactic operation, and are thus licensed syntactically by a syntactic gap or a resumptive pronoun. Shi argues that the Chinese topics should be derived the same way, thus denying the existence of dangling topics, which are not syntactically related to any syntactic gap inside the comment. Shi discusses six types of dangling topics in Chinese, and he claims that, upon closer investigation, they are not dangling topics but subjects, since a structural relationship does exist between 382
Do &quot;Transitive Adjectives&quot; Really Exist? Since Kuno(1973), it seems to have become part of the conventional wisdom of Korean and Japanese syntacticians that certain stative verbs like silhta 'dislike' or cohta 'like', which are often called "psychological" predicates (or psych-verbs in short), take a nominative direct object such as ton-i `money-NOM'(1) Okca-kalnun ton-i/*ton-ul silhta/cohtalkomapta. Okcha-NOM/TOP money-NOM/money-ACC loathsome/good/thankful 'It is to Okcha that money is loathsome/good/thankful. (lit.) (=Okcha dislikes/likes/appreciates money.)'According to this view, the first nominative is the subject and the second nominative is the direct object. Considering that ordinary transitive verbs require accusative direct objects, we can ask why these stative verbs take nominative direct objects. An answer was offered by Kuno (1973): they take nominative objects because they are adjectives. But this answer begs a question: why do adjectives require nominative objects? I am not aware of any answer to this question.In this paper, I challenge the claim that the so-called psychological predicates require a nominative direct object. I argue that those adjectives in (1) requires no direct object (or no patient argument in semantic terms), and that the nominative NP ton-i 'money-NOM' is the subject of the sentence, not the direct object.I will examine all the arguments for the notion of "transitive adjectives," 2 some of which are attributed to Kuno (1973), in some detail and show that none of them are convincing, suggesting that the notion itself is misguided and can be eliminated from Korean syntax. This will lead us to argue for a more traditional, a more common sense view concerning the syntactic nature of the second nominative of a double nominative construction such as (1): the second nominative is the subject. As for the first I The conventional wisdom was accepted by even Kim(2000), one of the most up-to-date, constraint-based approaches to multiple nominative constructions. (See Footnote 5 in Section 3 for my critical comment on it on other accounts.) 2 Kuno(1973) did not use the term « transitive adjective », but I will use it in this paper to refer to stative verbs which Kuno claimed required a nominative direct object. nominative, then, I suggest that it is the "focused" subject. I will mention how focusing is different, in "packaging" information in the sense of Chafe (1976), from topicalization (i.e. Okca-nun as opposed to Okca-ka in (1)), though I do not go deeply into the issues involving focus and topic in this paper..My new analysis is a constraint-based, lexical approach couched in terms of HPSG developed by Sag and Pollard(1991), Pollard and Sag(1994), Sag(1997), and Sag and Wasow(1999). I will show that the so-called psychological predicates have an option of taking an extracted (or "SLASHed" in HPSG terms) locative-dative or, more simply, "loca-dative" complement, 3 which is ultimately realized as a focus or a topic. 2 Problems in the Previous Arguments I argue that the so-called psychological predicates like komapta &apos;thankful,&apos; mwusepta &apos;fearful,&apos; silhtàsilhtàloathsome,&apos; or kulipta &apos;missing&apos; require a nominative subject and a locative or dative complement, challenging the claim, a conventional wisdom originated from Kuno(1973), that they are two-place &quot;transitive adjectives&quot; requiring a nominative direct object. I also show that those adjectives are subject to having the locative-dative complement extracted, which is ultimately realized as a focused subject or a topic. Thus, in this type of double nominative constructions, the first nominative is a focused subject, and the second nominative forms an embedded clause with the psychological predicate, which functions as the predicate of the whole sentence.
This Adverbial Accusative: A Corpus-Based Observation and Morel Ever since the traditional grammar (cf. Curme 1931), it has been observed that certain types of English noun phrases can behave like an adverb or an adverbial prepositional phrase on their own, i.e. without any support by a preposition. Such noun phrases have been dubbed adverbial accusatives. They can play virtually the same range of adverbial functions as the "real" adverbials:(1) Time adverbial I stayed there all the summer. I'd like to start Wednesday, the first jury day. Spatial adverbial Let's go some place. Come this way, please. Measure adverbial I should not mind a bit. She used to laugh a good deal Manner adverbial Don't look at me that way. They cook (the) French style.Of these adverbial accusatives above, this paper focuses o adverbial noun phrases which are preceded by a demonstrative article this 
The Loom-LAG for syntax analysis Adding a language-independent level to LAG  The left-associative grammar model (LAG) has been applied successfully to the mor-phologic and syntactic analysis of various european and asian languages. The algebraic definition of the LAG is very well suited for the application to natural language processing as it inherently obeys de Saussure&apos;s second law (de Saussure, 1913, p. 103) on the linear nature of language, which phrase-structure grammar (PSG) and categorial grammar (CG) do not. This paper describes the so-called Loom-LAGs (LLAG)-a specialisation of LAGs for the analysis of natural language. Whereas the only means of language-independent abstraction in ordinary LAG is the principle of possible continuations, LLAGs introduce a set of more detailed language-independent generalisations that form the so-called loom of a Loom-LAG. Every LLAG uses the very same loom and adds the language-specific information in the form of a declarative description of the language-much like an ancient mechanised Jacquard-loom would take a program-card providing the specific pattern for the cloth to be woven. The linguistic information is formulated declaratively in so-called syntax plans that describe the sequential structure of clauses and phrases. This approach introduces the explicit notion of phrases and sentence structure to LAG without violating de Saussure&apos;s second law and without leaving the ground of the original algebraic definition of LAG. LLAGs can in fact be shown to be just a notational variant of LAG-but one that is much better suited for the manual development of syntax grammars for the robust analysis of free texts.
A Japanese Compound Verb V -te-iku and Event Composition It is widely recognized that Japanese manner of motion verbs do not tolerate a so-called GOAL expression -ni, as observed in (1) (see e.g. Yoneyama (1986), Kageyama and Yumoto (1997), Ueno and Kageyama (2000)). 1 ' 2 The same is true of Korean (cf. Lee (1999)).(1) Japanese: a. *?Taro wa gakko-ni arui-ta Taro TOP school-GOAL walk-PAST`( PAST`(Lit.)Taro walked to school' b. *?Taro wa gakko-ni hasi-tta Taro TOP school-GOAL run-PAST`( PAST`(Lit.)Taro ran to school' Korean: c. *?Taro-nun yek-e keless-ta Taro-TOP station-GOAL walk-PAST`( PAST`(Lit.)Taro walked to the station' d. *?Taro yek-e tallyess-ta Taro-TOP station-GOAL run-PAST`( PAST`(Lit.)Taro ran to the station' On the other hand, in English, the expressions corresponding to (1) are natural.(2) a. John walked to school.b. John ran to school. The intended situations in (2) should be realized in Japanese with a V-V compound or a V-te-V compound, such as arui-te-ikùgo by walking' and hasi-tte-iku 'go by running'. In Korean, as in Japanese, we should use compound verbs. We will call V -te-V compounds in Japanese TE-compounds, to distinguish them from V-V compounds. 3' 4 (3) Japanese:a. Taro wa gakko-ni arui -te -i-tta Taro TOP school-GOAL walk-TE-go-PAST 'Taro walked to school' b. Taro wa gakko-ni hasi-tte-i-tta Taro TOP school-GOAL run-TE-go-PAST 'Taro ran to school' Korean: c. Taro-nun yek-e kele-kass-ta Taro-TOP station-GOAL walk-go-PAST 'Taro walked to the station' d. Taro-nun yek-e tallye-kass-ta Taro-TOP station-GOAL run-go-PAST 'Taro ran to the station' Thus, one of the goals of this paper is to answer the question why -ni in (1) is not allowed in Japanese and Korean, while its counterpart construction in English is allowed, and why in (3), the -ni phrase and -e phrase are grammatical. In the present paper, our focus will be mainly placed on Japanese data, but we would like to mention Korean data, when our theory is considered to be extended to them.Another question posed in this present paper is how TE compounds are represented in the lexicon. V-te-iku cannot be simply analyzed into [V + iku], for the compound verb is not fully compositional in meaning. We will, thus, investigate what kind of relation holds between V and V-teikuliku. Special emphasis is placed on the event structures of these verbs, based on the observations of the aspectual properties of these verbs.The organization of this paper is as follows: In the next section, we will observe the aspectual difference between (1) and (3), and show that this difference yields the grammatical contrast between them. In section 3, we turn our attention to the compositional mechanism of V -te-iku, and show that also in Japanese, a generative lexical operation co-composition (Pustejovsky 1995a(Pustejovsky , 1995b) is available at morphological level. In the latter half of this section, we will argue the syntactic realizations of the arguments. Section 4 will be the conclusion.2. The Event Structures of Aruku vs. Arui-te-iku 2.1 The Licenser of -ni Recall first the contrast between Japanese and English which was noted in section 1. As is often noted, in English, the attachment of the GOAL expression changes the aspectual value of the event (cf. Tenny 1994). Consider the following. b. John walked to school f*for 30 minutes/in 30 minutes). In (4a), the event described by the verb walk co-occurs with a durative adverb but not with a time-bounded adverb, which implies that the event (or verb) is atelic. On the other hand, in (4b), only the time-bounded adverb can modify the event, suggesting that it is telic. Pustejovsky (1995a) regards this aspectual alternation as a functional mapping of to phrase (i.e. co-composition, in his terminology). As we have already (ii) Hito -ga tsugitsugini sin-deiku people-NOM successively die-TEIKU-PRES`People PRES`People are successively dying.' The present article will not discuss this aspectual use of V-teiku. observed in section 1, the Japanese -ni phrase clearly does not have this function, given the semantics of walk and aruku are the same. Rather, as we discuss in this paper, -ni phrase must be licensed by some semantic component in the sentence.The idea that -ni is licensed by some element could be spelled out as the statement that it is an argument of a predicate. Nakau (1999) actually stands in this position.' His argument goes on by comparing -ni with -de 'at/in', clarifying the difference between them. 6 The first difference between -ni and -de is that -ni does not go with action verbs when they do not take it as an argument, while -de co-occurs with action verbs (5) a. Taro-wa tukue-(ni /*de} hon-o oi-ta Taro-TOP desk-{GOAL/LOC} book-ACC put-PAST`Taro PAST`Taro put the book on the desk' b. Taro-wa kooen-{*ni /de) hon-o yon-da Taro-TOP park-{GOAL/LOC} book-ACC read-PAST`Taro PAST`Taro read a book at the park' In (5a), the verb oku 'put' requires a GOAL ex pression.. which is realized as -ni phrase. -De phrase cannot occur in the place of -ni, for it does not mark an argument. On the other hand, in (5b), the verb yomu 'read' does not take as its argument a GOAL expression, therefore -ni phrase is not allowed, while -de phrase, being a non-argument, felicitously marks the location.However, when it comes to an intransitive verbs, the argumenthood of -ni phrase is not easily maintained, for intransitive verbs usually take only one argument, which is realized as a subject. ?Vi phrase can co-occur with intransitive verbs, as shown in the next examples. (6) a. Ha-ga.zimen-ni oti-ta leaf-NOM ground-GOAL fall-PAST 'A leaf (leaves) fell on the ground' b. Taro-wa isu-ni suwa-tta Taro-TOP chair-GOAL sit down-PAST 'Taro sat down on the chair' Here, we argue that .the licenser of -ni is deterraitve-d-by its atpectual featuit, not by its argument structure.From the aspectual point of view, the verbs in (5a) and (6) are all telic (i.e. they have a terminal point). The telicity of these events is verified by the uncancellability of the terminal point. (7) a. liTaro-wa tukue-ni hon-o oi-ta ga, hon-wa tukue-ni nora-naka-tta Taro-TOP desk-GOAL book-ACC put-PAST but book-TOP desk-GOAL onto-not-PAST 5 Takezawa(2001) also argues the difference between -ni and -de; from a different point of view. The crucial point of his argument lies in that -ni needs to construct some predication relationship within VP, while -de is a real adjunct. 6 In Korean, they have counterparts for -ni and -de, which show the similar contrast (Lee 1999). Consider the examples in (iii) and (iv). The corresponding Korean postpositions are -ey for -ni, and -else for-de.(iii) a. Taro-nun tayhak-ey issta Taro-TOP university-GOAL be-PRES 'Taro is at the university' b. *Taro-nun tayhak-eyse issta Taro-TOP university-LOC be-PRE S (iv) Yumi-ka ttul{-ese /*-e} nol-ko iss-ta Yumi-NOM garden{-LOC/-GOAL} paly-PROG-PRES`YumiPRES`Yumi is playing in the garden' 423`Taro 423`Taro put a book on the desk, but the book was not on the desk' b. #Ha-ga zimen-ni oti-ta ga, ha-wa zimen-ni tuka-naka-tta leaf-NOM ground-GOAL fall-PAST but leaf-TOP ground-GOAL arrive-not-PAST`A PAST`A leaf fell on the ground, but the leaf was not on the ground' c. #Taro-wa isu-ni suwa-tta ga, Taro-wa isu-ni tootatu-si-naka-tta Taro-TOP chair-GOAL sit down-PAST but Tam-TOP chair-GOAL reach-not-PAST`Taro PAST`Taro sat down on the chair, but he wasn't on it' On the other hand, the verb in (5b) does not have a terminal point of the action. (8) Taro-wa hon-o yon-da ga, yomi-kira-naka-tta Taro-TOP book-ACC read-PAST but read-off-not-PAST`Taro PAST`Taro read a book but he could not read it off' The relevant feature above is considered to be telicity However, the locative -ni is allowed not only with telic verbs, but also with stative verbs (e.g. existential iru (with animate subject) I aru (with inanimate subject) cbe').7 (9) a. Kodomo-ga kooen{-ni/*de} iru children-NOM park-{GOAL/LOC} be 'There are children in the park' b. Kabin-ga genkan-{ni/*de} aru vase-NOM entrance-{GOAL/LOC} be 'There is a vase at the entrance' Then, what is the relevant property in the licensing of -ni phrase? To answer this question, let us consider the event structure of the verbs observed above. Pustejovsky (1995aPustejovsky ( , 1995b proposes that at the level of event structure_ t here are at least three types of events; transition, process, and state. Transition is composed of the other two types of events; process and state. Furthermore, the event structure has specifications of the order of sub-events and headedness, which provides a way of indicating a type of foregrounding and backgrounding of event arguments. Headedness is annotated by * in the event structure.Following the sub-eventual analysis presented by Pustejovsky (1995aPustejovsky ( , 1995b, the verbs in (5)-(9) are roughly represented as follows: (10) a. oku:P(el*) S(e2) P(el) S(e2*) e e T is an abbreviation for transition, P for process, and S for state. In the traditional terminology (Vendler 1957), transition corresponds to Achievement or Accomplishment and process corresponds to Activity. The difference between Achievement and Accomplishment is represented by the headedness.Based on the. above typology of events, the contexts where -ni phrase is licensed are (10a,b,d). The common feature of these event types is that they have a state component in their event structures. Thus, we conclude from this that -ni is licensed by a state component. To summarize this section, the licensing condition of a -ni 7 The reversed situation come to be true when the subject is an event nominal (Nakau 1999). Consider the following.(v) Sotugyoosiki-ga koodoo-{de/*ni} aru graduation ceremony-NOM hall-{LOC/GOAL} bèbèThe graduate ceremony will be held in the hall' Following Nakau (1999), we consider aru in this case as an 'active' be, not an existential be.phrase is stated below. (11) If a Japanese GOAL expression -ni is allowed in an expression describing an event, the event contains a state component in its event structure.2.2 Aruku 'walk' vs. Arui-te-iku 'go by walking' Based on the discussion above, we will observe the difference between aruku 'walk' and ctrui-te-iku 'go by walking' in this section. Before going into the discussion of aruku and arui-te-iku, we would like to spare some space for the description of ikùgo'.First observe that ikùgo' licenses a -ni phrase. (12) Taro-wa gakko-ni i-tta Taro-TOP school-GOAL go-PAST 'Taro went to school' In (12), we cannot cancel the terminal point. In (16b), if we use with a phrase which delimits the activity, such as 50 meetoru o '50 meters', the -de phrase would be acceptable, but without such a phrase, the lack of a terminal point prevents the -de phrase from being used in this sentence. Our conclusion is that aruku 'walk' has the same event structure as (10c). Now, we would like to show that the event structure of a compound verb arui-te-iku is the same as iku in that it does have a terminal point. Thus, its terminal point cannot be cancelled, nor it is modified by durational adverbs. (17) a. #Taro-wa gakko-ni arui-te-i-tta ga, gakko-ni tuka-naka-tta Taro-TOP school-GOAL wa1k-TE-go-PAST but school-GOAL arrive-not-PAST`Taro PAST`Taro walked to school, but he didn't reach there' b. Taro-wa gakko-ni {??30 punkan/30 pun-de} arui-te-i-tta Taro-TOP school-GOAL {30 minutes-for/30 minutes-in} walk-TE-go-PAST`Taro PAST`Taro walked to school for {30 minutes/in 30 minutes} The compositional verb arui-te-iku inherits this property from iku, obeying Righthand Head Rule (Williams 1981), in that the compositional verb gains a terminal point, which is inherited from iku.At this point, it is clear why -ni is licensed in (3) but not in (1). Since -ni is licensed by a state component as stated in (11), only arui-te-iku, containing a state component, licenses its occurrence.In this section, we have claimed that a Japanese GOAL expression -ni is licensed by a state component, and that only arui-te-iku 'go by walking', contrasted with arukùarukùwalk', has a state component, which allows -ni to occur in the construction. 
Syntactic Interference in Chinese-English Bilingual Children Interference is different from language borrowing. Language borrowing is systematic and collective; it is related to integration, which means features of one language are adopted as part of the other language. Monolinguals used these foreign features but are not likely to know anything about the language from which some features are borrowed. These loan words may only occur in one dialect of the 'languages but not other dialects. That is, the loan words appearing in South American English may not be used at all in other places where people speak English. As the examples provided in Mackey 1959, Banat German have diverse determiner uses in different villages. One village may use die Butter, die Auto while another may use der Butter; der Auto.Another term interlanguage' is also needed to be clarified. It was employed by Selinker 1972 and he proposed that the attempted production of a Target Language by a second language learner should be considered as a separate linguistic system. Adult language learners typically experience difficulty when learning a second language; the difficulties they encounter are due to the features appearing in L 1 but not in L2 or vice versa. Hence, the language they are acquiring is called interlanguage. The interlanguages are natural languages and systematic through their developments. As conceived, interlanguages are products of the interaction between two languages and the features in L 1 (the first language) and L2 (the second language) are expected to occur in such interlanguage. The researches have shown that L 1 plays the key role in the construction of interlanguage.Adjemian 1976 singled out three important characteristics of Interlanguage Hypothesis.The first one is`systematicityis`systematicity', which refers that interlanguages are natural languages and therefore they are not random collections of items. Based on this property, interlanguages have systematic structures and could be linguistically analyzed. The second is 'permeability', which means interlanguages are susceptible to L 1 or L2 rules. Native languages are usually stable and unsusceptible to other linguistic systems; however, interlangugages are constantly affected by Ll or L2 features. In other words, the rules and forms may be improperly acquired or generalized while the native languages permeate the developing interlanguage grammar in various levels. This is the major difference between interlanguages and native languages. The third property is 'fossilization', pointed out by Yip 1995, "the persistence of plateaus of non-target-like competence in the interlanguages ( Selinker &amp; Lakshamanan 1992)". That is, once the permeability is lost, the features in interlanguages will become fossilized. Which properties in interlanguages are susceptible to be fossilized are interesting to many second language learning researchers. The question whether second language learners could attain the full L2 competence are not of consensus; nevertheless, it is granted L2 learners may attain native-like competence in certain areas but completeness of acquisition in other areas of grammar remains an impossibility (Yip 1995).Interference is individual and contingent. (Mackey 1959) As Mackey describes " In the speech of bilinguals the pattern and amount of interference is not the same at all times and under all circumstances. The interference may vary with the medium, the style, the register, and the context which the bilingual happens to be using." Medium refers to spoken or written occasions. Style means the different discourses such as descriptive, narrative or conversational. Register is the social role that a speaker plays in any given occasion. In general, the occurrence of interference in bilinguals is usual but the types of interference are associated with contexts. Therefore, the analysis of interference should take contexts into consideration. Interference in each text is usually divided into six levels: cultural, semantic, lexical, grammatical, phonological and phonetic of a language. Grammatical interferences often include the introduction of foreign parts of speech, grammatical categories and function forms into the other language. An item belonging to a mistaken part of speech may be created.Interferences of grammatical categories may include the misuse of gender e.g. a bilingual may carry over the gender use in one language into another) or misuse of concord and government, for instance, a Chinese-English bilingual may say 'he sing well' because Chinese does have third person singular agreement. Interferences of function forms could be free or bound.Interferences of free forms indicate misuse of prepositions, conjunctions, determiners and etc.Bound forms include misuse of inflectional or derivational morphemes, zero modification, or reduplication. In this study, we focus on grammatical level, trying to find out the types of interference tokes in the Chinese-and-English bilinguals who acquire English as a second language at school. On the basis of the interference types, we assess and compare the language developments of the bilinguals with those of Chinese and English monolinguals so as to see whether learning a second language in an early stage affect their native language learning. language. At around age four, they began their English learning at Jump Start. Now they are aged around 7, and they have studied English for more than two and a half years. In other words, the four bilinguals are learning L2 in a quasi-immersion-program environment, where the instruction language is L2 and no classmates are native speakers of L2. (Dobrovolsky 1996) The four subjects are George, Angel, Andy, and Alex. C.. The English monolingual, Olivia, is around the same age and so is Wendy, the Chinese monolingual. 
Argumentness and Probabilistic Case Structures Researches on building Korean Case frames have been done by several organizations such as the National Academy of the Korean Language, Language Research Institute of Seoul National University, and the Center for Linguistic Informatics Research of Yonsei University, including the SEJONG 21 National Project. For English, there are several comprehensive Case frames such as FrameNet, COMLEX corpus, and LDOCE (Longman Dictionary of Contemporary English), etc. All these researches distinguish optional arguments from obligatory ones in the logic of black or white, assuming that arguments are the participants minimally involved in the activity or state expressed by the predicate, and they discuss only obligatory arguments (Dusan 1998, Hong 1997, Lee 1997, Song 1999). We note, however, that the term minimally is very vague. Furthermore, it is very dubious that such a simple binary distinction of obligatory vs. optional arguments is objectively well-grounded (Nam 1993). Concerning this issue, the present study recognizes that there is a significant difference in native speakers' intuition on whether or not an argument can be omitted. By establishing a cognitive model from a situation to an utterance, we explain why arguments' ellipsis occurs, though the traditional term ellipsis is not suitable under our new concepts. Here we devise two filters: an individual cognitive filter and an individual linguistic filter. Then, we claim that the binary distinction is not appropriate. Instead, we propose two types of new concepts, namely argumentness and probabilistic Case structures by adapting the prototype theory. Finally, we show that these concepts have several merits for NLP. 2 Cognitive Process of Arguments' Ellipsis This paper proposes that the argument structures be stated in a way that uses probabilities derived from a corpus to replace a Boolean-value system of subcategorization. To do this, we make a cognitive model from a situation to an utterance to explain the phenomena of arguments&apos; ellipsis, though the traditional term ellipsis is not suitable under our new concepts. We claim that the binary distinction is neither rational nor suitable for a real syntactic analysis. To solve this problem, we propose two new concepts argumentness and probabilistic Case structures by adapting the prototype theory. We believe that these concepts are effective in the syntactic analysis of NLP.
An Important Issue in Data Mining-Data Cleaning Data mining techniques are well accepted for discovering potentially useful knowledge from the large datasets. Our past research work on studying aspects of data mining includes improving the performance of the rule generation [SS95, SWC96], extending the scope of association rule mining [RSC99] and data generation [SLL96].Prior to data mining process, data cleaning is essential in that the quality of rules derived from the mining process is subject to the quality of data. Recently, significant attention is paid to record deduplication----an important branch of data cleaning. Various reasons are behind different representations of identical record: typographical errors, purposeful entry of false names, inconsistent data formats, incomplete information and registrant moving from one place to another.[LL+99] is a milestone paper in the area of record de-duplication. Experiments on real-world data demonstrate that the methods of de-duplicate records presented in [LL+99] are efficient. Further study indicates that there is still room for improvement in the core part of its whole technology----the algorithm of the calculation of the Field Similarity. Our paper is to introduce a new algorithm to calculate Field Similarity. Theoretical analysis, concrete examples and experimental result shows that our algorithm can significantly improve the accuracy of the calculation of Field Similarity.The rest of the paper is organized as follows. Section 2 gives a background description of the algorithm of calculating Field Similarity presented in [LL+99]. Section 3 proposes our algorithm of calculating Field Similarity and exhaustively compares the new algorithm with the previous one. Section 4 provides an experiment to prove the performance improvement with the introduction of the new algorithm. 
Building a Domain-Specific French-Korean Lexicon Korean government has chosen the French TGV as a high-speed transportation system and the first service is scheduled at the end of 2003. TGV-relevant documents are consisted of 700 thousands pages, of which 170 thousands pages are written only in French language and the rest has been translated in English. A few locomotive engineers or railroad officers have reached a certain level of proficiency in comprehending French. As a language itself, English has more advantages to be better understood than French in Korea. Unfortunately however, a large part of the English version is not comprehensible without referring to the original French version. The successful translation of manuals of high-tech machines needs (1) the fluency in object and target languages, (2) and the expert knowledge in the specific domain in question. It is hard, however, to find out human translators who have both of those 2 totally different capacities. The ideal situation might be that a pair of the language expert and the domain expert work together. But it is certainly a time-consuming and costly job, if there are a great number of object documents. The time effectiveness is a very important factor in the translation of manuals, since the new technology described in the manuals might be no longer valuable some time after. One of the solutions for the efficient and effective translation consists in classifying those pair works in 3 steps, provided those experts are equipped with appropriate tools. In the first step, the language experts translate the target documents into the first and rough version of the object language, with the domain-specific terminology lexicon. In the second step, the domain experts, who have not a good mastery of the object language, can examine the correctness of contextual meaning of roughly translated documents, using the bilingual or multilingual lexicon. In the third and final step, both experts can focus on deciphering jointly the incomprehensible parts. These processes could be done manually. But it would be certainly less time-consuming and more efficient that there is a kind of network-based translator's workbench where they can do differentiated jobs quasi-simultaneously. A domain-specific terminology lexicon is a prerequisite for these processes (Zingle, 1999).However, there are some technical problems in developing domain-specific terminology lexicon, especially when it contains European languages in Korea. In order to support the input/output of diacritics used in most European languages, a professional knowledge of computation is highly required in Korean computational circumstances (Jeong &amp; Yoon 1998, Jeong &amp; Yoon 1999, Yoon &amp; alii 1999, Yoon &amp; alii 2000. Coupled with the technical problems, there are no developmental tools to be easily used. Some SGML (Standard General Markup Language)-Based tools, which have been developed so far by computer scientists, provide a fixed DTD for specific dictionaries and there are not concern about the solution for European languages (Choi &amp; alii 1996, Kang 1996) Since developing a domainspecific terminology lexicon is a time-consuming work, it is likely that a widely separated group of language experts and domain knowledge experts work together. In order that one can communicate one's ideas and work with others effectively, the developmental tool can provide them the interconnectivity.The goal of this paper to demonstrate how DiET (Dictionary EdiTor) 2.5 makes it possible to build with ease domain-specific terminology lexicon that may contain multimedia and multilingual data with multi-layered logical information. The functions of DiET 2.5' will be presented and described in detail as a powerful lexicon builder. DiET is a network-based developmental tool for multilingual lexica or corpora, which can support also multimedia data with multi-layered text data, so that domain experts who are not good at computation or computer use can easily construct their expert knowledge in structured information. Section 2 overviews the system architecture of DiET which enables developers to work jointly from different places, and shows how to define a DTD and to design a database for a selected domain-specific terminology lexicon. Section 3 introduces solutions for the multilingual and multimedia support. Then, we conclude with notes on the future work, in Section 4. Korean government has adopted the French TGV as a high-speed transportation system and the first service is scheduled at the end of 2003. TGV-relevant documents are consisted of huge volumes, of which over than 76% has been translated in English. A large part of the English version is, however, incomprehensible without referring to the original French version. The goal of this paper is to demonstrate how DiET 2.5, a lexicon builder, makes it possible to build with ease domain-specific terminology lexicon that may contain multimedia and multilingual data with multi-layered logical information. We believe our wok shows an important step in enlarging the language scope and the development of electronic lexica, and in providing the flexibility of defining any type of the DTD and the interconnectivity among collaborators. As an application of DiET 2.5, we would like to build a TGV-relevant lexicon in the near future. 1 Introduction Korean government has chosen the French TGV as a high-speed transportation system and the first service is scheduled at the end of 2003. TGV-relevant documents are consisted of 700 thousands pages, of which 170 thousands pages are written only in French language and the rest has been translated in English. A few locomotive engineers or railroad officers have reached a certain level of proficiency in comprehending French. As a language itself, English has more advantages to be better understood than French in Korea. Unfortunately however, a large part of the English version is not comprehensible without referring to the original French version. The successful translation of manuals of high-tech machines needs (1) the fluency in object and target languages, (2) and the expert knowledge in the specific domain in question. It is hard, however, to find out human translators who have both of those 2 totally different capacities. The ideal situation might be that a pair of the language expert and the domain expert work together. But it is certainly a time-consuming and costly job, if there are a great number of object documents. The time effectiveness is a very important factor in the translation of manuals, since the new technology described in the manuals might be no longer valuable some time after. One of the solutions for the efficient and effective translation consists in classifying those pair works in 3 steps, provided those experts are equipped with appropriate tools. In the first step, the language experts translate the target documents into the first and rough version of the object language, with the domain-specific terminology lexicon. In the second step, the domain experts, who have not a good mastery of the object language, can examine the correctness of contextual meaning of roughly translated documents, using the bilingual or multilingual lexicon. In the third and final step, both experts can focus on deciphering jointly the incomprehensible parts. These processes could be done manually. But it would be certainly less time-consuming and more efficient that there is a kind of network-based translator&apos;s workbench where 465
  
PACLIC17 Conference Committees  
Virtual Linked Lexical Knowledge Base for Causality Reasoning  The A virtually linked knowledge base is designed to utilize a pre-constructed knowledge base in a dynamic mode when it is in actual use. An algorithm is proposed for causality reasoning based on a set of lexical knowledge bases that contain information about such items as event role, is-a hierarchy, relevant relation, antonym, and other features. These lexical knowledge bases have mainly made use of lexical features and symbols in HowNet. Several types of questions are experimented to test the effectiveness of the algorithm here proposed. In particular, questions of the form why are treated to show how causality reasoning works.
Focus-Marking in Chinese and Malay: A Comparative Perspective In the literature, `Focus'-related issues have been studied from different perspectives. Following Culicover andRochemont (1983, 1990) and Horvath (1986), we in this paper assume that this essentially semantic conception of 'Focus' can be characterized as a purely formal syntactic feature [+Focus] or [+F], which gets assigned to constituents at a certain appropriate level of syntactic representation, triggering such syntactic operations such as 'Movement' and 'Adjoining' under the general syntactic principles and constraints. This paper is organized as follows: In Section 2 we will first review some basic assumptions about the formal characterization of [+FOCUS], then moving quickly onto the question of how [-I-FOCUS] is reflected in the formal syntax, especially how it is marked syntactically. Section 3 is devoted to a discussion of the nature of Focus Mark shi A in Chinese. Comparable phenomena in Malay are viewed from a comparative perspecitve in Section 4. We will demonstrate that question words are inherently assigned the focus feature in Section 5. Section 6 is a short one on some remarkable and relevant phenomena from historical and dialectal grammars of Chinese. Our major conclusions are summarized briefly in Section 7. Cross-linguistically, there are two devices for grammar to process the focus mark [+FocusJ: the fronting of focused constituents and the insertion of a Focus Mark such as the English &apos;be&apos; before focused constituents. In this mode of formulation, a comparative study of Focus and Focus-Marking in Chinese and Malay has been conducted. These two languages are similar and different. They are similar in opting for the use of Focus Mark instead of focused constituents movement. They are different in the nature of Focus Mark itself Focus mark is the copular verb SHI A in Chinese, but, as the language simply does not have a copular verb, in Malay two complementary particles KAH/LAK are chosen, and all other contrasts in Focus-Marking between the two languages are demonstrated to follow from the difference in the nature of Focus Mark and some other independently motivated conditions in a modularized theory of grammar.
A Constraint-Based Analysis of Association with Focus in Japanese  In this paper, I present an explicit analysis of association with focus in Japanese. The proposed formal analysis, basically couched in Rooth&apos;s (1985) alternative semantics supported by the syntactic devices of HPSG, captures the behaviors of focus particles in Japanese in a fairly straightforward manner. Section 1 is devoted to a survey of data and definitions of key terms and concepts in this paper. In section 2, I present my analysis and explain in detail how it accounts for the observed phenomena.
On the Event Structure of Indirect Passive in Japanese Linguists normally assume that the active sentence in (1 a) and the passive sentence in (lb) designate the same situation, rejecting the idea that the active and passive sentences denote distinct events.(1) a. The assassin killed the senator. b. The senator was killed by the assassin.The situation described by the English active and passive sentences in (1) can be expressed as an active sentence and its passive counterpart in Japanese as shown below:(2) a. Ansatusha-ga giin-o korosita. assassin-Nom senator-Acc kill-past "The assassin killed the senator."b. Giin-ga ansatusha-ni koros-are-ta. senator-Nom assassin-by kill-pass.-past "The senator was killed by the assassin."Both of the active and passive sentences denote the same single event. Passive sentences of this type are called direct passives.Japanese has yet another type of passive sentences, called indirect passives. Indirect passives can be formed on the basis of either transitive or intransitive verbs. The sentence in (3b) is an example of the indirect passive based on the intransitive verb in (3a). (3) a. Kodomo-ga nai-ta. child-Nom cry-past "The child cried."b. Taroo-ga kodomo-ni nak-are-ta Taro-Nom child-by cry-pass.-past "Taro was adversely affected by the child's crying."The intransitive verb nak (cry) can be passivized as shown in (3b). The active and passive pair in (3) denote different situations. In the passive, a new participant (the syntactic subject) is added to the event denoted by the base verb.The sentence in (4b) is an instance of indirect passive based on a transitive verb.(4) a. Tonari-no gakusei-ga piano o asa made hiita. neighboring-Gen student-Nom piano-Acc morning-until played "The neighboring student played the piano until morning."b. Hanako-ga tonari-no gakusei-ni piano"-o asa-made hik-are-ta. Hanako-Nom neighboring-Gen student-by piano-Ace morning-until play-pass.-past "Hanako was adversely affected by the neighboring student's playing the piano until morning."The indirect passive sentence in (4b) contains the transitive verb hik (play). The object of the verb (the piano) remains in the passive. And the event denoted by the passive sentence involves an extra participant, represented as the syntactic subject, which is not originally involved in the event denoted by the transitive verb. The indirect passive generally has an adversative meaning. As indicated in the English glosses, the indirect passive sentences have an implication that the new subjects (Taroo in (3b) or Hanako in (4b))are adversely affected by the evenst denoted by the active counterparts. Lexical semantic research in the last several decades has developed the idea that the meaning of a verb can be analyzed in terms of a structured representation of the event that the verb denotes. The representation is often referred to as Event Structure. I would like to show that the indirect passive exhibits the event structure and qualia structure which follow from the causative lexical conceptual paradigm proposed within the model of generative lexicon (Pustejovsky 1995). Focusing on two types of passives in Japanese, I will suggest that a mapping condition between event structure and syntax explains the syntactic realization of arguments, and that an elaboration of the agentive qualia role is needed to account for the selection of verbs in the passive. This paper presents an analysis of indirect passives in Japanese in terms of event structure and qualia structure proposed in the framework of the generative lexicon (Pustejovsky 1995). On the assumption that the event structure of the indirect passive construction is based on the causative structure, the present analysis accounts for the adversative interpretation of indirect passive sentences, the selection restriction on verbs, and the obligatory presence of the adjunct phrase.
An Event-Based Interpretation of Japanese Honorific Constructions Using RRG Operators The contemporary subject and non-subject honorific constructions involving the combination of the prefix o or go and the ren'you form of verbs (called the gerundive by Martin), which came to be widely used only in the late 19th century, are relatively latecomers in the history of honorific expressions in Japanese (Tuzimura 1968). The predicates of the two constructions have the following forms:(1) a. The subject honorific predicate o/go-V ni naru b. The non-subject honorific predicate o/go-V suru V in both predicates is the ren'you form of a verb, and o and go are prefixes of honorification of presumably Chinese descent. Ni in (la) is the dative case marker, and naru is a verb which I will argue is a grammaticalized version of the verb naru meaning 'to happen'. Suru in (lb) is a verb which I will also claim to be a grammaticalized version of the verb suru 'to do '. In this paper, I will approach this problem from the viewpoint of the universal operator hierarchy of RRG. I will present an analysis of the subject honorific construction based on a functional analysis of existential and copulative verbs, and relate it to adjectival verbs. I will also sketch a possible origin of the non-subject honorific construction.According to RRG, a clause has a tripartite structure.(2) (((Nucleus ) Core) Periphery)The innermost layer, the nucleus, is composed of the predicate of the clause, while the next layer, the core, is composed of the predicate and its core arguments. The outermost layer, the periphery, is made up of all other constituents of the clause. Each layer is associated with a distinct set of operators, which have the layer as their scope. Traditional grammatical categories such as tense, aspect and mood are among these operators. For example, RRG holds that these three operators belong to different layers, which is seen in the different ordering possibilities of these operators. The following is the universal layered structure of the clause as proposed in RRG. The contemporary Japanese verbal honorific constructions can be motivated in view of the RRG clausal organization and its operator hierarchy. I will present an analysis of the subject honorific construction based on a functional analysis of existential and copulative verbs, and relate it to adjectival verbs. I will also sketch a possible origin of the non-subject honorific construction.
Multiple Nominative Constructions in Japanese and Their Theoretical Implications In a derivational theory of syntax, problems of choice arise when there are two or more potentially possible steps at a single stage of a derivation. Chomsky (1995,2000) proposes that, if Attract/Move and Merge are both potentially possible, Merge is chosen (Merge-over-Move). It has also been proposed in the literature that, when there are two or more elements that can potentially be moved, the element closer to the target than the other(s) is moved (Attract/Move the Closest). Discussing multiple nominative constructions in Japanese, I argue that the set of principles of this kind contains a principle that minimizes the size of moved elements, and show a new piece of evidence for Merge-over-Move.Multiple-nominative sentences in Japanese are classified into (at least) two types: those that involve a relation of inalienable possession between the nominative DPs ((1), MNC1), and those that do not ((2), MNC2).(1) a.Taro-ga te-ga naga-i Taro- Nihon-de itiban Tokyo-ga kootuu-j iko-ga oo-i Japan-LOC most Tokyo-NOM traffic accident-NOM many-PRES`In PRES`In Japan, Traffic accidents most often occur in Tokyo.' c.Kono bangumi-ga yuumei-na haiyuu-ga yoku shutuen-su-ru this program-NOM famous actor-NOM often appearance-do-PREs`Famous PREs`Famous actors often make appearance on this program.' I consider only sentences with two nominative DPs for simplicity and refer to the first one as the NDP1 and the second one as the NDP2. 1 I assume that the MNC1 and the MNC2 are derived differently (Takahashi 1994;Tateishi 1994): the MNC1 is derived from a source in which the NDP1 is contained in the [Spec, D] of the NDP2, like genitive possessors; the MNC2 is derived by inserting the NDPs each into different positions. I consider (i) what the overt structure of the MNC1 and that of the MNC2 are like and (ii) why only the proposed structure can be derived from their respective underlying source with the other potentially possible options being blocked. This paper studies the derivation of multiple nominative constructions (MNC) in Japanese. First, discussing the MNC-sentences in which there is a relation of inalienable possession between nominative noun phrases, I will argue that the set of local economy principles that choose among potentially possible steps at a single stage of a derivation contains a principle that minimizes the size of moved elements. Second, considering the derivation of the MNC-sentences in which there is no relation of inalienable possession between nominative noun phrases, I will show a new piece of evidence for the Merge-over-Move principle.
Automatic Learning of Stemming Rules for the Indonesian Language The Internet and other modem communication infrastructures can be the media for either the predominance of cultural globalization or for the blooming of cultural diversity and culture interchange. The choice of the kind of information society that we are creating is, in part, in the hands of technologists and researchers devising techniques and designing tools for facilitating the maintenance, management of multi-cultural information. One of the central issues in the management of multicultural information or the management of information in a multicultural context is the processing of natural language. There are still more than 6000 living languages at the dawn of the twenty first century. Tools for the processing of speech and text often require large collections of reference data such as dictionaries, morphological rules, grammars, and phonemes. Such collections are expensive to build since they normally require human expert intervention. Our research is concerned with the economical and therefore semi-automatic or automatic acquisition of such linguistic information necessary for the development of other-than-English or multilingual information systems. Our motivation comes from the desire to build computational linguistics and information retrieval tools for the processing and retrieval of documents written in the Indonesian language. Indonesian is the official language of the republic of Indonesia. Although several hundreds regional languages and dialects are used in the Republic, it is spoken and understood by more than two hundred and forty million people. Indonesian is written using roman script and there are more than 1,000,000,000 documents in Indonesian Language on the public World Wide Web.Stemming is the process of finding the stem (root) of a word, by stripping away the affix attached to the word. In many languages words are often obtained by affixing existing words or roots. Stemming is particularly useful in applications in which morphologically-related words are processed or treated in the same way regardless of their forms, for example in a text classifier, information retrieval system, and in dictionary lookup tool. Studies have shown that in application like information retrieval system, inflectional morphology plays a more important role as compared to derivational morphology. This is because derivational morphology gives rise to the generation of words belonging to different classes, which sometimes lead to a big difference in meaning. For example, consider the phrase "compute matrix product" and "usage of matrices in computational linguistic". When we use these two phrases as a search string in some search engine, we would expect matrix and matrices to be regarded as the same word. However, if we consider computational and compute as the same word, a search on "compute matrix product" will generate pages related to computational linguistics, which are quite unrelated to what we actually search for.However, in a dictionary lookup tool, particularly one for the Indonesian language, it turns out that both types of morphology are significant. The words listed as entries in an Indonesian dictionary are usually root words; the various affixes applicable to a root word are usually listed together in the corresponding entry. Thus, to find a word in the dictionary, we must first find its stem before proceeding to find the corresponding entry of the root word in the dictionary.Most stemming rules in the Indonesian language are simple. Prefixes and suffixes are attached to the front and the back of a word. However, there are certain characteristics of the stemming rules that complicate the process. Plurals are usually presented as duplications of the stem. Infixes, inherited from the Javanese language, although not frequently used anymore, are also known. Affixing frequently results in the transformation of a letter of the root word. The transformation can be in the form of morphing, insertion, or deletion of letter(s) as follows:• Morphing of letters) Morphing in the Indonesian language usually applies to the first letter of the root word, in which that letter is replaced by some other letter(s). An example of morphing is when the prefix "me-" is attached to the word "taxi", the letter 't' in the stem is morphed into 'n' such that the affixed word would be "menari". Another example, "me-" attached to "sapu" becomes "menyapu" (the letter 's' is replaced by`nyby`ny').• Insertion of letter(s) When the prefix "me-" is attached to the word "baca", the letter 'm' is inserted in between the prefix and the stem, such that the affixed word would be "membaca".• Deletion of letters)When a prefix ending with a certain letter is attached to a stem starting with the same letter, one of the duplicate letters is discarded. For example, the prefix "ber-" attached to "renang" becomes "berenang" instead of "berrenang". We present a method for the automatic learning of stemming rules for the Indonesian language. The learning process uses an unlabelled corpus. In the first phase the candidate (word, stem) pairs are automatically extracted from a set of online documents. This phase uses a dictionary but is nevertheless not trivial because of morphing. In the second phase the rules are induced from the thus obtained list of pairs of words with their respective stems. We evaluate the effectiveness of our method for different sizes of the training set with different settings on the thresholds of the support and confidence of each rule. We discuss how these variables affect the quantity and quality of the rules produced. 1 Introduction The Internet and other modem communication infrastructures can be the media for either the predominance of cultural globalization or for the blooming of cultural diversity and culture interchange. The choice of the kind of information society that we are creating is, in part, in the hands of technologists and researchers devising techniques and designing tools for facilitating the maintenance, management of multi-cultural information. One of the central issues in the management of multicultural information or the management of information in a multicultural context is the processing of natural language. There are still more than 6000 living languages at the dawn of the twenty first century. Tools for the processing of speech and text often require large collections of reference data such as dictionaries, morphological rules, grammars, and phonemes. Such collections are expensive to build since they normally require human expert intervention. Our research is concerned with the economical and therefore semi-automatic or automatic acquisition of such linguistic information necessary for the development of other-than-English or multilingual information systems. Our motivation comes from the desire to build computational linguistics and information retrieval tools for the processing and retrieval of documents written in the Indonesian language. Indonesian is the official language of the republic of Indonesia. Although several hundreds regional languages and dialects are used in the Republic, it is spoken and understood by more than two hundred and forty million people. Indonesian is written using roman script and there are more than 1,000,000,000 documents in Indonesian Language on the public World Wide Web. Stemming is the process of finding the stem (root) of a word, by stripping away the affix attached to the word. In many languages words are often obtained by affixing existing words or roots. Stemming is particularly useful in applications in which morphologically-related words are processed or treated in the same way regardless of their forms, for example in a text classifier, information retrieval system, and in dictionary lookup tool. Studies have shown that in application like information retrieval system, inflectional morphology plays a more important role as compared to derivational morphology. This is because derivational morphology gives rise to the generation of words belonging to different classes, which sometimes lead to a big difference in meaning. For example, consider the phrase &quot;compute matrix product&quot; and &quot;usage of matrices in computational
A Simplified Latent Semantic Indexing Approach for Multi-Linguistic Information Retrieval 1 Multi-linguistic Information Retrieval (MLIR for short, also "translingual" or "cross-language" IR) enables a query in one language to search document collection in another one or more languages. Many monolingual IR approaches can be extended to multi-linguistic environment and among them Latent Semantic Indexing (LSI for short, Deerwester et al., 1990) has proved effective (Y. Yang et al., 1997, Douglas William Oard, 1996). The particular technique used in LSI is singular-value decomposition (SVD for short), in which a large term-by-document matrix is decomposed into a set of orthogonal factors from which the original matrix can be approximated by linear combination. However, SVD on the large term-by-document matrix whose size increases with the size of the training corpus brings huge computation costs. This situation becomes even worse when we use LSI for MLIR because the training matrix consisting of various languages is always several times larger. To reduce the cost of SVD in LSI and thus make it feasible in MLIR, we try to exploit the semantic symmetry hidden in the training corpus. We find that theoretically if the term-by-document matrices of multi-linguistic training set have either a weak symmetry form or a strong symmetry form, the SVD step of LSI in multi-linguistic environment can be simplified. Both symmetry forms have clear meanings in the context of MLIR. Further, though we can never reach precisely either of two symmetry forms from real world data, two possible methods are raised to enhance the strong form symmetry of the term-by-document matrices. Our small-scale experiment gives a satisfying result though we only roughly keep the strong symmetry.In section 2 we will briefly introduce how LSI approach can be extended to multi-linguistic environment. In section 3 we will prove some theorems for the LSI simplification in the two symmetry forms, then discuss symmetry enhancement issues for real world data. Experiments and results will be 1 Supported by the National Natural Science Foundation of China (No. 60003004) given in section 4. Finally in section 5 we will draw our conclusions with some problems for future work. 2 LSI for MLIR LSI is based on the vector space mode (VSM), in which both queries and documents are represented as vectors of term weightsq is the query vector, d is the document vector, m is the number of unique terms (words, phrases or word clusters) in the corpus after stop-word removal and stemming, qi and di are term weights in the query and the document respectively. Terms are usually weighted by term frequency, term frequency inverted document frequency (TFIDF), information gain or other weighting schemes. In monolingual IR, the similarity between a query and a document is defined asLSI is a one-step extension of VSM. The claim is that neither terms nor documents are the optimal choice for the orthogonal basis of a semantic space, and a reduced vector space consisting of the most meaningful linear combinations of documents would be a better representative basis for the documents content.In The bilingual case above can be easily extended to multi-linguistic cases. When the training corpus has more than one target language, the matrix W will be composed of all the term-by-document matrices of the languages involved. Suppose that the training corpus consists of 1 languages, and for each language Li we choose mi terms and the same n matching documents. We define Ai as the term-by-document matrix for the training documents in the i-th language. That the documents of different languages are "matching" means out of n training documents of any language the i-th one has the same content but in different languages. Accordingly we align the corresponding columns representing those matching documents to the same position in A. Now the term-by-document matrix W representing the entire training corpus is defined as A1 = A2 Latent Semantic Indexing (LSI) approach provides a promising solution to overcome the language barrier between queries and documents, but unfortunately the high dimensions of the training matrix is computationally prohibitive for its key step of Singular Value Decomposition (SVD). Based on the semantic parallelism of the multi-linguistic training corpus we prove in this paper that, theoretically if the training term-by-document matrix can appear in either of two symmetry forms, strong or weak, the dimension of the matrix under decomposition can be reduced to the size of a monolingual matrix. The retrieval accuracy will not deteriorate in such a simplification. And we also discuss what these two forms of symmetry mean in the context of multi-linguistic information retrieval. Although in real world data the term-by-document matrices are not naturally in either symmetry form, we suggest a way to make them appear more symmetric in the strong form by means of word clustering and term weighting. A real data experiment is also given to support our method of simplification.
An integrated approach for Chinese word segmentation Chinese word segmentation aims to recognize the implicit word boundary delimiters in plain Chinese texts, which plays very important roles for most text-based applications, such as machine translation, information retrieval, text-t-speech synthesis and many more. During the past decades, many different techniques have been proposed for Chinese word segmentation, ranging from dictionary or rule based methods ( Liang and Zheng 1991;Yeh and Lee 1991), statistical approaches ( Fu and Wang 1999;Nie et al. 1995;Teahan et al. 2000;Wang et al. 2000;Yao 1997;Zhang et al. 2002), to machine learning approach (Hockenmaier and Brew 1998;Palmer 1997;Xue and Converse 2002). However, we are still faced with word boundary ambiguities and unknown words while developing a high-performance system for practical applications. What is more, ambiguity resolution and unknown word identification are often taken as two independent stages in previous word segmentation systems for Chinese. In this point, disambiguation is considered as a unique problem related to known word segmentation and unknown word identification is taken as a post-processing of known word segmentation. Although this two-stage strategy is simple and applicable, it usually fails to yield correct results for some complicated cases such as a mixture of ambiguities and unknown words. For instance, in a Chinese sentence 1 13 IT , the correct segmentation for the fragment 11:1 4T-LEX should be 11:1 X/ (zhonglhang2 chang2ge2, Bank of China/Changgel). However, Most two-stage systems cannot yield this segmentation because this string has been wrongly segmented as /IT -K/ / (zhongl hang2zhang3 ge2, middlelpresidentlGe6 in process of known segmentation and there is no a mechanism to re-segment the word IT (hang2zhang3, president) during unknown word identification. Here, f7 (zhonglhang2, Bank of China) is an abbreviation of organization name, and (chang2ge2, Changge) is a place name.Recently, a variety of methods have been reported for this problem. Wu and Jiang (1998&amp;2000) take word segmentation, including unknown word identification as an integral part of sentence analysis. This mechanism provides a word lattice to store all the possible words and use a full sentence parsing to achieve the final disambiguation. However, the parser coverage may restrict its applications in practical systems. Furthermore, this mechanism requires extra linguistics knowledge such as partof-speech to yield correct results for the input. Lately, Zhang et al (2002) presents a novel method for word segmentation based on role tagging. They define a set of unknown word roles about varied internal components and contexts in their system. As a result, their system can recognize different types of unknown words, including the case mentioned above, despite that their method is also a twostage segmentation. However, a role-tagged corpus is needed in their work to learn role knowledge, which is not always available in practice.To address above problems, this paper presents an integrated word segmentation approach for Chinese, which can perform disambiguation and unknown word identification simultaneously on the input. In this work, a hybrid model is used to score known word candidates and unknown word candidates equally, which incorporates the modified word-formation models (viz. word-juncture models and word-formation patterns) into word bigram models. In this way, different types of features are statistically computed and combined for the integrated segmentation, including internal wordformation power of component words of word candidates, affinity relations between these components and the external contextual information. Furthermore, a filter algorithm is also proposed to enhance correctness and avoid combination explosion in word candidate construction.The rest of this paper is organized as follows: Section 2 focuses on statistical modelling for integrated segmentation. Section 3 describes in detail the algorithm for integrated word segmentation. In section 4, we report our experiments on Peking University corpus, and in the final section we give our conclusions on this work. This paper presents an integrated approach for Chinese word segmentation, which can perform disambiguation and unknown word identification simultaneously on the input. In this work, a hybrid model is used to score known word candidates and unknown word candidates equally by incorporating the modified word-formation models (viz. word-juncture models and word-formation patterns) into word bigram models, with which different types of features are statistically computed and combined for this integrated segmentation, including internal word-formation power of components in a word, affinity relations between these components and the external contextual information. To enhance the precision and avoid the problem of combination explosion in word candidate construction, a filter algorithm is also given to block ineligible unknown word candidates. In this way, ambiguity and unknown word can be resolved effectively. The results of our experiment on Peking University corpus show that the integrated approach outperforms the other two-stage methods under discussion.
Korean Phrase Structure Grammar and Its Implementations into the LKB System Though there exist various morphological analysers developed for Korean, no serious attempts have been made to build its syntactic or semantic parser(s), partly because of its structural complexity and partly because of the existence of no reliable grammar-build up system. This paper presents a result of our on-going project to build up a computationally feasible Korean Phrase Structure Grammar (KPSG) and implementing it into the LKB (Linguistic Knowledge Building) system.The grammatical framework we adopt for KPSG is the constraint-based grammar, HPSG (Pollard andSag 1994, Sag andWasow 1999). The grammar HPSG (Sag and Wasow 1999) is well suited to the task of multilingual development of broad coverage grammars. HPSG is a constraint-based, lexicalist approach to grammatical theory that seeks to model human languages as systems of constraints on typed feature structures. In particular, the grammar adopts the mechanism of type hierarchy in which every linguistic sign is typed with appropriate constraints and hierarchically organized. The characteristic of such typed feature structure formalisms facilities the extension of grammar in a systematic and efficient way, resulting in a linguistically precise and theoretically motivated descriptions of Korean. In addition, we adopt a flat semantic formalism Minimal Recursion Semantics (MRS) in representing semantics ( Copestake et al. 2001). MRS is proved to be flexible and well work with the Korean typed feature structures too.The basic tool for writing, testing and processing the KPSG is the LKB system (downloadable from http://www-csli.stanford.edu/ aac/lkb.html, Copestake 2002). The LKB system is a grammar and lexicon development environment for use with constraint-based linguistic formalisms such as HPSG. 
Porting Grammars between Typologically Similar Languages: Japanese to Korean The Parallel Grammar project (ParGram) is an international collaboration aimed at producing broad-coverage computational grammars for a variety of languages ( Butt et al., 1999;Butt et al., 2002). The grammars (currently of English, French, German, Japanese, Norwegian, and Urdu) are written in the framework of Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982;Dalrymple, 2001), and they are constructed using a common engineering and high-speed processing platform for LFG grammars, the XLE ( Maxwell and Kaplan, 1993). These grammars, as do all LFG grammars, assign two levels of syntactic representation to the sentences of a language: a superficial phrase structure tree (called a constituent structure or c-structure) and an underlying matrix of features and values (the functional structure or fstructure). The c-structure records the order of words in a sentence and their hierarchical grouping into phrases. The f-structure encodes the grammatical functions, syntactic features, and predicate-argument relations conveyed by the sentence. F-structures are meant to encode a language universal level of analysis, allowing for cross-linguistic parallelism at this level of abstraction.The ParGram project attempts to test the LFG formalism for its universality and coverage and to see how far parallelism can be maintained across languages. Previous ParGram work and much theoretical analysis has largely confirmed the universality claims of LFG theory. The f-structures produced by the grammars for similar constructions in each language have the same major functions and features, with minor variations across languages (e.g., the f-structures for French nouns have a grammatical gender feature but that distinction is not marked in English f-structures). This uniformity has the computational advantage that the grammars can be used in similar applications and that machine translation (Frank, 1999) can be simplified.We have found that it takes roughly two person-years of effort to construct for a new language a grammar that approximates existing grammars in terms of coverage and accuracy (see ( Riezler et al., 2002) for a discussion of the coverage and accuracy of the current English grammar). This suggests that the deep-grammar construction task is not as difficult as many people have believed, and indeed may require less effort than would be needed to produce training materials for automatic learning procedures for shallower grammars. However, it is still interesting to explore methods for reducing the linguistic effort that grammar construction requires. To that end, we report here on a preliminary investigation of the difficulty of converting a grammar of one language into a grammar of a typologically similar language. In this investigation, we started with the ParGram grammar of Japanese ( Masuichi and Ohkuma, 2003) and used that as the basis for a grammar of Korean.Typologically similar but not necessarily genetically related languages are those that not only admit of similar f-structures, as LFG theory suggests is the case with all languages, but also have similar c-structure to f-structure mappings. Whether or not Japanese and Korean are genetically related (an issue that is in some dispute; see (Sohn, 1999) for some discussion), Japanese and Korean are typologically similar in at least the following ways: they both are verb final and more generally head final, have relatively free word order, use postpositions to mark grammatical functions, and exhibit rampant pro-drop. We report on a preliminary investigation of the difficulty of converting a grammar of one language into a grammar of a typologically similar language. In this investigation, we started with the ParGram grammar of Japanese and used that as the basis for a grammar of Korean. The results are encouraging for the use of grammar porting to bootstrap new grammar development.
Mandarin Chinese Shenme in Interaction*  
The Semantics of Shapes: A Study based on Mandarin Quanlzi5 (NH)  Mandarin shape nouns, such as afanglxing2 &apos;square&apos; and sanljiao3xing2 &apos;triangle&apos;, share a set of very interesting lexical semantic features. These nouns can refer to either the contour (i.e. the outside edge) or enclosed area of the shape. In this paper, we will try to explain this lexical semantic fact both in terms of the cognitive theory of grounding and the visualization. Our study will be focused on quanlzi5 &apos;circle&apos;, which has the typical semantic behaviors of the shape nouns but also allow two additional interesting meaning extensions. 1. Background: the meaning of shape nouns Shape is an instance of a visual configuration while people perceive or recognize an object (Zusne 1970). We name these visual forms in term of their stimulous properties that we identified, such as a triangle is composed by three co-terminated lines that form three angles, and a circle is a continuous curved line without angles. By studying the semantics of shape nouns, we can gain substantial knowledge about what prominent properties people perceive while identifying shapes, and what cognitive concept that underlying the meaning of shape nouns. Mandarin shape nouns, such as a fanglxing2 &apos;square&apos; and sanifiao3xing2 &apos;triangle&apos;, share a set of very interesting lexical semantic features. These nouns can refer to either the contour (i.e. the outside edge) or enclosed area of the shape. In this paper, we will try to explain this lexical semantic fact both in terms of the cognitive theory of grounding and the visualization. Our study will be focused on quanlzi5 &apos;circle&apos;, which has the typical semantic behaviors of the shape nouns but also allow two additional interesting meaning extensions. In the following, Section 2 presents the data of quanizi5. Section 3 describes the sense distinction of the meaning of quanlzi5. Section 4 explains the lexical semantics of shapes in term of the figure/ground theory and the visualization. Section 5 is a conclusion. 115 2. The Meaning of Quanlzi5 &apos;Circle&apos;: the data Like all shape nouns, quanlzi5 has the basic meanings of a circular contour (1) or the area enclosed by the circular contour (2). In addition, its area meaning can be extended metaphorically to refer to an area defined by human activity (3), an abstract confinement (4), and a set of related people often defined by their social strata (5). And its linear contour meaning can be extended to refer to a path where the end point coincides with the start point (6). (1) tamen weicheng yige quanzi tiaowu they surround-be a circle dance `They made a circle to dance.&apos; (2) &apos;t gg ni zhi neng zhan zai quanzi nei bu keyi paochulai you only allow stand circle inside NEG allow get out `You can only stand inside the circle. Don&apos;t get out.&apos; (3) ta de shenghuo quanzi jiushi taida he gongguan she DE living circle just NTU and GongGuan `Her usual circle of activity just covers NTU (campus) and GongGuan.&apos; (4) .-3ze.. 4s)c-T-7-ft A jiaoyou shi kuoda shenghuo quanzi de yizhong fangshi make-friends is expand life circle DE a way `Making (new) friends is a way to expand (your) circle of life.&apos; (5) Rig *A-141 ta zai meiguo de huaren quanzi hen youming she BE America DE Chinese-people circle very famous `She is famous in the Chinese societies in America.&apos; VO, women zai shanshang milu le yizhi zai rao quanzi we at mountain lost-way LE always ASP circle circle `We got lost in the mountain and were going around in circles.&apos; 116 quanlzi5 in (1) and (2) displays an original and physical meaning respectively, and has more abstract meaning in (3)-(6). Like all lexical polysemy, ambiguity between physical and metaphorical meanings of quanlzi5 is attested, such as in (7). Quanlzi5 has three possible readings: a professional
 The language of economy frequently involves conceptual metaphors. Charteris-Black (2000), for instance, suggested that the metaphoric lexis in the Economist were higher in frequency than in a general magazine. In describing the movement of the economy, in particular, there are expressions relating to the directions of the economic situation (up and down) and the speed of the economic development (Chung, Ahrens and Huang, 2003b). In this paper, two Mandarin Chinese newspaper corpora will be examined in terms of occurrences of conceptual metaphors related to the target domain of gushi 'ti r -fy 'STOCK MARICET.' Among the recurring metaphors found, the metaphor STOCK MARKET IS OCEAN WATER constitutes the majority of the linguistic expressions. This paper focuses on this conceptual metaphor and compares it with the English and Spanish data analyzed by Charteris-Black and Ennis (2001).In Charteris- Black and Ennis (2001), the metaphor MARKET MOVEMENTS ARE NAUTICAL OR ARE WAYS OF MOVING IN THE WATER is suggested for expressions relating to the domain of OCEAN WATER. This paper looks at their examples in detail and compares the Chinese data with their analysis. Working within the Conceptual Mapping (CM) Model (Ahrens, 2002), this paper proposes while the source and target domains may be similar across different languages, the linguistic expressions that are mapped in each language can differ within each language. These differences in mappings can be captured using the CM Model by postulating a Mapping Principle for the sourcetarget domain mapping. The Conceptual Mapping (CM) ModelThe CM Model is a model based within the Contemporary Theory of Metaphor ( Lakoff andJohnson 1980, Lakoff 1993), and shares with it the idea that metaphors have systematic source to target domain pairing. However, it goes beyond the CTM to postulate that the systematic pairings are principled and that these principles can be formulated from a linguistic analysis of the conventional metaphorical expressions (Ahrens 2002). For instance, using the CM Model, Ahrens (2002) analyzed IDEA AS BUILDING via five main stages. First, within the target domain of IDEA, native speakers generated all items related to IDEA. Then, these items were categorized into similar source domains such as BUILDING and FOOD. Third, for each source domain, three questions were asked in order to generate the conceptual real world knowledge regarding this source domain. These questions are as follows (with sample answers taken from Ahrens' analysis for IDEA AS BUILDING in Mandarin Chinese):1. What entities does the SD have?--(for BULDINGS: foundation, structure, model, base, etc.) 2. What quality does the SD or the entity in the SD have?--(shaky, high, short, strong, etc.) 3a. What does the SD do?--(to protect, to etc.) b. What can somebody do to the SD? --(to live in, to build, etc.)The fourth stage involved filtering out non-conventional expressions obtained in stage one. At the fifth stage, the actual mappings between IDEA and BUILDING were compared with what could possibly be mapped in the real world. By comparing the linguistic data (obtained at stage one) with the expressions from the real world knowledge, one can postulate why a target domain uses a particular source domain. For IDEA AS BUILDING in Mandarin Chinese, the following examples were given (Ahrens, 2002: 277-278 Idea (originally capitalized) is understood as building because buildings involve a (physical) structure and ideas involve an (abstract) structure.Chung, Ahrens andHuang (2003a, 2003b) have looked at economy metaphors occurring within the source domains of PERSON and TRANSPORTATION DEVICE (such as MOVING VEHICLES and AEROPLANE) respectively. They suggested that the most prototypical mappings in the corpora for these conceptual metaphors are used in formulating the Mapping Principle. This paper also adopts an empirical analysis of corpora by formulating Mapping Principle from the most frequent mappings in the Mandarin Chinese corpora. Both of these sources provide news reported in Taiwan over roughly the same period of time as Charteris-Black and Ennis's (2001) corpora. The search for huashishingwen (WEVIIMM) was limited to the front page news but the search for gungshangshibao (Inli4C) covered all the sections of the newspaper. A total of 120 financial reports were collected from these two sources. Among the 120 financial reports collected, 22 articles were taken from the huashishingwen spoken data (accompanied by written reports and the analysis was based on the written reports) and 98 were taken from gungshangshibao. These two corpora contain 97,156 characters (with MSW word count). This amount of characters was divided by 1.4 to make the Chinese characters comparable to words citation. This produced 69, 397 words in total.For the analysis, we first searched for the key term gushi 'stock market' ()R") using the search engine in Microsoft Word. All the instances of gushi were extracted. A metaphor is identified when there is a mapping from a mapping of gushi to a concrete domain such as (OCEAN) WATER in the following example.Latih America stock market also follow big-rise Literal: 'The Latin American stock market also followed suit in the rising.'The underlined word belongs to the domain of OCEAN WATER and the bracketed words belong to the domain of the STOCK MARKET. These identified metaphors were then categorized according to the different of metaphors (STOCK MARKET IS OCEAN, STOCK MARKET IS DISASTER, etc.). The instances of expressions within each type of metaphor were counted. We then calculated the frequency of each instance of metaphor by finding out the percentage these metaphors constitute in the two corpora.Our analysis yielded 135 tokens of economy metaphors, with 4 recurring patterns of economy metaphors. These 4 metaphors are shown in Table 1. From Table 1, the source domain of OCEAN WATER constitutes the majority of the total number of metaphors found. This is followed by the source domain of WAR. The domain of DISASTER comprises two sub-domains, i.e., the source domains of STORM and EARTHQUAKE. The fourth recurring pattern occurs within the domain of PERSON. These recurring metaphors are defined by their frequency of tokens found in the two news corpora. Other metaphors with lower frequency of occurrences are THE FALL OF THE STOCK MARKET IS CHAIN (1 token) and STOCK MARKET IS GAMBLING (3 tokens).The following examples (5) to (9)  The examples of metaphors with lower frequencies are shown in (10) and (11) The source domains of CHAIN and GAMBLING are also found in related target domains such as FINANCE and CURRENCY. However, for the purpose of this paper, the delimitation of the target domain to the key term gushi JR-1 2ff 'stock market' has reduced the frequencies of these two metaphors in this paper. In related economy metaphors (i.e., FINANCE and CURRENCY), there are more mappings using these source domains.In addition to these conceptual metaphors represented in examples (5)   Most of the expressions in Table 2 have to do with the rising of tides. There are also examples which refer to the up-and-down of the ocean water (chilli ka:M 'up-down' and chichifufuE,C{tMùp To what degree do different languages share similar conceptual metaphors? Charteris-Black and Ennis (2001) examined this question by running a comparative, corpus-based study of metaphors in Spanish and English financial reporting. They argue that both languages show considerable similarity in the choice of conceptual metaphors with the same linguistic expressions but that there is a differing degree of frequency. They attribute this similarity to the common cultural identity of the two languages such as the similar economic system and their Latinate origins. In this paper, we use a corpus-based approach in comparing the choice of conceptual metaphors in Mandarin Chinese with those of Spanish and English. We focus our discussion on the conceptual metaphor STOCK MARKET IS OCEAN WATER in Mandarin Chinese and compare it with the Spanish and English data. We carry out our analysis within the framework of the Conceptual Mapping (CM) Model (Ahrens 2002). With this model, we are able to demonstrate that although different languages share similar conceptual metaphors, they differ in what is mapped linguistically. These differences can be shown in their specific Mapping Principles.
Subject Positions and Derivational Scope Calculation in Minimalist Syntax: A Phase-Based Approach  This paper proposes a new scope calculation system named a phase-based approach. The new system treats scope calculation as a feature-matching operation between more than one interpretable feature related to quantification (henceforth Fqunt1). We call this matching operation Fq.nrmatching. It is shown that the working space of Fquanrmatching is restricted by a syntactic unit phases. Given the matching operation for scope calculation in CHL, scope interpretation can be derivationally determined in narrow syntax as far as it is permitted by the Phase Impenetrability Condition (PIC) proposed in Chomsky (2001). It is demonstrated that various mysterious scope facts in both English and Japanese are reducible to our phase-based scope system without any other special implement.
Context-rule Model for Pos Tagging Part-of-speech tagging for a large corpus is a labour intensive and time-consuming task. In order to achieve fast and high quality tagging, algorithms should be high precision and in particular, its tagging results should require less manual proofreading. There is lots of work on part-of-speech tagging such as Hidden Markov Models (HMMs), Maximum Entropy Models (MEs), and Support Vector Machines (SVMs), etc. Most of works addressed on the high accuracy of tagging results only. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. Part-of-speech tagging for a large corpus is a labour intensive and time-consuming task. In order to achieve fast and high quality tagging, algorithms should be high precision and in particular, its tagging results should require less manual proofreading. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. We compared the tagging precisions between Markov bi-gram model and context-rule classifier. According to the experiments, context-rule classifier performs better than those two other algorithms. Also, it covers the data sparseness problem by utilizing more context features, and reduces the amount of corpus that is need to be manual proofread by introducing the confidence measure. 1 Introduction Part-of-speech tagging for a large corpus is a labour intensive and time-consuming task. In order to achieve fast and high quality tagging, algorithms should be high precision and in particular, its tagging results should require less manual proofreading. There is lots of work on part-of-speech tagging such as Hidden Markov Models (HMMs), Maximum Entropy Models (MEs), and Support Vector Machines (SVMs), etc. Most of works addressed on the high accuracy of tagging results only. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. 2 Tagging Algorithms In this study, we are going to test two different tagging algorithms based on same training data and testing data. The two tagging algorithms are Markov bi-gram model, and context-rule classifier. For Markov bi-gram model, we propose a new form named word-dependent Markov bi-gram model, which will be described later. The training data and testing data are extracted from Sinica corpus, a 5 million word balanced Chinese corpus with pos tagging (Chen et al., 1996). The confidence measure will be defined for each algorithm and the best accuracy will be estimated at the constraint of only a fixed amount of testing data being proofread. It is easier to proofread and make more consistent tagging results, if proofreading processes were done by checking the keyword-in-context file for each ambivalence word and only the tagging results of ambivalence word need to be proofread. The words with single category need not be rechecked their pos tagging. For instance, in Table 1, the keyword-in-context file of the word &apos;1111: F9: (research), which has pos-categories of verb type VE and noun type Nv, is sorted according to its left/right context. The proofreader can see the other examples as references to determine whether or not each tagging result is right. If all of the occurrences of ambivalence word have to be rechecked, it is still too much of the work. The common terms used in the following tagging algorithms were defined as follows: W k The k-th word in a sequence C k The pos-category associated with k-th word wk 146
Chinese Word Segmentation Based on Contextual Entropy Unlike English there is no explicit word boundary in Chinese text. Chinese words can comprise one, two, three or more characters without delimiters. But almost all techniques to Chinese language processing, including machine translation, information retrieval and natural language understanding are based on words. Word segmentation is a key step in Chinese language processing.Several approaches have been developed for Chinese word segmentation. In general two main approaches are widely used: the statistical approach (Gua and Gan, 1994, Sproat and Shih, 1990, , Teaban, Wen, McNab and Witten, 2000, Peng and Schuurmans, 2001) and lexicon-based approach (Yeh and Lee, 1991, Palmer, 1997, Cheng, Yong and Wong, 1999).Some statistical approaches are based on the mutual information (Sproat and Shih, 1990), which only captures the dependency among characters of a word. Some need large pre-tagged corpus for training (Teaban, Wen, McNab and Witten, 2000), which is too expensive to construct at present. Rule-based approaches require a pre-defined word list (dictionary, or lexicon). The coverage of the dictionary is critical for these approaches. Many researches use a combination of approaches (Nie, Jin and Hanna 1994). These are supervised approaches that require extensive human involvement. Some (Sproat and Shih, 1990, de Marcken, 1996, Peng and Schuurmans, 2001) used unsupervised approaches and required little human intervention.It has been long known that contextual information can be used for segmentation (Harris 1955). Dai, Kgoo and Loh (1999) used weighted document frequency as contextual information for Chinese word segmentation. Zhang, Gao and Zhou (2000) used the context dependency for word extraction. Twig and Lee (1994) used contextual entropy to identify unknown Chinese words. Chang, Lin &amp; Su (1995) and Ponte &amp; Croft (1996) used contextual entropy for automatic lexical acquisition. Hutchens &amp; Alder (1998) and Kempe(1999) used the contextual entropy to detect the separator in English and German corpus.In this paper we will present a simple purely statistical approach using contextual entropy for word segmentation. Details about our approach are given in section 1 and 2. Chinese is written without word delimiters so word segmentation is generally considered a key step in processing Chinese texts. This paper presents a new statistical approach to segment Chinese sequences into words based on contextual entropy on both sides of a bigram. It is used to capture the dependency with the left and right contexts in which a bigram occurs. Our approach tries to segment by finding the word boundaries instead of the words. Experimental results show that it is effective for Chinese word segmentation.
Topic Segmentation for Short Texts Topic segmentation, which aims to fmd the boundaries between topic blocks in a text, is an important task for semantic analysis of texts. In general, they rely on such knowledge as word recurrence, collocations, thesaurus, linguistics cues, or the combination of those, to measure the similarity between sentences, and estimate whether topic shift occurs (Ferret(2002)). The studies are also classified into two schemes: one is based on locally comparing adjacent blocks of sentences, while the other method is based on considering topic block boundary decision as a global optimization (Bigi et.a1(1998)).Although the approaches for topic segmentation have been worked well for long texts, the assumptions it requires limit most of useful applications. First, before topic segmentation, the thresholds, coefficients, or parameters of formulas in some methods must be estimated beforehand depend on the characters of text sets or the experience of users. It not only limits applications to be fully automatic but also causes difficulty in many domains.These methods perform especially poor when they are applied to such short texts as internet news and student's writings (Ponte and Croft(1997)). Since keywords from sentences are quite few and not reliable for short texts, the errors in measure naturally occur more frequently. It results in rapid decrease of the accuracy of topic segmentation.The accuracy of these methods not only relies on the characteristics of the short texts, but also depends on the languages. For Chinese, the usage of punctuation marks such as comma mark is often ambiguous (Chen(1993)). For instance, the sentence ending with comma mark is not always considered a complete sentence on syntax or semantics. For incomplete sentences, the step of extracting reliable keywords is far more difficult. This causes, due to the characteristic of the language, further decrease of the accuracy of topic segmentation.Apparently, short texts are troublesome and quite difficult for existing methods in topic segmentation. However, given a theme, a huge collection of reference texts for short texts can easily be obtained. This large corpus or thesaurus allows us to overcome the above mentioned limitations and define similarity between sentences. This paper will present a new method to segment topics for short texts. Section 2 reviews previous studies for text segmentation. Section 3 discusses the proposed technique in detail. Section 4 shows the performance of the method on some experiments. Section 5 discusses the conclusion. Topic segmentation, which aims to fmd the boundaries between topic blocks in a text, is an important task for semantic analysis of texts. Although different solutions have been proposed for the task, many limitations and difficulties exist in the approaches. In particular most of the methods do not work well for such case as short texts, internet news and student&apos;s writings. In this paper, we focus on the short texts and present a method for topic segmentation. It can overcome the limitations in previous works. In preliminary experiments, the method show the accuracy of topic segmentation is increased effectively.
Cross-Lingual Text Filtering Based On Text Concepts And kNN With the rapid growth of the Internet and other networked information, there is an increasing need for reliable automatic texts filtering. As the Internet is boundless, Cross-Lingual Information Filtering(CLIF) system is also eager to access information which may be important to the user. Information filtering includes three subtasks: collecting information from information sources, selecting information which may interest the user and presenting the information to the user. In this paper, we investigate how a cross lingual texts filtering model could be structured to acquire a user profile which enables it to distinguish between relevant and irrelevant documents in texts form on the Internet. This user profile is then used to accomplish the task of filtering documents from information sources automatically in English or in Chinese.The task of texts filtering entailed: building a model of the features in the text predicting the relevance of the text to the user's interest. This asks for text representation which is discussed in section 3 and the user profile which is described in section 4. Techniques to compare between the text representation and the user profile to decide whether or not to keep the text and to notify the user are discussed in section 5. The architecture of the model is described in section 2. In section 6, report about some experiments is displayed to evaluate the value of this system. Conclusion about this model is made at the end of this paper. All techniques discussed below are suit to texts both in Chinese and in English. Some differences when handling information in Chinese and English will be discussed separately.2 Multilingual texts filtering model Figure 2 is the architecture of multilingual texts filtering model. First of all , the user provides sample texts as his interest to the system. The system analyses the sample texts by reducing them into sememes, a basic concept that will be described late. After calculating the sememes, we express the user's interest as a vector in the sememe vector space. And if a text is available, we analysis the text by reducing it into sememes and expressed it as a vector too. We calculate the similarity of the two vectors to show whether or not the text is relevant to the user's interest. As mentioned above, this model calculates the similarity of a text to an user's interest based on sememes other than words. This model is based on HowNet [l], a knowledge database developed by Mr. Zhendong Dong, which comprises more than 53000 Chinese words and 57000 English words. In the philosophy of the HowNet, sememe refers to the smallest basic semantic unit that cannot be reduced further. Taken for instance, 'human being' , despite being a most complex concept composing a set of attributes, it can be regarded as a sememe. All concept can be reduced to the relevant sememes. Mr. Zhendong Dong has extracted a set of over 800 sememes that are used in HowNet and believed that all concepts in the world can be expressed by this set of sememes. If a word has several meaning, HowNet gives every concept a definition in its DEF entry by this set of sememes. Reducing a word to sememes can settle the problem synonymy of different words having the same concept which has puzzled us for several years with good results. Synonym is a well-known limitation of the word-based techniques that can make it difficult to find relevant documents. For example, word`1=word`1=-4 )A' and`-and`-id--A L' are both explained as 'computed g m-4'. From this point of view, we can think Chinese words and English words are a kind of synonym too. We can take words rt g f, `171-0: 191: and 'computer' as the same word without considering their different appearance. So we can handle texts in Chinese or in English without translation. Reducing the word into sememes is also useful for disambiguity which will be discussed in section 3.Further more, we divided this set of more than eight hundred sememes into two group: sememes that are useful in classifying the relevance and sememes that are useless in classifying the relevance. We called the former group of sememes classifiable sememes(C.S.) and the latter group of sememes unclassifiable sememes. Most unclassifiable sememes are so frequently appear in the explanation of concepts, such as location, time, fact and or so, that if they were used in classifying, it would mislead us that most of the texts are somewhat alike. Classifiable sememes play important role in the process of filtering in our system. This paper presents the model that can be used to filter the texts which the user is interested in from a large scale of source texts in Chinese or in English. Each text which the user is interested in can be represented as a vector in the vector space of classifiable sememes. The text to be sifted is represented as a vector too. The relevance of the text to the user can be measured by using the cosine angle between the text and its k nearest neighbor in the vector space. Experiments have been done and their results show that this scheme yields good results .
The Semantics of Onomatopoeic Speech Act Verbs Based on the Speech Act Theory (Austin 1992;Reiss 1985;Searle 1969Searle , 1975Searle , 1979, speech act is what speaker performs when producing the utterance. Researchers suggest that speech act is not only an utterance act but contains the perlocutionary force. If the speaker has some particular intention when making the utterance such as committing to doing something or expressing attitude or emotions, the speech act is said to contain illocutionary force. On the other hand, if the utterance has a particular effect on the addressee, the utterance is regarded to contain perlocutionary force. In addition to the familiar speech act verbs, we suggest that speech act can also be carried out by a group of syntactically independent small vocalizations. Since they are imitations of sound generated by certain actions, we describe them as 'onomatopoeic speech act verb'. Example (1) illustrates this kind of verb: In example (1), speaker A uses t (ei) 'hey' to attract B's attention. When B responds, A continues to say what s/he intends to say. As the example demonstrates, when A utters the word t (ei) 'hey', s/he not only performs the utterance act but performs a perlocutionary act. This word will bring about effects on the audience, i.e. to make the audience pay attention to the speaker. This paper attempts to explore the semantics of onomatopoeic speech act verbs. Language abounds with small bits of utterances to show speaker's emotions, to maintain the flow of speech and to do some daily exchange routines. These tiny vocalizations have been regarded as vocal gestures and largely studied under the framework of 'interjection'. In this paper, we describe a subset of interjection. We term these subset verbs as onomatopoeic speech act verbs. It refers to a syntactically independent monomorphemic utterance which performs perlocutionary forces. An onomatopoeic speech act verb normally brings about effects on listener by having the recipient to do something or to solicit recipient's response or reaction. They are onomatopoeic because most of them are imitation of the sounds produced by doing some actions.The paper is organized in the following way. Section 1 is the introduction. Section 2 turns to review previous studies on interjection since onomatopoeic speech act verb is a subset of interjection. Definition as well as the classification on interjections will be brought up. Section 3 discusses the semantics of onomatopoeic speech act verbs and a selection of onomatopoeic speech act verbs is accounted for in details. Section 4 is the conclusion. This paper attempts to explore the semantics of onomatopoeic speech act verbs. Language abounds with small bits of utterances to show speaker&apos;s emotions, to maintain the flow of speech and to do some daily exchange routines. These tiny vocalizations have been regarded as vocal gestures and largely studied under the framework of &apos;interjection&apos;. In this paper, the emphasis is placed on the perlocutionary force the vocal tokens contain. We describe their conventionalized lexical meaning and term them as onomatopoeic speech act verb. An onomatopoeic speech act verb refers to a syntactically independent monomorphemic utterance which performs illocutionary or perlocutionary forces. It is normally directed at the listener, which making the recipient to do something or to solicit recipient&apos;s response or reaction. They are onomatopoeic because most of them are imitation of the sounds produced by doing some actions.
Mandarin Adverbial Jiu In Discourse*  
A Synchronous Corpus-Based Study of Verb-Noun Fluidity in Chinese The problem of part-of-speech (POS) or categorial ambiguity, especially between nouns and verbs, as well as adjectives and verbs, has been widely discussed in the literature. Most contemporary Chinese grammarians would suggest that Chinese words have predefined lexical categories mainly based on their syntactic properties, which should not be contingent upon the grammatical function the words take in particular sentences (e.g. Zhu, 2001). For example, the word' MI huai2yi2, with a verb sense ("to suspect") to start with, should only appear as a verb in a dictionary, despite its variable usages found in real contexts, as in (1)1.(1) a. RigititAv (I suspect he is a thief) --verbal, predicate wo3 huai2yi2 tal shi4 zei2 b. ft 1 7 4. ,; .-t Pir i (He wears a suspicious look) --adjectival, modifier tal man3lian3 huai2yi2 biao3qing2 c. aRAftni (This is only my suspicion) --nominal, head of object noun phrase zhe4 zhi3shi4 wo3 des huai2yi2We call this relative flexibility of a word being used for different grammatical functions and possibly different POSs "categorial fluidity". This phenomenon thus poses a dilemma on tagging Chinese corpora. On the one hand, the theoretical and lexicographic concern is to keep the whole classification of Chinese syntactic categories straightforward, so that all words can be prescribed a category and all categories can be adequately described. On the other hand, the practical concern is to obtain authentic data on the distribution of word uses as this is the ultimate purpose of corpus tagging. Chinese POS tagging in the past had handled this problem in different ways (e.g. CKIP, 1993;Yu et al., 1998;Xia, 2000). For instance, regarding the fluidity between verbs and nouns, Xia (2000) assigned a verb tag if the word is used as a verb and a noun tag if it is used as a noun; whereas CKIP (1993) assigned the same 1 The digits following the Hanyu pinyin indicate the tone. tag to the same word, but used features to encode specific grammatical functions, such as using [+nomj to indicate a nominalised usage of a verb. We suggest that the two concerns could be compromised, possibly via the tagset, as in Kwong and Tsou (2003), and maintain the mutual relation between the treatment of categorially fluid words and empirical evidence. Thus in the current work we further our empirical study of categorial fluidity between verbs and nouns. Moreover, we try to compare this phenomenon with respect to texts collected from various Chinese speech communities.In Section 2, we first discuss the problem of POS ambiguity in Chinese. Then in Section 3, we consider the shift of a word from one POS to another a transitional process and identify three types of POS ambiguity along the categorial fluidity continuum. In Section 4, we describe the synchronous corpus-based study and report on the results obtained. Finally in Section 5, we discuss the results further with respect to their implications on lexicography, POS tagging, and other natural language processing tasks. The problem of verb-noun categorial ambiguity is critical and relatively unique for non-inflectional languages, especially Chinese. We consider the verb-noun categorial fluidity a continuum and any categorial shift a transitional process. A synchronous corpus-based study was conducted to compare the phenomenon with respect to news texts collected from Hong Kong, Beijing, and Taiwan. It was found that about 15% of the verbs in the Hong Kong and Taiwan texts were undergoing the verb-noun categorial shift; whereas Beijing texts had more than 18% of the verbs undergoing this shift. The results also have important implications on various natural language applications, including lexicography, part-of-speech tagging of Chinese, as well as other natural language processing tasks.
Non-monotonic Negativity  The main aim of this paper is to provide a new analysis of licensers of negative polarity items (NPIs). The problems with Fauconnier-Ladusaw&apos;s downward entailment analysis have been argued since Linebarger (1980). I will show that there exists a class of weak NPI licensers characterized by non-monotonicity and exclusivity. Weak negation, which is monotone decreasing, has been known to license weak NPIs such as any and ever (Zwarts 1993). However, non-monotonic items also trigger these wideners. Exclusivity or uniqueness characterizes non-monotonic operators, such as only, exactly n, superlatives, ordinal numerals, the determiner the, generic NPs, and also if and only if clauses, hope, happy, glad and others. Many of them function as generalized quantifiers which prohibit either downward or upward entailment. As Jespersen (1917) traces the origin of NPIs back to the strengthening of negation, non-monotonic contexts also favor strengthening by these words. We begin by considering the limited distribution of polarity items. The following section presents shortcomings of previous analyses, and then, non-monotonic expressions and their exclusivity are discussed.
The SVM With Uneven Margins And Chinese Document Categorisation Document Categorisation (DC), the problem of assigning documents to predefined categories, is an active research area in information retrieval and machine learning. For one category, the DC problem is actually a binary classification problem by classifying a document to the category or not. Many machine learning algorithm have been applied to the DC problem, using a training set of categorised documents to obtain a classifier for one category and then judging the relevance of a document to the category by feeding the document into the classifier.The support vector machine (SVM) is a well known learning algorithm for linear classifier and has achieved state of the art results for many classification problem, including document categorisation (see Joachims (1998), Yang and Liu (1999)). However, it has been noticed the the performance of the SVM for small category (i.e. category with a small number of relevant documents in collection) was quite poor. Small category in DC problem corresponds to the classification problem with uneven datasets, where the numbers of positive and negative examples are very different. Several kinds of adjustments have been made to the SVM to deal with the uneven datasets. The algorithm we present in this paper, the SVM with uneven margin, is the latest one.People who are concerned with Chinese information retrieval might notice that, whereas extensive studies have been done to English document categorisation, relatively few results have been reported on Chinese document categorisation ( He et al. (2003)). This is mainly because of a lack of Chinese document collection designed particularly for document categorisation. Fortunately, a multilingual corpus RCV2 created recently by Reuters contains a Chinese document collection. The Chinese corpus was manually categorised in the similar way and with the similar high quality with the Reuters English corpora such as Reuters-21578 and the RCV1 corpus. So the Reuters Chinese collection is comparable to the two commonly used English document collections with respect to DC problem. Regarding big difference between English and Chinese, it is interesting to see whether the good methods such as the SVM for English DC problem are able to achieve similar performances for Chinese as for English.In Section 2 we overview the previous works about adapting the SVM towards uneven datasets. Section 3 describes our new algorithm the SVM with uneven margins to tackle the unbalanced classification problem and explain some interesting relationship between the SVM with uneven margins and the SVM. Section 4 presents the experimental results on the benchmark dataset, the Reuters-21578 corpus, showing our new algorithm outperforms the SVM and justifying the introduction of a margin parameter into the SVM. Section 4 also reports the results of the SVM and the SVM with uneven margins for Chinese DC problem using the Chinese document collection of the RCV2. We propose and study a new variant of the SVM-the SVM with uneven margins, tailored for document categorisation problems (i.e. problems where classes are highly unbalanced). Our experiments showed that the new algorithm significantly outper-formed the SVM with respect to the document categorisation for small categories. Furthermore, we report the results of the SVM as well as our new algorithm on the Reuters Chinese corpus for document categorisation, which we believe is the first result on this new Chinese corpus.
The development of tagged Uyghur corpus Processing raw corpuses is fundamental to the research of the corpus linguistics. Since the first corpus was built in USA in 1970's, many other corpuses had been developed world wide. In the early days, a corpus usually contains only one million words and only text corpuses are collected. From 1980's, research in linguistic corpuses becomes very active. Some major projects at that time are: the Lancaster Treebank, the tagged LOB Corpus and the Birmingham Corpus. In the 1980's the corpuses are large and there are more varieties. Before 1990s, almost all are English corpuses.From 1986 to 1994, several different part-of-speech (POS) automatic tagging tools such as CLAWS1, CLAWS2, CLAWS3 and CLAWS4 are developed. At the same time, many progresses had been made in the corpus syntax analysis and tagging. The PennTreebank, developed by the Pennsylvania University of USA combined and integrated many corpus processing tools such as: the Church POS tagging tools, the Fidditch syntax analysis kit of the Hindle system and etc. These tools greatly enhanced the efficiency of the corpus tagging.After 1990s, many non-English corpuses are developed. These include the Japanese ER corpus and the news manuscript corpus of NHK.In the past ten years, much progress has been made in the development of Chinese corpuses. These include: the automatic syncopating [LZH93]) and etc. From 1992, researchers in the Institute of Computational Linguistics in Peking University have been working in the integrated processing of the Chinese corpuses. They have developed many new technologies and tools for the Chinese corpus processing.From the viewpoint of the sources of a language, a corpus can be divided into text or speech, spoken language or written language and single language or multi languages.So far, many tagsets have been developed for POS tagging, phrase tagging, dependency relation tagging, case relationship, syntax tree, semantic relationship and so on. As much as we know, the beginning of building Uyghur corpus is in 2001. So there are far more works to be done for this language. The history and development of Uyghur language is introduced. After a brief introduction to the development of Uyghur words, morphology and syntax, we explain our developing of a computer-aided contemporary Uyghur language tagging system. The coverage of this corpus, the resources building, the rules for syncopating and tagging etyma and termination, and the tagging of a corpus using a small tagset are explained. Some practical methods solving problems in Uyghur language tagging are also proposed.
A Vector-Based Algorithm for Chinese Text ClassificationEll  In this paper, vector-distance-weighted algorithm and representative-vector-distance algorithm are described and used to implement the process of automatic text classification. Two experiments have been done by means of the algorithms (experiment) is based on vector-distance-weighted algorithm and experiment2 is based on representative-vector-distance algorithm). Characters are selected as features. The average precision of experiment) and experiment2 is 80.36% and 69.27%, respectively. Comparing the two experiments, it can be concluded that the efficiency of text classification can be improved by means of vector-distance-weighted algorithm.
A Large-scale Lexical Semantic Knowledge-base of Chinese Semantic resources play an important role in many areas of Natural Language Processing (NLP). The The Semantic Knowledge-base of Contemporary Chinese (SKCC) is a large scale Chinese semantic resource developed by the Institute of Computational Linguistics of Peking University. It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts. Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering. The descriptions of semantic attributes are fairly thorough, comprehensive and authoritative. The main work in this paper is to introduce the outline of SKCC, and establish a multi-level WSD model based on it. The results indicate that the SCK is effective for word sense disambiguation in Chinese and are likely to be important for general NLP.
An Effective Combination of Different Order N-grams In Chinese natural language processing domain, the parameters of n-gram can be estimated by calculating the frequency of word pair in text corpus and then normalizing the frequency. This conventional language model cannot satisfy our requirements since it is dependent of discriminative capability. We propose discriminative estimation approach, which can directly relate the estimation of n-gram parameters to its discriminative capability. We optimize the parameters of n-gram on the criterion of discriminative estimation by using Newton Gradient method.When we establish N-gram we artificially introduce an assumption over the relationship among adjacent words. Uni-gram is based upon the assumption that all words appear in the corpus independently. Bi-gram assumes that only contiguous words correlate with each other and tri-gram puts a constraint on the language information that one word can be predicted only by its two predecessor words. Some words are free of context and some depend on short or long history information under some circumstances. In this sense single N-gram could model the language phenomena with some compromise. This paper addresses the impact of the different assumption from different order n-gram on the performance of language model and proposed the combination and optimal selection of different n-gram to battle with artificial assumption and possible data sparsity problem.In the following sections, we first bring up with the discriminative estimation criterion. Next we describe the assumption from N-gram. In the following section we introduce the scheme for combination of different order N-gram. In the next section, an approach to optimal selection of different order language model is proposed. At last we report the experimental results on the platform of conversion from Chinese pinyin to Chinese character. In this paper an approach is proposed to combine different order N-grams based on the discriminative estimation criterion, on which the parameters of n-gram can be optimized. To raise the power of modeling language information, we propose several schemes to combine conventional different order n-gram language model. We employ Newton Gradient method to estimate the assumption probabilities and then test the optimally selected language model. We conduct experiments on the platform of conversion from Chinese pinyin to Chinese character. The experimental results show that the memory capacity of language model can be remarkably lowered with hide loss of accuracy.
Efficient Methods for Multigram Compound Discovery Multigram language model has become important in Speech Recognition (SR), Natural Language Processing (NLP) and Information Retrieval (IR) as demonstrated in Siu and Ostemdorf (2000), Peng and Schuurmans (2002), and Chien (1999). It has also been used in evaluating NLP applications such as automatic Machine Translation and Text Summarization (Panineni, etc., 2002;Lin and Hovy, 2003). For a corpus of length N, the computing cost of a naïve algorithm for the frequencies over all substrings is at least 0(N2). In Yamamoto and Church (2001), an efficient method is given for computing the term frequency (/) and document frequency (di), as well as the Mutual Information (MI) and Residual Inverse Document Frequency (RIDF), for all substrings based on Generalized Suffix Array (GSA). The method groups all N(N+ 1)/2 substrings into up to 2N-I equivalence classes, and in this way, the computation is reduced to a manageable computation over these classes, that is, 0(NlogN) time and 0(N) space.It is natural to compare Generalised Suffix Tree (GST) and Generalised DAWG (GDAWG) with GSA since they all can be viewed as compact representations of suffix tries. Moreover, the construction complexities of GST and GDAWG are 0(N), while that of GSA is 0(NlogN). This raises the question:MI and RIDF by Yamamoto and Church (2001)  Where x and z are tokens, Y and xYz are ngrams (sequences of tokens).Are GST and GDAWG the same or more efficient data structures than GSA for multigram compound Discovery?In Crochemore and Rytter (1994), a set of properties has been identified such that a data structure D is said to be good if:(Property A) D has linear size.(Property B) D can be constructed in linear time.(Property C) D allows computing FACTORIN(x, text) in 0(1x1) time. Although the above properties are desired for multigram compound discovery, additional properties are required to provide a more precise assessment. Two important basic statistics: #-and df are important. The frequency of a substring in a collection of strings is called the term frequency (or /), and that of a substring occurred among different strings in the collection is called the document frequency (or dfi. In this paper, the following properties are identified, in addition to Properties A -C, to assess D: let Nbe the size of a set of strings TEXT:(Property D) D allows #-(tenn frequency) and df(document frequency) to be computed in 0(N) time. Multigram language model has become important in Speech Recognition, Natural Language Processing and Information Retrieval. An essential task in multigram language model is to establish a set of significant multigram compounds. In Yamamotto and Church (2001), an 0(NlogN) time complexity method based on Generalised Suffix Array (GSA) has been found, which computes the (term frequency) and df (document frequency) over 0(N) classes of substrings. The ff&apos;and df form the essential statistics on which the metrics, such as MI (Mutual Information) and RIDF (Residual Inverse Document Frequency)&apos;, are based for multigram compound discovery. In this paper, it is shown that two related data structures to GSA, Generalised Suffix Tree (GST) and Generalised Directed Acyclic Word Graph (GDAWG) can afford even more efficient methods of multigram compound discovery than GSA. Namely, 0(N) algorithms for computing ff-and df have been found in GST and GDAWG. These data structures also exhibit a series of related, and desirable properties, including an 0(N) time complexity algorithm to classify 0(N2) substrings into 0(N) classes. An experiment based on 6 million bytes of text demonstrates that our theoretical analysis is consistent with the empirical results that can be observed.
Translation Template Learning Based on Hidden Markov Modeling Example based machine translation (EBMT), originally proposed by Nagao (Nagao,M.A 1984), and is one of the main approaches to corpus-based machine translation. The main idea behind EBMT is that an input sentence in the source language is compared with the example translations in the given bilingual parallel text to find the closest matching examples so that these examples can be used in the translation of the input sentence. After finding the closest matches for the sentence in the source language, parts of the corresponding target language sentence are constructed using structural equivalences and deviances in the matches. Following Nagao's original proposal, several approaches using the example based method were presented. One of the approaches that applied the idea for translation from English to Turkey is learning translation template (Cicekli,I 1996)  (Gilvenir,H.A 1998). This method relies on the technique that uses the similarity and difference from a source sentence and a target sentence in the given bilingual corpus to build template rules for translation. The advantage of this method is that does not need any complex parsing such as syntactic parsing or semantic parsing and overcome the imperfectness of the rule-based machine translation. One of the disadvantages of the method is that a lot of templates can be matched with an input sentence. To overcome this problem, (Oz and Cicekli,I 1998) present a method which allows sorting template rules according to their confident factors. The translation results are sorted using its score through the value of confident factors. However, this method needs to evaluate all matching rules for each input sentence to obtain the output results, while much of them are redundant rules. The exponential calculation problem will arise when an input sentence is long and the number of template rules is large. Following that point, we present a novel method based on an HMM model that uses constraints for set of matching rules with each input sentence. Thus, the translation results of an input sentence are obtained by finding a set of template rules that is most likely with our HMM model. This paper addresses a novel translation method based on Hidden Markov Model using template rules after learning them from the bilingual corpus. The method can enhance the translation accuracy and ensure a low complexity in comparing with the pervious template learning translation method and draws a new perspective for applying statistical machine learning on example based translations. domain.
News-Oriented Keyword Indexing with Maximum Entropy Principle With more and more information flowing into our life, it is very important to lead people to gain more important information in time as short as possible. Keywords are a good solution, which give a brief summary of a document's content. With keywords, people can quickly find what they are most interested in and read them carefully. That will save us a lot of time. In addition, keywords are also useful to the research of information retrieval, text clustering, and topic search ). Manually indexing keywords will cost highly. Thus, automatically indexing keywords from text is of great interests. Several methods have been proposed for extracting English keywords from text. For example, Witten(1999) adopted Naïve Bayes techniques, and Turney (1999) combined decision trees and genetic algorithm in his system. Due to the characteristics of the Chinese language, some researchers adopt the structure of PAT tree and make use of mutual information to obtain keywords ( Chien 1997, Yang 2002. Unfortunately, the construction of PAT tree will cost a lot of space and time. In this paper, aiming at the characteristics of news-oriented articles, resources and techniques available, we will introduce ME model to index keywords from text. Section 2 will describe the architecture of the whole system and review the ME model. In section 3, we will introduce how to obtain candidate keywords as input of the ME model. In section 4, we will illustrate the process of how to construct a feature set for the ME model. In section 5, experimental results will be given and analyzed. At last, we will end with the conclusion. In our information era, keywords are very useful to information retrieval, text clustering and so on. News is always a domain attracting a large amount of attention. Aiming at news documents&apos; characteristics and the resources available, this paper proposes to use Maximum Entropy (ME) model to conduct automatic keyword indexing. The focus of ME-based keyword indexing is how to obtain all the candidate items and select useful features for ME model. First, we make use of some relatively mature linguistic techniques and tools to obtain all the possible candidate items. Then, a feature set of ME model will be introduced. At last we test the model, and experimental results are given. 1 Introduction With more and more information flowing into our life, it is very important to lead people to gain more important information in time as short as possible. Keywords are a good solution, which give a brief summary of a document&apos;s content. With keywords, people can quickly find what they are most interested in and read them carefully. That will save us a lot of time. In addition, keywords are also useful to the research of information retrieval, text clustering, and topic search (Frank 1999). Manually indexing keywords will cost highly. Thus, automatically indexing keywords from text is of great interests. Several methods have been proposed for extracting English keywords from text. For example, Witten(1999) adopted Naïve Bayes techniques, and Turney (1999) combined decision trees and genetic algorithm in his system. Due to the characteristics of the Chinese language, some researchers adopt the structure of PAT tree and make use of mutual information to obtain keywords (Chien 1997, Yang 2002). Unfortunately, the construction of PAT tree will cost a lot of space and time. In this paper, aiming at the characteristics of news-oriented articles, resources and techniques available, we will introduce ME model to index keywords from text. Section 2 will describe the architecture of the whole system and review the ME model. In section 3, we will introduce how to obtain candidate keywords as input of the ME model. In section 4, we will illustrate the process of how to construct a feature set for the ME model. In section 5, experimental results will be given and analyzed. At last, we will end with the conclusion. 2 System Architecture Keyword indexing can also be called keyword extraction. The definition of a keyword is not restricted to one word in our conception. Here, a keyword might consist of more than one Chinese word, i.e., a term reflecting the main content of a document. In fact one document is composed of a set of terms every of which can be described by many features and must belong to a keyword or not. Thus, the
Extracting Chinese Multi-Word Units from Large-Scale Balanced Corpus Natural language processing is a project based on knowledge, thus human's linguistic knowledge must be stored in the computer and the process of human's comprehending and producing languages be formalized before the computer commands human's linguistic potency. Since multi-word units (words formed with at least two characters) are the primary embodiment of semantics (Pinchuck 1977;Sager 1990), research on these words is the starting point for different natural language processing applications. Automatic multi-word units (MWUs) extraction has great theoretical and practical significance to such language information processing research as information indexing, machine translation, voice recognition, document classification as well as thesaurus compiling. Presently, the rapid developments in different professional fields (e.g. computer science, medicine) mean continuous creation of new MWUs, and it is impossible to list them exhaustively in a lexicon. Therefore, automatic extraction of MWUs is a very important issue. Compared with western languages, as for Chinese there is no space between characters and words are hard to define, thus automatic Chinese MWUs extraction will surely confront even more difficulties.In this paper, we have proposed a statistical method based on a large-scale balance corpus to realize the automatic extraction of MWUs. The goal is to extract sets of words with exact meaning from the corpus. Our method mainly consists of three phases (the first two phases include 3 steps respectively and the third phase includes 4 steps). First, select "seeds" (two character word) ready for extension; then extend these seeds at the front or back by K characters; finally, by comparing these parameters, determine which are MWUs. We have assessed the experiment data by measuring precision rates, and the result indicates that our method is more efficient and robust compared with other approaches.The rest of this paper is structured as follows. In section 2, we describe in detail the method and all statistical parameters used. In section 3, we make a comprehensive analysis and just evaluation of the experimental data. In section 4, we outline the related works as well as their results. Finally, we give out conclusions and introduce part of our later research work in section 5.to measure the association ratio of adjacent characters. Automatic Multi-word Units Extraction is an important issue in Natural Language Processing. This paper has proposed a new statistical method based on a large-scale balanced corpus to extract multi-word units. We have used two improved traditional parameters: mutual information and log-likelihood ratio, and have increased the precision for the top 10,000 words extracted through the method to 80.13%. The results of the research indicate that this method is more efficient and robust than previous multi-word units extraction methods.
A New Sentence Reduction based on Decisions tree model Many researches in automatic text summarization were focused on extraction or identifying the important clauses and sentences, paragraphs in texts ( Math and Maybury, 1999). Meanwhile, humans used to produce summaries by creating new sentences that are grammatical, that cohere with one another, and capture the most salient parts of information in the original document. Sentence reduction is the problem of removing some redundant words or some phrases from the original sentence by creating a new sentence, in which the gist meaning of the original sentence was =changed .Methods of sentence reduction have been applied in many applications. Grefenstette (Grefenstette,S, 1998) proposed removing phrases in sentences to produce a telegraphic text that can be used to provide audio scanning services for the blind. Dolan (Donlan,W.B, 1999) proposed removing clauses in sentences before indexing document for information retrieval. Those methods removed phrases based on their syntactic categories without relying on the context of words, phrases and sentences around. Therefore, those methods are unsuitable for text summarization task.Sentence reduction for text summarization is pointed out by Mani and Maybury (Mani and Maybury,1999). The authors present a process of writing reduced sentences by reversing the original sentence with a set of revised rules. Jing (Jing,H, 2000) also studied a method to remove extraneous phrases from sentences by using multiple source of knowledge to decide which phrase can be removed. The multiple sources include syntactic knowledge, context information and statistic computed from a corpus that consists of examples written by human professional. Their method prevented removing some phrases that were relative to its context around and produced a grammatical sentence, and applied to the cut and paste summarization strategy.Recently, Knight and Marcus (Knight and Marcu,D, 2002) demonstrated two methods for sentence compression problem based on corpus. They devised both noisy-channel and decision tree approach to the problem. The decision tree approach has been applied in parsing sentence (Magerman,D,1995) (Hermijakob,U and Mooney,R, 1997) and defining the rhetorical of text documents (Marcu,D,1999) and achieved a good results in sentence compression as described in (Knight and Marcu,D, 2002).In almost previous methods, the order of reduced sentences is the same with the original sentence. Meanwhile, in summrizing document, human may perform a changeable order to ensure the summary document is smooth and coherence. This fact requires a new sentence reduction with the order of reduced sentence is different from the orignal. In addition to using sentence reduction for text summarization, the information of syntactic is not enough. The semantic information of original sentences should be incorporated with reduction process to enhance the accuracy of reduction process.This fact is also similar to the behavior of human in reduction sentence that they can understand the meaning of original sentences to ensure that important words is remained in reduced sentences. To satisfy the new requirements mentioned above, we proposed a new sentence reduction based on decision tree model where semantic information is used to support reduction process. The decision tree model is also extended to cope with the changeable order between original sentences and reduced sentences. The remainder of this paper will be organized as follows: Section 2 presents a new sentence reduction algorithm using semantic information to satisfy the new requirements of sentence reduction as mentioned above. Section 3 shows implementations and experimental results and section 4 gives conclusions and some outstanding problems to be solved in future. This paper addresses a novel sentence reduction algorithm base on decision tree model where semantic information is used to enhance the accuracy of sentence reduction. The proposed algorithm is able to deal with the changeable order problem in sentence reduction. Experimental show a better result when comparing with the original methods.
Japanese Parser on the basis of the Lexical-Functional Grammar Formalism and its Evaluation Deep grammatical analyses of input sentences based on theoretically sound grammar formalisms are essential for the further development of such NLP applications as machine translation, dialogue understanding, message extraction, etc. In this paper we report development and performance of a parser for the Japanese language based on the Lexical-Functional Grammar (LFG) formalism (Kaplan and Bresnan, 1982;Dalrymple, 2001), The Japanese LFG grammar used for this parser is being developed in relation to the Parallel Grammar (ParGram) project (Butt et al., 1999(Butt et al., , 2002). In this project, grammars for six languages, English, French, German, Japanese, Norwegian and Urdu', are under way, sharing various design decisions within the LFG formalism. LFG assumes two levels of syntactic representation for a sentence: a c(onstituent)-structure (a tree) and an functional)-structure (attribute value matrices: AVMs). Within LFG, an f-structure is meant to encode a language universal level of analysis, allowing for cross-linguistic parallel ism. 2 Our research goal is to construct a practical Japanese LFG parsing system with broad coverage and deep analysis for real-world text. In this paper we describe the details of the system, and show its coverage and accuracy. For the accuracy evaluation, we compared outputs of the Japanese LFG parsing system with outputs of standard bunsetsu3 dependency parsers for Japanese.I Korean has recently been added to this list. The Japanese LFG grammar is used as the basis for a grammar of Korean ( Kim et al., 2003  This paper is organized as follows. Section 2 describes the architecture of the Japanese LFG system. In Section 3 we present the Japanese grammar written in the LFG formalism for the system described in Section 2. We show the coverage of the system in 4.1, and explain our approach to evaluating the accuracy of the system and report experimental results in 4.2. We report a Japanese parsing system with a linguistically fine-grained grammar based on the Lexical-Functional Grammar (LFG) formalism. The system is the first Japanese LFG parser with over 97% coverage for real-world text. We evaluated the accuracy of the system comparing it with standard Japanese dependency parsers. The LFG parser shows roughly equivalent performance on the dependency accuracy to the standard parsers. It also provides reasonably accurate results of case detection.
A Statistical Approach to Chinese-to-English Back-Transliteration Machine transliteration is very important for research and applications in natural language processing, such as machine translation (MT), cross-language information retrieval (CLIR), and bilingual lexicon construction. Proper nouns are often domain specific and frequently created. It is difficult to handle transliteration using existing bilingual dictionaries. Unfamiliar personal names, place names, and technical terms are especially difficult for human translators to transliterate correctly. In CLIR, the accuracy of transliteration greatly affects the retrieval performance. Recent studies have made great strides toward machine transliteration for many language pairs, such as English/Arabic ( Stalls and Knight, 1998;Al-Onaizan and Knight, 2002), English/Chinese ( Chen et al., 1998;Wan and Verspoor, 1998;Lin and Chen, 2002), English/Japanese ( Knight and Graehl, 1998), and English/Korean ( Lee and Choi, 1997;Oh and Choi, 2002). Machine transliteration is classified into two types based on transliteration direction. Transliteration, forward-direction, is the process that converts an original word in the source language into an approximate phonetic equivalent word in the target language, whereas back-transliteration, backward-direction, is the reverse process that converts the transliterated word back into its original word. Most of the previous approaches require a pronunciation dictionary to convert a source word into its corresponding pronunciation sequence. Words with unknown pronunciations may cause problem for transliteration. In addition, using a language-dependent penalty function to measure the similarity between a source word and corresponding transliteration or using handcraft heuristic mapping rules to deal with transliteration may lead to problems when porting to other language pairs.In this paper, we focus on Chinese-to-English back-transliteration. The proposed framework requires no conversion of source words into phonetic symbols. The model is trained automatically on a bilingual proper name list.The remainder of the paper is organized as follows: Section 2 presents the proposed statistical transliteration model (STM) and describes the model parameters. In Section 3, we describe the framework to deal with back-transliteration. Experimental setup and the results of the evaluation are presented in Section 4. Concluding remarks are made in Section 5. This paper describes a statistical approach for modeling Chinese-to-English back-transliteration. Unlike previous approaches, the model does not involve the use of either a pronunciation dictionary for converting source words into phonetic symbols or manually assigned phonetic similarity scores between source and target words. The parameters of the proposed model are automatically learned from a bilingual proper name list. The experimental results for back-transliteration indicate that the proposed method provides significant improvement over previous work.
On the Sentence Category Transfer of Action-effect Sentences in Chinese-English Machine Translation The HNC (short for Hierarchical Network of Concepts) theory (Huang, 1998) put forward the idea of action-effect chain (AEC), which is composed of the following six links: action (X), process (P), transfer (T), effect (Y), relation (R) and state (5). It advocates that AEC can be used to reflect the maximum commonness of the world and to depict the fundamental laws underlying the existence and development of everything in the universe (Huang, 1998:29). Besides AEC, HNC expounds human thinking via the concept of judgment (D). As judgment is a kind of response made by the rational subjective world to the objective world and the emotional subjective world, judgment and AEC combine to describe the relationship between subjectivity and objectivity, between rationality and emotionality (Zhang Keliang, 2002). AEC+judgment can be called "the generalized action-effect chain" (GAEC), which serves as the basis both for the classification of principal primitive concepts (which constitute one of the eight HNC semantic networks) and for the semantic categorization of sentences. The semantic category of sentences is referred to in the HNC theory as sentence category (SC). The HNC SC system includes 57 basic SCs and, in theory, 3192 composite SCs and over 10 millions of compound SCs (Huang, 2001). Basic SCs are those that have no more than one eigen chunk (EK) which depicts just one of the seven links of GAEC . Composite SCs have only one EK, but their EK originates from the semantic blending of the EKs of basic SCs and depicts two or more of the links of GAEC. As to compound SCs, they come from the compounding of basic SCs and/or composite SCs, with two or more EKs that contain information related to corresponding links of GAEC.The 57 basic SCs fall into 7 main types: action, effect, process, transfer, relation, state and judgment. This paper focuses on a special subtype of action SC, i.e. action-effect SC, and investigates the rules underlying the SC transfer of sentences of this sort from Chinese to English. By this study we expect to gain some insights into the relationship between the syntactico-semantic structure of a ChineseIt should be pointed out that some basic SCs may not have any EK as far as a specific language is concerned. To Chinese, for example, there are four EK-free SCs (referential comparison-jugment jD021J/jD022J, restrictive comparison-judgment jDO1J/jD011J/jD012J, concise tendency jD2J, concise state SO4J) and a quasi EK-free SC (concise judgment jDOJ). sentence and that of its English translation and to facilitate the structural transfer of the HNC-based Chinese-English MT engine under construction. Of the 57 basic sentence categories (SC) defined in the HNC theory, action-effect SC is an important one with distinctive features. In the light of the HNC conceptual network, action-effect sentences in the Chinese language arise directly from causative verbs and compelling verbs, and indirectly from general acting verbs, i.e. via the use of the &quot; de(4)&quot; construction. In Chinese-English machine translation, action-effect sentences follow different SC and SF (sentence format) transfer rules. Therefore, different transfer frames should be adopted so as to ensure the generation of TL sentences with proper syntactico-semantic structures. Experiments show that the rules underlying the SC-SF transfer of action-effect sentences from Chinese to English can cover 90.3% of all the sentences in question.
Modelling Verb Order in Complex Multi-Verbal Predicate Constructions  abbodomo@hku.hk 1. Introduction) Verb order is an important issue in complex multi-verbal predicate constructions, for example, serial verb constructions (SVCs). With more than one verb in the construction, how are the verbs sequenced? What constraints are at play to govern their order? In this paper, we attempt to investigate several constraints that are related to the issue. We also propose a different ranking of these constraints for Cantonese, a Yue dialect spoken in the southeastern parts of China, and Dagaare, a Gur language spoken in the northwestern areas of Ghana, to account for the different verb orderings found in these two languages for SVCs.
The Structure of Spatial Expressions in Saisiyatl Recent research on the structure of spatial language ( Talmy 1983Talmy , 1985Talmy , 2000Bloom et al. 1996) suggests that the linguistic spatial representation is a window on the human conceptualization of the world. Many scholars have researched the grammar of space in English ( Leech 1969;Bennett 1975;O'Keefe 1996), and in Austronesian languages (Utsurikawa 1993;Senft 1997). However, the spatial representations in Saisiyat, an Austronesian language in Taiwan, have not been thoroughly studied. Utsurikawa (1993) has suggested that in Saisiyat, the orientations of the east and of the north relate to the motion of sun as well as to the direction of the chilly wind. Much of the structure of spatial concepts in Saisiyat has remained largely unexplored. Our goal in this paper is to fill in the gap in our understanding of the grammar of space in the language.In Saisiyat, there is a locative case marker ray before a location in general sentences as well as a location focus -an in location-emphasized sentences (Yeh 2000;Tanangkingsing 2003). When we compare Saisiyat with other Austronesian languages spoken in Taiwan, we found that Saisiyat is on a par with most of the other Formosan languages since it has only one locative marker, which is what most Formosan languages have. Both locative case marker and location focus are used in languages such as Pazah (Lin 2000), Tsou (Zeitoun 2000;Huang, S. 2002)  We start with an analysis of spatial terms including directionals and locatives in Section 2. Then in Section 3, we present the syntax of these spatial terms. In Section 4, we discuss Ground in motion-verbs, directionality in Path verbs, and serial verb constructions. Section 5 is the conclusion. In this paper we investigate the structure of spatial expressions in Saisiyat based in part on corpus data. Our corpus material includes both narratives and conversations that together run for approximately 100 minutes. First, the corpus was searched for syntactic patterns of spatial expressions. The structure of dynamic motion expressions is then examined with regard to types and tokens of motion verbs, expression of ground elements, directionality in path verbs, and serial verb constructions. We conclude that Saisiyat, like most of the Austronesian languages in Taiwan, is a verb-framed language.
A Constraint-based Grammar of Case: To Correctly Predict Case Phrases Occurring without Their Head Verb  The current paper argues that the phenomenon in Japanese that case phrases occur without their head verb before the finite complementizer would falsify the HPSG valence/
The Floating of Negative Factors and the Recognition of Semantic Patterns of HUAIYI Sentences in Mandarin 1.1 The usual method of natural language understanding is from form to meaning or meaning to meaning. In some language with developed-forms, different semantic patterns can be distinguished by different forms (inflections). Form is the natural formal mark of semantic understanding of sentences. However, semantic patterns are various, it's impossible for any language to have enough forms to mark all its semantic patterns. Thus it's important to explore the combined-marks of forms and form-meaning synthesis marks in our study.1.2 In the viewpoint of philosophy, generality is the abstraction of individual on one hand, individual reflects generality on the other hand. So, from generality to individual or from individual to individual are two feasible ways of exploring combined-marks of forms and form-meaning synthetic-marks. This article adopts the latter. Based on the exploration of form-meaning synthetic-marks and recognition program of semantic patterns of huaiyi sentences, this paper tries to build a bridge, which is friendly to people as well as to computer in the process of semantic understanding.1.3 Semantic recognition includes sentence meaning recognition and semantic patterns recognition. The semantic understanding of a sentences lies on the accomplishment of these two sides. However, in the our viewpoint, we can only solve one problem. This paper discusses the semantic patterns recognition of huaiyi sentence and the relative theoretical issues.2 Huaiyi Sentence and Its Negative Factors 2.1 Huaiyi sentence is the sentence whose predicate or object is huaiyi. e.g.(1) Ith---AMI/M. K.(2) A&amp;*:_=_I&amp;14, RA-LbOtl. (3) aintifiA/141.(4) ithffE3VNAA-16)E. (5) it'WEI:Mffn, Based on our study, the signification of huaiyi can be described as follows: huaiyi1=distrust; huaiyi2=suspect; huaiyi3=uncertain, puzzle. Different meanings of huaiyi reflect different semantic patterns of huaiyi sentences. For example, the meaning of huaiyi in e.g. (1) and (2) is huaiyi 1, the corresponding sentences meaning are He distrusts Xiao zhang all the time and I don't believe they can win three gold medal this time. the meaning of huaiyi in e.g. (3)and (4) is huaiyi2, the corresponding sentences meaning are I suspect that he is a thief and I suspect that this guy is unreliable. the meaning of huaiyi in e.g. (5) is huaiyi3, the corresponding sentence meaning is I was puzzled that how can I voice without speaking 2.2 In this paper, huaiyi 123 represents the signification of huaiyi which is corresponding to the semantic patterns of huaiyi sentences, and these semantic patterns can be marked by Shuaiyii 2 3 accordingly. As the thinking-judging sentence, different semantic patterns of huaiyi sentences represent different semantic function. Based on our research, a conclusion can be made: huaiyil is judgmental sentence, huaiyi2 is suspicion sentence, huaiyi3 is inquiry sentence. S huaiyi 1 (judgmental sentence) represents that a subject X of thinking forms a judgment on a object Y.Verb huaiyi is a judging word or opinion. E.g. (1) reports that he distrusts Xiao zhang, e.g. (2) reports that I don't believe that they can win three gold medals this time. Huaiyi is the viewpoint of the subject X. Other examples read as follows:(6) tTiv iJUMUtiffi, RafEWRItTo (7) " -A-A1: 4-Mrit -A" 449iVit 230 t fig9h4tpRiff4, 7A-1,EAMIAEOlito (8) -14 g ritiff fg-LEATf4TxtikiW4MAitIttOW3,tiPoto S huaiyi 2 (suspicion sentence) expresses the conclusion of thinking Y, represents that a subject X of thinking passes a judgment about possibility on a object Y. Verb huaiyi has the function of marking the character of conclusion Y, which is a kind of suspicion. In e.g. (3) and (4), Y-he is a thief and this guy is unreliable are two suspicions of subject marked with Huaiyi respectively. Other examples read as follows:t4WIAllEp4inA "Vt" gligRAig, MSAritCfaliri firdTIN LIAR.(11) -4 -.1i*M3.-_#1V14strit i h.q , WrilaiSiDivi4V)fin$4T-98-1± §1A-gAritTirm.Shuaiyi 3(inquir y sentence) reports question-Y in the form of question, which is inquired by subject X.g. Negative factors of judgmental sentence drift into the meaning of verb, which contains negative semantic element bu. the sentence meaning indicates negativity of judgment. Such as, e.g. (1),(2) ,(6), (7), (8) and their verb huaiyi.In typical suspicion sentences, negative factors drift into the conclusion Y whose content is generally derogatory, such as thief in e.g. (3), unreliable in e.g. (4), cheat in e.g. (9), the Y of non-typical suspicion sentences indicates that the result of thinking is opposite to the beginning. For example, the conclusion Y of e.g. (9) it will rain tomorrow is the conclusion of thinking, this conclusion is generally contrary to the opposite beginning of thinking, such as it will be a sunny day tomorrow or it will not rain tomorrow. different levels of semantic recognition of sentence. The recognition capacity and sequence of distinctive feature (a, b, c, d) is different at different levels, however, the distinctive feature of recognition should be examined by speech and demonstrated by theory.3.2 In the field of sentence pattern analysis of linguistic theory study, the capacity of four features (a, b, c, d) are equal, disordered and optional for recognizing the sentence pattern of huaiyi sentence, however, that does not mean that the function of each feature for explaining system is different.For instance, huaiyi is a thinking verb, which belongs to the same word family as the words renwei, guji, caice, juede. i In the family of thinking verbs, "huaiyi" which has negative factors is opposite to other thinking verbs without negative factors at meaning. Thus as the distinctive feature of huaiyi sentence pattern, an important function of negative factors (c, d) is to distinguish huaiyi sentences from non-huaiyi sentences.In comparison with negative factors, other features (a, b) don't possess the function of distinguish upper-patterns.In this paper, the classification and explanation of meaning of huaiyi is differ from any dictionary in hand. By demonstrating the correspondence of negative factors patterns (c) and its drift (d), and examining the fact of language, we divide the meanings of huaiyi into three items and describe it so. From the research of recognizing wording meaning, we conclude that don't take it for granted that the word meaning of a authoritative dictionary can be used directly for natural language processing. In fact, lexical items in most of these dictionaries are neither exhaustive nor objective. They are provided quite randomly. Negative factors and their floating are the semantic marks of objective identification of the meaning of huaiyi and sub-categorization of sentence patterns of the huaiyi sentences. This article discusses negative factors, the floating of negative factors and recognition of different patterns of huaiyi sentences. It has also bee argued that there is an operational mark of syntactic-semantic generalization which functions to identify the semantic patterns of sentences, and demonstrated the nature and use of this kind of mark. We argue that more reliable connection can be established between human beings and computers. Some related theoretical and methodological issues are also addressed in this article.
Using Mutual Information to Identify New Features for Text documents of Various Domains Most text processing systems, e.g., information extraction, text categorization, clustering, and machine translation, use words (rather than characters) as basic units to build their algorithms. Thus morphological process like segmentation and feature identification becomes an important step, extremely for languages like Chinese, which lacks morphological marks like space separator between words or capital letters at proper names. The quality of segmentation and feature identification greatly influences the performance of the overall text processing systems. Unknown words, i.e., words used in a document but not collected in a segmentation dictionary, and unknown proper names (persons, locations, organizations and their abbreviations) often reduce the precision and quality of a Chinese segmentation algorithm. Although there are ways to collect a balanced word list as the basis of a dictionary, a document always has new words and new names. Hence there must be an efficient way to identify these features. If they are not correctly identified, the real single-character words and the lone characters that actually combine into a new word, the common words and proper names will all be mixed together, which impediments followed processing steps.This paper presents a method of using document-scope mutual information to identify three types of features in a unified algorithm. These three types of features are:• proper names, including sub-categories of person names, place names, organization names, and their abbreviation forms; • unknown words, including common words used in a document but not collected in segmentation dictionary, product and brand names; • document terms, usually some phrases formed by multiple words. These are often key concepts of a document. Mutual information is used to evaluate the coherence level of two consecutive characters and words in a document, and those bigrams of higher mutual information are assumed as "seed" of possible features. Then these seeds are extended to both their right and left sides, still using mutual information as a criterion to determine how long to be extended, to form a list of candidate features. The sifted patterns are assigned a category type and a confidence value, according to their internal constructions, their contexts, and their distribution in the document scope.Since the method uses document-scope statistics as a major criterion to identify proper names, unknown words, and document terms, it's relatively easy to adjust to various domains, by adding relatively little domain-specific knowledge represented via rules. The document statistics results can also be, a helpful resource to assist generating these rules. The method proposed in this paper doesn't require a large manually-tagged corpus, but it could cooperate with such a corpus, in a way of learning useful domain-knowledge from corpus and then applying it to this method.The next section introduces some previous works on feature identification, focusing on that for Chinese language. Section 3 introduces the method to calculate entropy and mutual information values for arbitrary length of patterns. Section 4 introduces the overall steps of feature identification. Section 5 provides the results of some preliminary experiments, and the last section gives some thoughts about future works. The task of identifying proper names, unknown words and new terms, is an important step in text processing systems. This paper describes a method of using mutual information to collect possible segments as candidates of these three feature types in a document scope. Then the construction and context of each possible feature is examined to determine its type, canonical form and meaning. Adding very little domain-specific knowledge, this method adapts to various domains easily.
Applicability Analysis of Corpus-derived Paraphrases toward Example-based Paraphrasing Paraphrasing is a very important form of processing for natural language processing (NLP). A characteristic property of natural language is that various expressions can exist to express a single concept. So far, ad hoc paraphrasing methods have been applied to many NLP applications, such as machine translation, information retrieval, automatic summarization, and so on. Nowadays, however, many researchers recognize automatic paraphrasing as a very important technique for many areas of NLP, and work in this field has recently become more active.We view paraphrasing as a translation where the source language and the target language are the same. Thus, we can take a similar approach to that of machine translation for paraphrasing. In this study, we use an example-based approach to control paraphrasing.So far, a number of new methods for paraphrase extraction have emerged (e.g., (Lin and Pan- tel, 2001;Yamamoto, 2002;Barzilay and McKeown, 2001; Barzilay and Lee, 2003;Shinyama and Sekine, 2003;Pang et al., 2003)). There are several extracting methods: for example, one extracts paraphrases from a monolingual corpus, and another extracts from a multilingual parallel corpus. However, most of these methods have focused on efficiency or how many paraphrases would be extracted, and there have been few discussions on the types of paraphrases that were extracted.Meanwhile, a rule-based approach seems reasonable in terms of the application of paraphrasing ( Takahashi et al., 2001). Therefore, an investigation of how many paraphrasing examples are valuable, or difficult-to-write paraphrasing rules that cover the extracted examples are important. In this paper, we thus discuss the types of paraphrases that are extracted from a bilingual parallel corpus.We analyze paraphrasing examples extracted from a Japanese-English parallel corpus of travel conversations. The corpus is made for Japanese speakers traveling to English-speaking countries. We can extract paraphrases from a parallel corpus ( Barzilay and McKeown, 2001).The basic idea of this method is that one expression can be translated into several expressions, and the translated expressions are paraphrases of each other.In this paper, we introduce a method that extracts paraphrasing examples from a JapaneseEnglish parallel corpus. Then, we analyze the extracted examples and discuss several issues about them. The purpose of this study is to answer the following questions:(a) How many of the paraphrasing examples that are automatically extracted from a parallel corpus can be directly applied to paraphrasing?(b) What types of paraphrasing will be achieved by the extracted examples? Two kinds of paraphrases extracted from a bilingual parallel corpus were analyzed. One is from an adjectival predicate sentence to a non-adjectival one. The other is from a passive form to a non-passive form. The ability to extract paraphrases is strongly desired for paraphrasing studies. Although extracting paraphrases from multilingual parallel corpora is possible, the type of paraphrases extracted is unknown. We discovered what types of examples can be obtained, and what types of paraphrasing will be available for the two kinds of paraphrases.
A Word Selection Model Based On Lexical Semantic Knowledge In English Generation)  Word selection is an vital factor to improve the quality of machine translation. This paper introduces a new model for word selection based on lexical semantic knowledge, which could deal with the problem significantly better. Meanwhile, the construction of the English lexical semantic knowledge base required for the model in our Chinese-English machine translation system is also discussed in detail. 1 Word selection Methods Based on Lexical Semantic Knowledge in Generation The task of Vocabularies Handling in machine translation is to map source language words or phrases to their corresponding ones in target language. The task should be performed in almost every stage of machine translation, since words are basic elements of a sentence. A word in a source language can be translated into many different ones in the corresponding target language, since there exist 1 to N mapping between words in different languages due to the homophony and synonyms. But only one of them should be chosen according to the context. Such work is called Word selection. It is common practice that if one target word is selected improperly during the word selection, the sentence of the translation becomes quite unreadable, or even its meaning is much different from the source sentence. Word selection is regarded as one of the most important and difficult problem in machine translation. (Liu Xiaohu et al., 1998). With the development of machine translation, researchers realized that it is more important to consider its semantic constraints in dealing with the problem of word selection than syntax constraints of each word candidates, and are now paying more and more attention to applying of semantic knowledge in machine translation. The following (in 1.1 and 1.2) are two frequently used methods of this kind.
Corpus-based Ontology Learning for Word Sense Disambiguation An ontology is a knowledge base with information about concepts existing in the world, their properties, and how they relate to each other. An ontology is different from a thesaurus in that it contains only language independent information and many other semantic relations, as well as taxonomic relations. In this paper, we propose to use the ontology to disambiguate word senses.All approaches to word sense disambiguation (WSD) make use of words in a sentence to mutually disambiguate each other. The distinctions between various approaches lie in the source and type of knowledge made by the lexical units in a sentence. Thus, all these approaches can be classified into Al-based, knowledge-based, or corpus-based approaches, according to their sources and types of knowledge (Ide, 1998). AI-based WSD methods (Dahlgren, 1988) use a semantic network, or frames containing information about word functions and the relation to other words in individual sentences; or preference semantics, which specifies selectional restrictions for combinations of lexical items in a sentence. The difficulty with handcrafting the knowledge sources is the major disadvantage of AI-based systems. Knowledge-based methods (Resnik, 1995a;Yarowsky, 1992) have utilized machine-readable dictionaries (MRD), thesauri, and computational lexicons, such as WordNet. Since most MRDs and thesauri were created for human use and display inconsistencies, these methods have clear limitations. Corpus-based methods (Dagan, 1994;Gale, 1992) extract statistical information from corpora which is monolingual or bilingual, and raw or sense-tagged. The problem of data sparseness commonly occurs in the corpus-based approach, and is especially severe when processing in WSD. A smoothing and concept-based method is used to address this problem.Our WSD approach is a hybrid method, which combines the advantages of corpus-based and knowledge-based methods. We use our semi-automatically constructed ontology as an external knowledge source and secured dictionary information as context information. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words.The remainder of this paper is organized as follows. In the next section, we describe the semi-automatic ontology construction methodology briefly. The ontology learning is explained in Root (dummy node)   I   1  1  I  I  1  I  1  I  nature character change action feeling human disposition society institute thi gs  0  1  2  3  4  5  6 This paper proposes to disambiguate word senses by corpus-based ontology learning. Our approach is a hybrid method. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the least weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation.
On Intra-page and Inter-page Semantic Analysis of Web Pages In the recent years, the content and structure of Web pages become much more complex for business purposes with easy access and user-friendly. However, it brings a big challenge for automatic Web information processing system since most of the Web's content today is designed for humans to read, not for machines to manipulate meaningfully (Tim Berners-Lee 2001), and many problems occur in current Web machine processing due to lack of semantic analysis. According to these problems in practical application, two opposite kinds of semantic information of Web pages are analyzed in this paper.Web pages often represent a collection of different topics and functionalities that are loosely knit together to form a single entity. People can easily identify the information areas with different meaning and function in a Web page, but it's very difficult for automatic processing systems since HTML is designed for presentation instead of for content description. Now most of current Web IR (Information Retrieval) and DM (Data Mining) systems regard a Web page as an atomic unit and do not give due consideration to the intra-page semantic structure of a Web page, and in this case many problems occur. (Xiaoli Li 2002) illustrate the precision decline occurred when the query words scatter at different semantic information areas in a Web page. The use of templates has grown with the recent developments in Web site engineering, because the template is very valuable from a user's point of view since it provides context for browsing, however, (Ziv Bar- Yossef 2002 andSoumen Chakrabarti 2001) reveal that the template of Web pages skew ranking, IR and DM algorithms and consequently, reduce precision, and (Shian-Hua Lin 2002) describe the intra-page redundancy caused by the common template. Obviously, it is important to analyze the intra-page semantic of Web page for improvement of current Web application.On the other hand, Web is a hypertext environment, and hyperlink analysis has been widely used in the Web information processing system, such as Google (Sergey Brin 1998) and HITS (Jon M. Kleinberg 1999). However, these systems just regard the hyperlink as the simple reference and endorsement, actually from the point view of the author of the Web page, the hyperlink tells the diverse inter-page semantic information of Web pages, such as reference, related topic and document organizing structure, etc. Understanding the hyperlink semantic will provide better solution for Web content management and integration. For example, in actual Web data, a logical document (such as slides and BBS articles in one thread) discussing one topic is often organized into a set of pages connected via links provided by the page author as document structure links. In such a situation, a data unit for Web data integration and processing should not be a page but should be a connected sub-graph corresponding to one logical document. If we can identify the links pointing to the related contents, we will acquire better performance on focus crawling and computation of link analysis.To solve the problems mentioned above, we explore intra-page semantic of the Web page and partition the whole Web page to a finer granularity based on the repetitive pattern discovery and clustering. Furthermore, we synthesize hyperlink semantic of Web pages, and define an information organizing oriented hyperlink semantic category. We propose the corresponding feature selection and quantification methods according to presentation and context of the hyperlink carrier. We employ C4.5 decision-tree to recognize the hyperlink semantic automatically. We evaluate our ideas by experiment, and the result proves its feasibility.The reminder of this paper is organized as follows. In the next section, we briefly discuss the related work. Section 3 illustrates our analysis method of intra-page semantic structure and inter-page semantic structure in detail. Experiment evaluation and analysis are described in section 4. Last section presents our conclusion and future work. To make real Web information more machine processable, this paper presents a new approach to intra-page and inter-page semantic analysis of Web pages. Our approach consists of Web pages structure analysis and semantic clustering for intra-page semantic analysis, and machine learning based link semantic analysis for inter-page analysis. Based on the automatic repetitive patterns discovery in structure level and clustering in semantic level, we explore the intra-page semantic structure of Web pages and extend the processing unit from the whole page to a finer granularity, i.e., semantic information blocks within pages. After observing the various hyperlinks, we synthesize the Web inter-page semantic and define an information organizing oriented hyperlink semantic category. Considering the presentation of the hyperlink carrier and intra-page semantic structure, we propose corresponding feature selection and quantification methods, and then exploit the C4.5 decision-tree method to classify hyperlink semantic type and analyze the inter-page semantic structure. In our experiments, the results suggest that our approch is feasible for machine processing.
TOWARDS A MULTI-OBJECTIVE CORPUS FOR VIETNAMESE LANGUAGE Vietnamese speech has been created approximately 4000 years, closely related with Indo-European languages. Today there are around 80.000.000 people using this language. The main feature which makes it differ from Western languages is that it belongs among the group of mono -syllable languages. That means it never changes its morphology. In order to express grammatical sense we usually use means of the outside word as grammatical words, order words, etc. The other important feature that makes it differ from Eastern language is that it uses extended Latin based symbols.In Vietnamese language, the basic is "tiéng". There are totally around 8000 "tiéng" found in Vietnamese modern language [1]. For speech, in the complete form, "tiêng" has following model: (as In some cases, "ti&amp;ig" can be appeared without final sound, inter-sound, or initial sound. That means main sound and tone are the major components from which "ti8ng" is formed. There are 22 initial sounds, 14 main sounds and 10 final sounds coordinating with 6 tones. In order to distinguish "tiéng", initial sound, inter-sound, main sound and final sound are used. In cases that all of them are the same, tones are used. An example of "tiéne and tones are given in Fig. 2 and Fig. 3. For writing system, "tiêng" usually consists of two main components: consonant (corresponding to initial sound) and syllable (corresponding to inter-sound, main sound and final sound) coordinating with accent in the accent set (corresponding to tones). There are totally 27 consonants (table 1), 434 syllables (a part of them are shown in Table 2) and 6 accents ( Table 3).   u'ng u'de u'di &amp;UM u'dn ((din u'dp Waft u'ou tit WuIn general all Vietnamese words are created from "ti&amp;i.C. We can have a word with one, two, three, four or even five "tieng". However not all "tiéng" have meaning. For simply, we can suppose that a single word is a meaning word that contains only one "tieng". More than 90% of all single words belong to this kind (The others can be determined by hand). Compound word or word sequence is a meaning word which is formed from more than one "tiéng". For example if we make a query to find all compound words that begin from "tiéng" "hoc" (to study, to learn) from the dictionary we could have at least 79 alternative word sequences in which eight of them are combined from 3 consecutive "tieng", two of them are created from 4 consecutive "tieng" and the remainder are formed from two consecutive "tiéng" as shown in  The appearance of compound words raises several problems for Vietnamese natural language processing such as the part of speech tagging and parsing because we need a correct segmentation of those compound words in each sentence. Considering the sentence: "Hoc sinh hoc sinh vat" (Pupils learn biology) we could have following combination of correct word tags (see Table 4) while the correct segmentation should be "Hoc sinh/Noun/ hoc/Verb/sinh vat/Noun".In the previous works, we have introduced some approaches for the recognition of Vietnamese document images [3], handwritten images [4] and also isolated speech words [2]. However we soon recognized that in order to archive a better result, a corpus is needed. In the following sections, we will present the VnCorpus in details and give some first experiences in using this corpus for the segmentation of sentences into correct word sequences and for the recognition of Vietnamese continuous speech.101p.04111.1pArnare  Texts in written language are collected base on time and domain criteria. Due to the wide usage of QuocNgu (national language) and political events around the 20 6i century, we decided to collect data based on three periods: before 1945 (10%), from 1946 to1975 (40%) and from 1976 to present (50%). We classified texts into sub-domains as shown in table 5. Today, corpus plays an important role in development and evaluation language and speech technologies, such as part of speech tagging, parsing, word sense disambiguation, text categorization, named entity classification, information extraction, question answering, structure discovery (clustering), speech recognition and machine translation systems, etc. One can exploit valuable statistical parameters taken from corpus to train and evaluate those systems. Developing such a corpus has been a challenging work in context that a huge data needed to be processed and annotated. In this paper we first represent our developing method for a multi-objective Vietnamese language corpus, namely VnCorpus, together with the description of various kinds of sources from which we have used to build up this database. It then goes on to describe some first experiences in using this corpus for the segmentation of sentences into Vietnamese words and for the recognition of Vietnamese continuous speech. Upon completion the corpus will constitute a valuable resource for research in the fields of computational linguistics, language and speech technologies.
Using Zero Anaphora Resolution to Improve Text Categorization In Chinese text, anaphoric expressions are frequently eliminated, termed zero anaphor (ZA) hereafter, due to their prominence in discourse ( Li and Thompson, 1981). Zero anaphors are generally noun phrases that are understood from the context and do not need to be specified. Anaphors in Chinese can be classified as zero, pronominal and nominal forms, as exemplified in (1) by fì and #13respectively'. Zero anaphors are generally noun phrases that are understood from the context and do not need to be specified.( 1 ) a. WE r-r,J i , Zhangsan frightened and ran outside.b. co fiPJ -M (He) bumped into a person.c. In Chinese, anaphors are frequently omitted, termed zero anaphor (ZA), from text due to their prominence. Thus the information carried by ZAs in text can not be used to contribute the calculation of text categorization. In this paper, we employ a ZA resolution method to recover the omissions of anaphors in text. Then the resulting text is used as the input of a text categorization system. The experiment result shows that ZA resolution method enhances the accuracy of text categorization from 79% to 84%.
Dependency of Long-Distance Reflexives The locally bound reflexives have been explained by the traditional binding theory in Chomsky (1981), R&amp;R's predicate based theory (MR 1993), and the recent derivational theory (Hornstein 2001). Questions are raised with regard to the reflexive forms that seem to be bound across the clause boundary. Long-distance reflexives are found in many languages such as Chinese, Japanese, Korean and others. They are even found in English as seen below. Sumi-NOM Suji-NOM Younghee-NOM self-ACC silehan-ta-ko] sayngkakhan-ta-ko alkoi-ss-ta hate-DEC-COMP think-DEC-COMP know-PAST-DEC Sumii knows that Sujii thinks that Youngheek hates selfvyk' (4) Johni said that a picture of himself; is on sale For the constructions like (1), (2) and (3), some argued that they are actually pronouns, and some argued that they are reflexives that can be accounted for by the parameterization of the binding theory (Manzini and Wexler 1987). The sentence (4) is rather surprising, since the English reflexive himself is the most typical reflexive that is locally bound, but it turns out to be bound across the clause boundary. For (4), Chomsky (1981;1986) introduced the concept of the governing category, incorporating the phenomena into the local binding. In the movement theory (Chomsky (1986), Battistella (1989), Sung (1990), Cole et al. (1990), and Cole and Wang (1996)), it was claimed that the apparent long-distance binding between the reflexive and the antecedent is actually local with the covert movement of the reflexive across the clause boundary. All these approaches are seeking for syntactic accounts.On the other hand, it has been claimed that logophoricity plays as a licensing condition for long-distance reflexives (Kuno (1987), Sells (1987), and Zribi-Herts (1989)). The termìogophoric pronoun' was originally used for the analysis of African languages. The concept of logophoricity used for the long-distance reflexives is different from the original concept, including SOURCE, SELF, and PIVOT. In (1-3), both the matrix subjects and the embedded subjects seem to satisfy the logophoric conditions, being aware of the situation predicated. The high frequency in using the verbs of 'saying' and 'thinking' as a matrix subject could induce such a misconception. However, logophoricity cannot explain all binding phenomena, since there are languages such as Chechen and Ingush that do not require the logophoric conditions (Nichols 2001). Chinese and Korean also do not always require the logophoric conditions for antecedents as illustrated below.(5) Zhangsan; cong Lisij chu tingsuo Wangwuk bu xihuan Zhangsan from Lisi place hear Wangwu not like self`Zhangsani self`Zhangsani heard from Lisij that Wangwuk does not like selfilvk' (6) Chulswurka Youngswuj-lopute Youngheek-ka calcivw1u1Chulswu-NOM Youngswu-from Younghee-NOM self-ACC sileha-n-ta-ko tul-ess-ta dislike-PRES-DEC-COMP hear-PAST-DEC 'Chulswui heard from Youngswuj that Younghee k dislikes self y*j/k' In (5) and (6), the SOURCE NP does not serve as an antecedent. The mixed approach (Reinhart and Reuland (1993), Pollard and Sag (1992), Pollard and Xue (2001), Cole et al. (2001), Huang and Liu (2001) and others) thus comes in between, adopting both the syntactic accounts and the logophoricity-based accounts.This paper attempts to elucidate the licensing conditions on the long-distance anaphors by investigating both the syntactic conditions and the logophoric conditions. I basically follow the spirit of Reuland (2001) in that the syntactic binder is more easily available than the binder based on the logophoricity effects. For the mechanism of the syntactic binding, I follow the derivational approach supported by Hornstein (2001), Kayne (2002), and Zwart (2002). This paper aims to account for dependency of long-distance anaphors within the derivational approach. Dependency between the antecedent and the anaphor is determined by the universal operations, Merge, Move and Agree. Following Hornstein (2001) and Zwart (2002), anaphors in Korean, Chinese, and Japanese are argued to merge with antecedents to obtain anaphoricity. How such a merged complex participates in derivation is demonstrated using both local and long-distance binding examples. Logophoricity and discourse effects are obtained after computation within CHL when the antecedent and the anaphor are not merged at the outset.
Voicing Constraint and Segmental-Tonal Neighborhood Effects on Clusters in Thai  Investigating existing and non-occurring onset clusters in Thai led to the postulation of a voicing constraint. Native speakers were asked to give well-formedness judgments to novel words with and without violations of the constraint. The findings support the argument for the existence of the constraint in the speaker&apos;s mind. Furthermore, it was found that within all groups of novel words, categorized by whether or not they obey the constraint and whether or not they contain the existing clusters, there were segmental neighborhood effects. The novel words in dense segmental neighborhoods were rated significantly higher than those in sparse segmental neighborhoods. Finally, the present study puts forward the proposal and evidence that the degree of tonal neighborhood density also influences the speaker&apos;s perception of novel words. 1. Clusters in Thai Putting true Thai words in minimal pairs reveals that the language has 11 possible consonant clusters, which show up exclusively in the onset position (Naksakul 1998). The clusters consist of /pr p hr pl phl tr kr khr kl kh1 kw/ and /khw/. That is, the second consonant of a legal cluster is restricted to those in the set {r, 1, w}. Regarding the first consonant, they are drawn from the set of consonants belonging to the plosive class shown in (1). (1) Plosives in Thai Voiceless Unaspirated p t k Aspirated Ph th kh Voiced Unaspirated b d Tumtavitikul (1997: 312) mentions (2a) and (2b) as constraints on the occurrence of clusters in Thai: (2) a. C2 = {1, r, w}, and if C2 = [IN], then C1 = [k, kh] b. *[ocson] [awn] It is true that the existing clusters obey the constraints. However, the constraints do not rule out the following plosive and C2 combinations: (3) /bl br dl dr tl thl thr/ In the present work, I am interested in looking at the set of combinations seen in (3) in comparison with the set of legal clusters. The next section presents the issues concerning onset clusters in Thai that I will investigate. 441
The treatment of Japanese focus particles based on Lexical-Functional Grammar As has been observed in many studies, various functional relations are expressed by particles in Japanese. In particular, particles such as`kuraras`kurar , `bakarr , 'clake', `nomr, `hodo' and 'made' specify focus in sentences. It has been observed that these particles mark contrastive focus within sentences ( Numata et al., 2000). It is important for a Japanese parsing system to be able to treat these focus particles properly, because focus particles bear different syntactic functions depending on where they appear in a given Japanese construction. In this paper, we propose phrase structure rules and lexical entry constraints to treat Japanese focus particles on the basis of the Lexical-Functional Grammar (LFG) formalism (Kaplan and Bresnan, 1982;Dalrymple, 2001). The proposed rules and constraints for treating focus particles have been incorporated into a Japanese LFG system we are currently developing and have been used extensively for parsing real-world Japanese text. The purpose of this paper is to discuss treatment of &apos;focus particles&apos; in Japanese sentences to be incorporated in a Japanese parsing system based on the Lexical-Functional Grammar(LFG) formalism. Focus particle can follow nouns, quantifiers, verbs, other particles (postpositions) and auxiliary verbs. Thus, it is necessary for a large-scale grammar to treat focus particles properly. Furthermore, there are syntactic and semantic ambiguities caused by the particles. We propose phrase structure rules and lexical entry constraints which cover focus particles in various positions and account for the ambiguities.
Empty Category and the Effect of Teaching in Sentence Processing This paper proposes an experimental scheme which solves two independent problems in sentence processing. The first problem concerns the role of empty categories in sentence processing. The second is regarding the sentence processing in L2 acquisition. Different syntactic frameworks have different ways to deal with dislocation constructions. One major disagreement is whether or not empty categories should be assumed. Researchers have been working on this issue on the ground of &quot;psychological reality&quot; (of empty categories), yet have not come to an agreement. The first aim of this paper is to propose an experimental scheme to settle the issue of empty categories. Our second aim is to propose that the application of the experiment to L2 is beneficial to see the teaching effect in L2 acquisition. If native speakers&apos; result supports no-empty-category analysis, yet, L2 learners exhibit the different result, L2 teaching of &quot;wh-movement&quot; created a category which is not in native speakers&apos; mental grammar.
Foreword  In the foreword written on the occasion of PACLIC 14 held in 2000 at Waseda University, I wrote as follows: &quot;The past ten years was the time in which we have witnessed so many wars and so much strife among nations and races. One of the main causes of these tragedies was and still is language differences we employ.&quot; Regrettably, this is still true nowadays. Not a single day passes without the news of murder and killing among nations and races. Therefore, we cannot overemphasize the importance of language study since it is through language that we can help to create and achieve peace in the world. The PACLIC conferences, as is well known, have a long tradition of putting strong focus on the intersection of linguistics and computer science. This focus has a vital importance in the days when we experience a tremendous gap between several branches of language study and the research on computer science. While the PACLIC conferences have provided a sense of family-like community for the people who attend the conference, we have employed anonymous review process since PACLIC 10, sending out each paper to reviewers for evaluation.
Processing and Representing Temporally Sequential Events Events are often ordered in a temporal sequence and such sequential events are expressed in natural language either explicitly or implicitly. They are explicitly expressed by some subordinating conjuncts like "before" or "after" in English or by some unbound nouns like "cen-ey" (before) or "tawum-ey" (after) in Korean. But they may also be expressed in coordinate structures by the sequential occurrences of verbs. Here is a simple example in English, "Mia went to Tokyo, met a friend and stayed at her home." This sentence narrates three sequential events of Mia's going to Tokyo, meeting a friend and staying at the friend's home.The sequence of narration, however, does not always match that of events. One such case involves the mixed occurrences of durative events and punctual events. Consider a case "Mia was ill and took pills and took a nap. She then had a bad dream. She fell off a cliff." This sentence narrates five events, three durative events of being ill, taking a nap, and having a bad dream and two punctual events of taking pills and falling off a cliff.This paper aims at showing how such events can be processed and represented for the proper interpretation of their temporal sequences. For this aim, I make two basic assumptions: one is a minimal syntax as argued for in Lee (2002) and another, a minimal semantic representation that will be shown in this work. In a discourse, events are narrated one by one linearly. But the temporal sequence of events does not always match the linear sequence of narration. One of such cases involves the mixed occurrences of durative and punctual events, as illustrated by &quot;Mia took aspirin and slept, for she was ill. She then fell off a cliff in her dream.&quot; In this narration, five events are reported: three are durative events of sleeping, being ill and having a dream and two are punctual events of taking aspirin and falling off a cliff. This presentation aims at establishing some systematic way of processing such events and representing them in a reasonably understandable temporal sequence. For this, events are analyzed in terms of an interval semantics that allows them to be anchored to appropriate temporal intervals and be ordered in an appropriate temporal sequence. In order to provide a simple syntactic basis, the presentation attempts to develop a small computational program that derive representations in feature structure by analyzing a small fragment of Korean.
Machine Learning based NLP: Experiences and Supporting Tools  Coprus-based approaches to natural language analysis that utilize recent sophisticated machine learning algorithms have now become to achieve very good performance. In this talk I will overview and categorize machine learning based natural language processing tasks and our experiences of using machine learning to various tasks such as segmentation, POS tagging, phrase and NE chunking, and syntactic parsing. I then discuss pros and cons of machine learning approaches and future issues. Finally, I will introduce an ongoing project of annotated corpus maintenance tools for developing consistent data for corpus and machine learning based NLP research.
Towards a Proper Treatment of Adjuncts in Japanese  In this paper we will discuss interpretation of adverbs in Japanese. We will explore the division of labor between the syntactic requirements, semantic requirements, and discourse-contextual constraints involving adverbial interpretation. It will then be argued that this inter-modular approach utilizing LFG explains various elusive paradigms of the adverbs.
An Analysis of the Korean [manyak … V-telato] Construction: An Indexed Phrase Structure Grammar Approach In Korean there is a group of adverbials which show a correspondence to some specified element in the sentence. These adverbials, which are called "concord adverbials (CAs)," are comprised of such "modality adverbials" as those indicating 'condition' and 'concession.' They constitute "Concord Adverbial Constructions (CACs)" together with the corresponding elements. These corresponding elements are typically verbs which have some specified verbal endings (VEs).(1) a. pilok yengca-ka ttena-ess-telato/eto, (ke peulo-nun po-l manha-e.) {CA} Youngja-Nom leave-Past-{VE 'although'}, that program-Top see-Fut be worth-Decl 'Although Youngja doesn't appear, (that program is worth watching.)' b. *pilok yengca-ka ttena-ess-tamyen {CA} {VE 'if'} Here the CA pilok requires that the clause-final verb have an ending of a concessive meaning, i.e. -telato or -eto. It does not allow a conditional ending like -tamyen. These constructions show some special properties which cannot be easily accounted for. One such property in (1) is that the CA and (the verb with) the VE are not always adjacent. Furthermore, as we will see in section 2, they exhibit two different types of unbounded relationships. The purpose of this paper is two-fold. Firstly, after examining the analysis of Korean CACs in Chae (2003,2004), we will propose a new feature of LICENSEE for a better analysis of them. The former analysis employs the LICENSOR feature under the "Indexed Phrase Structure Grammar (IPSG)" framework, which is introduced in Chae (1992) following the spirit of Gazdar (1988). While the LICENSOR approach does not have any problems from a syntactic point of view, it is not very satisfactory from a semantic point of view. We will show that the new LICENSEE approach does not have any syntactic or semantic problems. Secondly, under this revised IPSG framework, we will provide an analysis of a subtype of CACs:(2) manyak chulswu-lul manna-telato (ke kes-ul mal haysenun antoyn-ta.) {CA 'if'} -Acc meet-{VE 'although'} that thing-Acc must not say-Decl 'Even if you meet Chulsoo, (you should not say about that thing.)'This type of CACs contains a CA and a VE which do not seem to be compatible semantically: the former is a conditional adverb while the latter is a concessive ending. Despite this apparent incompatibility, the sentence is grammatical. We will call this type of CACs [manyak … V-telato] constructions or "Conditional-Concessive Constructions (CCCs)." Unlike these constructions, expressions containing a concessive adverb and a conditional ending, e.g. [pilok 'although' … V-myen 'if'], are ungrammatical. In this paper, we will show that the CCC is basically a conditional construction despite its concessive meaning. Concord adverbial constructions in Korean show unbounded dependency relationships between two non-empty entities. There are two different types of unboundedness involved: one between a concord adverbial and a verbal ending and the other between the adverbial as a modifier and a predicate. In addition, these unboundedness relationships exhibit properties of &quot;downward movement&quot; phenomena. In this paper, we examine the Indexed Phrase Structure Grammar analysis of the constructions presented in Chae (2003, 2004), and propose to introduce a new feature to solve its conceptual problem. Then, we provide an analysis of conditional-concessive constructions, which is a subtype of concord adverbial constructions. These constructions are special in the sense that they contain a seemingly incompatible combination of a conditional adverbial and a concessive verbal ending. We argue that they are basically conditional constructions despite their concessive meaning.
An analysis of Japanese ta / teiru in a dynamic semantics framework and a comparison with Korean temporal markers a nohta / a twuta Japanese tense-asepct markers ta and teiru are both analyzed to have a perfect state meaning, among others. 1 However, in the literature, ta and teiru have been studied more or less individually, and the relation between their perfect state meanings is left to be investigated ( Kudo (1995), Ogihara (2001)). They exhibit an asymmetry as Inoue (2000) observes it (Glosses and translations are mine): 2 (1) a. [The water in the kettle comes to the boil while the speaker sees it.]Yoshi, o-yu-ga wai-{ta / ?? teiru}. All right, Hon-hot-water-Nom (come-to-the-)boil-{Past / State-Nonpast} o.k. 'All right, the water has (just) come to the boil.' / ?? 'The water is on the boil.'b.[The speaker put the kettle on the gas and left. Some time later he comes back and finds the water boiling.] Ah, o-yu-ga wai-{ta / teiru}. 'Oh, the water has come to the boil.' / 'Oh, the water is on the boil.'c.[The speaker comes to the kitchen and finds the water boiling. (He doesn't know who put the kettle on the gas.)]Are, o-yu-ga wai-{?? ta / teiru}. ?? 'Oh? The water has come to the boil.' / o.k. 'Oh? The water is on the boil.'In this paper, I analyze this asymmetry and show how ta and teiru with the perfect meaning are different (Section 2). In this light, I elaborate on the semantics of ta and teiru with other meanings, illustrating how the proposed analysis of the perfect meaning fits into the scheme of ta and teiru (Sections 3 and 4). I defend the intuition that ta and teiru make respectively an eventive and a stative description of eventualities and illustrate my analysis in a formal dynamic semantics framework, DRT (Discourse Representation Theory: Kamp and Reyle (1993)). I also analyze ta (with the perfect and the simple past meanings) and teiru in a discourse and illustrate the differences (Section 5). Furthermore, in DRT terms, I will compare Japanese ta / teiru with Korean temporal markers a nohta / a twuta as analyzed by   (Section 6) and conclude my arguments (Section 7).2 Ta and teiru with the perfect state meaning In this paper I will shed new light on the semantics of Japanese tense-aspect markers ta and teiru from dynamic semantics and contrastive perspectives. The focus of investigation will be on the essential difference between ta and teiru used in an aspectual sense related to a perfect. I analyze the asymmetry between ta and teiru with empirical data and illustrate it in the DRT framework (Discourse Representation Theory: Kamp and Reyle (1993)). Defending the intuition that ta and teiru make respectively an eventive and a stative description of eventualities, I argue that ta is committed to an assertion of the triggering event whereas teiru is not. In the case of teiru, a triggering event, if there is any, is only entailed. In DRT, ta and teiru introduce respectively an event and a state as a codition into the main DRS. Teiru may introduce a triggering event only as a codition in an embedded DRS. I also illustrate how the proposed analysis of the perfect meaning fits into a more general scheme of ta and teiru. and analyze ta and teiru in a discourse. Furthermore, in DRT terms, I will compare Japanese ta / teiru with Korean perfect-related temporal markers a nohta / a twuta in light of Lee (1996).
Relational Nouns as Anaphors It has recently been argued that the relational nouns has peculiar syntactic and semantic properties quite different from those of regular nouns (see Jacobson 1999Jacobson , 2000Partee and Borschev 2000;Vikner and Jenssen 1999;Asudeh 2003, among others). Relational nouns denote a wide range of "relations" like kinship relations as in mother, child, etc., whole-part relations as in hand, height, etc., location relations as in neighbor, local bar, or ownership relations as in (my) book, (my) car, etc. These nouns, however, share common semantic properties we will see shortly. Also this group of nouns have to do with the occurrence of certain constructions peculiar to Japanese. Consider multiple subject sentence (1a), indirect passive (1b) and relativization clause (1c):(1) a. Taroo-ga zikka-ga yuufuku-dearu-(koto). Taroo-NOM parents-home-NOM rich-be-PRESS-(FACT) '(the fact that) Taroo's parents' home is rich' b. Taroo-ga tuma-ni sakidat-are-ta. Taroo-TOP wife-DAT passed-away-PASS-PAST 'Taroo suffered his wife's death.' c.[Tuma-ni sakidat-are-ta otoko-wa] isyoku-nimo fujiyuu-o kanjir-u. wife-DAT passed-away-PASS-REL man-TOP food-or-clothing have-trouble-PRES 'The man who suffered his wife's death suffers hardships in food and clothing.'The existence of the constructions illustrated in (1) in Japanese, unlike in English, should partly be ascribed to the fact that possessors can "run away from home," i.e., the specifier position of relational nouns, or, putting it differently, relational nouns can occur bare without genitive possessor NPs. Though we do not take the genitive NPs preceding relational nouns to literally have the "possessor-relation" to the latter, we continue use the term "possessor", following a long tradition, even when the possessor appears in a position distant from a relational noun it is associated with, as in (2): (2) a. Taroo-no jikka-ga totemo kanemoti-da. Taroo-GEN.POSS family-home-NOM very rich-be-PRES 'Taroo's parents' home is very rich." b. Taroo-ga totemo jikka-ga kanemoti-da. Taroo-NOM.POS very family-home-NOM rich-be-PRESWe will argue that relational nouns are referentially dependent on other (referential) nouns, i.e., they have some sort of anaphoric properties. This hypothesis will explain some interesting phenomena like quantification as shown in (3):(3) Kono roojin-hoomu-deha subeteno dansei-ga tuma-ni sakidat-are-teiru. In this nursing home, all men-NOM wife-DAT die-earlier-PASS-PAST 'In this nursing home, all men suffered their wives's death."In sentence (3), though only the man variable is universally quantified, the value of wife variable must covary with a particular choice of a man, that is, tuma 'wife' can never take scope over dansei 'man'. Instead of the unselective binding or movement analysis, we propose that relational nouns contain variables to get bound in the course of derivation, and show the analysis in which constructions containing relational nouns, mostly concentration on indirect passive sentences, can be assigned sound model-theoretic interpretations 'on the fly', adopting a version of categorial grammar as our framework. This paper examines the syntactic and semantic properties of a set of nouns recently called &quot;Relational Nouns&quot; like mother, neighbor, etc. Relational nouns denote relations between individuals, rather than sets of individuals regular nouns denote, and are referentially dependent on individual-denoting expressions. In Japanese relational nouns may appear &apos;bare&apos; with no genitive possessors in the noun phrases they project, i.e., but still require the possessors somewhere in sentences. The presence of bare relational nouns allow Japanese to have a lot of peculiar constructions like multiple subject sentence, indirect passives, etc. Assuming the a version of categorial grammar in which the syntax and semantics work in tandem, we discuss the proper way to provide model-theoretic interpretations for expressions containing relational nouns under direct compositionality.
Capturing and Parsing the Mixed Properties of Light Verb Constructions in a Typed Feature Structure Grammar  One of the most widely used constructions in Korean is the so-called light verb construction (LVC) involving an active-denoting verbal noun (VN) together with the light verb ha-ta &apos;do&apos;. This paper first discusses the argument composition of the LVC, mixed properties of VNs which have provided a challenge to syntactic analyses with a strict version of X-bar theory. The paper shows the mechanism of multiple classification of category types with systematic inheritance can provide an effective way of capturing these mixed properties. The paper then restates the argument composition properties of the LVC and reenforces them with a constraint-based analysis. This paper also offers answers to the the puzzling syntactic variations in the LVC. Following these empirical and theoretical discussions is a short report on the implementation of the analysis within the LKB (Linguistics Knowledge Building) system. 1 Issues The first main theoretical and computational issue we encounter in the analysis of the LVC is the status of the light verb and argument composition. One of the main properties the light verb ha carries is that it does not affect the argument structure of the VN it combines with.
Japanese Subjects and Information Structure: A Constraint-based Approach Japanese is a language in which topic and focus are identified by the use of the particles. In the case of subjects, they are either marked by wa or ga. The two particles are illustrated in the following examples: The fact that (1a) and (1b) are given the same semantic interpretation, does not tell us what wa and ga really mean. These particles are said to have two uses respectively. Kuno (1973), 1 for instance, distinguishes the use of wa by referring to (2a) as a thematic, and (2b) as a contrastive:(2) a. 'As for Ken, he left.' b. 'As opposed to other people, Ken left.'Much syntax-based work has been done on the use of these particles but the basic distinction between them is the same. Whatever the correct characterization of the uses of wa shown in (2) may be, the meaning of (1a) that is common to both uses is that the proposition that somebody left is true if that somebody is Ken. Whether a subject marked by wa is uttered in one use of wa or the other cannot be determined without knowledge of the context. In this paper, we demonstrate that the relation between syntactic constituency and contextual information is not a tight one-to-one mapping as purely syntax-based analysis assumes. Based on Vallduví and Endahl's Information Structure (Vallduví, 1992;Engdahl and Vallduví, 1996;Vallduví and Engdahl, 1996), we propose a constraint-based one-to-many mapping mechanism which captures some aspects of topic/focus interpretation. The study of information structure, we argue, is essential in addressing fundamental questions regarding the multi-dimensional grammar for Japanese. This paper is concerned with how topic/focus articulation should be optimally integrated into Japanese grammar. Based on Engdahl and Vallduví&apos;s (1996) Information Structure, we propose an analysis with the following characteristics: (i) information structure is an integral part of Japanese grammar and interacts in principled ways with both syntax and phonology, (ii) the representations of topic/focus in the information structure and its interactions with the particles wa/ga show one-to-many relation, and (iii) the ordering of grammatical functions and its interactions with other grammatical parts play an important role in determining the focus domain.
High WSD accuracy using Naive Bayesian classifier with rich features WSD is always a difficult and important task in natural language processing. Its task is to determine the most appropriate sense for an ambiguous word given a context. Approaches for this work include supervised learning, unsupervised learning, and combinations of them. Except for the expense involved in building labeled datasets, supervised based methods generally give results with higher precision. Many supervised learning algorithms have been applied, such as Bayesian learning, Exemplar-Based learning, Decision Trees, Decision Lists, and Neural Networks. Despite their simplicity, NB methods are still effective when applied to WSD (Mooney, 1996;Pedersen, 2000).Before presenting the previous related studies and describing our approach, we need to define some terms that are used throughout in this paper. These are topic context, local context, and collocation. The first kind of information, which is always used for determining the senses of a word, is the topic context represented by a bag of surrounding words in a large context of the ambiguous word. The other informative resource is collocation. There are various definitions of collocation, and for our approach we define collocation as a sequence of words including the ambiguous word. Several studies, such as Leacock and Chodorow (1998), used local context for disambiguating word senses. Like them, we define local context as the words (or tags of words) assigned with their position in relation to the ambiguous word in a local context. For example, suppose that we have a context of the ambiguous word interest as follows:"yields on money-market mutual funds continued to slide, amid signs that portfolio managers expect further declines in interest rates." Then the topic context includes the words: yields, money-market, mutual, funds, continued, . . .; Collocations include the expressions: interest rates, declines in interest, in interest rates, further declines in interest rate ,. . .; Local context is represented by the pairs: (declines,-2), (in,-1), (rates,1), (further, -3), . . .Note that words in collocations and local contexts can be replaced by their part-of-speech tags, and then we will have new features. We also use other terms in the same meaning: unordered words as surrounding words, and ordered words as the words assigned with their positions. Mooney (1996) compared six supervised algorithms including NB, Perceptron, Decision-Tree, k Nearest-Neighbor classifier, logic-based DNF (disjunctive normal form), and CNF (conjunctive normal form), and concluded that NB and Perceptron are the best methods for WSD. He used only the words surrounding the ambiguous word as features for the classifiers.Pedersen (2000) proposed a simple but effective approach using Ensembles of NB classifiers. He showed that WSD accuracy can be improved by combining a number of simple classifiers into an ensemble. He built nine different NB classifiers based on using nine different sizes of the left and the right windows of context: 0, 1, 2, 3, 4, 5, 10, 20 and 50. His method was tested on two datasets of the words interest and line and achieved 89% and 88% accuracy, respectively. He also used only topic context for making decisions.Only a few papers have considered information other than topic context when using the NB model. Leacock and Chodorow (1998) used an NB classifier, and indicated that by combining topic context and local context they could achieve higher accuracy. In comparing NB methods with Exemplar-Based methods, Escudero (2000a) utilized most of the features used in Ng and Lee (1996), and showed that exemplar-based algorithm outperforms the NB algorithm. However, these papers did not mention how to select appropriate features, so the features used in their papers do not contain enough information and some information, such as part-of-speech, may be redundant.In many WSD studies, authors use NB as a baseline method for comparison, but many of them use NB with only topic context while adding other information to their own methods. In this paper, we focus on two problems: The first is to determine whether a WSD system using NB will improve the accuracy of its prediction if more kinds of information than usual are used. The second is to discover which kinds of information will be useful for determining the senses of an ambiguous word. We first discuss which kinds of information will be most useful for sense determination, then use a forward sequential selection algorithm to extract the best subset of features.The experiments on some datasets widely used in WSD show that the accuracies will be much improved by combining three kinds of information: topic context, local context, and collocation. One more difference from previous studies is that we do not need to use information, such as part-of-speech tags, other than the words themselves in the context.The rest of this paper is organized as follows: Section 2 briefly presents the NB classifier. Section 3 discusses choosing features for word sense disambiguation and shows the algorithm for feature selection. Section 4 shows our experiments and compares the results to those of the best previous studies when testing on four words: interest, line, serve, and hard. Section 5 shows our results and comparison with the others on the DSO corpus. Section 6 discusses the obtained results, and finally our conclusions are presented in section 7. Word Sense Disambiguation (WSD) is the task of choosing the right sense of an ambiguous word given a context. Using Naive Bayesian (NB) classifiers is known as one of the best methods for supervised approaches for WSD (Mooney, 1996; Pedersen, 2000), and this model usually uses only a topic context represented by unordered words in a large context. In this paper, we show that by adding more rich knowledge, represented by ordered words in a local context and collocations, the NB classifier can achieve higher accuracy in comparison with the best previously published results. The features were chosen using a forward sequential selection algorithm. Our experiments obtained 92.3% accuracy for four common test words (interest, line, hard, serve). We also tested on a large dataset, the DSO corpus, and obtained accuracies of 66.4% for verbs and 72.7% for nouns.
Automatic Discovery of Telic and Agentive Roles from Corpus Data We present a study of methods for automatically discovering the telic and agentive roles of nouns based on corpus data. These relations form part of the qualia structure assumed in generative lexicon theory (Pustejovsky, 1995). The qualia structure of a given noun incorporates (at most) the following four roles:Formal role: the conceptual superclass of the noun. e.g., orientation, magnitude, shape, dimensionality, color, or position.Constitutive role: the internal constitution of the entity. e.g., material, weight, parts, or component elements.Telic role: the typical function of the entity. i.e. what the entity is for.Agentive role: the origin of the entity, or its coming into being e.g., creator, artifact, natural kind, or casual chain For example, for the noun book, publication is a formal role noun, text is a constitutive role noun, read is a telic role verb, and write is an agentive role verb.Research has been done on extracting the formal and constitutive roles of nouns. Hearst (1992), Widdows and Dorow (2002), and others developed methods of automatically acquiring noun hyponymscorresponding to the formal role-by identifying a set of frequently used and unambiguous lexicosyntactic patterns. Girju et al. (2003) proposed a method of learning part-whole relations, which correspond to the constitutive role. It is also possible to use lexical resources such as WordNet (Fellbaum, 1998) to determine formal (through hypernym links) and constitutive role data (through meronym links). Telic and agentive roles, on the other hand, have received relatively little attention in terms of automatic acquisition and are not available from any large-scale lexical resources. The only work we are aware of which directly targets the task of learning telic and agentive qualia data is that of Bouillon et al. (2002), who use symbolic learning to identify "qualia pairs"-token instances of noun-verb pairs which correspond to a some qualia role-in corpus data. Our work differs in that we can identify the qualia roles of an arbitrary noun, as suitable for the development of a lexical resource, and sub-classify noun-verb pairs according to the specific qualia role they constitute.An example application of telic and agentive roles is the interpretation of logical metonymy (Lapata and Lascarides, 2003), such as in Mary finished her beer. Under the standard interpretation of logical metonymy, finish here predicates over an unexpressed verb, which takes her beer as object. By accessing the qualia structure of beer, it is possible to resolve the unexpressed verb by way of telic and agentive roles, resulting, e.g., in the interpretation finished drinking her beer (from the telic role -although in other cases the agentive role may be more appropriate). Busa and Johnston (1996) proposed an interpretation-based method of translating complex nominals from English to Italian, interpreting the relation between the nouns based primarily on telic and agentive role data. Qualia structure is also useful for QA tasks. Choi et al. (2003) proposed a QA system that uses rich lexical semantic knowledge incorporating relations between nouns and verbs of the type manifested in qualia structure.In the qualia structure of a given noun, telic and agentive roles are described by a set of predicates (potentially specified for argument structure). For example, the prototypical telic role for book is normally considered to be read, and the prototypical agentive role is write. However, alternate predicates such as study and publish can also be considered to be telic and agentive roles, respectively. In line with this observation, we treat the telic and agentive roles of a given noun as a (partially) ranked list rather than a closed set of predicates. The purpose of this research is thus to generate a ranked list of verbs for a given noun for each of the telic and agentive roles, with the ranking encoding the relative prototypicality of the verb fulfilling the given role of the target noun. Verbs that rank high in this list can then be considered as the telic and agentive roles of the noun in question.In this paper, we propose two basic methods for extracting the telic and agentive roles of nouns from corpus data. These are in the same vein as Hearst's template-based strategy (Hearst, 1992), whereby we identify highly precise (generally low-recall) syntactic constructions that are indicative of a verb constituting the agentive or telic role of a given noun. An example of such a template is an N' modified by an infinitival relative clause, such as (a) book to read, wherein read represents the purpose of book and is thus a candidate for the telic role. We estimate the occurrence of different verbs with a given noun in these constructional templates by running a dependency parser (RASP Briscoe and Carroll (2002)) over the British National Corpus (BNC: Burnard (2000)). The first method uses hand-generated templates for each role type. The second employs a maximum entropy-based supervised learning technique which dynamically learns constructional and lexical preferences from the dependency data. In evaluation, we took a sample of 30 nouns, independently selected 50 verbs for each, and generated a ranked list of verbs for a given noun. We then evaluated the results using a variant of Spearman's rank correlation.In the remainder of this paper, we first introduce the resources used in this research ( 2). We then present the two methods we propose for extracting qualia structure ( 3). Finally, we provide details on the methodologies used to evaluate these methods ( 4), before concluding the paper ( 5). We present two methods for automatically discovering the telic and agentive roles of nouns from corpus data. These relations form part of the qualia structure assumed in generative lexicon theory, where the telic role represents a typical purpose of the entity and the agentive role represents the origin of the entity. The first discovery method uses hand-generated templates for each role type, and the second employs a supervised machine-learning technique. To evaluate the effectiveness of the two methods, we took a sample of 30 nouns, selected 50 verbs for each, and then generated a ranked list of verbs for a given noun. Using a variant of Spearman&apos;s rank correlation, we demonstrate the ability of the methods to identify qualia structure.
Bilingual Knowledge Extraction Using Chunk Alignment Corpus-based approaches based on bilingual corpora are promising for automatically acquiring translation knowledge (Alahawi et al,2000) ( Brown et al,1993) (Imamura,2002)( Menezes et al,2001) ( Utsuro et al,1993)( Watanabe et al,2000)( Yamada et al,2001). Most of statistical translation models based on IBM models( Brown et al,1993) are built from bilingual corpora without considering the structural aspects of the language (Brown et al,1993). They often output ungrammatical or unnatural translations.( Yamada et al,2001) modeled the translation process from a parse tree of source language into a target language sentence. But the computational complexity during alignment is very high because it must handle hierarchical structures. Some methods use parsed sentences in parallel sentence-aligned corpora to extract transfer rules or examples (Aramaki et al,2001) ( Watanabe et al,2000) ( Menezes et al,2001) (Imamura,2002). However, parse-to-parse matching, which regards parsing and alignment as separate and successive procedures, suffers from grammatical inconsistency between languages. Moreover, it costs a lot to develop parsers of both the source and target language.In this study, we propose a new method for effectively extracting translation knowledge which will reduce the computational complexity of alignment without losing the structural properties of each language. The main difference of our approach lies in exploiting a pair of a dependency parsed sentence and a POS tagged sentence as an input, and aligning those sentences in a chunk level as well as in a word level. By sharing the dependency relations of a source sentence, it is possible to automatically obtain a dependency parsed target sentence that will be structurally consistent with a given source sentence without using a parser of the target language. Ultimately, we can effectively acquire invaluable bilingual knowledge by exploiting the dependency relations among the aligned chunks and words.The proposed method was implemented into the system that consists of two sub-systems, a bilingual alignment system and a knowledge extraction system. For aligning chunks and words, we utilize bilingual dictionaries and take a divide-and-conquer strategy. That is, after aligning chunks, the alignment of word level is consecutively tried in chunk pairs to reduce the computational complexity and improve alignment accuracy.For extracting linguistic knowledge, we first obtain bilingual dependency parses by sharing the dependency relations between the chunk and word aligned sentences. Then we recursively traverse the dependency parses to get various bilingual knowledge from translation correspondences of singletons to surface verb sub-categorization patterns and apply a stepwise filtering method to obtain reliable ones.As a case study, we apply the proposed method to Japanese and Korean. For evaluating the extracted knowledge, we measure the coverage and the average ambiguities of them on the Basic Travel Expression Corpus( Takezawa et al,2002).From now on, we will show the details of bilingual chunk alignment and word alignment in section 2.1., knowledge extraction in section 2.2. and some experiments of extracting bilingual knowledge and evaluating them in section 3. Then, we will discuss some issues in bilingual knowledge extraction, and conclude this study. In this paper, we propose a new method for effectively acquiring bilingual knowledge by exploiting the dependency relations among the aligned chunks and words. We use a monolingual dependency parser to automatically obtain dependency parses of target language using chunk and word alignment. For reducing the computational complexity of structural alignment, we use a bilingual dictionary and adopt a divide-and-conquer strategy. By sharing the dependency relations of a given source sentence, we automatically obtain a dependency parse of a target sentence that is structurally consistent with the source sentence. Moreover, we extract bilingual knowledge bases from translation correspondences of singletons to surface verb sub-categorization patterns by exploiting the bilingual dependency relations. To acquire reliable ones, we take a stepwise filtering method based on statistical test.
Constructing English Reading Courseware There is a huge range of English reading materials for EFL (English as a foreign language) learners on the Internet. For example, current news stories can be read on web sites such as those of CNN, 1 TIME, 2 or the BBC. 3 Specialized reading materials for EFL learners are also provided at web sites like EFL Reading. 4 Given the vast amount of available reading materials, EFL teachers have to carefully select the most appropriate text to use in reading courseware for teaching English reading efficiently. Efficient teaching includes promoting the acquisition of new words, phrases, and/or syntax through reading.Our goal is to construct reading courseware automatically from a set of reading materials (a corpus) with the aim of achieving a specific course objective. The objective examined in this paper is the acquisition of vocabulary.We define efficiency in terms of the amount of reading materials that must be read to learn a required vocabulary. That is, efficient courseware is as short as possible, while containing the required vocabulary.Automatic construction of courseware will be especially useful in preparing materials for ESP (English for special purposes) courses. ESP teachers would benefit greatly if they could construct efficient courseware automatically from a corpus and a specialized vocabulary of a particular discipline such as medicine, engineering, or economics. There is a wide range of English reading materials for EFL (English as a foreign language) learners. However, it is difficult for teachers to select appropriate materials to construct course-ware that can be used for an English course. We propose a method for constructing courseware from a target vocabulary and a corpus. We used the specialized vocabulary for the Test of English for International Communication (TOEIC) and articles from The Daily Yomiuri newspaper to construct effective courseware. The constructed courseware consisted of articles in which the target vocabulary frequently occurred. Evaluation of the constructed courseware is ongoing. However, students have accepted it as an effective tool for learning the TOEIC vocabulary from real texts.
Tiny Corpus Applications with Transformation-Based Error-Driven Learning: Evaluations of Automatic Grammar Induction and Partial Parsing of SaiSiyat SaiSiyat is a Formosan Austronesian language with less than 4,677 speakers (1995 census data). It is an SOV language with four verbal voices, six case markers, but without declensions ( Yeh (2000)). As other Austronesian languages in Taiwan, SaiSiyat writing system is just officially standardised. 1 Few written materials are published in this language and the main source of its corpora is linguistic fieldwork in form of transcription of oral narration and conversation. The tiny scale of corpora makes it hard to do probabilistic natural language processing. Other affordable methods to build a syntactically tagged treebank are thus subjects to our work.SaiSiyat parallels to ancient Egyptian in terms of the description of Rosmorduc (published on Internet). Part of its grammar is still unsure. Grammatical errors are found in texts. The absence of punctuation makes the corpus impossible to be proceeded at sentence level. In order to partially parse this language, the applications of Kullback-Leibler divergence and transformation-based error-driven learning (TBL) are evaluated in the paper.NTU SaiSiyat corpus (?))contains 27 texts, 3702 intonation units (IUs), 12065 words. Its notation follows the convention of Du Bois (1993). Sixteen narrations are composed in the corpus, including 4 Pear Stories (a colour mute film), 8 Frog Stories (a sketchbook by Mayer (1980)) and 4 indigenous legends. The corpus is tagged with a TBL tagger in reduced Penn Treebank Tagset. The overall accuracy is 88.11%. ( Lin (2004)) Additional collected texts are added in our experiment to enlarge the corpus. An example of original and tagged data segment follows:1. kor-koring min-a'rem korkoring/NN mina'rem/VB Red-discipline MIN-rest "A child was asleep." Esperanto is planned as an international help language in 1887 by L. Zamenhof. 2 Large corpora of authentic journals, translated works and archives of Yahoo!Groups are available online for free. Its declension and conjugation are regular, permitting us to tag the texts easily, quickly and correctly. We choose "Monato" archive, a periodic written in the language, as a sentence-based contrast . See table 1 for corpora statistics. This paper reports a preliminary result on automatic grammar induction based on the framework of Brill and Markus (1992) and binary-branching syntactic parsing of Esperanto and SaiSiyat (a Formosan language). Automatic grammar induction requires large corpus and is found implausible to process endangered minor languages. Syntactic parsing, on the contrary, needs merely tiny corpus and works along with corpora segmented by intonation-unit which results in high accuracy.
Scalar Meanings of the Concessive (-to), the Contrastive Topic Marker (-nun) and -man &apos;only&apos; in Korean (and Japanese) The concessive -to (or mo)-marked (attached to an Indefinite) polarity phenomenon and the high tone -nun (or wa)-marked Contrastive Topic (CT) phenomenon show a close relatedness, with respect to underlying concessivity and thereby derived scalarity. The scalar but not concessive features of the exhaustivity focus marker -man (or dake) in inherently scalar numeral, quantifier and predicate contexts will also be described. Some relatedness between polarity/negativity and implicature (suspension) will be investigated. Negative CT Ss invoke affirmative weaker implicatures connected by a PA 'but' and metalinguistic negation constitutes a Contrastive Focus construction. I argue that both polarity, based on -to 'even,' and CT, -nun with a high tone, involve conventional scalar implicatures motivated by concessivity. This paper investigates the systematic relatedness between the concessive-to (or mo)-marked polarity phenomenon and the high tone-nun (or wa)-marked Contrastive Topic phenomenon with respect to underlying concessivity and thereby derived scalarity. I also investigate the scalar features of the exhaustivity focus marker-man (or dake) in inherently scalar numeral, quantifier and predicate contexts. I further try to show a relation between polarity/negativity and implicature (suspension). 1 Introduction The concessive-to (or mo)-marked (attached to an Indefinite) polarity phenomenon and the high tone-nun (or wa)-marked Contrastive Topic (CT) phenomenon show a close relatedness, with respect to underlying concessivity and thereby derived scalarity. The scalar but not concessive features of the exhaustivity focus marker-man (or dake) in inherently scalar numeral, quantifier and predicate contexts will also be described. Some relatedness between polarity/negativity and implicature (suspension) will be investigated. Negative CT Ss invoke affirmative weaker implicatures connected by a PA &apos;but&apos; and metalinguistic negation constitutes a Contrastive Focus construction. I argue that both polarity, based onto &apos;even,&apos; and CT,-nun with a high tone, involve conventional scalar implicatures motivated by concessivity.
On Argument-Adjunct Asymmetry of Sluicing in Mandarin Chinese  Sluicing is an elliptical construction which involves a remnant wh-phrase followed by an empty constituent. Such an elliptical constituent, in the standard assumption, is an IP, and the remnant wh-phrase can escape from being elided in that it has either undergone wh-movement or has been  I am indebted to both Prof. T.-H. Jonah Lin and Prof. Shu-Min Chang for their valuable comments and criticisms. All honors belong to them and all errors are my own responsibility.base-generated in [Spec, CP]. To account for such an escape of wh-phrase, two derivational theories have been generally adopted, those being PF Deletion Theory (Ross, 1969) and LF Copying Theory (CLM, 1995) respectively 1 . However, Sluicing in Mandarin Chinese is even more complicated since an argument-adjunct asymmetry in this wh-in-situ language is frequently demonstrated by shi-support, and little attention has been given to the point in the current related literature 2 . In this study, I very boldly but bravely, attempt to explore a little further into such an asymmetry in Mandarin Chinese Sluicing. My a n a l y s i s i s c r u c i a l l y b a s e d o n C L M' s ( 1 9 9 5 ) i n s i g h t f ul analysis of LF Copying Theory.Departing from their analysis in crucial respects, however, I argue that a covert wh-movement also takes place simultaneously with the operation of copying the antecedent IP. In addition, evidence from shi-support will help argue that Sluicing in Mandarin Chinese prefers LF copying rather than PF deletion. As for this, there will be a detailed discussion in the following subsections, and the concept of " barriers" (Chomsky, 1986) will be applied in this study as well so as to account for shi-support in Mandarin Chinese Sluicing, which has been touched from time to time but still remains unexplored and unexamined. This study attempts to account for the argument-adjunct asymmetry of Sluicing in Mandarin Chinese. Such an asymmetry is empirically demonstrated by a language-particular phenomenon, so-called shi-support, which is also the last resort (Chomsky, 1995a) of our linguistic mechanism. In the current related literature, shi-support is obligatory for wh-arguments but optional for wh-adjuncts (Wang, 2002). However, I argue that at the PF level shi-support is even optional for wh-arguments; t h a t i s , i t i s o n l y n e e d e d i n t h e d e r i v a t i o n a t t h e (1995) insightful analysis of LF Copying Theory. Departing from their analysis in crucial respects, however, I argue that a covert wh-movement also takes place simultaneously with the operation of copying the antecedent IP. For reasons of economy, such a non-overt movement is preferred and is of the least efforts (Procrastinate). In addition, evidence from shi-support argues that Sluicing in Mandarin Chinese prefers LF copying rather than PF deletion. To sum up, shi-support is compulsory for wh-arguments in that the ECP (Empty Category Principle) requirement must be satisfied at the LF level owning to the int e r v e n i n g &quot; b a r r i e r s &quot; (C h o ms k y , 1 9 8 6) .
A Contrastive Study of Function Verbs in English and Japanese -Cut and Kiru Cut and kiru, as main verbs, both express division sense such as in cut a cake and keeki-o kiru 'to cut a cake'.It is well known among Japanese linguists that a verb kiru 'to cut' in Japanese is also used as a function verb 1 , expressing performance of an event, like (1) (Himeno 1980 and others). It is interesting to note that the English counterpart to kiru, i.e. cut can participate in so-called Light Verb Construction when it co-occurs with event nominals, similarly to other well-established light verbs like take and have. For instance, parallel to (2a), we can say (2b) as well as (2c).(1) Taroo-ga biiru-o nomi-ki-tta. Taroo-NOM beer-ACC drink-cut-PAST 'Taroo drank the beer to the last drop.' (2) a. John looked at the clock quickly.b. John cut a quick look at the clock. c. John {had/took} a quick look at the clock.[Light Verb Construction]From observation of (3), however, it seems that these two verbs differ in selection restrictions on their complements. While kiru co-occurs with a constituent that denotes an activity of drinking, cut does not. Conversely, while cut takes a look as its complement, kiru does not combine with mi-'to look'.(3) a. Taroo-ga biiru-o nomi-ki-tta. (=1) 'Taroo drank the beer to the last drop.' b. *John cut a drink of the beer. c. *Taroo-ga subayaku tokei-o mi-ki-tta.Taroo-NOM quickly clock-ACC look-at-cut-PAST 'Taroo looked at the clock quickly and thoroughly.' d. John cut a quick look at the clock. (=2b) Based on the Generative Lexicon Theory, this paper analyzes the event structures of these two forms 'cut+an event nominal' and V-kiru, and accounts for the difference in selection restriction between the two. This paper gives a contrastive analysis of function verbs in English and Japanese, cut and kiru in the framework of the Generative Lexicon (Pustejovsky 1995). Despite their division sense, these two express performance of an event when they take constituents which denote eventuality. They differ, however, in the resulting event structures. On the one hand, &apos;cut+an event nominal&apos; can be dealt with as involving a generative operation called co-composition, and describes a temporally bounded event. On the other hand, V-kiru denotes an accomplishment event by means of another operation, type coercion.This analysis also accounts for the difference in selection restriction between the two forms.
Extraction of Cognition Results of Travel Routes with a Thesaurus  We are attempting to model travel route choice behaviour with language to describe the thinking process of travelers because words can directly and clearly reflect their psychological states from a bottom-up viewpoint. This paper shows a method that extracts impressions and feelings, i.e., cognition results of travel routes, out of open-ended questionnaire texts with a thesaurus. Complex words are also allowed as cognition results. Additional considerations and training contents are also reported. Finally, an experiment on the extraction of cognition results from unseen texts is reported. 1 Introduction 1.1 Background We are attempting to linguistically model spatial cognition and travel route choice behaviour. In the field of travel engineering, a unit of travel called a &quot;trip&quot; is expressed as movement from an origin to a destination; route choice behaviour is expressed as choice from the set of alternatives in each trip. Existing city and traffic infrastructure planning are mainly framed according to economical effects and demand estimations. Many studies have approached route choice behaviour from such top-down standpoints. These studies are more interested in the results of travel behaviour rather than the psychological state. Many have used numerical equations to explain behaviour quantitatively, and psychological factors are often expressed with internal variables. On the other hand, when developing new products, a company must analyze customers&apos; awareness from a bottom-up viewpoint. In the same way, it is also important for traffic infrastructure planning to analyze from a bottom-up viewpoint, observing travelers&apos; psychological states when making choices. We expect to handle psychological factors more clearly with words. Psychological states of route choice behaviour can be explained by words with analyzing open-ended questionnaire texts.
Adaptive Word Sense Tagging on Chinese Corpus  This study describes a general framework for adaptive word sense disambiguation. The proposed framework begins with knowledge acquisition from the relatively easy context of a corpus. The proposed framework heavily relies on the adaptive step that enriches the initial knowledge base with knowledge gleaned from the partially disambiguated text. Once adjusted to fit the text at hand, the knowledge base is applied to the text again to finalize the disambiguation decision. The effectiveness of this approach was examined through sentences from the Sinica corpus. Experimental results indicated that adaptation significantly improved the performance of WSD. Moreover, the adaptive approach, achieved an applicability improvement from 33.0% up to 74.9% with a comparable precision. 1 Introduction Word sense disambiguation is a long-standing problem in natural language understanding. Statistically acquiring sufficient knowledge about a language to build a robust WSD system is extremely difficult. For such a system to be efficient, a large mass of balanced materials must be gathered to cover many idiosyncratic facets of the language. Three issues must be addressed in a lexicalized statistical word sense disambiguation (WSD) model: data sparseness, lack of abstraction, and static learning. First, a word-based model has a multiplicity of parameters that are difficult to measure consistently, even with an extremely large corpus. Under-trained models lead to low precision. Second, word-based models lack a crucial degree of abstraction for a broad coverage system. Third, a static WSD model is probably neither robust nor portable, since it is difficult to construct a model relevant to a broad range of unrestricted texts. Several WSD systems have been created that apply word-based models to a specific domain to disambiguate senses appearing in generally easy contexts with a large number of typically salient words. In an unrestricted text, however, the context is usually diverse and difficult to capture with a lexicalized model; therefore, a corpus-trained system is unlikely to transfer suitably to a new domain. Generality and adaptability are, therefore, essential to a robust and portable WSD system. An adaptive system, armed with an initial knowledge base extracted from defined words, is superior in two ways to static word-based models trained on a corpus. First, the initial knowledge is sufficiently rich and unbiased for a large portion of text to be disambiguated correctly. Second, based on the initial disambiguation, an adaptation step can then be implemented render the knowledge base more relevant to the task, thus resulting in broader and more precise WSD. This study explores in detail whether word-based knowledge provides a general solution for disambiguating contexts of unrestricted texts. This method assumes that a major part of a given text is easy or prototypical and, therefore, understandable using general knowledge. Adapting contextual representation of word senses to those in the easy context, will hopefully allow us to PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo-267
Developing an Automated Test of Spoken Japanese According to the Japan's Agency for Cultural Affairs, in 2002, the number of learners studying Japanese as a second language in Japan was 126,350. This is twice as many students as 10 years ago. Similarly, the Japan Foundation (2003a) reported that a little over 2 million people learned Japanese outside Japan in 2003, which is 18.5 times more than in 1979. The Japan Foundation also administers the Japanese Language Proficiency Test. Approximately 227,000 people took the test in 2001, which is about 4 times more than the number of test-takers in 1996 (The Japan Foundation, 2003b).Currently, one of the major focuses of language instruction is to enhance learners' ability to communicate, that is, to enhance their oral communication skills. Therefore language assessment should emphasize the competent use of language in spoken communication. Oral Proficiency Interviews (OPIs) are often viewed as assessments well-aligned with this goal. However, administering OPIs is time-consuming and often expensive because each interview may take 20-40 minutes and must be administered and scored by human raters. With the rapid increase of students learning Japanese, there is a growing need for a quick but reliable and accurate assessment instrument in the field of teaching Japanese. However, at present, no such test exists.Ordinate Corporation, a language testing company in California, develops fully automated tests that measure the speaking and listening skills of non-native speakers. The Ordinate testing system is currently delivering tests that measure the spoken language skills of non-native speakers of English and non-native speakers of Spanish. A series of studies has proven that the both tests are highly reliable (The reliability of SET-10 (the Spoken English Test), is 0.97 and the reliability of SST (the Spoken Spanish test) is 0.96). Building on Ordinate's existing testing system, Ordinate Corporation in the U.S. and the Institute for DECODE at Waseda University in Japan are collaborating to develop a fully automated test of spoken Japanese, the Spoken Japanese Test (SJT).In this paper, we first describe Ordinate's testing system, in general, including the test development processes including test construct, then we describe the structure of the SJT test, and item development, data collections, and validation. Note that we refer to Ordinate's existing tests such as SET-10 and SST (existing English and Spanish tests) to provide more concrete descriptions, as necessary. , and the test-taker responds by speaking. For example, SET-10 takes approximately 10 minutes to complete. SJT will also be 10-15 minutes to complete.A score report becomes available on Ordinate's website usually within a few minutes after a test has been completed. For example, both the SET-10 and the SST score report consist of one Overall score and four subscores: Sentence Mastery, Vocabulary, Fluency, and Pronunciation. In other words, the SET-10 measures two aspects of the spoken skills: what the test-taker said and how the test-taker said it. Sentence Mastery and Vocabulary are the what aspects of the scores and Fluency and Pronunciation are the how aspects of the scores. The scores are reported in the range of 20-80 and each aspect counts for 50% of the Overall score. A SJT score report will be similar to the SET-10 and the SST score report.These Ordinate's general test administration procedures are schematized in Figure 1. These procedures will be applied to SJT as well. Figure 1. In order to assess spoken skills of learners of Japanese effectively and more efficiently the Institute for DECODE (Institute for Digital Enhancement of Cognitive Development) at Waseda University is collaborating with Ordinate Corporation to develop and validate an automated test of spoken Japanese, SJT (Spoken Japanese Test). The SJT is intended to measure a test-taker&apos;s facility in spoken Japanese, that is listening and speaking skills in daily conversation, in a quick, accurate and reliable manner. In this paper, we discuss the purposes for developing the SJT, the mechanism of a fully automated test, and the test development processes, including item development and implementation.
Three English Learner Assistance Systems Using Automatic Paraphrasing Techniques Studies on paraphrasing (IWPT, 2001;Murata and Isahara, 2001) are relevant to a range of research topics including sentence generation, summarization, and question-answering in natural language processing (Katoh and Uratani, 1999;Takahashi et al., 2003). Several techniques have been constructed for paraphrasing. We used automatic paraphrasing techniques based on natural language processing to develop three learner-assistance systems for English language learners and beginners. One system extracts an individual's personal error patterns in using English. The second transforms English sentences containing the letters "l" and "r" into sentences with fewer instances of these letters, which Japanese people have trouble pronouncing properly in English. The third system transforms difficult English words into easier ones by providing definitions of the difficult words that clarify their meaning. These systems and studies using them may also have applications in studies on second language acquisition ( Dulay et al., 1982;Larsen-Freeman and Long, 1991;Granger, 1998). We developed three systems based on automatic paraphrasing techniques to help En-glish learners and English-language beginners. One system extracts personal error patterns in the user&apos;s English usage. The second transforms English sentences containing the letters &quot;l&quot; and &quot;r&quot; into sentences containing fewer instances of these letters, which Japanese people have trouble pronouncing properly in English. This system could be used, for example, to transform a draft of a presentation that a Japanese speaker was to present to an audience. The third is an annotation system that provides definition sentences of difficult English words, making them easier to understand. We believe that these systems will be useful both for learners of English and in studies on second-language acquisition.
Committees PACLIC Steering Committee Co-Chairs Acting Chair  
LPath + : A First-Order Complete Language for Linguistic Tree Query In recent years, a great variety of linguistic query languages have been proposed, most of them specialised for linguistic trees (Lai and Bird, 2004), and applied to corpora such as the Penn Treebank ( Marcus et al., 1993). Despite this considerable effort, relatively little is known about the formal expressiveness of these languages, or the computational resources required to process them as the size of the data grows. One reason for this is that much of the work in this area has taken place in isolation from well-understood database query languages such as SQL and XPath (Clark and DeRose, 1999).Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees ( Bird et al., 2006). It augments the navigational axes of XPath with three additional tree operators, and it can be translation into SQL for efficient execution. In this paper we investigate the expressiveness of LPath with respect to Core XPath and to a first-order complete language called Conditional XPath. We also extend LPath to permit simple closures, and argue that this new language supports all the navigational and closure requirements of linguistic tree query.This paper is organised as follows. Section 2 reviews LPath, XPath, and Conditional XPath, and Section 3 examines the LPath operators to see which of them can be expressed in XPath or Conditional XPath. Section 4 presents an extended language, Conditional LPath, or LPath + , and discusses its merits as a linguistic tree query language. Annotated linguistic databases are widely used in linguistic research and in language technology development. These annotations are typically hierarchical, and represent the nested structure of syntactic and prosodic constituents. Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees. We establish the formal expressiveness of LPath relative to the XPath family of languages. We also extend LPath to permit simple closures , resulting in a first-order complete language which we believe is sufficiently expressive for the majority of linguistic tree query needs.
Form-Meaning Interface in Constraint-based Unified Grammar: Prosody and Pragmatics Two aspects of prosody-pragmatics interface in Korean, intonation vs. speech act and stress vs. topic-focus articulation (TFA) are presented in descriptive terms: first, four types of terminal contours (fall, mid, rise, and high rise) are set up and mapped to the speech-act types of asserting, asking, requesting, etc. on the basis of the prosodic features of echo utterances and tag questions. Second, four types of stress (0, 1, 2, 3) are also set up and mapped to four types of TFA: zero topic, topic, (narrow) focus and contrastive topic/focus. The interface between prosody and pragmatics is then presented in formal terms in the framework of Constraintbased Unified Grammar, analyzing discourse-oriented examples of nonstandard questions and a dialogue exchange to explicate building up of lexical to phrasal constructions along with grammar-internal principles and conventions such as principle of ordering, stress lineup, and TFA composition. The present work aims at a formal description of the interface between prosody and pragmatics in the framework of Constraint-based Unified Grammar CUG. It focuses on two main aspects of this interface: (1) intonation patterns vs. speech acts and (2) stress vs. topic-focus articulation. First, four terminal contours are proposed: fall, mid, rise, and high rise. They are then mapped into the speech-act types of asserting, asking, requesting, etc., centering on the grammatical constructions of nonstandard questions. Second, four levels of stress are also proposed: 0, 1, 2, 3. They are linked to the four types of topic-focus articulation: zero topic, topic, narrow focus and contrastive topic/focus. With these prosodic markings, various discourse-oriented data are analyzed to illustrate and support the principle of compositionality for building up of lexical to phrasal constructions as well as various principles and conventions in CUG such as principle of ordering, stress lineup, and TFA composition.
Automatic Acquisition of Knowledge About Multiword Predicates  Human interpretation of natural language relies heavily on cognitive processes involving meta-phorical and idiomatic meanings. One area of computational linguistics in which such processes play an important, but largely unaddressed, role is the determination of the properties of mul-tiword predicates (MWPs). MWPs such as give a groan and cut taxes involve metaphorical meaning extensions of highly frequent, and highly polysemous, verbs. Tools for automatically identifying such MWPs, and extracting their lexical and syntactic properties, are crucial to the adequate treatment of text in a computational system, due to the productive nature of MWPs across many languages. This paper gives an overview of our work addressing these issues. We begin by relating linguistic properties of metaphorical uses of verbs to their distributional properties. We devise automatic methods for assessing whether a verb phrase is literal, metaphorical, or idiomatic. Since metaphorical MWPs are generally semi-productive, we also develop computational measures of their individual acceptability and of their productivity over semantically related combinations. Our results demonstrate that combining statistical approaches with linguistic information is beneficial, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in a computational lexicon.
People in the State of the Union: Viewing Social Change through the Eyes of Presidents In the latter half of the twentieth century, American presidents have had the enviable task of shaping the way Americans think about themselves by delivering a State of the Union address near the beginning of each calendar year. This speech is broadcast live across the nation on major television and radio channels. In the address, the president emphasizes his accomplishments to date and sets out a new agenda for the year. Topics touched upon may include both foreign and domestic policy, and run the gamut from justification for war to a fervent plea to pass an education bill. The complete text of the address appears the following day in major newspapers and in on-line news resources.Thus, these addresses constitute a narrow, but influential media genre, since subsequent discourse in the news media often reports on the proposals put forth by the president in his own terminology (Barrett 2000). This terminology reflects the ideology of the ruling political party, and it is this ideology that is used to exercise power "through the manufacture of consent" (Fairclough, 2001). Moreover, as Van Dijk (1993) notes, "More control over more properties of text and context, involving more people, is thus generally (though not always) associated with more influence, and hence with hegemony" (p. 257). Thus, a linguistic analysis of presidential speeches has the potential to shed light on how the president views (and wants the country to view) economic, political and social issues of the day.However, it is only recently that the advent of on-line corpora has facilitated the collection and analysis of presidential speeches. Kowal et al. (1997), for example, had to create a corpus of Inaugural Addresses in order to look at the interaction between literacy and orality in presidential speeches. Charteris-Black (2004, 2005), on the other hand, was able to examine the use of metaphor as well as rhetorical devices used by British and U.S. politicians in their speeches based on the corpus of U.S. Presidential Inaugural Addresses found on www.bartleby.com. However, to date there has been no systematic analysis of changes in lexical use within the scope of presidential speeches. Thus, it is the goal of this paper to demonstrate that by combining presidential speeches into a corpus, subtle changes in language use over time can be determined by examining the frequency of occurrence of key words as well as their associated collocations (Stubbs, 1996). In order to examine this issue, I will explore language use pertaining to 'people' in all of the State of the Union addresses (SOU corpus) from 1945 to 2005 by analyzing the tokens: humankind, mankind, man, men, woman, women, mother, father and parent. I will demonstrate that while there was clearly a shift twenty years ago to systematically portraying human beings as being made up of two genders, or being subsumed under a gender-neutral term such as person or people, other aspects of gender, such as parenthood, are still stereotyped by American presidents. On-line databases of presidential speeches now allow for a diachronic exploration of language use at the highest political levels. This allows for a contrast between legislative and legal advances for minorities and the integration of those advances into the presidential lexicon. In this paper, I explore language use pertaining to &apos;people&apos; in the American State of the Union addresses from 1945 to 2005. I demonstrate that while there was clearly a shift two decades ago to systematically portraying human beings as being made up of two genders, or being subsumed under a gender-neutral term, other aspects of gender, such as parenthood, are still stereotyped by American presidents. In short, analyzing lexical instances related to &apos;people&apos; in the State of the Union address allows us not only to reflect on the values held by U.S. presidents, but also to systematically uncover how they use language to exercise power on the very people they are elected to serve.
A Framework for Data Management for the Online Volunteer Translators&apos; Aid System QRLex For many years, specialists and researchers have become pessimistic in regards to the prospects for fully automatic translation capable of producing high quality translations equal to human translator. It is well known that the ALPAC report in 1966 evaluated the performances of Machine Translation (MT) systems negatively. Martin Kay stated:"...this happens when the attempt is made to mechanize the non-mechanical or something whose mechanistic substructure science has not yet been revealed..." [6] This situation has promoted a shift in emphasis of research from fully automated machine translation to computer aided human translation, which exploits the potential of computers to support human skills and intelligence [5]. Many industries have made a large investment in developing useful translation-aid tools, which has resulted in commercial Computer-Aided Translation (CAT) systems such as Translation Memory (TM) in various forms, dictionaries and terminology database techniques. However, these systems are not designed to be used by all translators. On the one hand, the commercial feature is a barrier for many translators. On the other hand, these tools do not provide content and functions that fully satisfy some translators. Online volunteer translators, to whom we specifically address our system, are among those excluded from commercial CAT systems. There is thus a real need to aid online volunteer translators and their communities by providing them with a free environment with a rich linguistic content and improved process and data management.In section 2, we first outline the status and conditions of online volunteer translators and how they work in translation. Section 3 outlines the framework and technical modules we have defined on the basis of analyzing online translators' requirements and, in the process clarified the basic requirements for the data management module. In section 4, we present the XML structures that we defined for our data management module. A new framework for a system that aids online volunteer translators is proposed. As regards this proposal, first, the current status and conditions of online volunteer translators and their translation environments are examined, and general requirements for a system that would aid these translators are given. Our proposed approach for dealing with heterogeneous data, which involves providing a new XML structure that we have developed for maximizing efficiency and functionalities, is then described.
A Two-level Morphology of Malagasy  We present a two-level model of Malagasy nominal and verbal morphology (Beesley and Karttunen, 2003), based primarily on the discussion of Malagasy morphology in Keenan and Polinsky (1998) and Randriamasi-manana (1986). Words in Malagasy are built from roots by means of a variety of morphological operations such as affixation and reduplication. The present paper analyzes productive patterns of nominal and verbal morphology , describing genitive compounding and suffixation for nouns, and various derivational processes involving compounding and affixation for verbs. 1 Overview of Malagasy Morphology Malagasy is an Austronesian language spoken by about six million people on the island of Madagascar. With Welsh, it is a focus of the Verb-Initial Grammars sub-project (users.ox.ac.uk/˜cpgl0015/pargram/) within the PARGRAM initiative, a collaborative project to develop computational lexicons and grammars within the shared linguistic framework of Lexical Functional Grammar (Butt et al., 2002). Because of the complicated and productive patterns of Malagasy verbal and nominal morphology, the development of such a grammar relies heavily on a computational component for morphological analysis. As with any finite-state morphological transducer, our Malagasy morphological analyzer is bidirectional: it can be used in grammatical analysis to produce morphologically analyzed input to a parser, or in generation to produce a surface form from a specification of lexical properties (Beesley and Karttunen, 2003).
A Small Fan and a Small Handful of Fans Exploring the Acquisition of Count-mass Distinction in Mandarin  The count-mass distinction often served as a test case for asking how syntax and semantics are related, whether knowledge of one helps the acquisition of the other. Virtually no studies examined this distinction in classifier languages which supposedly lack the distinction. However, Cheng and Sybesma (1998) argued Mandarin as a classifier language encodes this distinction at the classifier level: classifiers can be categorized as &quot;mass-classifiers&quot; or &quot;count-classifiers,&quot; which categories denote different semantic meanings and occur in different syntactical constructions. The current study undertook Cheng and Sybesma&apos;s framework to compare Mandarin adults&apos; and children&apos;s interpretation of classifiers. Experiment 1 and 2 asked whether count-classifiers select individuals and mass classifiers non-individuals and sets of individuals. Adult data indicated that there is indeed such distinction in Mandarin, but 4-to 6-year-olds had not fully mastered the distinction. Experiment 3 tested participant&apos;s syntactic sensitivity. Participants saw two contrasting pictures and had to match the &quot;one-ADJ-CL-de-N&quot; mass phrase to one and the &quot;one-CL-ADJ-N&quot; neutral phrase to the other. Adults were near perfect whereas six-year-old children were evenly split between those who were at-chance and those who knew the syntax. Our experimental data with children suggests that the mastery of the distinction appears quite late (6-or 7-years of age) relative to English-speaking children.
Integration of Dependency Analysis with Semantic Analysis Referring to the Context When we have a talk, we presuppose the situation and particular context. 1 The situation/context enables us to understand utterances without talking in great detail. The situation/context also makes it possible to convey a wide variety of contents with a certain linguistic expression. This means that a language understanding component in a dialog system should not interpret a given sentence alone; rather, it should interpret a sentence using the context information.In our previous work, we have developed a Japanese dialog system for hotel search and reservation ( Noguchi et al. 2002). The system accepts free input from the keyboard. We are planning to build an audio input module into the system. Audio input raises the importance of the context information because the effective use of the context information may circumvent possible recognition errors. In this paper, we focus on how a language understanding component can perform syntactic parsing and semantic analysis using the contextual information. It may be possible to integrate syntactic parsing with semantic analysis by taking advantage of LFG ( Kaplan et al. 1982) and HPSG (Pollard et al. 1994). However, they do not consider the use of the context information. In this paper, we propose the method to integrate dependency analysis with semantic analysis referring to the context.Recently, many researches have adopted a statistical approach toward parsing, and it has been shown that the statistical approach is effective in obtaining correct dependency structure. There are some free tools available (e.g. Charniak 2000;Kurohashi 1998;Kudo et al. 2002). In the study of semantic analysis, word sense disambiguation (Charniak 2000) and analysis of case (Harada et al. 2000) have been pursued. The syntactic parsing and semantic analysis, however, are often studied independently of each other though correct parsing of a sentence often requires the semantic information on the input and/or the contextual information prior to the input. Accordingly, we propose a sequential parser that determines syntactic structure based on semantic analysis referring to the context and on relevant statistical information. This paper describes how to perform syntactic parsing and semantic analysis using the contextual information as a language understanding component in a dialog system. Although syntactic parsing and semantic analysis are often conducted independently of each other, correct parsing of a sentence often requires the semantic information on the input and/or the contextual information prior to the input. We therefore merge syntactic parsing with semantic analysis, which enables syntactic parsing to take advantage of the semantic content of an input and its contextual information. To use contextual information, the semantic representation of an input should have a comparable form to the semantic content of the preceding context. Accordingly, we employ a framework for semantic representations that achieves such comparison. We take dialogs of hotel search and reservation for example, and demonstrate the effectiveness of the proposed method. The experimental results confirm that the proposed system achieves high accuracy in parsing and generation of semantic representations.
Constructing Filler-Gap Dependencies in Chinese Possessor Relative Clauses As human languages are replete with dependency relations within and across sentences, one crucial task of the language parser concerns the efficient recovery of such relations and their correct interpretation. This process involves various factors, including the required on-line processing load (often discussed in terms of working memory), the complexity of the processed materials, and human syntactic knowledge, etc. 1 Most previous research focused on the first two factors. When a sentence consumes more processing resources, it is assumed to be more difficult and thus takes longer to understand. Similarly, when a sentence is more complicated or less usual, it requires longer processing time. However, the third factor, i.e. the structural properties of different syntactic positions in strategic on-line processing, has been less studied. Studies of focus positions and their salience for processing are both examples of this vein of inquiry (see for example Birch et al., 2000, andFrazier et al., 2005). In this paper, we present data that supports structural knowledge as a prominent factor in sentence processing.By structural or syntactic knowledge, we mean knowledge about the function of specific syntactic positions. This knowledge allows the human parser to recover structure in efficient fashions. For example, in a probe-goal model such as Chomsky (2001), this might mean specific and direct access to certain syntactic positions (e.g. a probe to spec-TP), followed by more general top-down search for matching goals. As the experimental data with possessor relative clauses (PRCs) will suggest, surface subject positions are the most likely candidates for direct gap-probing. In what follows, section 2 reviews findings on processing Chinese relative clauses (RCs), and classical effects such as locality and canonicity. Section 3 introduces Chinese PRCs and issues regarding filler-gap dependencies. Section 4 presents three experiments on sentence comprehension. Section 5 discusses the implications of the experimental results and proposes a structure-based theory of gap-searching. Crosslinguistic evidence from Japanese and Turkish is also discussed. This article explores the construction of filler-gap dependencies in Chinese possessor relative clauses (PRCs), which are different from typical relative clauses (RCs) considered in the literature because Chinese PRCs contain no overt missing arguments (i.e. gaps). As Chinese RCs are prenominal, the gaps precede the head noun fillers. It has been suggested that when the gaps are close to the filler, the dependency is easier to construct; there is, thus, a processing advantage for object RCs over subject RCs (Hsiao &amp; Gibson, 2003). The PRC data presented show that even in Chinese, a language with RCs that are head-final, it is possible to have a subject gap preference (over object) despite the fact that the subject is further away from the filler. Three experiments confirmed this subject preference with respect to naturalness and grammaticality ratings (Experiment 1), paraphrasing tasks (Experiment 2), and self-paced reading tasks (Experiment 3). The results support a theory of gap-searching which operates top down. Issues regarding locality and canonicity will also be discussed.
Analysis of Machine Translation Systems&apos; Errors in Tense, Aspect, and Modality Tense, aspect, and modality are difficult to translate appropriately using machines (Shirai et al., 1990;Kume et al., 1990;Dale et al., 2000;Nirenburg et al., 2002). We investigated the error patterns produced by translation systems when translating Japanese tense, aspect, and modality expressions into English. We compared the performance of six translation systems on the market and our new translation systems for tense, aspect, and modality. We found that our systems outperformed the other systems, and we detected error patterns that the other systems often made and our systems rarely made. These results can be used to improve translation systems on the market. Moreover, we extracted error patterns peculiar to each translation system. These errors can be corrected easily because their corresponding sentences can be translated correctly by other systems. These results are useful for improving each translation system. Errors of the translation of tense, aspect, and modality by machine translation systems were analyzed for six translation systems on the market and our new systems for translating tense, aspect, and modality. The results showed that our systems outperformed the other systems. They also showed that the other systems often produced progressive forms rather than the correct present forms. Our systems rarely made this mistake. Translation systems on the market could thus be improved by incorporating the methods used in our systems. Moreover, error analysis of the translation systems on the market identified information that would be useful for improving them. 1. Introduction Tense, aspect, and modality are difficult to translate appropriately using machines (Shirai et al., 1990; Kume et al., 1990; Dale et al., 2000; Nirenburg et al., 2002). We investigated the error patterns produced by translation systems when translating Japanese tense, aspect, and modality expressions into English. We compared the performance of six translation systems on the market and our new translation systems for tense, aspect, and modality. We found that our systems outperformed the other systems, and we detected error patterns that the other systems often made and our systems rarely made. These results can be used to improve translation systems on the market. Moreover, we extracted error patterns peculiar to each translation system. These errors can be corrected easily because their corresponding sentences can be translated correctly by other systems. These results are useful for improving each translation system.
Multiply Quantified Internally Headed Relative Clause in Japanese: A Skolem Term Based Approach This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese paying particular attention to the effect of quantification on its interpretation. (1) illustrates the basic form of the construction: 1 (1) Taroo-ga [Hanako-ga ringo-o muita] no-o tabeta. Taro-NOM Hanako-NOM apple-ACC peeled NML-ACC ate 'Taro ate the apple that Hanako peeled.'The bracketed string Hanako-ga ringo-o muita 'Hanako peeled an apple' is a clause followed by nominalizer no. The whole NP obtained is marked with accusative particle -o, and is construed as the object of the matrix verb tabeta 'ate'. Since the verb requires as its semantic restriction that the object be an edible thing, it anaphorically picks up the referent of ringo 'apple' from the embedded clause. A brief note on terminology is in order. If we say the antecedent of an IHRC, we mean the referent of the IHRC which functions as the argument of the matrix predicate. And the head of the IHRC refers to the linguistic element in the IHRC which describes the antecedent. So for example, in (1) the antecedent is the apple that Hanako peeled, and the head is the noun ringo 'apple'. This paper is organized as follows. In section 2, we discuss the interpretational characteristics of IHRC, drawing on the study of Kikuta (2000). We then review the observation made by Shimoyama (1999) and her E-type analysis of IHRC. We address the problem that the E-type analysis would raise focusing on multiply quantified IHRCs. Section 3 introduces the notion of generalized Skolem term, which provides a straightforward account for multiply quantified IHRC. Finally, section 4 concludes. [Tuti-o hotta] no-o ue-kara nozokikonda. soil-ACC dug NML-NOM up-from looked:into 'I dug the soil, and lookd into (the hole).' This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese within the framework of Combinatory Categorial Grammar (Steedman 2000). Shimoyama (1999) argues that when an IHRC appears within the scope of a universal quantifier, the interpretation of the IHRC exemplifies E-type anaphora and that the LF representation of the IHRC should have a variable bound by the quantifier in the matrix clause. To accommodate this argument Shimoyama posits a free variable of functional type to which the bound variable is applied, and whose denotation is determined by the context-dependent assignment function. However since there is no limit to the number of quantifiers in the matrix clause (and accordingly that of bound variables in the IHRC), the semantic type of the free variable would be highly ambiguous if the IHRC occurs within the scope of multiple quantifiers. The current analysis assumes that the interpretation of IHRCs exhibits an instance of generalized Skolem term (Steedman 1999, 2004), a term whose denotation varies with the value of bound variables introduced by scope-taking operators, but which is interpreted as a constant in the absence of such operators. This paper provides a straightforward account for the semantics of the construction without invoking the complexities of the type ambiguity of free variables.
Learning Translation Rules from Bilingual English -Filipino Corpus The demand for language translation has greatly increased as globalization takes place. Some businesses have turned to machine translators (MT) that usually provide fast and consistent translation results as compared to human translators. However, the quality of such translation services is usually poor as compared to text translated by humans. Several MT paradigms have attempted to improve the quality of translation such as rule based, example based, and statisticalbased MT. Rule based MT systems usually have impressive results for a given domain. However, creating the translation rules is tedious and time consuming, requiring a linguist who thoroughly knows the construct of both languages. In addition, since language is constantly changing, these rules should be updated regularly in order to maintain the system. Example based MT systems presents a different approach. It uses a bilingual corpus as the basis for translation. Although this approach is more flexible, the quality of the translation greatly depends on the quality of the examples. A new approach in Statistical-based MT is by using probabilities to determine if a phrase or word occurs in the current context. Using a corpus, statistical-based MT allows flexibilty to accomodate different domains. However, it still has its drawbacks. Usually the output is ungrammatical and usually unacceptable to human linguist. It is still important to incorporate abstract syntax rules to improve the quality of translation.The paper presents an approach that combines these paradigms. Rather than building the rules by hand, the system incorporates a training phase where it learns transfer rules by examples found in a bilingual corpus. As such, since the system learns from examples, there is no need for a human linguist to generate the rules for translation. It also allows the MT system to translate in different domains. Aside from this, the corpus can be updated to accommodate changes in language. A substantial amount of research has been done on the area of rule learning and extraction. One example is the ALLiS Algorithm by Herve (2002) that uses rule induction to generate new rules.Patterns generate certain rules and training data is used to test it. Accurate rules are kept whereas the other are deleted. The paper by Probst (2002) presented another learning approach to handle automatic rule learning for low-density languages. The key idea is for the system to read from a training corpus and allow the system to deduce the seed rules. It uses Seeded Version Space Learning to generate seed rules and performs compositionality. Previously learned rules can be used to translate part of the seed rule to remove specificity of the rule and to make it more general.The system presented is based on the Seeded Version Space by Probst (2002). This is applied to the problem of learning rules for translating English to Filipino. Implementation of the algorithm has been modified to allow learning of non-lexically aligned languages and to adapt to the complex free word order of the Filipino language. Most machine translators are implemented using example based, rule based, and statistical approaches. However, each of these paradigms has its drawbacks. Example based and statistical based approaches are domain specific and requires a large database of examples to produce accurate translation results. Although rule based approach is known to produce high quality translations, a linguist is necessary in deriving the set of rules to be used. To address these problems, we present an approach that uses the rule based approach in translating from English to Filipino text. It incorporates learning of rules based on the analysis of a bilingual corpus in an attempt to eliminate the need for a linguist. The learning algorithm is based on seeded version space learning algorithm as presented by Probst (2002). Implementation of the algorithm has been modified to allow learning of non-lexically aligned languages and to adapt to the complex free word order of the Filipino language. 1. Introduction The demand for language translation has greatly increased as globalization takes place. Some businesses have turned to machine translators (MT) that usually provide fast and consistent translation results as compared to human translators. However, the quality of such translation services is usually poor as compared to text translated by humans. Several MT paradigms have attempted to improve the quality of translation such as rule based, example based, and statistical-based MT. Rule based MT systems usually have impressive results for a given domain. However, creating the translation rules is tedious and time consuming, requiring a linguist who thoroughly knows the construct of both languages. In addition, since language is constantly changing, these rules should be updated regularly in order to maintain the system. Example based MT systems presents a different approach. It uses a bilingual corpus as the basis for translation. Although this approach is more flexible, the quality of the translation greatly depends on the quality of the examples. A new approach in Statistical-based MT is by using probabilities to determine if a phrase or word occurs in the current context. Using a corpus, statistical-based MT allows flexibilty to accomodate different domains. However, it still has its drawbacks. Usually the output is ungrammatical and usually unacceptable to human linguist. It is still important to incorporate abstract syntax rules to improve the quality of translation. The paper presents an approach that combines these paradigms. Rather than building the rules by hand, the system incorporates a training phase where it learns transfer rules by examples found in a bilingual corpus. As such, since the system learns from examples, there is no need for a human linguist to generate the rules for translation. It also allows the MT system to translate in different domains. Aside from this, the corpus can be updated to accommodate changes in language. A substantial amount of research has been done on the area of rule learning and extraction. One example is the ALLiS Algorithm by Herve (2002) that uses rule induction to generate new rules.
Discourse Segment and Japanese Referring Expressions: Are These Bare NPs or Proper Names? Forms of Japanese referring expressions are mainly divided into four types: bare NPs, demonstrative NPs (i.e. either as determiner or as pronoun) and zero pronouns. Out of the four types of the referring expressions, bare NPs are the most common type as a subsequent mention and zero pronouns are used only in limited conditions. In English, on the other hand, NPs are the major type of reference used in the discourse and there seems to be a parallelism in distribution between English and Japanese when the topic entity is in the process of being established in discourse. Based on the discourse understanding model, I will argue that the forms of anaphor are not always shorter, and the center of attention is maintained by the chain of NPs rather than (zero) pronouns both within the discourse segment and over the segment boundaries. Forms of Japanese referring expressions are mainly divided into four types: bare NPs, demonstrative NPs (i.e. either as determiner or as pronoun) and zero pronouns. Out of the four types of the referring expressions, bare NPs are the most common type as a subsequent mention and zero pronouns are used only in limited conditions. Based on the centering framework, the results suggest that the center of attention is maintained by the chain of NPs rather than zero pronouns, and the chain of NPs is correlated with the global focus of discourse coherence. There is no doubt that bare NPs mainly carry the topic entity and continue to be used both within the border of discourse segment and over the discourse segment boundaries, while the zero pronoun can carry the topic entity only in the limited context of discourse and is likely to discontinue within the discourse segment. This result is not fully explained by the existing anaphora resolution and it is difficult to predict the typical pattern of referential transitions in naturally occurring discourse. It is worth noting that the repetitive use of bare NPs tends to function as proper names in the Japanese spoken discourse.
Japanese Bare Nouns as Weak Indefinites This paper discusses the behavior of Japanese Bare Nouns (JBNs) 1 . In particular, we focus on their interpretation when they appear as a theme of accomplishment verbs.It is well known that in English, VPs headed by an accomplishment verb can be telic or atelic, depending on the nominal expression the verb takes (Verkyul (1972), Dowty (1979), Krifka (1998), Rothstein (2004) among others).(1) a. Bill ate an apple in one minute/*for one minute.b. Bill ate bread *in one minute/for one minute. c. Bill ate apples *in one minute/for one minute.Adverbs of duration, which modify unbounded events, apply to VPs in which a mass noun or a bare plural is in the object position of accomplishments. Adverbs like 'in one minute' target bounded events and apply to VPs with indefinites. Chierchia (1998) points out that JBNs are like English mass nouns. Both appear in the argument position without any determiner nor quantifier; both denote kinds; and both require classifiers or measure phrases to count. However, they do not always behave like mass nouns. Fromkin (ed.) (2000) shows that JBNs can be used with both in-and for-adverbs. Consider (2) 2 .(2) a. Bill-wa ip-pun-de pan/ringo-o tabe-ta. -Top one-minute-in bread/apple-Acc eat-Past 'Bill ate bread/apple in one minute.' b. Bill-wa ip-pun-kan pan/ringo-o tabe-ta. -Top one-minute-for bread/apple-Acc eat-Past 'Bill ate bread/apple for one minute.'In (2a), Bill consumed in one minute a certain amount of bread/apples recognized as a unit or a group. (2b) implies that apple/bread-eating activity by Bill took place for one minute. Thus, JBNs seem to have dual roles; they sometimes behave like indefinites and sometimes behave like mass nouns 3 . The question to be concerned here is what is semantics of JBNs and how they can appear with two types of adverbs. Japanese Bare Nouns (JBNs) appear to behave like indefinites in some contexts, and mass nouns in some other contexts. For example, when accomplishment verbs have JBNs as their theme, they are modified by both in-adverbs and for-adverbs. This paper first claims that JBNs are weak indefinites and introduce a variable to be bound by Existential Closure. Second, it shows how telic predicates are derived when accomplishment verbs take JBNs as their theme. Third, we observe that verbs of creation do not appear with for-adverbs when coupled with JBNs, and that an event is considered repeated when accomplishments appear with for-adverbs. Two possible explanations for the facts follow; (i) JBNs are not inherently kinds; (ii) sum-formation is restricted in Japanese.
An Approach to Improve the Smoothing Process Based on Non-uniform Redistribution  In the paper, an effective technique, based on the non-uniform redistribution probability for novel events (the unknown events), to improve the smoothing method in language models is proposed. Basically, there are two processes in the smoothing methods: 1) discounting and 2) redistributing. Instead of uniform probability assignment to each unseen events used by most smoothing methods, we propose new technique to improve the redistribution process. Referring to the probabilistic behavior of all seen events, the redistribution process for novel events in our method is non-uniform. The proposed technique is exploited on well-known and frequently-used Good-Turing smoothing method. The empirical results are demonstrated and analyzed for two n-gram models. The improvement is apparent and effective for smoothing methods, especially on higher unseen event rate.
Speech-Activated Text Retrieval System for Cellular Phones with Web Browsing Capability Cellular phones are now widely used and those with Web browsing capability are becoming very popular. Users can easily browse information provided on the World Wide Web such as news, weather, and traffic report with the cellular phone screen in mobile environment. However, obtaining necessary information from large database such as user's manual or travelers' guide is quite a task for users since searching for appropriate information from seas of data requires cumbersome key operations. I n m o s t cases, users have to carefully navigate through deep hierarchical structures of menus or have to type in complex combination of keys to enter some keywords.Text retrieval by voice input is one of the solutions for this problem. This paper presents a telephone-based voice query retrieval system in Japanese which enables cellular phone users to search through the user's manual. This system accepts spoken queries over the cellular phone with large vocabulary continuous speech recognition (LVCSR) and retrieves relevant parts from the user's manual with text retrieval. T h e r e s u l t s a r e p r o v i d e d t o t h e u s e r a s a W e b p a g e b y s y n c h r o n o u s l y activating the Web and the voice systems ( Yoshida et al., 2002). Users can input queries without complicated keystrokes and can view the list of results on the cellular phone screen.With respect to voice input systems, a large number of interactive voice responses (IVR) systems and spoken dialogue systems has been designed and developed over the years ( Zue, 1997). As for user's manual retrieval systems which accept voice input, Kawahara et al. (2003) has developed a spoken dialogue system for appliance manuals. However, they mainly focus on the dialogue strategy to select the appropriate result on screen-less systems such as VTR and FAX. On the other hand, retrieval methods for voice input have been examined on a TREC query set ( Barnett et al., 1997;Crestani, 2000). .However, text retrieval in TREC mainly aims to search open domain documents from long queries, while our system is required to search closed domain documents such as user's manuals based on short queries spoken over the cellular phone.In order to apply text retrieval technique to speech-activated user's manual retrieval, we have investigated queries for searching manuals in addition to the text of the manuals from a linguistic viewpoint. We found that text retrieval for a user's manual has the following three difficulties.1) The difficulty of identifying passages in a user's manual based on an individual word.2) The difficulty of distinguishing affirmative and negative sentences which mean two different features in the manual.3) The difficulty of retrieving appropriate passages for a query using words not appearing in the manual.This paper presents how we overcome these difficulties using three techniques: 1) utilizing word pairs with dependency relations, 2) distinguishing affirmative and negative expressions by auxiliary verbs, and 3) converging synonyms with synonym dictionary.The rest of the paper is organized as follows. Section 2 describes the system configuration of our speech-activated text retrieval system and how it works. Section 3 discusses the difficulties in text retrieval in our system and presents our proposed techniques in detail. Section 4 shows the developed prototype system and Section 5 reports its evaluation results. Finally Section 6 concludes the paper. This paper describes a text retrieval system for cellular phones with Web browsing capability, which accepts spoken queries over the cellular phone and provides the search result on the cellular phone screen. This system recognizes spoken queries by large vocabulary continuous speech recognition (LVCSR), retrieves relevant document by text retrieval, and provides the search result on the World Wide Web by the integration of the Web and the voice systems. The text retrieval in this system improves the performance for spoken short queries by: 1) utilizing word pairs with dependency relations, 2) distinguishing affirmative and negative expressions, and 3) converging synonyms. The LVCSR in this system shows enough performance level for speech over the cellular phone with acoustic and language models derived from a query corpus with target contents. The system constructed for user&apos;s manual for a cellular phone navigates users to relevant passages for 81.4% of spoken queries. 1. Introduction Cellular phones are now widely used and those with Web browsing capability are becoming very popular. Users can easily browse information provided on the World Wide Web such as news, weather, and traffic report with the cellular phone screen in mobile environment. However, obtaining necessary information from large database such as user&apos;s manual or travelers&apos; guide is quite a task for users since searching for appropriate information from seas of data requires cumbersome key operations. I n m o s t cases, users have to carefully navigate through deep hierarchical structures of menus or have to type in complex combination of keys to enter some keywords. Text retrieval by voice input is one of the solutions for this problem. This paper presents a telephone-based voice query retrieval system in Japanese which enables cellular phone users to search through the user&apos;s manual. This system accepts spoken queries over the cellular phone with large vocabulary continuous speech recognition (LVCSR) and retrieves relevant parts from the user&apos;s manual with text retrieval. T h e r e s u l t s a r e p r o v i d e d t o t h e u s e r a s a W e b p a g e b y s y n c h r o n o u s l y activating the Web and the voice systems (Yoshida et al., 2002). Users can input queries without complicated keystrokes and can view the list of results on the cellular phone screen. With respect to voice input systems, a large number of interactive voice responses (IVR) systems and spoken dialogue systems has been designed and developed over the years (Zue, 1997). As for user&apos;s manual retrieval systems which accept voice input, Kawahara et al. (2003) has developed a spoken dialogue system for appliance manuals. However, they mainly focus on the dialogue strategy to select the appropriate result on screen-less systems such as VTR and FAX. On the other hand, retrieval methods for voice input have been examined on a TREC query set (Barnett et al., 1997; Crestani, 2000).
Enhancing Usability of Information Extraction Results with Textual Data Profiling Most IE researches are concerned with producing better performance and facilitating development portability [2][5] [1]. Even with the state-of-the-art IE techniques, IE task results are expected to contain errors. If the IE results are to be used for applications with data validity concerns, IE errors need to be detected and corrected before any value-added processing can be applied. For a typical IE application with tens of thousands, or even hundreds of thousands, extracted entities, manual error detection and correction are labor intensive and time consuming in checking IE results with original documents. This validation cost remains a major obstacle to actual deployment of practical IE applications. In some cases, the validation cost may be so high that IE techniques appear to be hardly superior to direct human work in which both extraction and validation may be performed at the same time. Unfortunately, issues of ensuring highly validated IE results with acceptable cost are little addressed in the IE community. If IE techniques are to be translated into adequately supported actual deployment, we must have some ways to reduce validation cost and ensure data quality of IE results.Data cleansing, also called data cleaning, considers the problem of identifying and removing errors and inconsistencies in data sets [3]. Due to the wide range of possible data errors and inconsistencies, data cleansing involves a variety of research efforts to deal with invalid data, missing data, and duplicated data that occur in single source or when integrating multiple sources. One of the primary focuses is on data integrity analysis or data auditing. The goal is to analyze the actual instances of data to obtain indicative data characteristics or value patterns. The results of data analysis are then used to locate potential errors and inconsistencies based on anomaly and conflict detection. Two related approaches, data profiling and data mining, have been attempted for data analysis [4]. Data profiling focuses on individual attributes and derives information such as data type, length, value range, variance, value frequency, occurrence of null values, typical string pattern, etc. Data mining helps discover specific data relationships between several attributes, such as dependencies or domain-specific business rules. Both approaches provide indispensable tools for data cleansing tasks.In this paper, we present a string feature-based approach to textual data profiling and invalid data detection. A set of generic string features is proposed to provide characteristic profiling for textual data. Each string feature describes a surface property of a string without concerning its literal meaning. Textual data are audited by examining their string features and locating potential errors with atypical feature values. We consider two complementary strategies to establish the classification standards. The first strategy relies on specifying attribute constraints with a-prior domain knowledge. The second strategy employs the concept of statistical majority. The approach was applied to detecting invalid data in IE results from the task domain of government personnel directives. Performance evaluation shows the approach is capable of effectively classifying data validity and anomaly. Finally, we discuss certain observations and issues from the experiments, followed by suggestions for future explorations. Given a targeted subject and a text collection, information extraction techniques provide the capability to populate a database in which each record entry is a subject instance documented in the text collection. However, even with the state-of-the-art IE techniques, IE task results are expected to contain errors. Manual error detection and correction are labor intensive and time consuming. This validation cost remains a major obstacle to actual deployment of practical IE applications with high validity requirement. In this paper, we propose a string feature-based approach to textual data profiling and invalid data detection. The approach is based on the observation that values of an attribute in IE results are symbolic form variations of a concept in the IE task subject and may exhibit a certain congruity with some string features. We conducted experiments to verify that effective detection of IE invalid values can be achieved by using the surface-form string features. 1. Introduction Most IE researches are concerned with producing better performance and facilitating development portability [2][5][1]. Even with the state-of-the-art IE techniques, IE task results are expected to contain errors. If the IE results are to be used for applications with data validity concerns, IE errors need to be detected and corrected before any value-added processing can be applied. For a typical IE application with tens of thousands, or even hundreds of thousands, extracted entities, manual error detection and correction are labor intensive and time consuming in checking IE results with original documents. This validation cost remains a major obstacle to actual deployment of practical IE applications. In some cases, the validation cost may be so high that IE techniques appear to be hardly superior to direct human work in which both extraction and validation may be performed at the same time. Unfortunately, issues of ensuring highly validated IE results with acceptable cost are little addressed in the IE community. If IE techniques are to be translated into adequately supported actual deployment, we must have some ways to reduce validation cost and ensure data quality of IE results. Data cleansing, also called data cleaning, considers the problem of identifying and removing errors and inconsistencies in data sets [3]. Due to the wide range of possible data errors and inconsistencies, data cleansing involves a variety of research efforts to deal with invalid data, missing data, and duplicated data that occur in single source or when integrating multiple sources. One of the primary focuses is on data integrity analysis or data auditing. The goal is to analyze the actual instances of data to obtain indicative data characteristics or value patterns. The results of data analysis are then used to locate potential errors and inconsistencies based on anomaly and conflict detection. Two related approaches, data profiling and data mining, have been attempted for data analysis [4]. Data profiling focuses on individual attributes and derives information such as data type, length, value range, variance, value frequency, occurrence of null values, typical string pattern, etc. Data mining helps discover specific data relationships between several attributes, such as dependencies or domain-specific business rules. Both approaches provide indispensable tools for data cleansing tasks.
Language Identification for Person Names Based on Statistical Information The technology of language identification has become more important with the growth of the WWW. As Grefenstette reported in their paper (Grefenstette 2000) non-English languages are growing in recent years on the WWW, and the need for automatic language identification for both documents and phrases are increasing. An easy method for language identification must be a key for better accuracy rate in natural language processing, such as information retrieval and machine translation.Language identification is not a new topic in natural language processing. Bastrup proposed a unigram-based decision-tree method for language identification (Bastrup 2003). Dunning reported that 20 bytes are enough to obtain 92% accuracy in language identification (Dunning 1994). His method was based on statistical information, and it did not use any accented characters which would be a great help in identifying. This result is very encouraging for applying statistical language identification to proper nouns. Language identification is also well examined as speech recognition task (Matrouf 1998, Schultz 1996, Hazen 1994a, Hazen 1994b, Lamel 1994, Berkling 1994. Caseiro and Trancoso introduced a method using one language phone recognizer and less linguistic information for the language identification of speech (Caseiro 1998). Language identification has been an interesting and fascinating issue in natural language processing for decades, and there have been many researches on it. However, most of the researches are for documents, and though the possibility of high accuracy for shorter strings of characters, language identification for words or phrases has not been discussed much. In this paper we propose a statistical method of language identification for phrases, and show the empirical results for person names of 9 languages (12 areas). Our simple method based on n-gram and phrase length obtained more than 90% of accuracy for Japanese, Korean and Russian, and fair results for other languages except English. This result indicated the possibility of language identification for person names based on statistics, which is useful in multi-language person name detection and also let us expect the possibility of language identification for phrases with simple statistics-based methods.
XNLRDF, an Open Source Natural Language Resource Description Framework  XNLRDF represents an unseen attempt to collect, formalize and formally describe language resources on a large scale so that they can be used automatically by computer applications. XNLRDF is intended to become a free software distributed in XML-RDF. This software is designed to be accessed by computer applications like Web-browsers, mail-tools, Web-crawlers, information retrieval (IR) systems or Computer Assisted Language Learning (CALL) systems. It proposes to replace idiosyncratic ad-hoc solutions for Natural Language Processing (NLP) tasks by a standard interface to XNLRDF. The linguistic information in XNLRDF covers a wide range of written languages and extends the information offered by Unicode so that basic NLP tasks like language recognition, tokenization, stemming, tagging, term-extraction etc can be performed. With more than 1.000 languages used in the Internet and their number continually rising, the design and development of such a software becomes a pressing need. In this paper we introduce the basic design of XNLRDF, the type of information the first prototypes will provide and describe the current state of the project.
On the Web Communication Assist Aide based on the Bilingual Sign Language Dictionary Although monolingual sign language dictionary systems for American, Spanish, Japanese and others have already existed, there is none to translate each sign languages. In this paper we describe a Japanese-to-American sign language translation system. This system contains Japanese indices to American signs. We have made the first trial Japanese-to-American sign language translation system. We plan also to develop this translation system further to facilitate access and language acquisition for sign language learners.Recently some digital sign language dictionaries are available either through Internet or digital devices. Many of those utilize animation to show the sign language despite animation's reputation for being friendly for beginners but inadequate at showing each sign in detail. In sign languages, it is important to display the face expression and also the specific finger movement. Motion pictures introduce how signs are used in each sign language structure and present clear, explicit directions. So we decided to use human motion pictures instead of animation in order to show each sign in more detail.The maximum length for a regular paper is 12 A4 pages, and that for a short paper is 8 A4 pages, including Acknowledgements and References. We discuss the basic ideas behind a Japanese to American Sign Language Translation System for the Japanese users, which assists Japanese Deaf people to communicate. Our discussion covers two main points. The first describes the necessity of a Sign Language Translation System. Since there is no &quot;universal sign language&quot; or real &quot;international sign language,&quot; if Deaf people should learn at least three languages: they want to talk to people whose mother tongue is different from their owns, the mother sign language, the mother spoken language as an intermediate language, and the sign language in which they want to communicate. The second describes the use of computer, especially WWW which is very popular today. As the use of computers becomes widespread, it is increasingly convenient to study through computer software or Internet facilities. Our translation system provides Deaf people with an easy means of access using their mother-spoken language. It also provides a way for people who are going to learn American sign language to look up new vocabulary. We are further planning to examine how our system could be used to educate and assist Deaf people.
Using Speech Recognition for an Automated Test of Spoken Japanese  Various kinds of IT and computer technology have enabled language tests to be delivered online or on computer, and to have much faster scoring time. Such computer-based tests can well assess three skill areas such as reading, writing, and listening comprehension. However, in many cases, speaking ability is still inferred by the scores obtained for those three skill areas. Ordinate Corporation and the Institute for Digital Enhancement of COgnitive DEvelopment (the Institute for DECODE) at Waseda University have been jointly working to develop a completely automated test of spoken Japanese, Spoken Japanese Test (SJT) 1. SJT is intended to provide automated test administration and scoring service by delivering the test over the telephone and by scoring the test using speech recognition technology and other computerized systems. Ordinate and the Institute for DECODE are currently collecting speech data from native and non-native speakers of Japanese to develop a speech recognizer optimized for the Japanese language and for non-native speakers of Japanese.
Predicate Composition and the Determination of Scope  In this paper we argue that the Japanese causative constructions should be viewed as a complex predicate having two possible c-structure realizations (i.e. a single monoclausal f-structure, and monoclausal and biclausal c-structures). Along with the Optimality Theory architecture, we suggest that the emergence of two c-structures in predicate composition is a consequence of the interactions that regulate the parallel representations of clause structure. The proposed analysis can account for various grammatical phenomena including adverb scope.
Which Is Essential for Chinese Word Segmentation: Character versus Word Chinese text is written without natural delimiters, so word segmentation is an essential first step in Chinese language processing. In this aspect, Chinese is quite different from English in which sentences of words delimited by white spaces. Though it seems very simple, Chinese word segmentation (CWS) is not a trivial problem. Actually, it has been active area of research in computational linguistics for almost 20 years and has drawn more and more attention in the Chinese language processing community. To accomplish such a task, various technologies are developed [1] [2].In the early work of Chinese word segmentation, word-based method once played the dominant role, in which maximum matching algorithm is the most typical method. Here, the term, word, means those known words are shown in known lexicon or training corpus (also are called in-vocabulary(IV) words.). Explicit known word information was still important learning object even after statistical methods were introduced in CWS [1].To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 2006 1 , and there were 12, 23 and 23 participants, respectively [3], [4], [5]. Four segmentation corpora were presented in each Bakeoff. Thus, twelve corpora are available from Bakeoff 2003Bakeoff , 2005Bakeoff , and 2006. A summary of these corpora is shown in Table 1.In all of proposed methods, character-based tagging method [6], instead of traditional word-based one, quickly rose in Bakeoff-2005 as a remarkable one with state-ofthe-art performance. Especially, two participants, Ng and Tseng, gave the best results in almost all tracks [7], [8]. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. Researchers turned to character-based method from traditional word-based method only with four years.The success of Bakeoffs not only gave some public consistent segmentation standards, but also proposed a corpus-based segmentation standard representation, instead of the representation of known word lexicon and segmentation manual before. Thus Chinese word segmentation becomes more like corpus-based machine learning procedure in this sense.With the supply of common segmentation standards of Bakeoffs, the comparison problem on word-based method and character-based method are still remained. Though most effective Chinese word segmentation techniques are turned to pure characterbased methods, some researchers are still insisting that character-based method alone can not be superior to the method that combines both word information and character information [9] [10] [11]. In this paper, we will briefly explore the linguistic background of such turnaround in Chinese word segmentation and give an empirical comparison of these methods. The remainder of the paper is organized as follows. The next section reviews the track of character-based method. We discuss the linguistic background of characterbased features (especially for unigram feature) in Section 3. We evaluate unigram feature through CWS performance comparison in Section 4. In Section 5, the experimental results between word-based method and character-based method are demonstrated. We summarize our contribution in Section 6. This paper proposes an empirical comparison between word-based method and character-based method for Chinese word segmentation. In three Chinese word segmentation Bakeoffs, character-based method quickly rose as a mainstream technique in this field. We disclose the linguistic background and statistical feature behind this observation. Also, an empirical study between word-based method and character-based method are performed. Our results show that character-based method alone can work well for Chinese word segmentation without additional explicit word information from training corpus.
Multilinguality in Temporal Annotation: A Case of Korean  The aim of this paper is to apply TimeML, an annotation scheme for events and temporal expressions, to the annotation of Korean in an attempt to test its multilingual extendability. TimeML has been well validated by the successful annotation of a corpus of 186 news articles and some other documents in English. One of its remaining tasks, however, is multilingual extension. This paper aims at contributing to this task and also at promoting TimeML as an international standard.
Towards a Neuro-Cognitive Model of Human Sentence Processing Recent developments in non-invasive measurement of brain activities have paved the way for directly observing how linguistic information is processed in the human brain. However, no attempts have hitherto been made to model human sentence comprehension based on the data provided by those methods from a unified formal point of view. This paper proposes a parser which behaves in a similar manner to humans in eliciting different event-related potentials (ERPs) depending on whether NPs are unambiguously case marked or not. The crucial role therein is performed by syntactic underspecification in incremental processing, which is made possible by Dynamic Syntax ( Kempson et al. 2001) adopted as our formal linguistic theory. A formal sentence processing system is proposed which simulates different event-related potential (ERP) elicitation between sentences with and without unambiguous case marking. The electroencephalographical data are based on German subordinate clauses and Japanese sentences. As a formal framework we adopt Dynamic Syntax (Kempson et al. 2001), which enables incremental update of information by underspecifying tree information. This is augmented with default syntactic and semantic specifications which reflect shallow but efficient human sentence processing.
Discovering Relations among Named Entities by Detecting Community Structure Relation extraction among named entities (NR) is one of major tasks in information extraction (IE).The goal of relation extraction is to find out the relations among named entities(NE) in documents. In recent years, such technology has been widely used in many fields, such as: information retrieval, questionanswering systems, biology technology, construction of ontology, etc. Many methods have been proposed for relation extraction, including supervised learning methods ( Zelenko et al., 2002) [8], weakly supervised learning methods (Brin 1998;Agichtein 2000;Sudo 2003) [9][10] [11] and unsupervised learning methods (Hasegawa 2004; Chen Jinxiu 2005) [1] [2]. In this paper, we propose an unsupervised method for relation extraction.Currently, unsupervised learning methods for relation extraction have some difficulties. For example, in Hasegawa et al.'s method, they eliminated less frequent NE pairs, collected the contexts of NE pairs, clustered the contexts using complete linkage method, and finally selected the most frequent word in a cluster to label the relation in this cluster. However, the less frequent NE pairs might have relations, and it is difficult for complete linkage method to select the threshold.In Chen Jinxiu et al.'s method, they firstly got the NE pairs labeled with relation in ACE corpus and collected their contexts, clustered the contexts using stability-based method, and finally used DCM method to label the clusters. However, we can't select NE pairs with relation in the unannotated corpus. And the relations of NE pairs might be in hierarchical structure; but the stability-based method can not discover the hierarchical structure.Based on the issues mentioned above, this paper proposes a method to discover relations among NEs based on networked data mining. The advantages of this method is that there is no need to eliminate less frequent NE pairs and select the NE pairs which have relations. In addition, our method can automatically present the relations of NE pairs in the hierarchical structure.The rest of this paper is organized as follows. Section 2 talks about the construction for networked structure. Section 3 explores the procedure of relation discovering and labeling. Section 4 describes experiments and evaluation of experiment results. Section 5 gives a discussion of the problems in existing methods. Finally, Section 6 gives a summary and talks about future prospect. This paper proposes a networked data mining method for relations discovery from large corpus. The key idea is representing the named entities pairs and their contexts as the network structure and detecting the communities from the network. Then each community relates to a relation the named entities pairs in the same community have the same relation. Finally, we labeled the relations. Our experiment using the corpus of People&apos;s Daily reveals not only that the relations among named entities could be detected with high precision, but also that appropriate labels could be automatically provided for the relations. 1 Introduction Relation extraction among named entities (NR) is one of major tasks in information extraction (IE).The goal of relation extraction is to find out the relations among named entities(NE) in documents. In recent years, such technology has been widely used in many fields, such as: information retrieval, question-answering systems, biology technology, construction of ontology, etc. Many methods have been proposed for relation extraction, including supervised learning methods (Zelenko et al., 2002) [8], weakly supervised learning methods (Brin 1998; Agichtein 2000; Sudo 2003) [9][10][11] and unsupervised learning methods (Hasegawa 2004; Chen Jinxiu 2005) [1][2]. In this paper, we propose an unsupervised method for relation extraction. Currently, unsupervised learning methods for relation extraction have some difficulties. For example, in Hasegawa et al.&apos;s method, they eliminated less frequent NE pairs, collected the contexts of NE pairs, clustered the contexts using complete linkage method, and finally selected the most frequent word in a cluster to label the relation in this cluster. However, the less frequent NE pairs might have relations, and it is difficult for complete linkage method to select the threshold. In Chen Jinxiu et al.&apos;s method, they firstly got the NE pairs labeled with relation in ACE corpus and collected their contexts, clustered the contexts using stability-based method, and finally used DCM method to label the clusters. However, we can&apos;t select NE pairs with relation in the unannotated corpus. And the relations of NE pairs might be in hierarchical structure; but the stability-based method can not discover the hierarchical structure. Based on the issues mentioned above, this paper proposes a method to discover relations among NEs based on networked data mining. The advantages of this method is that there is no need to eliminate less frequent NE pairs and select the NE pairs which have relations. In addition, our method can automatically present the relations of NE pairs in the hierarchical structure. The rest of this paper is organized as follows. Section 2 talks about the construction for networked structure. Section 3 explores the procedure of relation discovering and labeling. Section 4 describes experiments and evaluation of experiment results. Section 5 gives a discussion of the problems in existing methods. Finally, Section 6 gives a summary and talks about future prospect. 42
A Full Inspection on Chinese Characters Used in the Secrete History of the Mongols *  The Secrete History of the Mongols (SHM) is a special Mongolian historical document transliterated with Chinese characters as phonetic symbols, which feature causes many multivariate analyses in later ages. The discussion of this paper mainly focuses on the data of used Chinese characters, the frequencies, and the rules of transliteration on SHM. All the statistic data and the analysis are based on the electronic text of SHM, which provides those important data in an all-round way, including the whole Chinese-transliterated characters, aligned Chinese glosses by the side of Chinese-transliterated characters, and Chinese translational paragraphs. Furthermore, four types of Chinese characters as phonetic symbols and their statistic information have been discussed, which are type C, xC, Cy, and xCy as in the main body of the text: , , , and. Shortly, this statistics is so far the most completed inspection on the text of the Secrete History of the Mongols.
An Information Retrieval Model Based On Word Concept Searching in archives of documents becomes increasingly frequent for most of people. So, how to provide useful and efficient IR (Information Retrieval) tools becomes more and more important. Since 1968, when the first formal model for IR [1] comes into being, a number of IR models have been developed, such as vector space, probabilistic, fuzzy, logical, inference, language and so on [2][3][4][5][6][7][8][9][10] .While considering all the former methods, we can find most of the methods are based on occurrences of terms in a document (or TF: Term Frequency) and seldom on the content of the document. These algorithms analyze only term occurrences and do not attempt to resolve the meaning of the terms. As we all know, the meaning of words is very helpful to IR. If we disregard the context and dispose the words separately, the performance of IR system will be lowered for the ubiquitous existence of word sense ambiguity. A word may have a lot of meanings. A particular meaning can also be represented by more than one word. Therefore whether we can determine the meaning of the word in a document will affect the accuracy of an IR system. Meanwhile, many studies [11][12][13][14][15] have shown that people understand things by comprehending the concepts represented by the things. The language works in the same way [16] . Consequently, some research teams began to investigate the language conceptual space and the expression of the space using symbolic system. The main part of the IR method we present is to discuss how to draw the conceptual expressions of a word and a sentence based on a symbolic system [16][17][18][19] . We consider the advantages of case grammar [20] , transformational generative grammar [21][22] and Wordnet [23] fully, and try to form a strategy to extract the conceptual expressions, and then, process the information in a semantic way based on the conceptual expression.The key rudder of Section 2 is to discuss how to use the concept symbols and the sentence category expressions to represent the words and the sentences via some analyzing strategies and knowledge bases.Owing to the fact that a simple search usually retrieves a large collection of documents, clustering becomes an efficient tool. In fact, clustering methods [24] [25] have been intensively studied in information retrieval for textual documents since 1990s. All these clustering methods need users to evaluate, more or less, the thresholds (e.g.: the value k in k-means). The values of thresholds will affect the results of clustering to some extend. Furthermore, the document vectors may always be of large dimension and sparse. So a fixed-threshold is certain to be inappropriate in some situations. Consequently, an autothreshold detection clustering method is proposed in the paper. This clustering method uses the simulated curve of the document distances to find the thresholds. The processing object of this clustering method is the concept instead of the word.In order to do cluster searching, a searching method is proposed. The method here uses the dispersion among individual maximum likelihood, partial maximum likelihood and global maximum likelihood to measure the appearance probability of queries, then to fulfill the task. This method will be more accurate. Section 3 will discuss the clustering method which is used to generate clustering of the documents, and the searching method which is used to measure the document query similarity.Section 4 compares the experiments using the proposed methodology and the traditional indexing schemes. The conclusions are given in Section 5. Traditional approaches for information retrieval from texts depend on the term frequency. A shortcoming of these schemes, which consider only occurrences of the terms in a document, is that they have some limitations on extracting semantically exact indexes that represent the semantic content of a document. However, one word can always represent more than one meaning. The word sense ambiguities will also affect the system behavior. To address this issue, we proposed a brand new strategy-a concept extracting strategy to extract the concept of the word and to determine the semantic importance of the concepts in the sentences via analyzing the conceptual structures of the sentences. In this approach, a conceptual vector space model using auto-threshold detection is proposed to process the concepts, and a cluster searching model is also designed. This auto-threshold detection method can help the model to obtain the optimal settings of retrieval parameters automatically. An experiment on the TREC6 collection shows that the proposed method outperforms the other two information retrieval (IR) methods based on term frequency (TF), especially for the lower-ranked documents 1 Introduction Searching in archives of documents becomes increasingly frequent for most of people. So, how to provide useful and efficient IR (Information Retrieval) tools becomes more and more important. Since 1968, when the first formal model for IR [1] comes into being, a number of IR models have been developed, such as vector space, probabilistic, fuzzy, logical, inference, language and so on [2-10] .While considering all the former methods, we can find most of the methods are based on occurrences of terms in a document (or TF: Term Frequency) and seldom on the content of the document. These algorithms analyze only term occurrences and do not attempt to resolve the meaning of the terms. As we all know, the meaning of words is very helpful to IR. If we disregard the context and dispose the words separately, the performance of IR system will be lowered for the ubiquitous existence of word sense ambiguity. A word may have a lot of meanings. A particular meaning can also be represented by more than one word. Therefore whether we can determine the meaning of the word in a document will affect the accuracy of an IR system. Meanwhile, many studies [11-15] have shown that people understand things by comprehending the concepts represented by the things. The language works in the same way [16]. Consequently, some research teams began to investigate the language conceptual space and the expression of the space using symbolic system. The main part of the IR method we present is to discuss how to draw the conceptual expressions of a word and a sentence based on a symbolic system [16-19]. We consider the advantages of case grammar [20] , transformational generative grammar [21-22] and Wordnet [23] fully, and try to form a strategy to extract the conceptual expressions, and then, process the information in a semantic way based on the conceptual expression. The key rudder of Section 2 is to discuss how to use the concept symbols and the sentence category expressions to represent the words and the sentences via some analyzing strategies and knowledge bases. Owing to the fact that a simple search usually retrieves a large collection of documents, clustering becomes an efficient tool. In fact, clustering methods [24][25] have been intensively studied in information retrieval for textual documents since 1990s. All these clustering methods need users to evaluate, more or less, the thresholds (e.g.: the value k in k-means). The values of thresholds will affect the results of 56
Discriminative Reranking for Spelling Correction * Spelling correction is used to suggest one or several hypothetical corrections for the assumed error once a spell checker detects some misspelling, which is typically identified when it cannot been found in a pre-compiled lexicon. In this paper, we focus on the problem of interactively correcting non-word errors (e.g. teh for the). We do not deal with real word errors (such as from is written when form is intended). We propose an approach that can be applied to text processing applications, such as the spelling corrector used in Microsoft Word or Aspell [2]. In these applications several suggestions (5 or 10 in most cases) are provided in an interactive manner, letting the user choose the desired one. In most cases the suggestion ranked first is preferred. For this reason top 1 suggestion is our most concern. We also care about the top 5/10 accuracies since they are presented to the user, too. Aspell [2] is a spelling program widely used on different platforms. As shown in [1], the top 5 accuracy of Aspell is more than 85%. However, it suffers from its poor performance on top 1 accuracy, which is less than 60%.Our motivation is to improve the initial ranking of Aspell's output to gain a higher performance. The main contributions are as follows:1. We adapt a discriminative model (Ranking SVM) to rerank the output of Aspell's N-best candidates, adding some global features as evidence. The model is general-purposed so that we can integrate state-of-the-art techniques in spelling correction into one model, including edit distance, n-gram, phonetic similarity and noisy channel model. Encouraging results are gained on this task. 2. A novel approach is proposed to actively solicit training pairs from query log chain. The quality of this approach is verified in the evaluation that follows. The rest of the paper is organized as follows: in Section 2, we conduct a literature research into spelling correction. The reranking problem is formulated in Section 3. Section 4 shows the experimental evaluation. In this section we also specify the details to automatically extract training data from query log chain. The last section contains the conclusions and suggestions for possible further developments of the proposed model. This paper proposes a novel approach to spelling correction. It reranks the output of an existing spelling corrector, Aspell. A discriminative model (Ranking SVM) is employed to improve upon the initial ranking, using additional features as evidence. These features are derived from state-of-the-art techniques in spelling correction, including edit distance, letter-based n-gram, phonetic similarity and noisy channel model. This paper also presents a new method to automatically extract training samples from the query log chain. The system outperforms the baseline Aspell greatly, as well as previous models and several off-the-shelf spelling correction systems (e.g. Microsoft Word 2003). The results on query chain pairs are comparable to that based on manually-annotated pairs, with 32.2%/32.6% reduction in error rate, respectively.
A User Interface-Level Integration Method for Multiple Automatic Speech Translation Systems There have been many researches on speech-to-speech translation systems, such as NEC speech translation system [1], ATR-MATRIX [2] and Verbmobil [3]. These speech-to-speech translation systems include at least three components: speech recognition, machine translation, and speech synthesis. However, in practice, each component does not always output the correct result for various inputs.In actual use of a speech-to-speech translation system with a display, the speaker using the system can examine the result of speech recognition on the display. Accordingly, when the recognition result is inappropriate, the speaker can correct errors by speaking again to the system. Similarly, when the result of speech synthesis is not correct, the listener using the system can examine the source sentence of speech synthesis on the display.On the other hand, the feature of machine translation is different from that of speech recognition or speech synthesis, because neither the speaker nor the listener using the system can confirm the result of machine translation. Thus, an error in the machine translation is critical in a speech-to-speech translation system.Instead of machine translation, there is a parallel text-based translation which uses parallel bilingual sentences registered in the system. This retrieves the corresponding translation by referring to the registered source sentence. However, in parallel text-based translation, although the quality of translation is largely guaranteed, only a limited number of sentences can be translated, because it is impossible to cover all the utterances of users by the parallel text of source language. When no registered sentences correspond to what the user says, he or she has to choose a most preferable one among retrieved sentences that roughly reflect the utterance; otherwise the speaker may give up the attempt.Accordingly, an integrated method which is easier to use is required in which the accuracy of translation becomes compatible with coverage by integrating these two translation components. This paper proposes a new method to unify the automatic speech translation system with free-style sentence translation component and that with parallel text-based translation component at the user interface-level in order to further ease the operation of users compared to the previous unified approach. We propose a new method to integrate multiple speech translation systems based on user interface-level integration. Users can select the result of free-sentence speech translation or that of registered sentence translation without being conscious of the configuration of the automatic speech translation system. We implemented this method on a portable device. 1 Introduction There have been many researches on speech-to-speech translation systems, such as NEC speech translation system[1], ATR-MATRIX[2] and Verbmobil[3]. These speech-to-speech translation systems include at least three components: speech recognition, machine translation, and speech synthesis. However, in practice, each component does not always output the correct result for various inputs. In actual use of a speech-to-speech translation system with a display, the speaker using the system can examine the result of speech recognition on the display. Accordingly, when the recognition result is inappropriate, the speaker can correct errors by speaking again to the system. Similarly, when the result of speech synthesis is not correct, the listener using the system can examine the source sentence of speech synthesis on the display. On the other hand, the feature of machine translation is different from that of speech recognition or speech synthesis, because neither the speaker nor the listener using the system can confirm the result of machine translation. Thus, an error in the machine translation is critical in a speech-to-speech translation system. Instead of machine translation, there is a parallel text-based translation which uses parallel bilingual sentences registered in the system. This retrieves the corresponding translation by referring to the registered source sentence. However, in parallel text-based translation, although the quality of translation is largely guaranteed, only a limited number of sentences can be translated, because it is impossible to cover all the utterances of users by the parallel text of source language. When no registered sentences correspond to what the user says, he or she has to choose a most preferable one among retrieved sentences that roughly reflect the utterance; otherwise the speaker may give up the attempt. Accordingly, an integrated method which is easier to use is required in which the accuracy of translation becomes compatible with coverage by integrating these two translation components. This paper proposes a new method to unify the automatic speech translation system with free-style sentence translation component and that with parallel text-based translation component at the user interface-level in order to further ease the operation of users compared to the previous unified approach. 72
Efficient language model development for spoken dialogue recognition and its evaluation on operator&apos;s speech at call centers Call centers are particularly appropriate for use as speech recognition application domains because almost all of their operations are based on spoken dialogue. The many attempts made to date to apply speech recognition technology to call center applications have mostly been based on IVR (Interactive Voice Response) including automatic call-routing, which is designed for taking over human operator work in recognizing and responding to customer speech. On the other hand, we are developing another speech recognition application that recognizes operator speech and helps human operator work. For example, product names are extracted from speech recognition results of the operator speech and the information about them is displayed to the operator. An application focusing on operator speech would seem more promising because of the advantage it has for ease of speech recognition. While customer speech is telephone-transmitted, operator speech can be recorded directly from a headset microphone, and the sound quality is much higher. Further, the enunciation of professionally trained operators can be expected to be clearer from the beginning. Even for the recognition of operator speech at call centers, however, a large training corpus is needed to build a language model. The development of such a corpus requires the recording and transcription of a very large number of utterances. The cost of such work is high because fully-trained transcribers are needed and, even for them, the transcription will require more than 4 times the amount of time that was needed for the original recording. Moreover, because spoken dialogue in business situations is often highly confidential, collecting such dialogue can itself present a problem, and strict management will be required even when such collection has been possible. In order to reduce these costs and increase the possibilities for applying speech recognition technology, a method is needed for using inexpensively produced, target-specific data-sets to adapt previously existing language models for use on specific target tasks. Such method is one of the methods known as "language model adaptation" and reviewed in [1]. Especially when combining conversational-style data with document-style data, adding unigram probabilities from document-style data to an existing class bigram learned from conversational-style data is proposed in [2]. In [3] specific-task-oriented data are collected from WWW. Search queries which are comprised of the phrases from existing conversational-style corpora and the words about the topic of the specific-task are used to get the data that match the style and topic of the target recognition task. In [4], if a small amount of specific-task-oriented data is available, similarity measure is used for selecting the training data from the data collected from WWW. In [5] verbs and noun phrases of the target-task are extracted from the data collected from WWW by syntactic analysis, and the artificial sentences that include the verbs and noun phrases are generated with templates. In [6] task related predicates and arguments are extracted from task related documents by semantic analysis, and conversational-style data are generated with templates. Statistical transformation model is estimated from parallel aligned corpus of the conversational-style transcripts and their document-style texts with statistical machine translation technique, and the task related documents are transformed into conversational-style data by the model [7].In this paper, we propose a method for combining previously existing spoken dialogue corpora with key phrases (i.e. phrases that contain keywords) extracted from task related documents. Even though the added data is from documents related to the target dialogue, since it consists of key phrases, stylistic differences (between document data and the actual dialogue to which the model will be applied) are not a problem. We have evaluated our method in recognition tests on actual spoken dialogue collected at call centers. While a language model for recognition of spoken dialogue is ideally built from a very large, specific-task-oriented corpus, a great amount of time and effort is required to develop such a corpus, and this involves both the audio recording and written transcription of large amounts of speech data. Training data for a language model should match the target task in both topic and style. What is needed, then, is a method to utilize previously existing spoken dialogue corpora that are not necessarily related to the specific target-task. Such corpora would be combined with documents related to the topic of the target-task to develop a language model for the target spoken-dialogue. In this paper, we propose a method for combining previously existing corpora with key phrases (i.e. phrases that contain keywords) extracted from task related documents. Even though the added data is from documents related to the target dialogue, since it consists of key phrases, stylistic differences (between document data and the actual dialogue to which the model will be applied) are not a problem. We have produced a model using this method and have evaluated it in use on actual spoken dialogue collected at call centers. Experimental results show that a relative 13% reduction in word error rate could be achieved with the addition of key phrases. This performance is nearly as good as that which would be achieved on the basis of a large, expensive transcript-corpus, and the cost of producing the key phrase data is essentially negligible. Such cost reduction achieved by our method will enable speech recognition applications to be more widely used.
Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling Chinese text is written without natural delimiters, so word segmentation is an essential first step in Chinese language processing. In this aspect, Chinese is quite different from English in which sentences of words delimited by white spaces. Though it seems very simple, Chinese word segmentation is not a trivial problem. Actually, it has been active area of research in computational linguistics for almost 20 years and has drawn more and more attention in the Chinese language processing community. To accomplish such a task, various technologies are developed [1].To give a comprehensive comparison of Chinese segmentation on common test corpora, two International Chinese Word Segmentation Bakeoffs were held in 2003 and 2005, and there were 12 and 23 participants respectively [4], [5].In all of proposed methods, character based tagging method [2] quickly rose in two Bakeoffs as a remarkable one with state-of-the-art performance. As reported in [5], the results of Bakeoff-2005 shows a general trend to a decrease in error rates from 3.9% to 2.8% compared to the results of Bakeoff-2003. Especially, two participants, Ng and Tseng, gave the best results in almost all test corpora [6], [7].We continue to improve CRF-based tagging method of Chinese word segmentation on the track of Ng and Tseng in this study. It is different in our method that we consider This work was finished while the first author visited Microsoft Research Asia both feature template selection and tag set selection, instead of feature template focused only methods in previous work. That is, feature template selection was the main work if it was not the unique one before, while tag set is empirically specified aforehand.There are two kinds of test schemes in Chinese word segmentation Bakeoff, open and closed test. In the open test participants are allowed to use training data and any other linguistic resource including other training corpora, proprietary dictionaries and so forth. In the closed test only training data are allowed to used for the particular corpus. No other data is allowed. In this study, we will limit our comparison in closed test because additional linguistical resource often varies from system to system.The remainder of the paper is organized as follows. The next section is a simple introduction to conditional random field. Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. This paper is concerned with Chinese word segmentation, which is regarded as a character based tagging problem under conditional random field framework. It is different in our method that we consider both feature template selection and tag set selection, instead of feature template focused only method in existing work. Thus, there comes an empirical comparison study of performance among different tag sets in this paper. We show that there is a significant performance difference as different tag sets are selected. Based on the proposed method, our system gives the state-of-the-art performance.
Type grammar meets Japanese particles As the need of software modules performing natural language processing tasks is growing, in depth grammatical analyses of sentences must be properly carried out. Grammatical analyses based on theoretically sound grammar formalisms are thus essential.Treatment of case particles constitute an essential part of a grammar for the Japanese language, where the word order is relatively flexible. The role of case particles is functionally determined within a sentence: they indicate that the accompanying noun functions as subject, object, etc. But because case components are often scrambled or omitted and because case particles disappear when case components are accompanied by the topic marker wa or other special particles, it makes it difficult to syntactically analyze Japanese sentences.Various studies in the literature discuss about the Japanese argument case marking and the treatment of Japanese focus particles. Here, we explore the treatment of Japanese particles within the Lambek style pregroup grammar.The application of pregroups in natural language processing provides a rigorous formulation of the grammar of a given language. Pregroup calculations are very simple from a computational point of view. Furthermore, in analyzing a sentence, we go from left to right and imitate the way a human hearer might proceed: recognizing the type of each word as it is received and rapidly calculating the type of the string of words up to that point.The reader might be curious to see a comparison of our grammar formalism with other existing formalisms such as HPSG. Indeed, it would be interesting to write our proof-theoretic analysis in terms of the model-theoretic HPSG framework. We could perhaps follow the HPSG analysis of Japanese presented by Siegel in [13], where particles are analyzed as heads of their phrases and the relation between case particle and nominal phrase is a head-complement relation. To account for the omission and scrambling of verbal arguments, Siegel introduces the attributes SAT, which denotes whether a verbal argument is already saturated, optional or adjacent, and VAL, which contains the agreement information for the verbal argument. Siegel also presents a Japanese head-complement schema which accounts for optional and scrambable arguments as well as for obligatory and adjacent arguments. Due to limited space, however, page-filling representations in the HPSG framework will not be further discussed. This paper presents a computational analysis within the framework of a type grammar for the treatment of Japanese particles. In Japanese, particles express a number of functional relations; they follow a word to indicate its relationship to other words in a sentence, and/or give that word a particular meaning. We explain our parsing technique and discuss about various constructions using case particles and focus particles. We show how troublesome phenomena such as scrambling and omission of case particles are treated.
  
Tense Markers and -ko Constructions in Korean The purpose of this paper is two-fold. On the one hand, we will identify the morpho-syntactic status of the tense markers in Korean. On the other, we will provide a new system for accounting for the tenserelated phenomena in "-ko coordinate constructions." The element -ko is generally regarded to be a typical coordinate marker in Korean. Our analysis is based on two important observations, which are not new findings but have not been duly appreciated thus far. Firstly, the marker -ko is actually ambiguous between a coordinate marker and a subordinate marker. Secondly, the non-past tense marker is not -n or -nun but a null form element -ø. That is, the marker does not have an explicit form, contrary to the general belief. We will see that these observations lead to a natural explanation of the tense phenomena in -ko constructions.As for the morpho-syntactic status of the Korean tense markers, two large groups of different analyses have been proposed: "lexical analyses" and "syntactic analyses." The tense markers are generally assumed to be inflectional affixes in the former group, while they are assumed to be "clitics" in the latter. Clitics are "grammatical units with some properties of inflectional morphology and some of independent words" ( [17], [18]). They cannot stand alone phonologically just like inflectional affixes, but they are words on their own syntactically just like regular words. We can easily see that tense markers are inflectional affixes in such head-initial languages as English because the verb together with a tense marker comes before its complements. However, it has long been an issue of hot disputes to correctly identify the morpho-syntactic status of tense markers in Korean, a head-final language. It is not clear whether they are attached to the verb (V) or to the verb phrase (VP) concerned. In the former case, they should be regarded as inflectional affixes, while, in the latter case, they should be regarded as clitics. There have been heated debates on whether tense markers in Korean are inflectional affixes or syntactic words. One of the strongest pieces of evidence for the latter position comes from tense-related phenomena in-ko constructions. These constructions are usually assumed to consist of two coordinate clauses. In this paper, we will see that the tense phenomena can be accounted for naturally by correctly identifying the non-past tense marker and by seriously taking consideration of the fact that-ko is ambiguous between a coordinate marker and a subordinate marker. It will be shown that the tense marker in the second conjunct does not have any effect on the first conjunct when-ko functions as a coordinate marker, which will render the position very weak that tense markers are syntactic words.
Topic-Comment Articulation in Japanese: A Categorial Approach  This paper deals with the topic-comment articulation of information structures conveyed by sentences. In Japanese, the topic marker WA is attached not only to an N(P) but also to a PP or a clause, forming the information structure of a sentence, where the topicalized part represents a restriction and the remaining part of the sentence represents a nuclear scope. I propose the type-raised category for WA which embodies flexible constituency to realize divergent topic-comment structures. With our categorial definition for the topic marker and the combinatory rules in Combinatory Categorial Grammar, which derive the tripartite representation of the information state of a sentence, our grammar architecture can dispense with an independent level representing the information structure.
A language-independent method for the alignement of parallel corpora As more and more countries and cultures develop a popular use of the Internet, the amount of distinct languages composing the information found online has grown tremendously in the last five years. Once a mostly English-speaking medium totally based on the latin alphabet, Internet has now become a place where information is represented in radically different ways, in languages that for example may not even share the basic notion of " word ".Being able to find relevant information for a given need in any language, whatever language the need is expressed in, is one of the challenges associated with that evolution. A much explored way to solve it is, of course, automatic translation; the problem of that approach is that each new language to take into account involves a huge amount of human work for the creation of lexicons, grammars, etc., which soon becomes un-tractable as the number of languages increases. But full text translation is not necessary to know that two documents address a same topic: often, a few known keywords or statistical properties of the considered texts are sufficient for that task. The interest of that more " loose " approach is that the information needed can be automatically acquired from training data presented as aligned parallel corpora.Parallel corpora are instances of a same text written in different languages, and aligning them means exposing the links between portions of text bearing the same meaning. The interest of that information for the automatic acquisition of knowledge concerning semantic equivalences between languages is of course very important. Although parallel corpora are quite frequentfor example, multilingual news outlets -, aligning them still requires work. Doing so manually is extremely tedious and time-consuming, and studies have therefore been conducted in order to do it automatically.Church and Gale [1] were among the first to propose a system to perform the task of sentencelevel parallel text alignment. Given two texts carrying the same message, they make the assumption that the ratio between the lengths of sentences (in numbers of characters) having the same meaning is roughly constant. Using the DTW (dynamic time warping) algorithm, they then progressively match sentences according to their lengths, while taking into account the fact that a single sentence may be translated as two, or reciprocally, or that a sentence may be deleted. Many works have used this basis, sophisticating it by using, for example, lists of known translations, or looking for corresponding language independent text elements, like numbers. The main difficulty raised by this system is that it is language-dependent, since the sentence length ratio for the considered languages must be provided. The authors claim that simply setting it to 1 has little impact on the system performance: that is true as long as the actual ratio remains relatively close to 1, as is the case for European languages, but performance does degrade in the case of more radically different sentence lengths (typically when the notion of " character " changes, as when aligning English and Chinese texts).Kay and Röscheisen [3] introduce a different approach that considers globally the repartition of words over text sentences. Words having a similar distribution are assumed to be translations of one another, and the correspondances between their occurrences are used as anchors for sentence alignment. The weakness of that approach, which we call " lexical alignment ", is that it features two potential sources of mistakes: computed word matches may be wrong, and even when they are not, occurrence matches may be incorrect too. Moreover, the performance of that system degrades if the texts to be aligned are in languages having a very different conception of what is a " word " (e.g. flexional vs isolating language).The work we present is inspired by both of those fundational studies, but attempts to create a truly language-independent parallel text alignement system. Contrary to many works carried on in the field on parallel corpus alignment, we do not wish to improve upon the results of Church and Gale so much as to broaden the range of application of their method: our system must be applicable to any language without any prerequisite, for example in situations where dictionary-based approaches such as [4] cannot be used, due to a lack of electronic dictionaries in the considered languages.We present in Section 2 our modification of the Church-Gale algorithm to make it depend on the studied texts only, and our improvement on previous work by Romary and Bonhomme [5] to take into account the logical structure of documents. We then introduce in Section 3 an attempt at making that system cooperate with another based on lexical alignment. We finally give in Section 4 the results of several evaluations; one is performed using the text of the novel Le petit prince in French, English and Vietnamese, and two come from the ARCADE II [2] evaluation campaign: the first between European languages, the second between French and more " exotic " languages such as Russian or Persian. The automatic alignment of parallel corpora is a very rich source of information for automatic translation, multilingual document indexing, information retrieval, etc. The rapid growth of the use of &quot; minority &quot; languages in online documents makes it necessary to develop methods that can easily adapt to any language. We present an evolution over previous works, notably by Church and Gale [1], that performs the alignement of parallel texts in any language without any need for information concerning these languages, as is often the case in existing systems. We also introduce a more experimental system, that shows promise for the alignment of degraded translations. The systems have taken part to the ARCADE II evaluation campaign [2], of which we present the results.
The Current Status of Sorting Order of Tibetan Dictionaries and Standardization *  This paper discusses the problem of sorting orders of Tibetan dictionaries and Tibetan electronic databases. The alphabetical sequence of Tibetan language has being gradually formed by popular usage in the long history, in which many cognitive senses and cultural connotations lie embedded of Tibetan people, also there are a few influence of foreign elements and irrational elements. However, on the bases of the analyses of the background, the paper proposes three standardizing principles for compiling Tibetan dictionaries: agreement, compatibility, and rationality. In light of the three principles, the paper designs a set of digital codes for each letter or character and assigns distinctive sorting values to all existing words in the electronic dictionary with corresponding algorithm, which revises some serious errors in a previous paper. The method of compiling Tibetan word order mentioned above has been accomplished in our software system. 1 The Background Although written Tibetan is a kind of phonemic language, its sorting order, in a dictionary or in an electronic database, is still not a simple issue. 1 The alphabet sequence of a language always shows by the word sequence of its dictionary. A dictionary sequence is not innate from the beginning, but gradually formed through common usage in a long history. So before discussing a compiling word order of a Tibetan dictionary, it is necessary to find out the development of its order, which may help readers to comprehend where the problem of the Tibetan alphabet order is. Firstly, to let the issue clear, it is necessary to introduce briefly the Tibetan graphic structure. In figure 1, a Tibetan word or morpheme consists of not more than 7 letters, in which the base letter(Ba) is the core letter and the first one in structural sorting order, then the rest of the order is prefixed letters(Pr), head letters(Up), subjoined letters(Lw), vowels(V), suffixed letters(Sx). However, except the base letters, any other letters may be absent in the structure. Therefore, any structures with vacant letters will make special Tibetan graphic structures, which will change the order of different graphic forms (Figure 2). Furthermore, there exists different letter number in each position of graphic structures and the letters are in the order of alphabet sequence of their own. 2 The process of compiling order of a graphic form is that, choose letters by the alphabet sequence within the lowest position, such as position 7, after alternating all the letters in the layer, then enter into the above layer, and do the alternation again, after each change, return to the lowest layer, and run the previous circle. After the alternation goes through each position and each layer until the base letter changes, then a sorting order shifts to next graphic form with another base letter by the alphabet sequence. Read the complicated process in detail with reference [1] and [2].
A Text Classifier Based on Sentence Category VSM With the rapid increase of electronic documents, especially the growth of the internet and intranet, automatic text processing becomes more and more important. Text categorization is usually the basic and important components of text processing applications. It will benefit other applications, such as IR, Q&amp;A.Statistical categorization is the major method to resolve the categorization problem. Vector Space Model (VSM) was created by G. Salton in 1960s and now widely used in text representation [1] . All categorization methods based on VSM commonly include feature vector generation, dimensionality reduction of feature space, machine learning and categorization execution [2][3][4] .The idea of VSM representation is to discretize a consecutive text and form a vector in particular feature space. Words are the most common features of VSM and there are also models that take phrases, terms or Chinese character as features [5] . The weight of every element of a vector is usually the number of occurrences or its transform. Tfc-weighting is a transform algorithm.Typically, the dimensionality of the feature space is more than tens of thousands. It is too high to result in too much calculation time and memory space. So dimensionality reduction is necessary. Feature selection and feature merger are two useful approaches. Feature selection is to discard some features non-informative or non-dipartite. Document Frequency Threshold, Information Gain, CHI, Mutual Information are means of feature selection [6][7][8][9][10] . Feature merger is an approach to compress the feature space. Latent Semantic Indexing (LSI) and using Ontology such as WordNet, HowNet, thesaurus and HNC concept primitive woods are all feature merger [11][12] . With machine learning methods, we can get the parameters of the classifier and decide an incoming document belong to which category. Simple vector distance, naïve Bayes, k-Nearest Neighbor (k-NN), Support Vector Machine (SVM), Artificial Neural Network (ANN), Decision Tree, Voting and so on are all applied methods [13] . VSM is a mature model of text representation for categorization. Words are commonly used as dimensions of feature space of VSM, but words only provide little semantic information. Sentence category theory is an important component of HNC theory and can provide abundant information about meaning, structure and style of a sentence. We use sentence categories as dimensions of feature space, reduce the dimensionality by dividing mixed sentence categories and reform the weights by tfc-weighting algorithm. By simple vector distance calculation, we can get the parameters of the classifier and execute the categorization. The average precision and recall of our classifier are acceptable and can be improved by other HNC techniques.
Research on Hypothesizing and Sorting the Eg Candidates in Chinese Semantic Parsing The global eigen semantic chunk (abbreviated as 'Eg' in the following) is the head dominant words at the top layer of a sentence. Generally the Eg roots in the head verb. However, it is possible that several verbs would appear in the sentence, like 'have also continued', 'seek, reveal and develop' and 'guides' in the example sentence: 'They have also continued, in the practice of historical activities and by making comparisons, to seek, reveal and develop the truth that guides their advance'. In Chinese sentences, it is more difficult to find out the Eg because of changeless form of Chinese character, flexible parts of speech (POS), lack of case, plural and tense. In order to get the right Eg from all possible verbs in a Chinese sentence, the approach of 'HypothesisTest' is adopted. When parsing a sentence, the parser firstly hypothesized the Eg according to some special concepts. Then the parser hypothesized the SC of the sentence according to the Eg. Finally, the parser tests the hypothesized SC of the sentence according to the knowledge-base in the computer. If all semantic chunks in the sentence are corresponded with the transcendental concepts in the knowledgebase, the SC is confirmed. Otherwise, the SC is rejected. Therefore, the first important step of parsing the SC of a sentence is to hypothesize the Eg from the possible verbs. It is necessary to find out the right Eg in a sentence by hypothesizing and sorting the Eg candidates. This paper discussed how to establish the rules for hypothesizing and sorting the Eg candidates in Chinese Sentences and how to implement it by rules in computer programs. The results of experiments on the Chinese sentences selected from true articles indicated that the approach of 'Hypothesis-Test' is valid. According to the formalized rules, the accuracy rate was 87.2% in which the right Eg was sorted into the first Eg candidate. This paper introduced a semantic parsing model constructed above a symbolic system of concepts for understanding natural language. Based on the model a sentence can be mapped into the corresponding semantic category. In order to obtain the semantic category of a sentence (SC), it is necessary to get the global eigen semantic chunk (Eg) in a sentence. The Eg is like the head verb in a sentence but has its own constitution. It is more difficult to find out the Eg in a Chinese sentence than in English. This paper puts forward an approach and rules to hypothesize and sort the Eg in a Chinese sentence. The approach is named &apos;Hypothesis-Test&apos;. The experiments on sentences in which multi-verb appeared showed that the approach was valid and the accuracy rate of the semantic parsing system was 87.2%.
Mining the Relation between Sentiment Expression and Target Using Dependency of Words Opinion mining is also called sentiment analysis. It is to analyze whether an author expresses a favorable or unfavorable sentiment for a specific target. Now, it has been extensively used in text filtering, public opinion tracking, customer relation management, etc. [8], [9], [12], [19]. For example, during mining the opinion about a product of customers, we need to find what the customers remark on.It means we should not only extract the sentiment expressions, but also find the targets. Therefore, it needs to mine the relation between sentiment expression and target [9]. This paper will focus on it.The rest of the paper is arranged as following. Section 2 introduces some previous work. Section 3 provides how to gain subjective words. Section 4 introduces the dependency relation between words.Section 5 and section 6 put forward our method on mining the relation. Experiment and result are discussed in the last section. In some applications of opinion mining in text, it is important to distinguish what an author is talking about from the subjective stance towards the topic. Therefore, it needs to find the relation between sentiment expression and target. This paper proposes a novel method based on dependency grammars to mine the relation. In this method, the process of mining the relations is turned into a procedure of searching in the dependency tree of a sentence. The result of our experiments shows that word dependency relation based methods is more flexible and effective than some previous surface patterns based methods.
A Constraint-based Morphological Analyzer for Concatenative and Non-concatenative Morphology Morphology is the area of linguistics that deals with the internal structure of words -how they are systematically formed from smaller units. Computational morphology deals with the processing of words and word forms in both written (graphemic) and spoken (phonemic) form. A morpheme is the smallest meaningful unit of a language. It could be a word stem or affixes. A free morpheme can stand alone, while bound morpheme functions only as part of a word. Affixation is the process of adding a bound morpheme to a free morpheme attached in front (prefix) or at the end (suffix) of a stem. The combination of a prefix and a suffix is called a circumfix. The infix is an affix where the placement is defined in terms of some phonological conditions. Reduplication requires copying of some portion of the stem. It can be complete or partial (prefixal, infixal, or suffixal reduplications). Infixation and reduplication are non-concatenative morphological processes.Much of the work in Philippine linguistics focused on the Tagalog language [1]. Tagalog language exhibits a rather complex verbal system. A single verb may contain reduplicated syllables, prefixation or suffixation and at the same time infixation. Reduplication in Tagalog can be partial or full. There could also be a combination of such phenomena in one word especially seen in Tagalog verbs. For instance, the word pinanglilibang-libang has root word libang and prefix pang (with infix in), partial reduplication of syllable li and full word reduplication of libang.Research on computational morphology has been predominantly on concatenative morphology and on finite-state models of morphotactics [2]. Although attempts were made to handle non-concatenative phenomena, it has been on a limited capacity only [3]. Optimality Theory is a phonological approach that is proven effective in handling non-concatenative morphology [4]. It has been applied for the generation process and never been used in morphological analysis. Morphological analysis in the current methods, such as finite-state and unification-based, are predominantly effective for handling concatenative morphology (e.g. prefixation and suffixation), although some of these techniques can also handle limited non-concatenative phenomena (e.g. infixation and partial and full-stem reduplication). A constraint-based method to perform morphological analysis that handles both concatenative and non-concatenative morphological phenomena is presented, based on the optimality theory framework and the two-level morphology rule representation. Although optimality theory has been proven effective in handling non-concatenative phenomena, it has been applied for the generation process, and in this study, it has been shown to be effective also in morphological analysis. The method was tested on 1,600 Tagalog verb forms (having 3 to 7 syllables) from 50 Tagalog roots which exhibit both concatenative and non-concatenative morphology. The resulting method is able to produce the correct underlying forms of the surface forms of 96% of the test data, having a 4% error, which is attributed to d-r alternation.
Statistical Survey of Monophthong Formants in Mandarin for Students Being Trained as Broadcasters There have been researchers on the monophthong formants characteristics of Mandarin (Putonghua) since 1960's [1][2] [3] . Because the number of the speech samples was limited, the results were insignificant in statistics and can not be used to describe the group characteristics of the monophthong formants of Mandarin.The computer aided Putonghua Shuiping Ceshi (Mandarin level testing) technique has been studied in Communication University of China since 1996. To develop computer aided PSC technique, it needs a statistical survey of the Mandarin speech characteristics with a large number of samples.Using the Mandarin speech database recorded in the Communication University of China, the monophthong formants of 212 trained female students and 126 trained male students were measured and analyzed. The result is meaningful in statistics. It should be helpful to the study of the group characteristics of Mandarin monophthong formants. It is also helpful for the study of parametric speech standard of Mandarin. To study the group characteristics of Mandarin monophthong formants, the monophthongs of 212 female students and 126 male students from the department of broadcasters were recorded. The monophthong formants were measured using LPC method. The mean value and deviation of formants were given with statistical meaning. The results show the difference from the previous measurements by other researchers decades ago.
Construction of Adverb Dictionary that Relates to Speaker Attitudes and Evaluation of Its Effectiveness Japanese adverbs are mainly classified into two kinds. The first is adverbs that indicate the state and mode of motion. The second is adverbs that indicate the psychological attitude or feeling of the speaker. [1,2].Reputation analysis 1 , which excludes adverbs from the objects being processed, has been used in previous research to handle sensitive information such as the extraction of evaluative expressions. Therefore, little research has been done to investigate the importance of adverbs in language processing. Adverbs are mostly excluded during information retrieval as stop words, because they are not part of the propositional content of sentences.Adverbs are not used in language processing such as reputation analysis or opinion extraction, because it involves the same process as information retrieval. In addition, when doing such processing, either adverb forms are not defined, or their relative frequency is not sufficiently high. Few adverbs have been extracted as important words to determine reputations in reputation analysis. We need to define adverbs manually in advance in a dictionary for reputation analysis since it is different from other evaluations of expressions in that it does not use automatic acquisition or automatic classification.We therefore constructed an adverb dictionary that describes speakers' attitudes. We found that the dictionary provides effective data for analyzing reputations.We verified how effective the dictionary we constructed was. First, we examined its coverage ratio using a Web text. Next, we verified the accuracy of the classifications using randomly selected dictionary data from the Web text. Finally, we assessed how effective the adverb dictionary was. Adverbs can express a speaker&apos;s attitude in a given situation on a specific matter. We constructed an adverb dictionary in which the attitude of the speaker is described. We also looked into whether or not adverbs could effectively be used as basic data to analyze reputations. We conducted three kinds of experiments to verify how effective and precise our dictionary was. First, we calculated the coverage ratio of the dictionary by comparing the ratios of appearances of all adverbs to the ratios of appearances of our dictionary items. Next, we attached a tag to 988 adverbs, and found that the coverage ratio of the tagged adverbs was 97.76% in the open data. Finally, we classified whether sentences were positively or negatively represented using the adverb dictionary and calculated that the accuracy of the classification was 86.5%.
An Activation-based Sentence Processing Model of English In the theoritical syntactic literature, hierarchical structures have played a central role in accounting for various phenomena. However, that does not mean that linear order has no role to play. Given that sentences are processed from left to right, it would be rather surprising if linear order played absolutely no role when native speakers make (un)acceptability judgments. In fact, some researchers (e.g. Hawkins 1994) have attempted to explain syntactic phenomena in terms of real-time processing. In this paper, we will deal with certain phenomena that could only be explained in terms of word order. Some of the linear order effects we deal with have already been pointed out in the literature, but to the best of our knowledge there exists no formalized account of the observed linear order effects. This paper reports our ongoing attempt to construct a formalized model which is based on the notion of working memory and activation value, and demonstrate how our model successfulluy accounts for the data in question. The structure of this paper is as follows: first, we summarize the kinds of linear order effects to be accounted for by our model, pointing out that they could not be accounted for in structural terms. Then we illustrate the observational generalizations in processing terms. In section 3, we propose a formalized sentence processing model and in section 4, we discuss a relation between our model and (un)acceptabilities. In section 5 we mention how our model parse sentence and in section 6, we demonstrate how our model accounts for the data in question uniformly. In section 6, we mention some remaining problems and concludes the paper. In this paper, we argue that linear order plays crucial role in an adequate account of acceptability judgment and further that the linear order effects we observe should be seen in terms of real-time processing, rather than static syntax. To capture the linear-order effects, we formulate a memory-based model that predicts fine-grained degrees of acceptability.
Platform for Full-Syntax Grammar Development Using Meta-grammar Constructs Full parsing of natural language sentences is a complicated task that provides a transition from unstructured text into structural information suitable for all forms of information retrieval. Any kind of higher level language understanding and/or semantic processing must rely on the results of syntactic parsing. The quality of state-of-the-art syntactic parsers [1,2] is still not completely satisfiable even for analytical languages like English. In case of free-word-order languages like Czech, the situation is even more complicated.At the Centre for Natural Language Processing at the Faculty of Informatics, Masaryk University in Brno, a full syntactic parsing system is being developed since 1997. The core of the system, the syntactic parser synt is based on the meta-grammar formalism (e.g. [3][4][5]), which allows to specify complex sentence constituent combination rules in a maintainable way. The composition of the formalism resembles structures in Lexical functional grammars [6] -the meta-grammar rules are expanded within the guidance of combinatorial constructs plus contextual constraints and are supplemented with additional actions and agreement tests.The described full grammar development platform consists of the following components:-the Czech morphological analyser ajka [7]. This tagger provides non-disambiguated output of plain text words. The ajka tagger covers almost 400.000 Czech word lemmas, which generates over 6 million word forms. -the VerbaLex verb valency lexicon tools [8]. The contextual constraints that safeguard the syntactic analysis of free-word order language need extra lexical-semantic information about the sentence constituents. The creation of new verb valency lexicon which contains so called complex valency frames has thus become a part of the grammar development platform. -the deep syntactic parser synt, see [9][10][11]. The parser development concentrates on high coverage on general corpus sentences (&gt; 90 %). The parser is able to work with several parsing algorithms (GLR parsing, top-down, bottom-up and headdriven chart parsing). The synt parser also introduced a new variant of the headdriven technique, the head-driven dependent dot move [12] algorithm. This parsing technique allows to parse natural language sentences within median time of less than 0.1s/sentence. -the user interface for linguists -the Grammar Development Workbench, GDW [13].The parser synt provides a command line interface, which is suitable for all forms of batch processing. However, the grammar development conducted by linguistic experts combines working with synt input parameters (the meta-grammar, the corpus text, parameters guiding the analysis) as well as thorough studying of synt outputs (tagged sentences, syntactic or dependency trees, chart graphs). All these tasks can be solved via GDW.In the following sections, we describe the functionality of the parsing platform in more detailed way and demonstrate the cooperation of its components on several examples. This paper describes a combination of tools necessary for full or deep syntactic parsing of natural language-the syntactic parser synt, the graphical Grammar Development Workbench, GDW and the VerbaLex verb valency lexicon tools. We describe the development of the mentioned tools and how they integrate into one system that allows a team of experts (computational linguists as well as programmers) to cooperate on the development of grammar covering all Czech language phenomena.
The stock index forecast based on dynamic recurrent neural network trained with GA The stock market is a very complicated nonlinear dynamic system, it has both the high income and high risk properties. So the forecast of stock market trend has been always paid attention to by stockholders and invest organization. However, because of the high nonlinearity of the stock market, it is difficult to reveal the inside law by the traditional forecast methods, so we are not satisfied with many of the applied effect for forecast analysis method [1] .In recent years, the rapid development of computer and artificial intelligence technology provide many new technology methods for the modeling and forecast of the stock market [1] . The neural network gains wide application in the aspect of nonlinear forecast due to its broad adaptive and learning ability. There is many neural networks applied in forecasting stock price, at present, the most widely used neural network is BP NN, but BPNN has many shortcomings such as the slow learning rate, large calculate amount, easy to get into local minimum and bigger randomicity and so on. This affects the predicted results of the stock price. RBF neural networks is also a very popular method to predict stock market, this network has better calculational and spreading abilities, it also has stronger nonlinear mapped ability [2] . But the stock market is not only with nonlinearity but also chaos, and it is a dynamic system related to time (time is a independent variable). Therefore the network for predicting itself is a dynamic system, it requires us to introduce short-time memory function in network. According to short-time the dynamic neural network can be divided into time delay NN and local or whole feedback NN. This paper uses an improved Elman dynamic recurrent neural network model [3][4] to carry modeling forecast for stock index, this neural network has better effect in dealing with time varying information aspect, the results show that using it to predict time varying stock index will have satisfactory effect. This paper use dynamic recurrent neural network to describe mathematics model, compared to feedforward neural network, it conquers the defect of feedforward neural network without dynamic property. It can make the trained network possess nonlinear mapped and dynamic characteristics. We introduces genetic algorithm to adjust model constantly during learning process, it makes the network model more intelligent [5] . ：In order to forecast the stock market more accurately, according to the dynamic property for the stock market, propose the real time modeling forecast via dynamic recurrent neural network and use GA to study online, then it improves the network performance and better describes the dynamic characteristic of stock market. By forecasting Shanghai negotiable securities index, it shows better validity.
Using the Swadesh list for creating a simple common taxonomy One of the main goal of 'Developing International Standards of Language Resources for Semantic Web Applications' [16], an international project sponsored by Japan's NEDO foundation, is to implement standards of language resources that can be very robust when applied to different languages of the world. In addition, the project concentrates on Asian language resources. Hence the project plans to construct lexica of Japanese, Mandarin, Thai and Italian; and integrate them as parts of a multilingual resource linked to the original Princeton WordNet (WN) [2]. It is therefore comparable with projects such as EuroWordNet [17], in which the coherence and homogeneity across different languages remained the main issue that hampered the true inter-operability of the final resource. Since the project is exposed to these risks by its design, a priority for the project members is therefore to tackle this issue from the very beginning. For this purpose, one of the measures taken is to build jointly an upper-level ontology 1 that will play the role of a structured interlingual index. The NEDO participants are currently exploring several ways for selecting a basic vocabulary that will serve as a starting point for designing this language-independent core of the resource. This paper describes some of the preliminary experiments we are currently conducing. More precisely we are (i) using the Swadesh list [15] as a basic core vocabulary and (ii) exploring the possibilities offered by this list for creating a simple common ontology. These practical objectives confront us however with a complex discussion of contemporary linguistics, namely the relativist/universalist debate [4], and more precisely its consequences for the lexical organization. In the context of this project the existence and the nature of a common "universal" structure is a background question that we would like to contribute to answer. The long-term goal of the research described in this paper is to develop a multilingual language resource linked to the Princeton WordNet. The paper describes the experiments we are conducing for determining a basic vocabulary and for designing a language-independent core for the future resource. More precisely, in this paper we use the universality of the Swadesh list [15] for selecting it as a basic core vocabulary and we present several options for designing a minimal upper ontology underlying the list.
-0 Transformation Form of UTF-8 Based on Multilevel Mark Theory(1, 2)£¬a mixture object search method is realized, it can search various objects just like searching characters, and raise Chinese text search to word level. (3, 4) UTF-8 provides a UNIX compatible transformation format of Unicode Code System (UCS), and makes UNIX systems support multi-lingual text in a single encoding with multi-bytes. The mechanism of realizing UTF-8 influences the average length of UTF-8 and the transformation efficiency. The original form of UTF-8 £¨5£¨5, 6£©was proposed in 1992, since then not any other forms have been proposed. In the original proposal of UTF-8, the first byte of multi-byte code determines the number of bytes of the code by setting the number of bits in high order to 1 and following a 0 bit; and the high-order bit of other byte is set to 1 and following a 0 bit. So, this form can be called as 110-10 form of UTF-8. Based on Multilevel Mark Theory(1, 2)£¬110-10 form of UTF-8 can be simplified to 11-10 form of UTF-8, and 11-10 form of UTF-8 can be further simplified to 1-0 form of UTF-8. Based on Multilevel Mark Theory£¬11-10 and 1-0 transformation form of UTF-8 are proposed in this paper. The transformation between UCS and 1-0 form of UTF-8 is introduced, then, the transformation between Local Code and 1-0 Form of UTF-8 is discussed in detail.
  A small vocabulary, isolated word speech recognition system in java has been realized. In this system we have done the extraction of feature parameter, the training of speech model parameter and the recognition of the recorded speech. MFCC is used as feature parameter, HMM model is used for speech model in this article and do some study on Uighur speech recognition based on them. Keywords： Speech recognition. Hidden Markov models. Speech feature parameter. Ⅰ.Introduction Based on achieving English speech recognition system ,we study in Uighur language and Chinese language voice recognition system. We using Jbuilder and Jdk 1.4 development tools to study how to deal with the Uighur text characters. Isolated word speech recognition system, with broad application prospects, such as computer control, industrial control orders, family services, banking services, personal information Identification, personal items such as mobile communications, the application of this technology will greatly facilitate the daily lives of people. The work of Uighur language recognition system further study instructive and help.
Machine Transliteration Recently, much research has been done on machine transliteration for many language pairs, such as English/Arabic [1] and English/Korean [2]. Most of the above approaches require a pronunciation dictionary for converting a source word into a sequence of pronunciations. However, words with unknown pronunciations may cause problems for transliteration. On the other hand, much research has focused on the study of automatic bilingual lexicon construction based on bilingual corpora.Proper names and corresponding transliterations can often be found in parallel corpora or topic-related bilingual comparable corpora. However, many methods dealt with this problem based on the frequencies of words appearing in corpora, an approach which cannot be effectively applied to low-frequency words. Fung, used different approaches to create translation pairs from parallel and comparable corpora [3]. We have exploited the pattern matching method of [3] to extract transliteration pairs from English -Arabic parallel corpus and we used it as a base line method. In the present study, we present different approaches for transliteration proper noun pair&apos;s extraction from parallel corpora based on different similarity measures between the English and Romanized Arabic proper nouns under consideration. The strength of our new system is that it works well for low-frequency words. We evaluate the presented new approaches using an English-Arabic parallel corpus. Most of our results outperform previously published results in terms of precision, recall and F-Measure.
Automatic Target Word Disambiguation Using Syntactic Relationships Target word disambiguation is a task in machine translation where a decision has to be made on which of a set of alternative target-language words is the most appropriate translation of a source-language word [1], a process familiarly known as translation selection. For instance, the correct translation of the word 'wash' in Tagalog could be hilamos, hugas, laba, etc. depending on the object noun of the source verb 'wash'.Several methods have been developed for target-word disambiguation on different types of corpora using different nature of word translations. Techniques exploit monolingual corpora on either the target language (target language based) or the source language to resolve lexical ambiguities. Target language based approaches include the use of statistics on lexical relations [3], estimation of translation probability using a language model of the target language [5], [1]. Other methods exploit information from the source language for disambiguation such as distributional clustering [6].A more recent and novel approach in translation selection is the hybrid method [2] based on the "word-to-sense and sense-to-word" relationship between source word and its translations, the method selects translation through two levels: sense disambiguation of a source word and selection of a target word. Other techniques worth mentioning in this field is the use of dependency triples on an unrelated monolingual corpus to select among translations of a given verb [4]. A more recent approach exploits content-aligned bilingual corpora for phrasal translations based on monolingual similarity and translation confidence of aligned phrases of two languages [7].This research addresses resolving word translation ambiguity based on the idea of "word-to-sense and sense-to-word" relationship between source word and its translation using a bilingual dictionary and syntactic relations (subject-verb, verb-objects and adjective noun) on un-tagged, monolingual corpora in the target language with word sense disambiguation on source words. Multiple target translations are due to several meanings of source words, and various target word equivalents depending on the context of the source word. Thus, an automated approach is presented for resolving target-word selection, based on &quot;word-to-sense&quot; and &quot;sense-to-word&quot; source-translation relationships, using syntactic relationships (subject-verb, verb-object, adjective-noun). Translation selection proceeds from sense disambiguation of source words based on knowledge from a bilingual dictionary with sense profiles and word similarity measures from WordNet, and selection of a target word using statistics from a target corpus. Test results using English to Tagalog translations showed an overall 64% accuracy for selecting word translation with a standardized precision of at least 80% for generating expected translations using 200 sentences with ambiguous words (an average of 4 senses) in three categories: nouns, verbs, and adjectives, using 145,746 word pairs in syntactic relationships, extracted from target corpora (317,113 words).
Learning Translation Rules for a Bidirectional English-Filipino Machine Translator Filipino is the national language of the Philippines. The 1987 Constitution of the Philippines stated that "The national language of the Philippines is Filipino. As it evolves, it shall be further developed and enriched on the basis of existing Philippine and other languages." This characteristic poses problems for developers of Filipino machine translators.Our goal is to develop a bidirectional English Filipino machine translation system. Our approach is to use a hybrid of rule-based and example based paradigms by learning rules from examples. Several researches have incorporated machine learning such as those by [1], [2], which automatically generates these rules from bilingual corpus. This technique is most applicable for languages with limited resources such as Filipino.The first phase of development was an attempt to build an English to Filipino Machine Translator based on the Seeded Version Space Learning presented by [1]. Using the same approach, several issues were realized for the development of the second phase, the Filipino to English translation. Filipino is a changing language that poses several challenges. Our goal is to develop a bidirectional English-Filipino Machine Translation (MT) system using a hybrid approach to learn rules from examples. The first phase was an English to Filipino MT system that required several language resources. The problem lies on its dependency over the annotated grammar which is currently unavailable for Filipino, which makes reverse translation impossible. Phase 2 addresses this limitation by using information taken from English and Filipino POS Taggers. The seed rules are generated by aligning the POS tags from English and Filipino examples, including their constraints. To perform compositionality, the system deduces the constituent labels by using the longest adjacent POS tags found in both the English and Filipino rule. The system groups together similar rules and generalizes it to encompass a wider range of unseen examples.
A Visualization method for machine translation evaluation results Machine translation evaluation activities have accompanied the MT research and system development. The ALPAC report is the first historical MT evaluation activity. In 1990s, because of the prosperity of machine translation research, great amount of evaluation activities are carried out according to the intelligibility and fidelity metrics followed DARPA methodology. The ISLE takes a software engineering point of view, which focuses on how an MT system serves the follow-on human processing rather than on what it is unlikely to do well.Since manual evaluation is labor-intensive and time-consuming, many researchers are making efforts towards reliable automatic MT evaluation methods. A problem is that the methods can not be characteristic by its precision and recall as in other natural language processing activities such as POS tagging or phrase identification [4] . A new quality system is necessary. This paper aims for the better illustration of machine translation evaluation results and ensemble of different evaluation methods. To make it easier to understand the machine translation evaluation results, a curve is utilized to stand for the performance of a machine translation system. The position of the curve in the graph depicts the quality of the system. The upper left curve stands for higher translation quality. System clustering is made and its dendrogram illustrates the quality difference between systems. These two methods visualize the machine translation evaluation results.
Language Model Based on Word Clustering Statistical language modeling [5] (SLM) has been successfully applied to many domains such as speech recognition, machine translation, information retrieval, and spoken language understanding. The dominant technology in SLM is n-gram models. Typically n-gram models are trained on very large corpora. In constructing n-gram models, we always face two problems. First, for a general domain model, large amounts of training data can lead to models that are too large for realistic applications. On the other hand, for specific domains, n-gram models usually suffer from the data sparseness problem, because large amounts of domain-specific data are usually not available.When n-gram models are used, we can define clusters for similar words in a corpus. We thus augment word-based n-gram models to cluster-based n-gram models. This has been demonstrated as an effective way to handle the data sparseness problem. There are many different clustering algorithms [1] , but they can be classified into a few basic types. There are two types of structures produced by clustering algorithms, hierarchical clustering and flat or non-hierarchical clustering. Flat clustering simply consists of a certain number of clusters and the relation between clusters is often undetermined. Most algorithms produce flat clusters and improve them by iterating a reallocation operation that reassigns objects. The tree of a hierarchical clustering can be produced either bottom-up, by starting with the individual objects and grouping the most similar ones, or top-down, whereby one starts with all the objects and divides them into groups so as to maximize within-group similarity. This paper proposes a bottom-up hierarchical clustering algorithm based on similarity.This paper also discusses the problem of the design of vari-gram [3] . The cluster n-gram model has been demonstrated as an effective way to deal with the data sparseness problem which exists in the word n-gram model, but this method sacrifice some predictable ability. We can improve the system predict performance by enhancing n, because the number of clusters is much less the number of words. However, it also has a few drawbacks, including: exponential growth in parameters as a function of n, which Increases the storage and computer costs of the recognition search; the problem of sparse data for parameter estimation; and the inability to model long distance relationships. To solve this problem, people propose vari-gram language model, in which model, the value of n varies according to the different ability that history words predict for the current word. But the construction of vari-gram model is a very complicated issue. In this paper, an Absolute Weighted Difference method is presented and is used to construct vari-gram model which has good predictable ability.The classic task of language modeling is to predict the next word given the previous words. The n-gram model is the usual approach. It states the task of predicting the next of word as attempting to estimate the conditional probability.An estimate of the probabilityis given by Equation (2) The most common metric for evaluating a language model is perplexity. Formally, the word perplexity w PP of a model is the reciprocal of the geometric average probability assigned by the test set. It is defined [5] as:Where w N is the total number of words in the test set. Category-based statistic language model is an important method to solve the problem of sparse data. But there are two bottlenecks about this model: (1) the problem of word clustering, it is hard to find a suitable clustering method that has good performance and not large amount of computation. (2) class-based method always loses some prediction ability to adapt the text of different domain. The authors try to solve above problems in this paper. This paper presents a definition of word similarity by utilizing mutual information. Based on word similarity, this paper gives the definition of word set similarity. Experiments show that word clustering algorithm based on similarity is better than conventional greedy clustering method in speed and performance. At the same time, this paper presents a new method to create the vari-gram model.
Make Word Sense Disambiguation in EBMT Practical Example based machine translation (EBMT [1] ) is a method of translation by the principle of analogy.When given an input sentence, the EBMT system first retrieval the bilingual aligned corpora and select one or some example sentences as translation templates whose source sentence parts are similar to the given input sentences. And system then modifies the target parts of these translation templates to get finial translation result. These modification operations can be classified as: delete, replace, and insert.There are many methods for retrieving some translation templates whose source parts are similar to the input sentences. When we have finished this step and begin to the second step: modify the translation templates, the ambiguity problem comes. For example, for a Chinese-Japanese EBMT system, the input Chinese sentence is and what's we retrieved translation template is: , we can see there is a Chinese word doesn't exist in the source part of the translation template, to get the right translation of the input sentence, we must insert the sense of in a proper place of the target part of the translation template (the proper insert place can be gotten by comparing two string). If from a bilingual dictionary we know, there are different senses for the Chinese word , which sense should we choose? The same situation will occur also in replacement operation. This is the problem of word sense disambiguation EBMT. Traditional word sense disambiguation methods don't work here. They will neither meet a knowledge bottleneck [3][4][5] , nor lower performance [2] . And it can't satisfy the requirement in an EBMT system.Here we propose a new word sense disambiguation method, which is based on the N-gram language model. We think a proper word sense must be the sense that makes the whole sentence looks frequent, and we use N-gram language model [6,7] to measure this fluency. That is to say in our disambiguation method, we select the word sense that make the whole sentence fluent most.Our paper is organized as following: in section 2, we give the detail description of our disambiguation method, and its computation algorithm; in section 3, are our experiments; and at last, in section 4, we drew our conclusions. In an EBMT system, we will meet the word sense disambiguation problem. The disambiguation methods used at present can&apos;t be used easily in EBMT. We propose a new method for word sense disambiguation in EBMT: it is based on a language mode of the target language. Its main idea is that a proper word sense can make the whole sentence fluent. We use a language model to measure this fluency, and use dynamic programming method to compute the proper words sense sequence in EBMT. It has the virtues of easily being used, and doesn&apos;t need extra lingual knowledge, besides, the general performance of it can be improved by using more target language resource to train. And experiment shows our method works well.
Implementing a Japanese Semantic Parser Based on Glue Approach The glue approach to semantic interpretation (glue semantics) provides the syntax-semantics interface where its semantic composition is represented as a linear logic derivation [1,2]. Glue semantics realizes semantic ambiguity with multiple proofs from the same set of premises corresponding to syntactic items in a sentence, and then it requires no special machineries such as storages. Employing linear logic, it guarantees the semantic completeness and coherence of its results. In other words, all of the requirements of the premises are satisfied and no unused premises remain in linear logic derivations. In addition, glue semantics is not restricted to any specific formalism although it was primarily developed for Lexical-Functional Grammar (LFG), and it can be used with both various syntactic formalisms and meaning representations [3]. I will present the implementation of a Japanese semantic parser based on glue approach. The implementation of glue semantics for English was constructed and incorporated into real-world application systems [4,5]. However, the theory has not been applied to other languages than English in broad coverage. The Japanese parser is constructed in the same framework as the English parser. Both parsers are designed as domain-dependent, and they produce fully scoped higher-order intensional logical expressions of the kind familiar to traditional formal semanticists as the meanings of sentences. The Japanese parser, as well as the English counterpart, aims to cover wide-ranging real texts, and is built on top of an industrial broad-coverage Japanese grammar based on LFG [6]. The Japanese grammar has been developed through the Parallel Grammar (ParGram) project. The ParGram project uses the Xerox Linguistic Environment (XLE), an efficient parser and grammar development platform based on LFG, and aims to test the formalism for its universality and coverage limitations and see how far parallelism can be maintained across languages [7]. This paper describes the implementation of a Japanese semantic parser based on glue approach. The parser is designed as domain-independent, and produces fully scoped higher-order in-tensional logical expressions, coping with semantically ambiguous sentences without storage mechanism. It is constructed from an English semantic parser on top of Lexical-Functional Grammar (LFG), and it attains broad coverage through relatively little construction effort, thanks to the parallelism of the LFG grammars. I outline the parser, and I present the analyses of Japanese idio-syncratic expressions including floated numerical quantifiers, showing the distinct readings of dis-tributive and cumulative, as well as a double-subject construction and focus particles. I also explain the analyses of expressions that are syntactically parallel but semantically distinct, such as relative tense in subordinate clauses. 1 Introduction The glue approach to semantic interpretation (glue semantics) provides the syntax-semantics interface where its semantic composition is represented as a linear logic derivation [1, 2]. Glue semantics realizes semantic ambiguity with multiple proofs from the same set of premises corresponding to syntactic items in a sentence, and then it requires no special machineries such as storages. Employing linear logic, it guarantees the semantic completeness and coherence of its results. In other words, all of the requirements of the premises are satisfied and no unused premises remain in linear logic derivations. In addition , glue semantics is not restricted to any specific formalism although it was primarily developed for Lexical-Functional Grammar (LFG), and it can be used with both various syntactic formalisms and meaning representations [3]. I will present the implementation of a Japanese semantic parser based on glue approach. The implementation of glue semantics for English was constructed and incorporated into real-world application systems [4, 5]. However, the theory has not been applied to other languages than English in broad coverage. The Japanese parser is constructed in the same framework as the English parser. Both parsers are designed as domain-dependent, and they produce fully scoped higher-order intensional logical expressions of the kind familiar to traditional formal semanticists as the meanings of sentences. The Japanese parser, as well as the English counterpart, aims to cover wide-ranging real texts, and is built on top of an industrial broad-coverage Japanese grammar based on LFG [6]. The Japanese grammar has been developed through the Parallel Grammar (ParGram) project. The ParGram project uses the Xerox Linguistic Environment (XLE), an efficient parser and grammar development platform based on LFG, and aims to test the formalism for its universality and coverage limitations and see how far parallelism can be maintained across languages [7].
A Chinese Automatic Text Summarization system for mobile devices As the news websites grow rapidly, the on-line news becomes large and repeatedly, even some news websites quote the contents of other news websites [1]. It is more and more difficult for the reader to skim over websites and get a quick idea of their content. Therefore, based on reporting intensity and updating speed, we select several Chinese news Home sites as the resource of news collection.In the last few decades, researchers have brought up a number of automatic summarization methods [2][3] [4][5] [6]. Basically these methods fall into two categories. One is called human-imitation approach or generative approach, which constructs the summary based on the structure and semantics of the source text. The other group of approaches tries to extract important sentences based on information acquired from the surface clues of the text. This approach is termed the extraction approach. Therefore, at the current stage, it is better to use statistical information in automatic extraction. The method proposed in this paper also extracts important sentences based on textual statistical information. But our method not only uses statistical information but also employs text structural features to improve the performance of important sentences extraction. A large amount of on-line information and lengthiness information can&apos;t fit for the mobile devices. In order to save this problem, we propose a method which collects original news text from on-line information and extracts summary sentences from them automatically. On this basis, we adopt WML(Wireless Markup Language) to build a news website for mobile devices browsing through the news summary. The system is mainly made up by Automatic News Collection and Auto Text Summarization. Our experimental results proved the effectiveness of the means.
Are Topic Constructions Licensed by A&apos; Movement in Mandarin Chinese?-A Preliminary Study It is conventionally suggested that two types of topics exist in Mandarin Chinese: dangling topics (i.e.Chinese-style topics) and non-dangling topics. Non-dangling topics are accounted for in terms of an antecedent-pronominal analysis (e.g. by Xu and Langendeon 1985), but are interpreted as an instantiation of A'-movement (e.g. by Chen 1995, cf. Williams 1980. As for dangling topics, some studies treat them as semantically licensed (Li and Thompson 1976, Chafe 1976, Xu and Langendeon 1985, Tsao 1990, while some analyses propose that so-called dangling topics are syntactically licensed as non-dangling topics are (Shi 2000, Huang and Ting 2006, cf. Chen 1995. Given the controversy, the current study attempts to imply that A' movement probably could account for both types of topics. Xu and Langendeon (1985) observe that topicalization in Mandarin is not subject to the subjacency condition, as illustrated in (1) where the EC (e i ) crosses more than one bounding node, i.e. NP and S (or S') but the example is acceptable. They advance that the EC (e i ) in the comment clause, as in (1), should be interpreted as a non-overt pronominal whose antecedent is the coindexed topic (zhe-ben shu i 'this book') rather than as a variable. Topic structures are conventionally classified into two types in Mandarin Chinese, namely dangling and non-dangling topics. The present study concurs that non-dangling topics might be licensed by A&apos; movement (e.g. Chen 1995), in the sense that they might be interpreted as an operator A&apos;-binding their co-indexed variable, usually an empty category, in a corresponding comment clause. As regards so-called dangling topics, this study favors the traditional postulation of an A/A&apos; position and also lends support to the A&apos; movement analysis (e.g. Huang and Ting 2006).
Word Sense Disambiguation and Human Intuition for Semantic Classification on Homonyms * * This work was supported by the Second Brain Korea 21 Project.Word Sense Disambiguation (WSD) is the basis of the natural language processing and other AI-related problems (Ide and Vernois 1998). Much of WSD mainly appeals to efficient computational techniques or some interesting discord in human annotators' classifications. In previous studies, human intuitive computational model on the sense classification is explained on the basis of estimation or probabilistic model ( Lapata and Lascarides 2003;Lapata and Brew 2004). However, few of WSD work, if any, has addressed the issue of both linguistic and computational mechanism of the human intuition.This paper is a psycholinguistic research on the computational model of human sense classifications. The well-known Distributional hypothesis (Harris 1964) claims that a sense of an ambiguous word is dependent on collocations around it. In other words, an analysis on the collocations elucidates the sense of their keyword(s). This also includes either statistical or logical behaviors of collocations in the context. Computational model is to predict such behaviors of collocating words. We, in this paper, compare three different models, one from each of the three bases, namely, logic, estimations, and probability. The particular models that are chosen are the Boolean model, the probabilistic model, and the probabilistic inference model. Each model also consists of several different versions of WSD methods. Among them, we choose the Boolean search, the Maximum Likelihood Estimation (MLE), and the naïve Bayesian classifier. The reason we choose those kinds of WSD methods is that they have previously been used for WSD technique and sense classification model (Ide and Vernois 1998, Lapata and Lascarides 2003, Lapata and Brew 2004, and thus have been proven to be relevant.We conducted an experiment to observe the human intuition which provides us with clues to choice of the computational model. We investigate the correlation between each model's prediction and the experimental result. Also, we compare the decision made by each model with the sense classification made by humans.The rest of the paper is composed of three parts: Section 2 is about three computational models. Section 3 describes the design of the experiment. Section 4 deals with the discussions. This paper reports a psycholinguistic research for the human intuition on the sense classification. The goal of this research is to find a computational model that fits best with our experiments on human intuition. In this regard, we compare three different computational models; the Boolean model, the probabilistic model, and the probabilistic inference model. We first measured the values of each models found in the semantically annotated Sejong corpus. Then the experimental result was compared with the values in the initial measurements. Kappa statistics supports that this agreement experiment is homogeneously coincidental. The Pearson correlation coefficient test shows that the Boolean model is strongly correlated with the human intuition.
  
An evaluation of the Hand-held Electronic Dictionaries Used by Chinese EFL Learners 1 Background of the Study -What the Reviews Say 1.1 Students&apos; Ownership of Electronic Dictionaries  This paper compares four hand-held electronic dictionaries with two paper-printed versions. The results show that the four different pocket electronic dictionaries contain less information than their paper-printed counterparts. Based on the results of the study, some pedagogical implications are drawn. Due to rapid development in technology, the electronic dictionary is no longer a novelty for FL learners around the world. The tremendous improvement in technology has being overwhelming dictionary users with a variety of electronic dictionaries. With a &apos;scanning dictionary&apos;, users may simply scan the word with the dictionary which looks like a pen and get the translation on the screen in no time. Learners may even get the translation of a word by just speaking to a &apos;phonetic-access dictionary&apos;. (Koren, 1997) In China, the pocket/hand-held electronic dictionary is very popular among FL learners. In Nesi&apos;s (2002) survey on dictionary use by international students in a British university, he observed students from Asia and the Middle East using pocket electronic dictionaries frequently. This was confirmed by a Chinese researcher, Deng. In his (2005) survey, 70% of the subjects use pocket electronic dictionaries. The results of the two surveys show that Chinese students at home and abroad have a positive attitude to pocket electronic dictionaries and rely very much on them in their learning.
Scalable Deep Linguistic Processing: Mind the Lexical Gap * Over recent years, computational linguistics has benefitted considerably from advances in statistical modelling and machine learning, culminating in methods capable of deeper, more accurate automatic analysis, over a wider range of languages. Implicit in much of this work, however, has been the existence of deep language resources (DLR hereafter), that is resources which encode precise symbolic linguistic knowledge. DLRs aim either to capture a particular feature of a language, such as verb argument structure (e.g. COMLEX ( Grishman et al., 1994) or PropBank ( Palmer et al., 2005)) or lexical semantics (e.g. WordNet (Fellbaum, 1998) or FrameNet ( Baker et al., 1998)), or alternatively to model a language in its entirety, in the form of a precision grammar (e.g. the English Resource Grammar (Flickinger, 2002), various ParGram grammars ( Butt et al., 2002) or CCGBank ( Hockenmaier and Steedman, 2007)).In line with Baldwin (2005a), we consider the development of DLRs to be made up of two basic tasks: (1) design of a data representation to systematically capture the generalisations and idiosyncracies of the dataset of interest (system design); and (2) classification of data items according to the predefined data representation (data classification). In the case of a deep grammar, for example, system design encompasses the construction of the system of lexical types, templates, and/or phrase structure rules, and data classification corresponds to the determination of the lexical type(s) each individual lexeme conforms to. Naturally, the two tasks are tightly integrated, and actual DLR construction involves successive iterations over the design of analyses (with the data classification informing the system design), and the application of those analyses over data (with the system design informing the data classification).One common problem pervading all DLRs is that of coverage, which we define as the proportion of relevant data points in a representative text that are adequately described in a given DLR. Based on our bifurcation of the DLR development process, lack of coverage is caused by either: (a) deficiencies in the system design, e.g. a precision grammar not containing an analysis of a given construction, or a lexical semantic DLR not capturing a particular verb frame type; or (b) deficiencies in data classification, that is certain words simply not having been classified, or being only partially described in the DLR. This arises through simple resource constraints on DLR development, domain effects, or the DLR not capturing generalisations effectively (e.g. not supporting conversion between noun countabilities).While system design is a highly specialised, manually-intensive task with little scope for automation to battle the effects of coverage, 1 the data classification lends itself readily to automation, particularly once the system design is relatively mature and the DLR in question has been populated with seed instances. The focus of this paper is the task of automated data classification, or deep lexical acquisition (DLA), in the context of maximising the lexical coverage of a given DLR.The purpose of this paper is to give a brief introduction into the current state of DLA, centring around precision grammars. As part of this, we present a classification of DLA research along three axes: general-purpose vs. targeted, in vitro vs. in vivo, and token-vs. type-based. Note that while the bulk of our examples and references relate to English, all claims about the nature of DLA are intended to be language-inspecific.DLA research falls into two broad categories: general-purpose DLA and targeted DLA. Generalpurpose DLA is a generic approach to DLA which is applicable to any DLR, e.g. in learning the senses for a given word in a wordnet, or the lexical types (or equivalent) for a given lexical entry in a precision grammar. Targeted DLA, on the other hand, takes the form of a dedicated expert system or classifier for a specific lexical feature.DLA methods generally use preprocessing or feature extraction to model lexical items, providing another basis for classification according to the relationship between the preprocessing/feature extraction method and the target DLR: in vitro methods use secondary lexical resource(s) to model lexical items, whereas in vivo methods are embedded within the target DLR and extract features directly from it.Finally, DLA methods can be classified as either type-or token-based. Type-based methods make predictions about lexical items out of context, e.g. about whether a given verb can participate in various subcategorisation frames, while token-based methods make predictions about a lexical item in a given context, e.g. about the precise subcategorisation associated with a given lexical item in a given context.In the remainder of this paper, we first describe the recent confluence of DLR development and statistical natural language processing (Section 2.). We then briefly outline and review each of general-purpose and targeted DLA (Section 3.), in vitro and in vivo DLA (Section 4.), and typeand token-based DLA (Section 5.). Coverage has been a constant thorn in the side of deployed deep linguistic processing applications, largely because of the difficulty in constructing, maintaining and domain-tuning the complex lexicons that they rely on. This paper reviews various strands of research on deep lexical acquisition (DLA), i.e. the (semi-)automatic creation of linguistically-rich language resources, particularly from the viewpoint of DLA for precision grammars.
The Semantics of Semantic Annotation * The most interesting and challenging computer applications of natural language require the exploitation of semantic information. This is for example the case for intelligent spoken or multimodal dialogue systems, and for interactive question answering given a data base of natural language texts. Efforts to make computers exploit the semantics of utterances and texts have so far met with very limited success, however. This has two fundamental reasons: the ambiguity problem and the robustness problem.1. The ambiguity problem: Computing the meaning conveyed by natural language expressions requires the availabilty of a vast amount and wide range of context information, such as knowledge of the domain of discourse, knowledge of the interactive (or other) setting in which language is used, knowledge of what occurred earlier in the discourse, knowledge of what nonlinguistic sources of information are available (e.g. shared visual context), and so on. In the absence of such information, natural language expressions are massively ambiguous; it has been estimated, for example, that a printed sentence of average length in Dutch or English has more than half a million possible readings when considered in isolation (Bunt and Muskens, 1999). Well-established methods of formal semantics, such as Montague-style or DRT-style semantics, capitalize on the context-independent interpretation of syntactic structure and function words, and have hardly any devices for taking context information into account. It may be noted that the ambiguity problem that arises in context-independent interpretation of natural language is to some extent artificial: it is in part caused by the aim to derive 'disambiguated interpretations' within the framework of a formal logical system, which brings a level of granularity which forces one to deal with issues that are often irrelevant in practical situations.2. The robustness problem: The methods of formal semantics tend not to be robust enough when applied to practical uses of natural language, such as spoken dialogue, on-line chatting, sms messages, or dynamic web pages. This is because linguistic semantic theories have been developed as components of grammatical theories, and have been informed primarily by the analysis of carefully constructed, grammatically perfect sentences rather than by the informal, elastic way in which language is used in spoken and multimodal interaction, which commonly involves nonsentential and grammatically irregular utterances that work semantically and pragmatically quite well.In this paper I explore a novel approach to the computation of semantic information, namely through semantic annotation. I will argue and illustrate that this approach may make it possible to address both the amibguity problem and the robustness problem successfully. I will argue that the approach offers the exciting perspective of a practial, incremental approach to the automatic use of semantic information from natural language; a use which may become more and more powerful as more sophisticated semantic annotation tools and methods are developed. This is a speculative paper, describing a recently started effort to give a formal semantics to semantic annotation schemes. Semantic annotations are intended to capture certain semantic information in a text, which means that it only makes sense to use semantic annotations if these have a well-defined semantics. In practice, however, semantic annotation schemes are used that lack any formal semantics. In this paper we outline how existing approaches to the annotation of temporal information, semantic roles, and reference relations can be integrated in a single XML-based format and can be given a formal semantics by translating them into second-order logic. This is argued to offer an incremental aproach to the incorporation of semantic information in natural language processing that does not suffer from the problems of ambiguity and lack of robustness that are common to traditional approaches to computational semantics.
Deep Lexical Semantics: The Ontological Ascent * If we are going to have programs that understand language, we will have to encode what words mean. Since words refer to the world, their definitions will have to be in terms of some underlying theory of the world. We will therefore have to construct that theory, and do so in a way that reflects the ontology that is implicit in natural language.There are wrong ways to go about this enterprise. For example, we could take our underlying theory to be quantum mechanics and attempt to define, say, verbs of motion in terms of the primitives provided by that theory. A less obviously wrong approach, and one that has sometimes been tried, is to adopt Euclidean 3-space as the underlying model of space and attempt to define, say, spatial prepostions in terms of that.In this paper, I propose a general structure for a different underlying conceptualization of the world-one that should be particularly well suited to language. It consists of a set of core theories of a very abstract character. These theories are too abstract to impose many constraints on the entities and situations they are applied to. In fact, the reader may complain that they apply to anything. But the main purpose of the core theories is to provide the basis for a rich vocabulary for talking about entities and situations. The fact that the core theories apply so widely means that they provide a great many domains of discourse with a rich vocabulary.These will not be just any core theories, but theories that find their place in a schema I call the "Ontological Ascent". They include a theory of systems or composite entities and the figureground relation, which subsumes a theory of scales; a theory of change of state; a theory of causality, which provides support for a theory of goal-directed behavior; and very importantly a theory of shifts in granularity. The Ontological Ascent is described in Section 2. Then in the subsequent sections I sketch the outlines of each of the above theories.The enterprise is therefore to axiomatize these core theories in as clean a fashion as possible, and then to define, or at least characterize, various words in terms of predicates supplied by these core theories. For example, a core theory of scales will provide axioms involving predicates such as scale, &lt;, subscale, top, bottom, and at. Then, at the "lexical periphery" we will be able to define the rather complex word "range" by an axiom such as the following:(∀ x, y, z)range(x, y, z) ≡ (∃ s, s 1 , u 1 , u 2 )scale(s) ∧ subscale(s 1 , s) ∧ bottom(y, s 1 ) ∧ top(z, s 1 ) ∧ u 1 ∈ x ∧ at(u 1 , y) ∧ u 2 ∈ x ∧ at(u 2 , z) ∧ (∀ u ∈ x)(∃ v ∈ s 1 )at(u, v)That is, x ranges from y to z if and only if there is a scale s with a subscale s 1 whose bottom is y and whose top is z, such that some member u 1 of x is at y, some member u 2 of x is at z, and every member u of x is at some point v in s 1 . Many things can be conceptualized as scales, and when this is done, a large vocabulary, including the word "range", becomes available. Two methodological principles should be mentioned first. Above, I said "define, or at least characterize, various words". In general, we cannot hope to find definitions for words. That is, for very few words p will we find necessary and sufficient conditions, giving us axioms of the sortRather, we will find many necessary conditions and many sufficient conditions.However, the accumulation of enough such axioms will tightly constrain the possible interpretations of the predicate, and hence the meaning of the word.The second methodological point is that we need to be careful how we use an argument from the "naturalness" of an expression. Not all expressions that will be allowed by our core theories will sound natural. Our knowledge of language consists of thousands of very specific conventions, each of which has a rationale in terms of core theories. But not everything that has a rationale has been conventionalized. Conventional expressions sound natural. Other expressions with a rationale are interpretable, but may not sound natural. For example, it is conventional to say "at work" and "in progress", and recently in corporate America, the expression "on travel" has become conventional. There is no particular reason that these expressions are better than "on work", "on progress", and "at travel". It just happens that the latter did not become conventional. The account of lexical meaning given here is intended to provide a rationale for expressions, but not to explain why one version rather than another has been conventionalized. Concepts of greater and greater complexity can be constructed by building systems of entities, by relating other entities to that system with a figure-ground relation, by embedding concepts of figure-ground in the concept of change, by embedding that in causality, and by coarsening the granularity and beginning the process over again. This process can be called the Ontological Ascent. It pervades natural language discourse, and suggests that to do lexical semantics properly, we must carefully axiomatize abstract theories of systems of entities, the figure-ground relation, change, causality, and granularity. In this paper, I outline what these theories should look like.
A Syntactic Account of the Properties of Bare Nominals in Discourse * Case markers in Korean are omissible in colloquial speech. Many previous studies of Caseless bare NPs in Korean show that subject-object asymmetries are observed in various respects. For example, as observed in the wide range of conversational data (H. Lee 2006b-c), occurrence rate of bare NPs in complement position is higher than that of bare NPs in subject positions. The grammatical contrast in (1) further shows that the distribution of bare NP subject in (1b) is not only less common but also severely restricted in canonical subject position, namely, Spec-T, in contrast to the bare NP object in (1a) in canonical object position.(1) a. Mary-ka Chelswu-(lul) manna-ss-e.Mary-Nom Chelswu-Acc meet-Past-Dec 'Mary met Chelswu.' b. Chelswu-lul Mary-*(ka) manna-ss-e.Chelswu-Acc Mary-Nom meet-Past-Dec 'Chelswu, Mary met.'It is plausible to assume that the subject Mary-ka in (1b) is "frozen" in the subject position, Spec-T, due to the scrambled object John-ul. Thus, (1b) sharply contrasts with (1a) in that Nominative Case must be marked unlike Accusative.Another subject-object asymmetry is D-linking restriction some non-Case-marked wh-phrases show. As initially noted by Ahn &amp; Cho (2006), non-Case-marked subject wh-phrase nwukwu 'who' has only D(iscourse)-linked interpretation in the sense of Pesetsky (1987), as shown in (2).(2) a. Nwukwu-∅ Yenghi-lul manna-ss-ni? who Yenghi-Acc meet-Past-Q 'Who is such that he/she met Yenghi?' (only D-linked reading is possible) b. Nwu(kwu)-ka Yenghi-lul manna-ss-ni?who-Nom Yenghi-Acc meet-Past-Q 'Who met Yenghi?' (non-D-linked reading is also possible)However, such restriction isn't observed in the case of bare object wh-phrases in (3).(3) a. Yenghi-ka nwukwu-∅ manna-ss-ni? Yenghi-Nom who meet-Past-Q (non-D-linked reading is also possible) b. Yenghi-ka nwukwu-lul manna-ss-ni?Yenghi-Nom who-Acc meet-Past-Q 'Who did Yenghi meet?' (non-D-linked reading is also possible)Interestingly, if the Case marker is absent in the scrambled object wh-phrase, only D-linked interpretation is possible, as shown in (4).(only D-linked reading is possible)The third interesting asymmetry is found with specific/non-specific contexts. In (5a), the bare subject NP is not permitted with the non-specific modifier han/etten. Note, however, that this restriction does not apply to bare NP objects. Thus, in (5b), Acc Case on the object can be freely absent with non-specific modifier.(5) a. (Yeysnal-ey) han/etten namca-*(ka) sal-ass-ta.long.time-at a/a.certain man-(Nom) live-Past-Dec '(Long time ago) there was a man lived.' b. (Yeysnal-ey) Mary-ka han/etten namca-(lul) manna-ss-ta.long.time-at M.-Nom a/a.certain man-(Acc) meet-Past-Dec '(Long time ago) Mary met a man.'Note further that overt realization of Acc Case in (5b) tends to induce a "focalized/emphatic" reading, as observed in the previous discourse studies (Jun 2005, E. Ko 2000, H. Lee 2006b, Matsuda 1995. By contrast, overt realization of Nom Case in (5a) does not necessarily give rise to a focalized interpretation, which is another instance of subject-object asymmetry of Case realization.In sum, the subject-object asymmetries, D-linking asymmetries, non-specific adjective modification with regard to non-Case-marking mentioned so far are listed in Table 1    This paper aims to correctly predict various kinds of subject-object asymmetries of morphological Case realization under the formal syntactic treatment. Case markers in Korean are omissible in colloquial speech. Previous discourse studies of Caseless bare NPs in Korean show that the information structure of zero Nominative not only differs from that of overt Nominative but it also differs from that of zero Accusative in many respects. This paper aims to provide a basis for these semantic/pragmatic properties of Caseless NPs through the syntactic difference between bare subjects and bare objects: namely, the former are left-dislocated NPs, whereas the latter form complex predicates with the subcategorizing verbs. Our analysis will account for the facts that (i) the distribution of bare subject NPs are more restricted than that of bare object NPs; (ii) bare subject NPs must be specific or topical; (iii) Acc-marked NPs in canonical position tend to be focalized.
BEYTrans: A Free Online Collaborative Wiki-Based CAT Environment Designed for Online Translation Communities * Multilingual information exchange is rapidly increasing on the Internet. A substantial part of this multilingualization is carried out by volunteers (engaged in the translation of documents, articles, reports, etc. as well as the translation/localization of computer software). Many online translation communities are formed by translators, software engineers, and in general people sharing the same motivations and aims. In view of this, various useful projects for online translation communities have emerged. Yakushite.Net is one such example. In Yakushite.Net, the system aims to improve machine translation (MT) performance through the interaction with volunteer translators. Translators contributing in turn can take advantage of MT and other functions provided by Yakushite.Net, while they contribute to the improvement of the system by augmenting its language resources (YAKUSHITE, 2007).Open environments for developing free online lexical resources also exist. Wiktionary -The linguistic companion of the huge free Wikipedia encyclopedia -is one example (WIKTIONARY, 2007). Papillon, another example, aims also to construct a large-scale, free multilingual lexical database with the help of volunteers (Mangeot, 2002). In addition, free standalone translation memory (TM) systems such as Omega-T are now available (OMEGAT, 2007).While appreciating the importance of these online environments and systems for directly or indirectly promoting the multilingual exchange of information, we recognize the lack of an integrated environment to help online translator communities in a systematic way. Against this backdrop, a free online computer-aided translation (CAT) environment, BEYTrans (Better Environment for Your TRANSlation) has been developed, and the system is now in its experimental stage. This first experimental version has two levels of functionality. The first level corresponds to the translators, who act as separate entities and need specific functionalities (translation editor, linguistic help, etc.). The second level corresponds to the community, in which translators work as an integrated entity. Both functionalities have been integrated into our system using a collaborative Wiki-based technology which provides to volunteer translators with a user-friendly environment and helps them improve translation consistency ( Schwartz et al., 2004) ( Augar et al., 2004). This paper explains the basic background, concepts and functionalities of BEYTrans. In section 2, we describe the flow of linguistic data and the interactions among translators, on the basis of which the system requirements have been identified. Section 3 describes the different functionalities that should ideally be made available to 1) individual translators and 2) translation communities. In section 4, a detailed explanation of BEYTrans and its position among existing CAT environments is given. The system is currently being used experimentally by the DEMGOL project and other communities, which is briefly examined in the final section. This paper introduces BEYTrans (Better Environment for Your TRANSlation), the first experimental environment for free online collaborative computer-aided translation. The requirements and functionalities related to individual translators and communities of translators are distinguished and described. These functionalities have been integrated in a Wiki-based complete environment, equipped with all currently possible asynchronous linguistic resources and translation aids. Functions provided by BEYTrans are also compared with existing CAT systems and ongoing experiments are discussed.
A Pregroup Analysis of Japanese Causatives Japanese causatives have been analyzed from a number of points of view: transformational, lexical, and movement approaches among others. Here, I propose a computational method for analyzing the different types of causative constructions. Our analysis is based on the notion of pregroup grammar, which has been developed as an algebraic tool to recognize grammatically well-formed sentences in natural languages (Lambek, 1999). This paper is organized as follows: section 2 introduces the pregroup formalism; section 3 describes the properties and characteristics of Japanese causatives; section 4 presents the analysis of Japanese causatives based on pregroup grammar; and finally, section 5 discusses about the implementation of a practical parser. We explore a computational algebraic approach to grammar via pregroups. We examine how the structures of Japanese causatives can be treated in the framework of a pregroup grammar. In our grammar, the dictionary assigns one or more syntactic types to each word and the grammar rules are used to infer types to strings of words. We developed a practical parser representing our pregroup grammar, which validates our analysis.
Customizing an English-Korean Machine Translation System for Patent Translation * An English-Korean machine translation system has been developed in earnest in Korea since 1996. We have applyed it to different areas such as web translation (Choi, 1999) and broadcasting subtitle translation (Choi, 2001). Recently, the natural language processing(NLP) of intellectual property documents is attracting many researchers and NLP-related companies, because NLP techniques associated with specificity of patent domain have promise for improving the translation qualtiy.It is well known that a sentence style and a dominant translation for a word vary with domains. Therefore, if the domain to be translated is fixed to patents, a adaptation of bilingual dictionary to the patent domain and a customization of natural language analyzer to the linguistic specificity of patent style would be one of effective ways to improve the translation quality of MT system. There have been studies concerned specifically with patent MT using these domainspecific advantages ( Shinmori et al., 2003;Hong et al., 2005;Kaji, 2005;Shimihata, 2005).Though intensive research has been made on patent MT for the domain-specific advantages, there still remain many issues to be tackled. We focus on the several issues that have continusely been problems in existing English-to-Korean MT systems: (1) new terminology construction, (2) patent-specific probabilities of POS tagger, (3) long and complex sentence analysis, and (4) target word selection.This paper addresses the customization of an English-Korean MT system for patent translation. The English-Korean patent MT system described in this paper is based on an English-Korean MT system developed for the web translation in a general domain. English-Korean patent MT system belongs to basically the pattern-based methodology for machine translation. It has the formalism that does English sentence analysis in which English patent-specific patterns are used, matches the English patent pattern with its Korean patent pattern, and then generates a Korean sentence from it. English-Korean patent MT system consists of an English morphological analysis module based on lexicalized HMM, an English syntactic analysis module by patternbased full parsing, a pattern-based transfer, and a Korean morphological generation.According to experience of patent attorneys, it is said that they read about 7 English patent documents to examine one Korean patent document in average. It means that they examine about 1,000,000 English patent documents for new 150,000 Korean patent documents every year. Korean patent attorneys have required any machine translation system to solve language barrier because they prefer reading Korean translated patent documents to reading English patent documents in spite of such linguistic competency as English native speaker.In this point, th development of the English-Koran patent translation system is closely related to offering of English-to-Korean patent machine translation service through Internet. KIPO (Korean Intellectual Property Office) pushes on with on-line translation service of patent documents by using MT system.The English-to-Korean patent machine translation system described in this paper was developed by ETRI (Electronics and Telecommunications Research Institute) under the auspices of the MIC (Ministry of Information and Communication, Korea) during [2005][2006]. In 2006, the patent MT system started an on-line patent MT service in IPAC (International Patent Assistance Center) under MOCIE (Ministry of Commerce, Industry and Energy) in Korea. In 2007, KIPO (Korean Intellectual Property Office) tries to launch an English-Korean patent MT service.Section 2 describes the customization processes that relate to new terminology construction, patent-specific probabilities of POS tagger, long and complex sentence analysis, and target word selection, respectively. The experimental work is presented in section 3. Lastly, in section 4, we present some conclusions. This paper addresses a method for customizing an English-to-Korean machine translation system from general domain to patent domain. The customizing method consists of following steps: 1) linguistically studying about characteristics of patent documents, 2) extracting unknown words from large patent documents and constructing large bilingual terminology, 3) extracting and constructing the patent-specific translation patterns 4) customizing the translation engine modules of the existing general MT system according to linguistic study about characteristics of patent documents, and 5) evaluating the accuracy of translation modules and the translation quality. This research was performed under the auspices of the MIC (Ministry of Information and Communication) of Korean government during 2005-2006. The translation accuracy of the customized English-Korean patent translation system is 82.43% on the average in 5 patent fields (machinery, electronics, chemistry, medicine and computer) according to the evaluation of 7 professional human translators. In 2006, the patent MT system started an on-line patent MT service in IPAC (International Patent Assistance Center) under MOCIE (Ministry of Commerce, Industry and Energy) in Korea. In 2007, KIPO (Korean Intellectual Property Office) tries to launch an English-Korean patent MT service.
A New Type of NPI Licensing Context: Evidence from French Subjunctive and NE Explétif * Despite their huge contribution, previous studies on NPI licensing context are problematic in that they treat it as a simple filter. Moreover some contexts are still unexplainable within them.My proposal in this paper can provide an answer of why they license NPIs, also explaining the unexplainable.This paper is organized as follows: In section 2, I will briefly present the previous researches.In section 3, I show that French subjunctive is equivalent to nonveridicality specially focusing on its licensing property of weak NPIs. In section 4, I give you the answer of the question "why and where ne expétif comes about". In section 5, I will propose a new type of NPI licensing context. The purpose of this paper is to propose a new type of NPI licensing context through French subjunctive and ne explétif. The distribution of NPIs on previous studies does not exactly correspond to negative function types. French subjunctive and ne expletif are good guidelines for reclassifying NPI licensing context. My classification is by a hierarchy of strength in negative force: overtly negative proposition &gt; negative entailment &gt; negative implicature. A new type of NPI licensing context is: (i) I-domain for negative implicature (ⅱ) E-domain for negative entailment and (ⅲ) overt negation.
AutoCor: A Query Based Automatic Acquisition of Corpora of Closely-related Languages * A corpus is a term used to designate a body of authentic language data that can be used as a basis for linguistic research. 1 It is also applied to a body of language texts that exist in electronic format. It is estimated that there are currently over 4 billion pages on the world wide web (WWW) covering most areas of human endeavor. And as more information are becoming electronically available on the web, we need more effective methods and techniques to access these information. To date, there has been limited effort in taking advantage of this available information on the web for building natural language resources especially for sparse languages (or minority languages) like Tagalog and other Philippine languages. Unfortunately, to manually collect and organize a language specific corpus over the Web is difficult. The process is tedious and time consuming. To add, an expert in linguistics is needed to manually determine the language where the document collected is written.A system that automatically acquires language specific documents from the Web is one good solution in corpora building. Creating such a system requires knowledge in information retrieval and natural language processing. AutoCor is a method for the automatic acquisition and classification of corpora of documents in closely-related languages. It is an extension and enhancement of CorpusBuilder, a system that automatically builds specific minority language corpora from a closed corpus, since some Tagalog documents retrieved by CorpusBuilder are actually documents in other closely-related Philippine languages. AutoCor used the query generation method odds ratio, and introduced the concept of common word pruning to differentiate between documents of closely-related Philippine languages and Tagalog. The performance of the system using with and without pruning are compared, and common word pruning was found to improve the precision of the system.
Ambiguity in the Negative V+bo NP Construction in Taiwanese The V+bo NP is a special negative construction in TSM whose exact syntactic counterpart is not found in Mandarin Chinese. It has been widely acknowledged that the post-verbal negative marker bo 'not' in the VbN may form a resultative complement with the preceding verb (e.g. Cheng 1997;Li 1996;Teng 1992). While those works have shed light on the semantic characteristics of bo, the VbN construction remains ambiguous between an episode and a generic reading that each needs to be explained. On the episode reading, bo expresses the lack of a desired result such as (1). Sentence (1) means that the agent he intended to find someone, so he did the finding-event, but failed to find out the person. On the generic reading, the VbN is taken as an association with a potential property, as in (2). It expresses that the agent he does not have the ability to do the studying-event well.(1) I chue bo lang.he find not person 'He failed in finding the person.' (2) I thak bo chhe.he study not book 'He can not study well.'The main goal of this paper is to argue that the different interpretations of VbN construction in Taiwanese Southern Min may be due to different structural positions which the post-verbal negative marker bo occupies on the ground of Zanuttini's (1997) proposal that argues for there to be different structural positions for two kinds of post-verbal negative markers, namely presuppositional versus non-presuppositional, as stated in (3).(3) a. Presuppositional negative markers, which negate a proposition that is assumed in the discourse. b. Non-presuppositional negative markers, which negate a proposition that does not have a special discourse status. (Zanuttini 1997: 99) More precisely, bo in the episode reading context corresponds to the presuppositional negative marker whereas bo in the generic reading context corresponds to the non-presuppositional negative marker.The remaining sections of this article is organized as follows. Section 2 is a summary of Zanuttini's (1997) analysis of post-verbal negative markers. Section 3 shows an overview of VbN construction and provides an analysis of the distinction between an episode and a generic reading. Section 4 briefly reviews previous study on the post-verbal negative bo. Section 5 concludes this article. This paper examines some syntactic and semantic properties of the negative construction V+bo NP (VbN) in Taiwanese Southern Min (TSM). It finds out that there are ambiguities between an episode reading and a generic reading in VbN construction which require further investigations and explanations. Therefore, the goal of this paper is to account for the ambiguities lying in the negative VbN construction.
Initialness of Sentence-final Particles in Mandarin Chinese *  This paper gives a thorough investigation into Mandarin sentence-final particles (henceforth SFPs). First I induce core grammatical functions and semantic interpretations of SFPs. Based on Rizzi&apos;s (1997) Split CP hypothesis, I make some modifications to accommodate Mandarin SFPs and map them onto separate functional heads within a proper hierarchy. I also examine some empirical evidence of head directionality and tentatively assume Mandarin C is head-initial. To explain the surface head-final order, in light of Chomsky&apos;s (2001) Phase Theory and Hsieh&apos;s (2005) revised Spell-out hypothesis, I pose a CP complement to Spec movement. Following Moro&apos;s (2000) idea, I further claim the motivation behind is to seek for antisymetry.
Co-Event Conflation for Compound Verbs in Korean * Compound verbs in Korean pose interesting problems for morphosytax and lexical semantics , in that their distribution is constrained by the interaction between morphology and syntax, which is more or less predictable from meaning. C-H Lee (2006: 129) discusses four types of compound verbs in Korean.(1) Compound verbs in Korean:a. VV type: ttwi-nolta 'to run and play' b. V-e-V type: ttut-e-nayta 'to rip off' c. V-ko-V type: mil-ko-tangkita 'to push and pull' d. V-eta-V type: tol-ata-pota 'to look back' Among these four types, the V-e-V type shows certain degree of productivity depending upon kinds of preceding and following verbs, as shown in (2). Compound verbs in Korean show properties of both syntactic phrases and lexical items. Earlier studies of compound verbs have either assumed two homonymous types, i.e. one as a syntactic phrase and the other as a lexical item, or posited some sort of transformation from a syntactic phrase into a lexical item. In this paper, I show empirical and conceptual problems for earlier studies, and present an alternative account in terms of Talmy&apos;s (2000) theory of lexicalization. Unlike Talmy who proposed [Path] conflation into [MOVE] for Korean, I suggest several types of [Co-Event] conflation; e.g. [ Co-Event Manner] conflation as in kwul-e-kata &apos;to go by rolling&apos;, [ Co-Event Concomitance] conflation as in ttal-a-kata &apos;to follow&apos;, [ Co-Event Concurrent Result] conflation as in cap-a-kata &apos;to catch somebody and go&apos;, etc. The present proposal not only places Korean compound verbs in a broader picture of cross-linguistic generalizations, but, when viewed from Jackendoff&apos;s (1997) productive vs. semi-productive morphology, provides a natural account for classifying the compounds that allow-se intervention from those that do not.
What Makes Negative Imperative So Natural for Korean [psych-adjective +-e ha-] Constructions? *  ion. Regarding Korean psych-adjectives and their-e ha-counterparts, e.i., [psych-adjective +-e ha-] constructions, what is at issue is how to capture the semantic difference and similarity between the two. Concerning this issue, one of the most controversial and difficult problems is whether the psych-construction has Action (Agency) as part of its meaning. The purpose of this paper is to solve this problem by answering the question why psych-constructions are much more natural when they are used as negative imperative than when they are used as positive imperative. First, in order to figure out why positive imperative is not allowed, we show that-e ha-adds the meaning of non-volitional action to psych-adjectives, using Jackendoff&apos;s Conceptual Semantics. Secondly, in accounting for why negative imperative is so natural, we show, with Talmy&apos;s Force Dynamics theory, what the speaker requires from the hearer is internal volitional action.
On the Syntax and Semantics of the Bound Noun Constructions: With a Computational Implementation * Bound nouns (BN) exhibit various peculiar properties, not found in common nouns in the language. For example, unlike canonical nouns, bound nouns cannot occur independently: they obligatory select a complement (determiner or sentence). This is rather unusual when considering the language allows most of the arguments to be freely omitted with proper context:(1) a. *(i) kes this thing b. *(wuli-ka motu nollass-ten) kes we-NOM all surprise-MOD BN 'the thing that we all surprised'Bound nouns also place restrictions on the types of their complements. There are at least two different types of BNs with respect to their complements: BNs selecting only a dependent clause (Type I) and those selecting either a dependent clause or a determiner phrase (Type II) (cf. Cha 2001): 1 (2) a. Type I: cheyk ('pretense'), cwul ('method'), li ('reason'), cek ('experience'), ppen ('being close to doing something'), ba ('way'), etc.b. Type II: swu ('possibility), hwu ('after'), cen ('before'), etcFor example, unlike Type II BN hwu, Type I BN li selects only a dependent sentence, as observed in the following contrast:(3) a.[John-i cam-ul ca-n]/ku hwu-ka mwusep-ta John-TOP sleep-ACC sleep-MOD/that BN-NOM not.exist-PAST-DECL 'the time after John was sleeping/after the time' b. [John-i cam-ul ca-l]/*ku li-ka eps-ess-ta John-NOM sleep-ACC sleep-MOD/that BN-NOM not.exist-PAST-DECL 'It is not possible that John was sleeping'Bound nouns also place tight restrictions on the verb forms of their sentential complement. In the noun complement construction (NCC), the dependent clause places no strict constraints on the head verb's VFORM value:(4) [John-i cam-ul ca-n/ca-ss-ta-nun] sasil John-TOP sleep-ACC sleep-PNE/sleep-PAST-DECL-PNE fact 'the fact that John slept As in (4), the head verb of the dependent clause, functioning as the complement of the factive noun sasil 'fact', can be either in a short form ca-n or in a full form can-ss-ta-nun with the declarative ending. Meanwhile, in the BN construction, the head verb of the dependent clause cannot be in a full verb form: only a short form with restricted tense is allowed:In both cases, the full dependent form (with the declarative marking) is not possible. In addition, the BN swu requires the head verb of its complement clause be in the future form, whereas chey restricts the dependent verb to be marked with the present tense.Bound nouns are also peculiar in that they impose restrictions on the types of the predicates following them. For example, the BN swu can combine only with the predicate iss-'exist' or eps-'not exist' whereas the BN li requires only the latter eps-. Also BNs like tes, ppen, and ccek occur only with ha-'do', whereas BNs like kes can be followed only by the auxiliary verb kath-ta 'seem':  Bound nouns have an additional restriction on the occurrence with case markers, which may be related to the function of the dependent clause:(7) a. Either NOM or ACC can be attached to the BN: tey ('place'), pa ('way'), ccohk ('side'), etc b. Only NOM: nawi ('degree'), li ('reason'), swu ('possibility'), ci ('whether'), etc c. Only ACC: tung ('so forth'), yang ('pretense'), cwul ('way'), chey ('pretense'), etc d. Only DEL: tus ('seem'), man ('possible'), sang ('seem'), kes ('possible'), etcThe fact that the BNs in (7b) can occur only with NOM and those in (7c) only with ACC can be expected when considering the possible predicate they can be followed. Though all the BNs can occur with a delimiter, those in (7d) allow no case markers at all: As observed here and in the literature, BNs display complex combinatory possibilities with their complements, case/delimiter markers, and predicates following them. In addition to the constructional properties that each BNC shares, each BN also has its own idiosyncratic lexical properties. This implies that to process these BN constructions with intriguing properties, we need to develop an explicit syntactic and semantic analysis. The so-called Korean BNC (bound noun construction) displays complex syntactic, semantic, and constructional properties. This paper, couched upon a constraint-based approach , two different syntactic structures for the construction with articulated lexical properties for the BNs and relevant predicates. The paper reports an implementation of this analysis in the LKB (Linguistic Knowledge Building) system and shows us that this direction is robust enough to pare relevant sentences.
Weak Connectivity in (Un)bounded Dependency Constructions * This paper attempts to deal with filler-gap mismatches found in what Pollard and Sag (1994) calls the weak unbounded dependency construction. One of the key issues in the construction is that there is a feature or category mismatch between the filler and gap, as shown in (1).(1) a. That John will help us is hard for us to rely on ___.b. That Chomsky might be wrong is hard to think of _____.Prepositions like on or of would normally be subcategorized to take a nominal complement, but in (1) the filler site for the corresponding gap is occupied by a that clause, which would pose a puzzle to a constraint-based analysis as well as to a minimalist approach. This kind of mismatch is not peculiar to the missing object construction (MOC, hereafter). The same kind of mismatch can also be found in a topicalized construction as shown in (2).(2) a. That some passives lack active counterparts, no theory can capture ____. b. *No theory can capture that some passives lack active counterparts. c. That some passives lack active counterparts, few teachers were aware of ___.The verb capture does not take as its complement a that clause in an ordinary construction as shown in (2b), but in a topicalized construction like (2a), a that clause can initiate the construction with the same verb governing the gap site. This kind of weak connectivity is not restricted to the above two constructions. It can be found in passives as well.(3) a. That some passives lack active counterparts can be captured by no theory.b. That Chomsky might be wrong has never been thought of ___.cf. No one has ever thought (*of) that Chomsky might be wrong. c. That John will help us may not be relied on.As shown in (3a) and (3b), a that clause can appear as the subject in these passives, but their active counterparts cannot take the clause-type constituent as their complement. There is also a less deviant and less pernicious kind of problem, as in the ordinary missing object construction, as shown in (4) (4) a. He/*Him is easy to please ___. b. They/*Them will take only five minutes to boil ____.There arises a case clash between the subject and the corresponding gap in (4) but this issue has been more or less elegantly handled in the framework of HPSG. It is taken care of by requiring not the total identity but a partial identity of feature specifications. However, as we will see in section 2.1, such an account is not still satisfactory. This paper will argue that the filler-gap relations in these constructions can in general be semantically accounted for, since the categorical mismatch is sometimes inevitable in syntax and should be allowed so far as other components of grammar require such mismatch. This paper argues that various kinds of displaced structures in English should be licensed by a more explicitly formulated type of rule schema in order to deal with what is called weak connectivity in English. This paper claims that the filler and the gap site cannot maintain the total identity of features but a partial overlap since the two positions need to obey the structural forces that come from occupying respective positions. One such case is the missing object construction where the subject fillers and the object gaps are to observe requirements that are imposed on the respective positions. Others include passive constructions and topicalized structures. In this paper, it is argued that the feature discrepancy comes from the different syntactic positions in which the fillers are assumed to be located before and after displacement. In order to capture this type of mismatch, syntactically relevant features are handled separately from the semantically motivated features in order to deal with the syntactically imposed requirements.
A Transformation-Based Learning Method on Generating Korean Standard Pronunciation * This paper presents a Transformation-Based Learning (TBL) method on generating Korean standard pronunciation. Previous studies on the phonological processing have been focused on the computation of the phonological rule application and the representation of the finite state automata (Johnson 1984;Kaplan and Kay 1994;Koskenniemi 1983;Bird 1995). In case of Korean computational phonology, some former researches have approached the pronunciation generation based on the phonological rules ( Lee et al. 2005; Lee 1998) 1 . Unlike previous works, this study suggests a standard Korean pronunciation generation method on the basis of corpusbased and data-oriented TBL learning.The role of the computational phonology is to generate a legitimate output counterpart of the underlying phonological input. Phonological rules are involved in the process of phonological generation. The SPE style operations on the computational phonology have used the rewriting rule ordering or the finite state transducer (Bird 1995;Bird and Ellison 1994;Gildea and Jurafsky 1996;Kaplan and Kay 1994). Those approaches, however, should reduce complicated * This paper was supported by the Second Brain Korea 21.Copyright 2007 by Kim Dong-Sung and Chang-Hwa Roh orderings because of huge amount of rewriting rules and rule orderings among themselves ( Gildea and Jurafsky 1996). Other differently motivated approaches have suggested the dataoriented models, using a pronunciation corpus to derive legitimate outputs (Daelemans, Gillis and Durieux 1994;Johnson 1984).In this study, we use the learning method of TBL that was proposed by Brill (1995). We design a set of templates and abstract transformations of possible pronunciations. For the experiments, we set up an aligned corpus between the text based on the Korean standard orthography and the text based on the Korean standard pronunciation. We conducted an experiment on generating the standard pronunciation with the TBL algorithm, using this corpus. We use the phonotactic constraints to reduce the complexity of TBL process. As noticed in Hayes and Wilson (forthcoming), the phonological feature constraints can reduce the complication of phonotactics. We set up a list of constraints on the phonotatics, which is derived from the phonological features.The rest of the paper is composed of three parts: Section 2 is to introduce the TBL method into the phonological operation. Section 3 describes the experiment on Korean pronunciation. Section 4 deals with the experiment discussions. In this paper, we propose a Transformation-Based Learning (TBL) method on generating the Korean standard pronunciation. Previous studies on the phonological processing have been focused on the phonological rule applications and the finite state automata (Johnson 1984; Kaplan and Kay 1994; Koskenniemi 1983; Bird 1995). In case of Korean computational phonology, some former researches have approached the phonological rule based pronunciation generation system (Lee et al. 2005; Lee 1998). This study suggests a corpus-based and data-oriented rule learning method on generating Korean standard pronunciation. In order to substituting rule-based generation with corpus-based one, an aligned corpus between an input and its pronunciation counterpart has been devised. We conducted an experiment on generating the standard pronunciation with the TBL algorithm, based on this aligned corpus.
Transition and Parsing State and Incrementality in Dynamic Syntax * Incremental processing of a sentence as it is inputted from left to right has been taken as most accurately simulating human sentence processing. When we attempt theoretically to account for this incrementality of sentence processing, however, we notice that there is a difference between head-initial languages and head-final languages. What we need to take into account is that in headinitial languages, like English, parsers can look ahead to syntactic structures to be established to some extent as a word is consumed. By contrast, this is not the case in head-final languages such as Japanese, because in these languages the syntactic positions of NPs remain unfixed until the matrix verb is inputted finally in the sentence. The Dynamic Syntax approach (Kempson, Meyer-Viol, and Gabbay, 2001;Cann, Kempson, and Marten, 2005), which allows the parser to process a sentence in an incremental fashion, has settled this difficulty by adopting structural underspecification. However, Kempson, Meyer-Viol, and Gabbay (2001) and Cann, Kempson, and Marten (2005) do not provide an explicit algorithm for implementation of the framework as formally as Moot (1999) presents Grail, the toolkit for a parser of Categorial Grammar Logics. This paper will delineate the basic idea of a parser for Japanese based on the Dynamic Syntax (hereafter, DS) framework, providing the algorithm of the application of lexical rules and transition rules. This paper will also show that the parser is able to cope with null arguments or empty pronouns in embedded clauses. Currently the parser is implemented in Prolog.The outline of this paper is as follows: the subsequent section will provide a brief introduction to the DS formalism and address issues of parsing inefficiency and the algorithm of rule application, discussing the previous studies. Section 3 will illustrate the implementation of the DS formalism and our algorithm to process head-final languages like Japanese. Section 4 shows how the parser deals with the complex sentences with empty pronouns. Section 5 will be devoted to the discussion. Section 6 will be a conclusion. This paper presents an implementation of a gramar of Dynamic Syntax for Japanese. Dynamic Syntax is a grammar formalism which enables a parser to process a sentence in an incremental fashion, establishing the semantic representation. Currently the application of lexical rules and transition rules in Dynamic Syntax is carried out arbitrarily and this leads to inefficient parsing. This paper provides an algorithm of rule application and partitioned parsing state for efficient parsing with special reference to processing Japanese, which is one of head-final languages. At the present stage the parser is still small but can parse scrambled sentences, relative clause constructions, and embedded clauses. The parser is written in Prolog and this paper shows that the parser can process null arguments in a complex sentence in Japanese.
A Focus Account for Contrastive Reduplication: Prototypicality and Contrastivity * This paper examines the linguistic phenomenon of what is called Contrastive Reduplication (henceforth CR) ( Ghoemshi et al. 2004) in English. In many works on reduplication patterns, CR has been unfairly regarded as a subclass of mere repetition or lexical duplications that simply function as an intensifier. However, CR is understood to have distinguishable uniqueness apart from the family of reduplication forms shown in various languages which are intensively studied in phonological and morphological aspects. Interestingly, the semantic properties of CR allows itself to be more exposed to our everyday, mundane conversation than to written discourse. CR is not merely a repetition or duplicated form of a lexical element for the purpose of intensive use. Consider the examples of CR given in (1). 1 (1) a. I'll make the tuna salad, and you make the SALAD-salad.b. Oh, we're not LIVING-TOGETHER-living-together. c. My car isn't MINE-mine; it's my parents'. d. I had a JOB-job once.[a 'real' 9-to-5 office job, as opposed to an academic job]e. Are you LEAVING-leaving?(from Ghomeshi et al., 2004) As illustrated in the examples of (1), the reduplicated form SALAD-salad has the semantic function of elucidating possible interpretation of a word that is associated with the term in a context. So in (1a), the CR denotes plain, normal salad as opposed to other specific salads (e.g. chicken salad, shrimp salad, etc.) overall and (1b) is understood as living together as roommates (which is the general interpretation of living-together) not lovers. Besides the prototypical effect, additional CR semantic properties are that CR is not limited to nouns but can be produced in various lexical categories like verbs (LEAVING-leaving) or adjectives (He is HANDSOMEhandsome?), etc. In other cases, proper names (She's MARY-Mary? as in the Mary the speakers know) and lexicalized expressions (LIVING-TOGETHER-living-together) are also possible candidates for CR. Likewise, CR has loose morphological restrictions on the construction of the form unlike general reduplication patterns. Correspondingly, CR is a target for many lexical categories and also functions to determinate the prototypical, default meaning of the reduplicated item it targets and plays to make disambiguation of the distribution of the different senses of the same expression. So it is noteworthy that the semantic influence of this unique duplicated construction is to limit and "to focus the denotation of the reduplicated element on a more sharply delimited, more specialized range." (Ghomeshi et al., 2004;308) CR is considered to have a repeated word or a phrase (chunk) within the expression for the semantic effect of contrastiveness in a sense. The reduplicated form is not redundant as it has a construction that contains a repeated form with a distinct use from the unduplicated form. The difference is clearly shown in (2).(2) a. I'll make the tuna salad, and you make the SALAD-salad.b. *I'll make the tuna salad, and you make the salad.Without the reduplicated form placed in (2b), the sentence is ungrammatical as there exist no semantic effect of differentiation. Thus, this type of reduplicated form differs from other reduplication process in English as emphasizing or intensifying the simple form is not the exact consequence expected. Furthermore, CR is of interest for it displays a narrowing effect to a prototypical variant of the non-reduplicated form. The primary goal of this paper is to show an account of focus pragmatic framework for the CR in English along with introduction to other types of devices in various languages having resemblance in narrowing semantic effects. We make a distinction from the prior studies of focus by taking a pragmatic-focus framework rather than a semantic one to analyze CR. As the intended meaning of CR is of our interest, the issue of contrastive focus should shed light on this construction. It should be asserted that this type of contrast in informativeness is in concordance with contrastive focus which also requires a set of contrastive sets as alternatives. However, what is exactly contrasted here in meaning in CR remains a vague issue and the rightful approach of contrastiveness and focus should be under consideration in specific details. We argue that a pragmatic reason lies in the contrastiveness in CR, not a semantic one.The organization of this paper is as follows: The succeeding section introduces previous studies of the CR construction with some insights on contrastive focus. In section 3, the focus semantic approach on CR will be considered in specific details for a more adequate explanation of the phenomenon. Section 4 discusses a pragmatic view on CR with an advantage of the explanation of vagueness in meaning. The final section puts forth the conclusion of this paper with acknowledgment to the significance of the pragmatic approach on CR. This paper sets forth the phenomenon of Contrastive Reduplication (CR) in English relevant to the notion of contrastive focus (CF). CF differs from other reduplicative patterns in that rather than the general intensive function, denotation of a more prototypical and default meaning of a lexical item appears from the reduplicated form resulting as a semantic contrast with the meaning of the non-reduplicated word. Thus, CR is in concordance with CF under the concept of contrastivity. However, much of the previous works on CF associated contrastivity with a manufacture of a set of alternatives taking a semantic approach. We claim that a recent discourse-pragmatic account takes advantage of explaining the vague contrast in informativeness of CR. Zimmermann&apos;s (2006) Contrastive Focus Hypothesis characterizes contrastivity in the sense of speaker&apos;s assumptions about the hearer&apos;s expectation of the focused element. This approach makes possible adaptation to CR and recovers the possible subsets of meaning of a reduplicated form in a more refined way showing contrastivity in informativeness. Additionally, CR in other languages along with similar set-limiting phenomenon in various languages will be introduced in general.
Gei 3 ta 1 in Taiwan Mandarin---A Particular Construction * Taiwan Mandarin is the result of language contact between Mandarin Chinese and Taiwanese Southern Min(TSM). There have been plenty of investigations on novel usages brought about by influence from TSM, including Kubler(1985), Tsai(2002) and Zeng(2003). However, these previous investigations mainly focus on the differences between Mandarin and Taiwan Mandarin. Few efforts were made to provide an adequate account on these newly emerged sentence structures. In the present paper, I am going to be concerned with one particular construction which are newly developed structure. In modern Taiwan Mandarin there are two particular structures evoking gei 3 ta 1 . One is descriptive expressions, "gei 3 ta 1 + adjective". The other one is resultative structure, "verb + gei 3 ta 1 + complement". These two sentence structures could be regarded as the result of language contact. The former structure is more novel and is confined in ordinary utterances of younger generation. Instead of their content meaning of a giving verb and the third person pronoun, gei 3 ta 1 function like an infix for the purpose of denoting speakers' undesired and unpleasant attitude toward on the event mentioned. The corresponding combination of giving word and the third person pronoun in TSM do not have the similar function. Thus, he present paper aims at pinning down the function and features conveyed by the construction "(NP) + (intensifier) + gei 3 ta 1 + adjective" in TM from the perspective of construction grammar proposed by Goldberg(1995Goldberg( , 2006). Through careful investigation on this novel structure, it is proven once again that language has its own life. It will be influenced by surrounding languages and has its own development. The data used as examples are attracted from, mainly blogs, internet and radio programs in Taiwan. The paper is organized by the following parts. Chapter 2 tackle with the function and features of the structure "(NP) + (intensifier) + gei 3 ta 1 + adjective", including its syntactic variations and features of the compatible adjectives, the function of gei 3 ta 1 , the reference of ta 1 . Chapter 3 is conclusion.2. The structure"gei 3 + ta 1 + adjective" in TM 2.1 Construction meaning Based on Goldberg (2006), grammatical constructions are conventionalized pairings of form and function. She claims that constructions bear specific function, especially idiosyncratic structure. 1 While verbs are important in semantics of a construction, construction also denotes certain kind of meaning. Moreover, CG claims that different surface structure spells different meaning. Furthermore, CG also take information structure and pragmatics into consideration. In the step of mapping from semantics to syntax, not all roles in frame are necessarily profiled in surface structure. Some could be omitted if they are recoverable from the context or they are of no importance in information.In this study, I am going to investigate the newly developed construction in terms of CG on account of the following reasons. First, this construction is signified by its characteristics. This structure amplifies speaker's emotive evaluation on undesired or unpleasant event or situation. The event or situation must be realis. That is to say that, it must be fact. Second, the adjectives compatible with this structure must be adjectives with higher degree. General adjectives are seldom found in this structure.There are two kinds of special structures evoking "gei 3 + ta 1 " 2 in TM. Type I, descriptive expressions, "gei 3 ta 1 + adjective" and "gei 3 ta 1 + verb". The other one is resultative structure, "verb + gei 3 ta 1 + complement". In the present study, the focus is on "gei 3 ta 1 + adjective". The core part of the structure is "gei 3 ta 1 + adjective". By adding adverbs there are several variations 3 :1. gei 3  Tianyuchanshuo(a puppet play) really a little GEI TA boring speak Tianyuchanshuo is really very boring. The present paper investigates a particular structure in Taiwan Mandarin, &quot;(NP) + (intensifier) + gei 3 ta 1 &quot;give him/it&quot;+ adjective&quot; in terms of construction grammar. The structure is mostly observed in utterances of younger generation. Though it is not regarded as a grammatical or standard structure, it is still a register of language. The structure lays emphasis on speaker&apos;s attitude toward an undesired, unpleasant event. In most cases, the attitude tends to be negative. The events or propositions must have existed or been completed. The adjectives compatible with this structure belong to category of higher degree. The grammatical usage illustrates semantic bleaching of gei 3 ta 1. And the changes from giving to a grammatical particle denoting subjective belief is a kind of subjectification. Moreover, ta 1 could refer to events or situation expressed by a more complicated grammatical structure, or denotes nothing as a dummy word. Though many previous studies paid attention to the newly developed structure resulted from language contact, the adequate account was not provided. It is hoped through this investigation, we will get a better understanding of this particular structure.
Implementation of Presence and Absence of Blocking Effects: A Categorial Grammar Approach to Chinese and Korean * In some languages, long-distance reflexives are allowed in addition to their sentence-bound counterparts. Among the languages that allow long-distance reflexives, some languages have blocking effects, but others don't. Chinese is one language that has blocking effects and the sentence (1) demonstrates an example of blocking effects (Cole, et al., 2000:14 Here, the reflexive ziji cannot refer to Zhangsan because wo blocks co-reference between ziji and Zhangsan. Let's compare this sentence with (2). (2) is the Korean counterpart of sentence (1). 2 * Copyright 2007 by Yong-hun Lee 1 Cole et al. (2000) included state-of-art introduction to long-distance reflexives, and various approaches to long-distance reflexives are tried by Pica (1987), Manzini &amp; Wexler (1987), Battistella (1989), Katada (1991), Haung &amp; Tang (1991), Cole &amp; Sung (1994), etc. Discussions on Chinese long-distance reflexives are contained in Huang (1982), Hung (1984), Tang (1989), Haung &amp; Tang (1991), among others. 2 We have similar phenomenon in the following sentence (Moon, 1996:15 Here, the reflexive caki can refers to John across the pronoun nay. If Korean had blocking effects, this phenomenon would be impossible since the pronoun nay blocks the co-referential relations between caki and John. As the co-reference relations in (2) indicate, caki CAN refer to Chelsoo, though nay is located between them. Therefore, we can say that there is no blocking effect in Korean long-distance reflexives.The goal of this paper is to provide computational algorithms that can handle presence and absence of blocking effects. The algorithms will be developed by combining Steedman (1996,2000)'s Combinatory Categorial Grammar (CCG) and Chierchia's Binding Theory, which will be called a CCG-like system. This paper is organized as follows. In Section 2, Categorial Grammar and Chierchia (1988)'s Binding Theory will be introduced. Section 3 introduces a CCG-like system and demonstrates how blocking effects can be handled in the CCG-like system. Section 4 provides computational algorithms for presence and absences of the blocking effects, and Section 5 summarizes this paper. Among the languages that allow long-distance reflexives, some languages have blocking effects, whereas others don&apos;t. The goal of this paper is to provide computational algorithms that can handle presence and absence of blocking effects of long-distance reflexives. We will examine the blocking effects in Chinese and Korea and develop computational algorithms for handling blocking effects in those two languages. The algorithms will be developed by incorporating Chierchia&apos;s Binding Theory into Steedman&apos;s Combinatory Categorial Grammar (CCG). Through the analyses and implementations, this paper illustrates how blocking effects can be implemented computationally.
Using Non-Local Features to Improve Named Entity Recognition Recall * Named entity recognition (NER) is a subtask of information extraction that seeks to locate and classify predefined entities, such as names of persons, locations, organizations, etc. in unstructured texts. It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption ( Krishnan and Manning, 2006). But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency.There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features. However, in the first approach, heuristic rules are used to find the dependencies ( Bunescu and Mooney, 2004;Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc ( Finkel et al., 2005). Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, Krishnan and Manning (2006) propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are only used to improve label consistency.To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class (Kambhatla, 2006). In this paper, we employ non-local information to recall the missed entities. Similar to Krishnan and Manning (2006), we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification. Our features are also inspired by Wong and Ng (2007). They extract entity majority type features from unlabelled data with an initial maximum entropy classifier. Our approach is validated on the third International Chinese language processing bakeoff (SIGHAN 2006) MSRA and CityU NER closed track, the experimental results show that non-local features can significantly improve the recall of the state-of-the-art NER system using local context alone. The remainder of the paper is structured as follows. In Section 2, we introduce the first stage CRF with local features alone; then we describe the second stage CRF using non-local features we design in Section 3. We demonstrate the experiments in Section 4 and we conclude the paper in Section 5. Named Entity Recognition (NER) is always limited by its lower recall resulting from the asymmetric data distribution where the NONE class dominates the entity classes. This paper presents an approach that exploits non-local information to improve the NER recall. Several kinds of non-local features encoding entity token occurrence, entity boundary and entity class are explored under Conditional Random Fields (CRFs) framework. Experiments on SIGHAN 2006 MSRA (CityU) corpus indicate that non-local features can effectively enhance the recall of the state-of-the-art NER systems. Incorporating the non-local features into the NER systems using local features alone, our best system achieves a 23.56% (25.26%) relative error reduction on the recall and 17.10% (11.36%) relative error reduction on the F1 score; the improved F1 score 89.38% (90.09%) is significantly superior to the best NER system with F1 of 86.51% (89.03%) participated in the closed track.
Hierarchical Structure in Semantic Networks of Japanese Word Associations * As an approach to deepening our understanding of lexical knowledge, many areas of cognitive science, including psychology and computational linguistics, are seeking to unravel the rich networks of associations that connect words together. Key methodologies for that enterprise are the techniques of graph representation and their analysis that allow us to discern the patterns of connectivity within large-scale resources of linguistic knowledge and to perceive the inherent relationships between words and word groups.Although studies applying versions of the multidimensional space model, such as Latent Semantic Analysis (LSA) and multidimensional scaling, to the analysis of texts have been fairly fruitful, the methodologies of graph theory and network analysis are particularly suitable for elucidating the important characteristics of semantic networks.Recently, a number of studies have applied graph theory approaches in investigating linguistic knowledge resources (Church and Hanks, 1990;Dorow, Widdows, Ling, Eckmann, Danilo and Moses, 2005;Steyvers and Tanenbaum 2005;van Dongen, 2000;Watts and Strogatz, 1998). For instance, Dorow, et al (2005) utilize two graph clustering techniques as methods of detecting lexical ambiguity and of acquiring semantic classes instead of word frequency based computations.This paper applies graph theory and network analysis methods to the analysis of semantic network representations of Japanese word associations. After briefly outlining the two separate Japanese word association databases used-the Associative Concept Dictionary (Okamoto and Ishizaki, 2001) and the Japanese Word Association Database ( Joyce, 2005Joyce, , 2006Joyce, , 2007)-the paper calculates some basic statistical features, such as degree distributions, clustering coefficients and the average clustering coefficient distribution for nodes with degrees. We also apply the recently developed Recurrent Markov Clustering (RMCL) algorithm (Jung, Miyake and Akama, 2006) which enhances the bottom-up classification method of the basic MCL algorithm by making it possible to adjust the proportion in cluster sizes. Given this greater control over cluster sizes, the RMCL clearly provides a very appealing approach to the automatic construction of condensed network representations, which, in turn, can facilitate the creation of hierarchically-organized semantic spaces as a way of visualizing large-scale linguistic knowledge resources. This paper reports on the application of network analysis approaches to investigate the characteristics of graph representations of Japanese word associations. Two semantic networks are constructed from two separate Japanese word association databases. The basic statistical features of the networks indicate that they have scale-free and small-world properties and that they exhibit hierarchical organization. A graph clustering method is also applied to the networks with the objective of generating hierarchical structures within the semantic networks. The method is shown to be an efficient tool for analyzing large-scale structures within corpora. As a utilization of the network clustering results, we briefly introduce two web-based applications: the first is a search system that highlights various possible relations between words according to association type, while the second is to present the hierarchical architecture of a semantic network. The systems realize dynamic representations of network structures based on the relationships between words and concepts.
Two Types of Complex Predicate Formation: Japanese Passive and Potential Verbs * Syntax and semantics of complex verbs have long been the focus of attention in Japanese linguistics. This paper proposes a new approach to the formation of passive and potential verbs, both of which include the auxiliary verb -rare, and explore the relationship between the complex verb formation and projections of syntactic structures. The auxiliary verb -rare has been assumed to be semantically ambiguous among passive, potential, spontaneous and honorific interpretations. We will take up the passive and potential interpretations of the auxiliary verb, and argue that there is a crucial difference between the two use of this verb. First, observe the sentence in (1):(1) Kurisumasu-ni-wa takusan-no keeki-ga taber-are-ru. Christmas-at-Top a lot of cake-Nom eat-Can/Pass-Pres 'At Christmas, a lot of cakes are/can be eaten.'(1) can be interpreted as passive on one reading, i.e., a lot of cakes are eaten at Christmas. On another reading, (1) is taken to be a statement of the possibility; we (or arbitrary people) can eat a lot of cakes at Christmas.The multiple usages of the auxiliary verb -rare has been assumed to be due to the common origin, sharing the meaning of spontaneity (get, become, etc.), though there are different hypotheses concerning which use is original. The interpretations of -rare are ambiguous in some cases and it is necessary to take contextual information into account. We will argue, however, that the two complex verbs must be derived in a completely different manner, exploring the interactions between the word formation and honorification or choice of subjects in sentences projected from them. Honorification is a device to indicate that the speaker feels respect for the person referred to by the grammatical subject or object, using the discontinuous honorific morpheme comprising the prefix o-and the verb -ni-nar (a kind of auxiliary verb). This morpheme combines with (actually wrap) verbal roots (infinitives) without changing the argument structures of the latter. (2) illustrates the subject honorification we address in this paper. Hereafter, HP stands for the honorific prefix (o-) and HS the honorific suffix (-ninar).(2) Sensei-wa keeki-o o-tabe-nina(r)-tta. Sensei-Top cake-Acc HP-eat-HS-Past 'Sensei ate a cake.' Interestingly, the passive and potential complex verbs, both of which are formed by the concatenation with the auxiliary verb -rare, shows different processes of honorification. The honorific morpheme wraps derived complex passive forms, while it wraps only base verbs, and -rare follows the derived honorific forms in the potential verb formation. Compare the following examples: This paper provide a new account for the relation between syntactic structures and the complex verb formation, in which passive and potential complex verbs are formed in a completely different manner. We discuss how subjects with a wide variety of semantic relations to base verbs are derived in potential constructions and consider the properties of (major) subjects in stative sentences, the license of which should be distinguished from that of ordinary subjects. This paper deals with the complex verb formation of passive and potential predicates and syntactic structures projected by these verbs. Though both predicates are formed with the suffix-rare which has been assumed to originate from the same stem, they show significantly different syntactic behaviors. We propose two kinds of concatenation of base verbs and auxiliaries; passive verbs are lexically formed with the most restrictive mode of combination, while potential verbs are formed syntactically via more flexible combinatory operations of function composition. The difference in the mode of complex verb formation has significant consequences for their syntactic structures and semantic interpretations, including different combination with the honorific morphemes and subjectivization of arguments/adjuncts of base verbs. We also consider the case alternation phenomena and their implications for scope construals found in potential sentences, which can be accounted for in a unified manner in terms of the optional application of function composition.
Automatic Acquisition of Lexical-Functional Grammar Resources from a Japanese Dependency Corpus * We present a method to automatically annotate Lexical-Functional Grammar (LFG)-style functional structure equations (labelled dependencies) on the unlabelled Kyoto University Text Corpus version 4 (KTC4) ( Kurohashi and Nagao 1997), to acquire more abstract and (somewhat) less language-dependent LFG f-structure representations for Japanese sentences. We apply the algorithm to enrich the output of a Japanese dependency parser (KurohashiNagao Parser, KNP) ( Kurohashi and Nagao 1998), to construct f-structure representations for KNP output; the enriched parser output is available for further cross-linguistic research or applications such as machine translation.Our annotation method is based on the assumption that non-configurational, relatively free word-order languages, of which Japanese is one example, do not require phrase structure trees as an indispensable level of linguistic representation. Rather, the rich morphological information on each unit in a sentence, along with the unlabelled dependency between syntactic units in KTC4 and KNP output, provides us with as much information as what can be deduced from phrase-structure trees in other configurational, fixed word-order languages.Our method provides zero pronoun identification as a preliminary process for long distance dependency (LDD) resolution, based on the morphology of verbs and on the probability of subcategorization frames, associated with particular verbs. This paper has the following structure: in Section 2 we summarize the background of this research, including LFG and related work. In Section 3, we describe in detail our method of automatic annotation of f-structure functional equations on KTC4, and show how we approach the problem of zero-pronoun identification and present results of our f-structure annotation and parsing experiments. We discuss the overall results and their implications in Section 4, and conclude in Section 5. This paper describes a method for automatic acquisition of wide-coverage treebank-based deep linguistic resources for Japanese, as part of a project on treebank-based induction of multilingual resources in the framework of Lexical-Functional Grammar (LFG). We automatically annotate LFG f-structure functional equations (i.e. labelled dependencies) to the Kyoto Text Corpus version 4.0 (KTC4) (Kurohashi and Nagao 1997) and the output of of Kurohashi-Nagao Parser (KNP) (Kurohashi and Nagao 1998), a dependency parser for Japanese. The original KTC4 and KNP provide unlabelled dependencies. Our method also includes zero pronoun identification. The performance of the f-structure annotation algorithm with zero-pronoun identification for KTC4 is evaluated against a manually-corrected Gold Standard of 500 sentences randomly chosen from KTC4 and results in a pred-only dependency f-score of 94.72%. The parsing experiments on KNP output yield a pred-only dependency f-score of 82.08%.
Semi-Automatic Annotation Tool to Build Large Dependency Tree- Tagged Corpus * More recently, a large corpus annotated with linguistic information is used in natural language processing. By using this corpus, natural language processing systems have learn some linguistic phenomena automatically. Building such a corpus, however, is an expensive, laborintensive and time-consuming work. Furthermore, maintaining consistency of a constructed corpus is difficult. Therefore, we need an annotation tool for improving annotation efficiency and maintaining consistency. To help such work, some annotation tools (Atalay, 2003;Lim, 2002;Morton, 2003;Brants T. and Plaehn, 2000) have already been used. In this paper, we describe an annotation tool for building a Korean dependency tree-tagged corpus with linguistic information about the segmentation of word phrases (called eojeols in Korean), partof-speech (POS) tags, boundaries of chunks, and dependency links and relations. We design an annotation tool so that an annotator can carefully investigate them and edit errors on them through a GUI. We also design it so that errors in low level processing like POS tagging might not be propagated to higher level processing step like parsing. Moreover, the tool is characterized by the following features; 1) It is independent of special applications like information extraction. 2) It focuses on localizing errors to modules as far as possible, such as morphological analyzers. It can make annotators find and pay attention to errors related to the modules easily. 3) It has an error checking function to make possible that errors can not be stored as it can be. 4) It promptly shares annotated information among annotators so that annotators can keep consistency with others' annotation within a working group. 5) It has a user-friendly interface.This paper is organized as follows: In Section 2, we introduce other annotation tools for establishing corpora In Section 3, we describe the architecture of our annotation tool for building a Korean dependency tree-tagged corpus. In Section 4 and 5, we explain the implementation details of our tool and guide process of the annotation using our tool, respectively. Finally, we draw conclusions, and discuss future works in Section 6. Corpora annotated with lots of linguistic information are required to develop robust and statistical natural language processing systems. Building such corpora, however, is an expensive, labor-intensive, and time-consuming work. To help the work, we design and implement an annotation tool for establishing a Korean dependency tree-tagged corpus. Compared with other annotation tools, our tool is characterized by the following features: independence of applications, localization of errors, powerful error checking, instant annotated information sharing, user-friendly. Using our tool, we have annotated 100,904 Korean sentences with dependency structures. The number of annotators is 33, the average annotation time is about 4 minutes per sentence, and the total period of the annotation is 5 months. We are confident that we can have accurate and consistent annotations as well as reduced labor and time.
Multiple Sluicing in English * Human languages enjoy the common rule of economy: when some word or words are repeated, we do not verbally pronounce what is repeated. This linguistic phenomenon is called ellipsis or, in more technical terms, deletion, which is to delete or suppress the phonological features of repeated word/words in the course of syntactic derivation.Deletion is known to be rather widely available to the relevant contexts in English. It has been noted that there are three types of ellipsis depending on what constituent undergoes deletion: (i) VP ellipsis (or maybe vP ellipsis); (ii) NP ellipsis (in the DP system); (iii) T/IP ellipsis. What draws particular attention recently among the three types of ellipsis is the last kind, where wh-movement is mandatory before TP is elided. T/IP ellipsis or what Ross (1969) calls sluicing is illustrated by the following example:(1) John met someone, but I don't know [ CP [who] [ TP John met t]]The fact that wh-movement feeds sluicing raises a question of whether in contrast to English with single wh-fronting, multiple sluicing (sluicing with multiple survivors) is possible in languages with multiple wh-fronting. Bulgarian and Serbo-Croatian, which allow for multiple wh-fronting, make a test case for this question, and indeed multiple sluicing is attested in these two languages as follows:(2) Njakoj vidja njakogo, no ne znam [koj] [kogo] [vidja] Bulgarian someone saw someone, but not I-know who whom saw Richards (1997) (3) Neko je vidio nekog, ali ne znam [ko] [koga] [je vidio] Serbo-Croatian someone is seen someone, but not I-know who whom is seen Stjepanovic (2003) Returning to English, a question that arises is whether multiple sluicing is allowed in non-multiple wh-fronting languages like English. Bearing on this question, Bolinger (1978:109) reports several examples like (4), which is quite similar in appearance to those involving multiple sluicing as in Bulgarian and Serbo-Croatian.(4) I know that in each instance one of the girls got something for one of the boys. But [which] [for which] Bolinger (1978: 109) Incidentally, Bolinger notes that a certain restriction applies to the formation of more than one remnant wh-phrase. In particular, the second remnant wh-phrase is required to be PP, not DP, which is shown by the contrast between grammatical (4) and ungrammatical (5):(5) *I know that in each instance one of the girls got something for one of the boys. But [which] [which]The purpose of this paper is to probe into the nature of examples like (4). In particular, we will investigate whether examples like (4) are analyzed on a par with corresponding examples in Bulgarian and Serbo-Croatian. Although both Nishigauchi (1998) andLasnik (2007) take a different tack, we will argue in this paper that peculiar properties of examples like (4) can be accounted for by the hypothesis that they are genuine instances of multiple sluicing. This paper explores the nature of multiple sluicing in English, which has two or more remnant wh-phrases in clause edge position. At the beginning part of the paper we argue against Nishigauchi&apos;s (1998) and Lasnik&apos;s (2007) Gapping analysis of multiple sluicing, which says that two remnant wh-phrases each actually occupies the left and right edge of a clause, with the in-between string of words undergoing Gapping. We rather argue that multiple sluicing in English is the same kind as found in Bulgarian and Serbo-Croatian. In other words, multiple sluicing in English is also derived by multiple wh-fronting which otherwise does not apply. We demonstrate that some important properties of the construction noted by Lasnik (2007) under the Gapping approach to it can be accounted for in a principled way by our proposed analysis.
Prosodic Annotation in a Thai Text-to-speech System * Presently, widespread use of text-to-speech technology is limited by its inability to produce highquality speech. That is, intelligibility and naturalness of synthetic speech is still not quite at the level acceptable by human listeners. In particular, the naturalness issue can be attributed to the lack of sophisticated prosody-generating scheme.Prosody is often described as a suprasegmental feature of speech (a term for describing phonological features of those aspects of speech that involve more than single consonants or vowels). Acoustically speaking, prosody can be defined as change in the fundamental frequency (F 0 ), timing, and amplitude of a speech signal. Speakers control the prosody of an utterance in order to signal linguistic and affective information. Linguistic prosody is used by speakers to signal grammatical information at the syllable, word, or sentence level (e.g., stress, intonation). Affective prosody, on the other hand, is used to convey information that indicates speaker's intentions, attitudes, or emotional states. In addition to linguistic and affective information, prosody can also be used to convey non-linguistic information concerning speaker's personal characteristics such as age, gender, idiosyncrasy, speaking style, and physical condition. Such characteristics may or may not be under the speaker's volitional control. It is part of the intelligibility and naturalness of his/her speech. This paper will deal with linguistic prosody only.The role of linguistic prosody in spoken language is similar to that of punctuation in written language. Punctuation is used to divide a stream of text into smaller segments such as a phrase, clause, or sentence, and thus, helps readers interpret the message according to the intention of the writer. Likewise, prosodic information helps listeners interpret a spoken utterance in the way the speakers intends. The need for punctuation or prosody can be attributed in part to the inherent ambiguity of natural language.Intuition tells us that intelligibility and naturalness of speech can be attributed to prosody. Some words in an utterance are louder and longer than others. Because function words are acoustically less prominent than the semantically important content words, such as nouns and verbs, we can prosodically distinguish them. Pauses tend to be inserted at certain points in the utterance, and words at the end of the utterance are likely to be lengthened. This suggests the existence of prosodic constituents that are used in the overall prosodic structure or melody of an utterance. Linguists have posited units such as syllables, prosodic words, phonological phrases, and intonational phrases.The use of prosody by speakers, in attempting to sound intelligibly and naturally, can be best exemplified by considering its use in ambiguous sentences. When two sentences are segmentally identical, a problem of identifying the correct meaning arises for listener, especially when the contextual information is not adequate. In such cases, the listener can make use of another type of information, namely prosody. The question arises, from the speaker's point of view, as to how this information is decoded or associated with different meanings. At an abstract level, a commonly accepted hypothesis is that there is a direct relationship between the syntactic structure of a sentence and its prosodic structure as suggested by Selkirk (1984), andNespor (1986). This hypothesis implies that an ambiguous sentence will have a different prosodic structure for each syntactic structure, and as such it can be used to determine the correct meaning. At the phonetic level, the speaker tends to manipulate the acoustic correlates of prosody, such as F 0 , segmental and pause duration, amplitude, and spectrum of the speech signal in order to signal prosody. The listener, in turn, will try to translate changes in these physical correlates into abstract linguistic concepts in order to arrive at the intended meaning of the utterance.As in human speech, it is believed that prosodic information can help improve performance of a text-to-speech system. Prosodic information is particularly helpful in generating synthetic speech because of lexical and structural ambiguities of written forms. Prosodic information could be used by computers to generate phonetically similar, but syntactically different utterances.In the following sections, a novel method for annotating prosody in a text-to-speech system will be described. The process will be abstractly described and demonstrated by using structurally ambiguous sentences involving different types of compounds in Thai. Vongvipanond (1993) concluded that compounds are a major cause of structural ambiguity in Thai and often create problems because of their high frequency of occurrence. Compounding is the most widespread word formation process in Thai. Structural ambiguities often result from compounds because Thai words lack inflectional and derivational affixes to indicate, for example, subject-verb agreement. Nevertheless, compounds can be prosodically distinguished from syntactic phrases by differences in stress patterns. In addition, the process of generating durational patterns for the utterance based on the prosodic annotation process will also be described. This paper describes a preliminary work on prosody modeling aspect of a text-to-speech system for Thai. Specifically, the model is designed to predict symbolic markers from text (i.e., prosodic phrase boundaries, accent, and intonation boundaries), and then using these markers to generate pitch, intensity, and durational patterns for the synthesis module of the system. In this paper, a novel method for annotating the prosodic structure of Thai sentences based on dependency representation of syntax is presented. The goal of the annotation process is to predict from text the rhythm of the input sentence when spoken according to its intended meaning. The encoding of the prosodic structure is established by minimizing speech disrhythmy while maintaining the congruency with syntax. That is, each word in the sentence is assigned a prosodic feature called strength dynamic which is based on the dependency representation of syntax. The strength dynamics assigned are then used to obtain rhythmic groupings in terms of a phonological unit called foot. Finally, the foot structure is used to predict the durational pattern of the input sentence. The aforementioned process has been tested on a set of ambiguous sentences, which represents various structural ambiguities involving five types of compounds in Thai.
Relation Extraction Using Convolution Tree Kernel Expanded with Entity Features * Information extraction is an important research sub-field in natural language processing (NLP) which aims to identify relevant information from large amount of text documents in digital archives and the WWW. Information extraction subsumes three main tasks, including Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper will focus on the ACE RDC task 1 and employ kernel method to extract semantic relationships between named entity pairs. Many feature-based approaches transform relation instances into feature vectors of high dimension, and compute the inner dot product between these feature vectors. Current research (Kambhatla 2004, Zhao et al 2005, Zhou et al. 2005, Wang et al. 2006) shows that it is very difficult to extract new effective features from relation examples. Kernel methods are non-parametric estimation techniques that computer a kernel function between data instances. By avoiding transforming data examples into feature vectors, kernel methods can implicitly explore much larger feature space than could be searched by a feature-based approach. Thereafter, kernel methods especially on discrete structures (Haussler 1999) attract more and more attentions in relation extraction as well as other fields in NLP.Prior work on kernel methods for relation extraction includes Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005). Due to strong constraints that matching nodes be at the same layer and in the identical path starting from the roots to the current nodes, their kernels achieve good precision but much lower recall on the ACE2003 corpus. Zhang et al. (2006) proposed a composite kernel that consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. However, their method needs to manually tune parameters in composite kernels that are often difficult to determine. This paper describes an expanded convolution parse tree kernel to incorporate entity information into syntactic structure of relation examples. Similar to Zhang et al. (2006), we employ a convolution parse tree kernel in order to model syntactic structures. Different from their method, we use the convolution parse tree kernel expanded with entity information other than a composite kernel. One of our motivations is to capture syntactic and semantic information in a single parse tree for further graceful refinement, the other is that we can avoid the difficulty with tuning parameters in composite kernels. Evaluation on the ACE2004 corpus shows that our method slightly outperforms the previous feature-base and kernel-based methods.The rest of the paper is organized as follows. First, we present our expanded convolution tree kernel in Section 2. Then, Section 3 reports the experimental setting and results. Finally, we conclude our work with some general observations and indicate future work in Section 4. This paper proposes a convolution tree kernel-based approach for relation extraction where the parse tree is expanded with entity features such as entity type, subtype, and mention level etc. Our study indicates that not only can our method effectively capture both syntactic structure and entity information of relation instances, but also can avoid the difficulty with tuning the parameters in composite kernels. We also demonstrate that predicate verb information can be used to further improve the performance, though its enhancement is limited. Evaluation on the ACE2004 benchmark corpus shows that our system slightly outperforms both the previous best-reported feature-based and kernel-based systems.
Summarization and Evaluation; Where are we today?! * Today, having access to information summaries is one of the important needs for humans which may affect their lives. These summaries generally are produced with the help of other humans. The question arises here is that in this era where we are encountering a huge volume of increasing data and information, with limited resources to decide on, may it be really possible to use humans as information summarizers? We believe not.So, researchers have created methods to automatically do summarization task and extract the most important concepts of the information without any need to involve humans. Now a second question emerges. How could we know about the correctness and the completeness of the results generated in an automatic manner? The answer is hidden in evaluation techniques on which we are going to focus in this paper. Therefore besides the growth of summarization methods, evaluation techniques must improve and mature so as to provide acceptable scores for summaries according to some sort of evaluation metrics.Evaluation approaches mostly evaluate created summaries based on (1) ideal (gold) summary, (2) use in an application and (3) original document. Techniques from the first category compare system generated summaries with a summary known as the best possible summary! This ideal summary currently is created by humans and it could be influenced by subjective effects from judges. Application driven techniques evaluate content of summary by analyzing the level of information that could be obtained from it for a specific task. In this approach the performance of the application is analyzed when using the original document and * Copyright 2007 by Mehrnoush Shamsfard, Amir Saffarian, Samaneh Ghodratnama its summary separately. On the other hand evaluation by original document is really ambiguous because specifying evaluation parameters and metrics is hard but it is more natural.In this paper we focus on the first category of evaluation approaches; comparing with an ideal (gold) summary. Section 2 of this paper provides a brief explanation about the summarization methods and their high level architecture. Then in the following sections some recent methods for summary evaluation are introduced. The rapid growth of the online information services causes the problem of information explosion. Automatic text summarization techniques are essential for dealing with this problem. There are different approaches to text summarization and different systems have used one or a combination of them. Considering the wide variety of summarization techniques there should be an evaluation mechanism to assess the process of summarization. The evaluation of automatic summarization is important and challenging, since in general it is difficult to agree on an ideal summary of a text. Currently evaluating summaries is a laborious task that could not be done simply by human so automatic evaluation techniques are appearing to help this matter. In this paper, we will take a look at summarization approaches and examine summarizers&apos; general architecture. The importance of evaluation methods is discussed and the need to find better automatic systems to evaluate summaries is studied.
Refinement of Document Clustering by Using NMF * In this paper, we use non-negative matrix factorization (NMF) to improve the document clustering result generated by a powerful document clustering method. Using this strategy, we can obtain an accurate document clustering result. Document clustering is a task that divides a given document data set into a number of groups according to document similarity. This is the basic intelligent procedure, and an important factor in text-mining systems, from Berry (2003). Relevant feedback in information retrieval (IR), where retrieved documents are clustered, is a specific application that is actively researched by Hearst et al. (1996), Leuski (2001), Zeng et al. (2001) and Kummamuru (2004). NMF is a dimensional reduction method and an effective document clustering method, because a term-document matrix is high-dimensional and sparse, from Xu et al. (2003).Let X to be a term-document matrix, consisting of m rows (terms) and n columns (documents). If the number of clusters is k, NMF decomposes X to the matrices U and as follows:where U is , V is and is the transposed matrix of V. The matrix U and V are nonnegative. In NMF, each k dimensional column vector in V corresponds to a document. An actual clustering procedure is usually performed using these reduced vectors. However, NMF does not need such a clustering procedure. The reduced vector expresses its cluster by itself, because each column axis of V represents a topic of the cluster. Furthermore, the matrices V and U are k m × k n × t V obtained by a simple iteration, from Lee (2000), where the initial matrices and are updated. Therefore, we can regard NMF as a refinement method for a given clustering result, because the matrix V represents a clustering result. In this paper, we use NMF to improve clustering results. Providing NMF with an accurate document clustering result, we can ensure a more accurate result, because NMF is effective for document clustering. However, NMF often fails to improve the initial clustering result. The main reason for this is that the object function of NMF does not properly represent the goodness of clustering. To overcome this problem, we use another object function. After each iteration of NMF, the current clustering result is evaluated by that object function. We first need the initial clustering result. To obtain this, we perform min-max cut (Mcut) proposed by Ding et al. (2001), which is a spectral clustering method. Mcut is a very powerful clustering method, and we can obtain an accurate clustering result by improving the clustering result generated through Mcut,In the experiment, we used 19 data set provided via the CLUTO website. Our method improved the clustering result generated by Mcut. In addition, the accuracy of the obtained clustering result was higher than those of NMF, CLUTO and Mcut. In this paper, we use non-negative matrix factorization (NMF) to refine the document clustering results. NMF is a dimensional reduction method and effective for document clustering, because a term-document matrix is high-dimensional and sparse. The initial matrix of the NMF algorithm is regarded as a clustering result, therefore we can use NMF as a refinement method. First we perform min-max cut (Mcut), which is a powerful spectral clustering method, and then refine the result via NMF. Finally we should obtain an accurate clustering result. However, NMF often fails to improve the given clustering result. To overcome this problem, we use the Mcut object function to stop the iteration of NMF.
Constraints and Type Hierarchies for Korean Serial Verb Constructions -An Analytic Study within the HPSG Framework -* Sohn (1999:380) offers a general explanation to Korean Serial Verb Constructions (henceforth KSVCs) as the following.(1) Serial predicate constructions consist of two or more predicate (flanked by a complementizer) which denote sequential actions or states that denote a single coextensive or extended event.The purpose of this study is to provide an overall picture of KSVCs within the framework of the unification-based grammar 2 , in particular, Head-driven Phrase Structure Grammar (HPSG) 3 . This paper makes a fine-grained analysis of constraints on KSVCs, and also proposes the type hierarchies for KSVCs within the HPSG framework. This paper provides a fine-grained analysis of Korean serial verb constructions within the HPSG framework, and covers major descriptive characteristics of the phenomena. This paper discusses constraints on serial verb constructions in terms of four aspects; transitivity, argument structure, semantic properties, and complementizers. As a result, 17 constraints have been built, which support the type hierarchies for Korean serial verb constructions. This paper also presents a sample derivation on the basis of on the constraints and the type hierarchies.
Exploring the Microscopic Textual Characteristics of Japanese Prime Ministers&apos; Diet Addresses by Measuring the Quantity and Diversity of Nouns * In the field of stylistics, as computational approaches have been developed and many on-line corpora have been constructed, statistical textual characteristics have been systematically examined. These textual characteristics have traditionally been used for detecting the authors, registers, and chronological variations of texts. Recently, they have also been used for more practical applications such as spam filtering (Argamon, Whitelaw, Chase, Raj Hota, Garg, and Levitan, 2007). With this expansion in the scope of stylistics and the production of a wide variety of new types of texts especially with the growth of the Web ( Aitchison and Lewis, 2003), the microscopic characterization of different types of texts has become more and more important.Turning our eyes to the field of political science, the content of political speeches is regarded as important for analyzing the policies, attitudes and thoughts of political actors (Axelrod, 1976). Among such speeches, prime ministers' Diet addresses are recognized as the most important material for understanding Japanese politics as they reflect Japanese governmental policies, and prime ministerial attitudes and thoughts (Watanabe, 1974;Kusano, 2005). As the role of the media and the performance of politicians increase in importance in contemporary politics (Kusano, 2006), the style of prime ministers' speeches, as well as their content, have attracted more attention (Ahrens, 2005;Azuma, 2006).Against this background, this study explores the textual characteristics of Japanese prime ministers' Diet addresses, focusing on (a) the difference between the two types of Diet addresses and (b) the perceived changes made to these addresses by two powerful prime ministers, comparing the characteristics of their addresses with those of all prime ministers from 1945 to 2006. In order to clarify these points, we focus on the quantity and diversity of nouns, textual characteristics strongly related to the content of texts, instead of conventional contentindependent stylistic characteristics, because the purpose of this study is to analyze these characteristics in order to better understand two political phenomena: the difference and changes in political content. From the point of view of computational linguistics, this study can be seen as a case study for exploring microscopic textual characteristics. The rest of this paper is organized as follows. In Section 2, we review previous work, and in Section 3, explain our two research questions in detail. In Section 4, we describe our data, and in Section 5, discuss the results. In Section 6, we make concluding remarks. This study explores the textual characteristics, more precisely the quantity and diversity of nouns, of Japanese prime ministers&apos; Diet addresses. In the field of stylistics, textual characteristics independent of the content have been examined with the aim on detecting the authors, genres, and chronological variations of texts. This study focuses instead on textual characteristics related to the content of texts, namely the quantity and diversity of nouns, because our aim is to analyze texts to better understand two political phenomena: (a) the difference between the two types of Diet addresses delivered by Japanese prime ministers, and (b) the perceived changes made to these addresses by two powerful prime ministers. It is a case study of the microscopic characterization of texts, which has become more and more important with the expansion in the scope of stylistics and the production of a wide variety of new types of texts following the advent of the Web.
What L2 Learners&apos; Processing Strategy Reveals about the Modal System in Japanese: A Cue-based Analytical Perspective * It is cross-linguistically not uncommon for a single modal marker to represent both deontic modality (the conditioning factors being external to the relevant individual) (Palmer 2001: 9) and epistemic modality (speakers' judgments about the factual status of the proposition) (ibid: 8), as in English should. This phenomenon is defined as polysemy (Traugott &amp; Dasher 2002: 9). The deontic-epistemic polysemy is considered as a typologically prevalent tendency (Bybee et. al. 1994).Unlike this cross-linguistic tendency, deontic modality and epistemic modality are generally encoded by two distinctive modal markers in Japanese.This raises an intriguing question as to whether deontic is more prototypical than epistemic modality, or vice versa, or neither is. The aim of this study is to analogize the relationship between deontic modal markers and epistemic modal markers in Japanese based on the data of Chinese speakers' L2 acquisition.The phenomenon of polysemy between deontic and epistemic modalities is observable among typologically different languages, therefore this phenomenon is considered as a cross-linguistically prevalent tendency. There are two main approaches which propose to account for the deontic-epistemic polysemy prevalent across languages, i.e. (I) the polysemic approach and (II) the monosemic approach. Japanese does not exhibit deontic-epistemic polysemy which is recognized among typologically different languages. Hence, in Japanese linguistics, it has been debated which of the two types of modality is more prototypical. This study brings Chinese learner&apos;s acquisition data of Japanese modality to bear on the question of which of the two types of modality is more prototypical, using the Competition Model (Bates and MacWhinney 1981). The Competition Model notion of &apos;cues&apos; as processing strategy adopted by learners reveals the continuity/discontinuity between these two modality domains.
Movie Review Classification Based on a Multiple Classifier * The World Wide Web contains a huge number of on-line documents that are easily accessible.Finding information relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user's opinions for products. For example, buying products, users usually survey the product reviews. Movie reviews are likewise one of the most important information for users who go to a movie. More precise and effective methods for evaluating the products are useful for users.Many researchers have recently studied extraction of evaluative expressions and classification of opinions ( Kobayashi et al., 2005;Osajima et al., 2005;Pang et al., 2002;Turney, 2002). Studies of opinion classification are generally classified into three groups: (1) classifying documents into the positive (p) or negative (n) opinions, (2) classifying sentences into the positive or negative opinions, and (3) classifying words into the positive or negative expressions. In this paper, we focus on the classification of movie review documents. Pang et al. (2002) have reported the effectiveness of applying machine learning techniques to the p/n classification. They compared three machine learning methods: Naive Bayes, Maximum Entropy and Support Vector Machines. In their experiment, SVMs produced the best performance. Osajima et al. (2005) have proposed a method for polarity classification of sentences in review documents. The method is based on a score calculation process of word polarity and outperformed SVMs in the sentence classification task.The previous studies, however, used only a single classifier for the classification task. We (Tsutsumi 2006 et al.) have proposed a method consisting of two classifiers: SVMs and the scoring method by Osajima et al. (2005). The method identified the class (p/n) of a document on the basis of the distances that were measured from the hyperplane of each classifier. It obtained the better accuracy as compared with the single classifiers. This method, however, contained a problem for the determination of the final output, namely positive or negative. We needed to normalize the classifier's outputs manually because the scale of the scoring method was different from that of SVMs.To solve this problem, we apply the 3rd machine learning method (Maximum Entropy) into the method based on the scoring and SVMs. Figure 1 shows the outline of the proposed method. In this paper, we compare three processes for the method: naive voting, weighted voting and determination with SVMs. Figure. 1.The outline of our method. In this paper, we propose a method to classify movie review documents into positive or negative opinions. There are several approaches to classify documents. The previous studies, however, used only a single classifier for the classification task. We describe a multiple classifier for the review document classification task. The method consists of three classifiers based on SVMs, ME and score calculation. We apply two voting methods and SVMs to the integration process of single classifiers. The integrated methods improved the accuracy as compared with the three single classifiers. The experimental results show the effectiveness of our method.
Research on a Model of Extracting Persons&apos; Information Based on Statistic Method and Conceptual Knowledge * In news reports, person is one of the most important elements. On the internet politicians, famous enterprisers, celebrities and hot persons are always the focus what people want to know. It would be very useful and valuable to compile and extract the information about a person with the assistance of using automatic information extraction technology. ACE (Automatic Content Extraction) program is organized by NIST (National Institute of Standards and Technology, U.S.A). It is aimed to develop automatic content extraction technology to support automatic processing of human language in text form, including the detection and recognition of entity, value, time, relation and event. In the description of events, ACE defined the allowable roles of an event, such as person, place, position, etc. Persou, which is studied by Liu and etc. (2006), is a web search engine for searching persons' information. It can distinguish automatically or semi-automatically the persons who own the same name, and produce their resume and activities. A fame evaluation system studied by Zan and etc. (2003) can calculate the correlative degree between the basic information such as name, specialty, affiliation, character words of a person and the information of a web page. If the count of the strong correlative web pages is greater than a threshold, the relative person will be a famous person. In above studies, they all adopted statistic method. The advantage of using statistic models is high efficient performance to process large-scale data, but the precision is restricted by training set and it is not suitable to process sparse individual data, although the data is very important to the user.This paper proposed an model of information extraction based on statistic algorithm and conceptual semantic knowledge to extract persons' information from the text. The primary information of a person includes name, gender, age, nationality, department in organization, career, title, and so on. The name of a person was recognized by using maximal entropy statistic model. The text around the name was analyzed based on conceptual model. According to some associated key words about the primary information, the information was extracted from the context. If there are some events in the text, the main elements of events will be extracted based on the conceptual structures of a sentences and the conceptual knowledge base, which was stored in computer in advance. Figure 1 shows the whole process of extracting a person's name, his or her primary information and events about a person. In this process, recognizing persons' names is one of the key subprocesses. In order to recognize a person's name, there must be some tagged texts which indicate the right persons' names. In the tagged texts, the contexts around the names present some statistic characters. Therefore, recognizing a person's name was transformed into classifying a word belongs to a name or not. This paper focused on Chinese names. A Chinese name is composed of two parts. The first part is family name and the second is given name. Most family names are limited in about 100 Chinese characters. These characters are used to activate persons' names. The Chinese character (one or two) behind the family will be considered as candidates of a person's name. A classifier can divide all candidates into name or non-name according to the statistic model based on the tagged texts. Once a person's name is recognized, the surrounding sentences will be analyzed to extract the primary information and the events about the person. To extract the primary information of a person, there are some activating words to point out the information. For example, 'he' pointed to a person's name indicates that the person's gender is male, and 'president' points out the person's title. To extract the information of events about the person, we must analyze the related sentences at first. All sentences are mapped into the conceptual structures, and the main elements of events are extracted from text based on conceptual knowledge bases. There are three kinds of conceptual knowledge bases for words, sentences, and text respectively, as showed in figure 1. In order to extract some important information of a person from text, an extracting model was proposed. The person&apos;s name is recognized based on the maximal entropy statistic model and the training corpus. The sentences surrounding the person&apos;s name are analyzed according to the conceptual knowledge base. The three main elements of events, domain, situation and background, are also extracted from the sentences to construct the structure of events about the person.
Text Categorization for Authorship based on the Features of Lingual Conceptual Expression * Along with the progress of social information industry, especially with the Internet development, more and more documents exist in the electronical form. It provides convenience for information automatic processing. The text categorization is the basic work for the automatic processing, and it is the foundation for the information retrieval, information mining, and question-answer system too.In some applications, it is need to identify the text authorship. The identification need use the documents written by the author as reference. Since the literary style of an author is relatively steady in some time, if we can mine the style character of the author then the identification is realized. Of course, the more documents which are gathered, the better result which is achieved. In many cases, the text authorship identification is treated as text categorization as the researchers can accomplish the work according the text categorization way. In this way, the frequency of glossary, punctuation, n-Gram string, syntax feature, average sentence length, the length of paragraph, and so on, are be used as the features to identify the author of a text [Burrows J. F. 1987, Baayen R. H. 1996, David I Holmes 1997, Yuta Tsuboi 2002. The texts are often literature, and sometimes they are internet text, such as E-mail [Olivier de Vel 2001].Presently, there are a few research works in the Chinese text authorship identification, and many of them orient to the linguistic research for the concrete literature. Sun and Jin [Yi-jian Jin, Xiao-ming Sun, and Shao-ping Ma, 2003] use the vector space model (VSM) which takes the syntax structural words as feature to identify the text authorship, they achieved good result in novel author identification. The best precision of pattern matching, KNN algorithm and SVM algorithm are 89.51%, 91.54%, and 93.58% separately. Ma and Chang study the E-mail authorship identification successively [Jian-bin Ma 2004, Shu-hui Chang 2005. In addition, Wu introduces the HowNet knowledge base; he performs the text authorship identification according to the evaluation method of glossary semantic similarity. His best marco-average Fmeasure is 86.23% that is achieved in 202 People Daily texts written by 5 reporters [Xiao-chun Wu 2006].In text categorization, one important aspect is text expression. The text is mainly expressed as discrete glossary. As to Chinese, it is also expressed as Chinese character, phrase, term, n-gram string, etc. When the discrete unites are obtained, they can be used in feature vector constructing.These features that come from natural language directly can represent the topic of the text and the writing style of the author in some aspects. However, these features also arouse the spare data and too large feature space in the computing, because there are more than ten thousand basic words in natural language, and the total of words even come to million, the dimension of the corresponding vector space is huge. In order to perform the computing, it is necessary to reduce the dimension.Huang put forwards the linguistic conceptual space according his Hierarchical Network of Concepts (HNC, in short) theory [Zengyang Huang 1998. The HNC conceptual primitives of the linguistic conceptual space and its word knowledge expression can be used to reduce the dimension. In fact, the structure of HNC conceptual primitives is a tree list, which the nodes are semantic primitives. The word meaning is expressed with the primitives, and it can accomplish the computing of correlation in the glossary, finding the thesaurus, and reducing the dimension.Therefore, we explore the Chinese text authorship identification with the KNN classifier that adopts the HNC conceptual primitives as the features carrier. We explain the HNC conceptual primitives and the semantic expression in the following firstly. The text categorization is an important field for the automatic text information processing. Moreover, the authorship identification of a text can be treated as a special text categorization. This paper adopts the conceptual primitives&apos; expression based on the Hierarchical Network of Concepts (HNC) theory, which can describe the words meaning in hierarchical symbols, in order to avoid the sparse data shortcoming that is aroused by the natural language surface features in text categorization. The KNN algorithm is used as computing classification element. Then, the experiment has been done on the Chinese text authorship identification. The experiment result gives out that the processing mode that is put forward in this paper achieves high correct rate, so it is feasible for the text authorship identification.
Ambiguity of Reflexives and Case Extension * One of well-known distinctions concerning reflexives, and more generally anaphors, is the distinction, introduced in Reinhart (1983), between co-referential and bound reflexives (or anaphors). In English the distinction between co-referential and bound reflexive can be illustrated by the possible ambiguity of the sentence in (1):(1) Only Leo washed himself.This sentence can mean, at least theoretically, either (2a), the co-referential reading of reflexive, or (2b), the bound reflexive: (2a) Only Leo washed Leo. (2b) Leo is the only person who washed himself.Obviously (2a) and (2b) are not equivalent: if nobody in addition to Leo washed himself and Leo was washed, in addition to Leo himself, by someone else then (2a) is false and (2b) is true; and if nobody, except Leo himself, washed Leo and some other person, in addition to Leo, washed him/herself that (2a) is true and (2b) is false.This distinction is related to the distinction between sloppy and strict readings of some pronouns (Dahl 1973) and is illustrated in (3)-(5); the possible interpretation of these sentences is given in the corresponding (b) and (c) sentences: (3a) Only Leo loves his wife (3b) Leo is the only person such that that person loves his own wife (3c) Leo is the only person such that that person loves Leo's wife (4a) Leo loves his wife and so does Bill (4b) Leo loves his (own) wife and Bill loves Leo's wife (4c) Leo loves his (own) wife and Bill loves his own wife (5a) Leo considers himself competent, and so does Lea (5b) Leo considers himself competent and Lea considers Leo competent, too (5c) Leo considers himself competent and Lea considers herself competent tooIn (a) sentences we have a bound variable reading of the reflexive or possessive pronoun and thus the sloppy identity between the corresponding NPs (those related by the anaphor). In (b) sentences we have the strict identity and co-referential readings of pronouns.Notice that the ambiguity does not always arise. In (6) we have only sloppy reading:(6) Leo criticized himself and so did BillThis sentence cannot mean that Bill criticized Leo. In English the distinction between the bound variable reading and the co-referential reading of pronouns is not formally or lexically marked. In many other languages this distinction is more overt and thus it is easier to illustrate it since in those languages different reflexive pronouns can express the corresponding readings. For instance in Polish there are two (at least) reflexive pronouns, the clitic sie and the full pronoun siebie (Lubowicz 1999). As it will be suggested siebie corresponds to co-referential reflexive and sie -the anaphoric reflexive. The situation in Polish is, however, more complex since not all transitive verbs can take both reflexive as complements. For instance verbs of "saying" like criticise or blame or some "psychological" verbs, like hate (in Polish) can take only siebie as possible direct object complement.Of course the indicated difference between co-referential and bound reflexive pronouns occurs in many other languages and it has been often studied, in particular in Germanic (Scandinavian, cf. Hellan 1888) and, as indicated above, in some Slavic languages. Various attempts have been made to explain the difference, basically in the framework of binding theory, that is a syntactic theory. In addition the difference can be easily represented in the lambda calculus for instance. In this paper I will analyse the difference between co-referential and bound reflexives from the (formal) semantic point of view. For that purpose I will use some tools from the generalized quantifier theory, and, in particular the notion of the semantic case theory (SCT) and of the case extension of a quantifier, as introduced by Keenan (1987Keenan ( , 1988Keenan ( , 2007). It is suggested that the difference between co-referential and bound reflexive pronouns found in many languages can be accounted for by using the notion of the case extension of a type &lt;1&gt; quantifier. Given this proposal the co-referential pronouns get their meaning when the corresponding NP takes nominal case extension first. Bound reflexives are reflexivisers in the sense that they are not case extensions of quantifiers although they also transform binary relations into sets. Examples from Japanese and from Polish are discussed. Keywords: Co-referential and bound reflexives, case extension of a quantifier.
Hosted By  
The Discour se of Pr int Adver tising in the Philippines: Generic Str uctur es and Linguistic Featur es One of the most ubiquitous discourses is advertisements. When we watch TV in the comfort of our living rooms, we are bombarded with ads; when we read a newspaper or magazine, somehow our attention is distracted by one form of an ad or another. On our way to school or office, we come across ads in various shapes or colors. Indeed, advertising, whether print, broadcast, or any other type, is part of our everyday lives.It is no wonder then that advertising discourse has attracted the attention of scholars in over two decades. Simpson (2001) acknowledges that there has been "an enormous upsurge of interest in the linguistic and discoursal characteristics of advertising" (p. 589), adding that the studies conducted have been anchored on different traditions and perspectives, such as cognitive, cultural and anthropological, genre and register analysis, critical discourse analysis, and linguistic pragmatics (Simpson, 2001, p. 590). In recent years, research has focused on reader effects of poetic and rhetorical elements in ads from a relevance-theoretic perspective. For instance, van Mulken, van Enschot-van Dijk, and Hoeken (2005) aimed to find out whether slogans in ads are appreciated more than slogans without a pun, and whether puns containing two relevant interpretations are appreciated more than puns containing only one relevant interpretation (p. 707). To do this, 68 participants rated their appreciation of 24 slogans. The results showed that the presence or absence of puns had a significant impact on the respondents' appreciation of the slogans. Furthermore, whether the pun contained two relevant interpretations or only one did not influence the extent to which they were considered funny, but the former were considered a better choice than the latter (van Mulken, van Enschot-van Dijk, and Hoeken, 2005). Lagerwerf (2007), on the other hand, examined the effects on audiences of irony in ads and of sarcasm in public information announcements. Two studies were conducted. Sixty students took part in the first study, with stimuli consisting of 12 magazine ads, six of which were positively formulated and six negatively. In the second, there were 40 students who participated in the experiment, with stimuli consisting of ads that were partly based on the researcher's own designs and partly on actual ads. In advertisements for commercial products and services, irony was found in the use of negative captions where positive captions were expected. Sarcasm was used by placing a positive caption against a background displaying a harrowing picture. Such departures from common practice in the use of negative and positive wordings were regarded as inappropriate. It turned out that advertisements with ironic intent were appreciated more when the inappropriateness was re-interpreted correctly as irony (Study 1). Even so, irony and sarcasm may impede a proper understanding of the advertisements' informative intention. This has a negative impact on the assessment by an audience of the importance of the societal issues emphasized in sarcastic announcements (Study 2) (Lagerwerf, 2007).Working within the pragmatic construct of metadiscourse, Fuertes-Olivera, et al. (2001) analyzed the metadiscourse devices typically used by ad copywriters to construct their slogans and/or headlines. The researchers' analysis proceeded from the assumption that advertising English should be represented as a continuum of text functions fluctuating between "informing" and "manipulating" in accordance with the idea that advertising is an example of covert communication. Based on an examination of ads from a women's magazine, they concluded that both textual and interpersonal metadiscourse devices help copywriters to convey a persuasive message under an informative mask (Fuertes-Olivera, et al., 2001).The present study seeks to contribute to the ongoing interest in describing the discourse of advertising. In particular, it aims to describe magazine ads in the Philippines in terms of their generic structures and linguistic properties, including speech acts performed by utterances. The study is anchored on Simpson's (2001) 'reason'-'tickle' binary distinction between types of advertising discourse, which expands Bernstein's (1974) proposal. In Simpson's view, 'reason' ads are those which suggest a motive or reason for purchase. Furthermore, these ads follow a basic discourse pattern and the corresponding conjunctive adjuncts -conditional, causal, and purposive -which realize the pattern. This pattern parallels the notion of generic structure) (see discussion of generic structure below). 'Tickle' ads, by contrast, are those which appeal to humor, emotion and mood. Unlike reason ads which are stable in terms of structure and whose language is straightforward, 'tickle' ads are indirect, and therefore need the reader's inferencing strategies to figure out what they convey (Simpson, 2001). Figure 1 locates the 'reason'-'tickle' construct within three well-known pragmatic models: This paper aims to examine the generic structures and linguistic properties of ads in Philippine magazines. Taken from the Corpus of Asian Magazine Advertising: The Philippine Database, the corpus consists of seventy-four ads for consumer nondurables such as medicines, vitamins and food supplements, and cosmetic/beauty/personal hygiene products. The study found that the ads demonstrated preference for certain generic structures and linguistic features, making them &apos;reason&apos; (rather than &apos;tickle&apos;) ads which may be described as direct. The paper argues that the directness of these ads contributes to making them covert communication.
From archive to corpus: transcription and annotation in the creation of signed language corpora * A modern linguistic corpus is something more than just a reference dataset of written or transcribed texts of a particular language on which a description of a language is based. If anything, this is an old fashioned sense of corpus. Rather, a corpus in the modern sense is a collection of spoken and spoken texts in a machine-readable form that has been assembled for the purposes of studying the type and frequency of lexical items and grammatical structures and constructions in a language (McEnery and Wilson, 2001). A modern linguistic corpus contains linguistic annotations and appended sociolinguistic and sessional data (metadata) that describe the participants and the circumstances under which the data were collected. With the development of digitized video recording and multi-media annotation software, signed language corpora can now be described as sub-types of 'spoken' (i.e., 'face-to-face') language corpora.Signed language corpora promise to vastly improve peer review of descriptions of signed languages and make possible, for the first time, a corpus-based approach to signed language analysis. Corpora are important for the testing of language hypotheses in all language research at all levels, from phonology, through lexis, morphology and syntax to discourse (Baker, 2006;Halli- day et al., 2004;Hoey et al., 2007;McEnery et al., 2006;Sampson and McCarthy, 2004;Sin- clair 1991). There are several reasons why this is especially true of deaf signing communities. First, signed languages-inevitably young minority language communities-lack written forms and well developed community-based standards of correctness. Second, they have interrupted generational transmission and few native speakers. Third, the representation of signed language examples using written gloss-based text has meant that these data have remained essentially inaccessible to other researchers for meaningful peer review. Thus, although introspection and observation can still be of valuable assistance to linguists developing hypotheses regarding signed language use and structure, one must also recognize that intuitions and researcher observations may fail in the absence of clear native signer consensus of phonological or grammatical typicality, markedness or acceptability. The previous reliance on the intuitions of small numbers of informants has thus been problematic in the field. Despite the fact that research into signed languages has grown dramatically over the past three to four decades, progress in the field has been hindered by these obstacles to data sharing and processing.As with all modern linguistic corpora, signed language corpora should be representative, welldocumented (i.e., with relevant metadata) and machine-readable (i.e., able to be annotated and tagged consistently and systematically) (McEnery and Wilson, 1996;Meyer, 2002;Teubert and Cermáková, 2007). This requires dedicated technology (e.g., ELAN), standards and protocols (e.g., IMDI metadata descriptors), and transparent and agreed grammatical tags (e.g., grammatical class labels) (Crasborn et al, 2007). One aim of this paper is to describe these resources and to identify the principles that need to be adhered to in the creation of signed language corpora such that the goals and practices of corpus linguistics, as now generally understood, can be achieved and implemented with respect to signed languages.The guiding principle behind the annotations being created for the Auslan (Australian Sign Language) corpus is machine-readability, not transcription narrowly understood. The aim is to create an annotated signed language corpus, and not, contrary to the practice of many signed language researchers, a body of signed language texts which have been transcribed to a greater or lesser degree of detail. The reason is that one can now use multi-media annotation software to transform a video recording of signed language into a machine-readable text without it first being necessary to transcribe that text. Transcription is defined here as the encoding of face-toface language (signed or spoken) using a recognized notation system that represents the phonetic or phonological form of the signal, or using a dedicated writing script that represents the conventional units of the language. This is an important consideration in building signed language corpora because there is no standard or widely accepted signed language transcription system. Using this type of multi-media annotation software it is thus now possible to gain instant and unambiguous access to the actual form of the signs being annotated-the video recording-because they are both time aligned. In saying this, it should be noted, however, that this type of multi-media annotation software can only profitably be used to create a machinereadable corpus if signed units are consistently and uniquely identified before more detailed linguistic annotations and tags are appended to them. A secondary aim of this paper is to describe how this can be achieved, using the example of the Auslan corpus.The Auslan corpus annotations that have been created to date are intended primarily for investigations of grammar and discourse, rather than a basic phonological or lexical analysis of the language. The investigation centres on the modification of indicating verbs in terms of frequency of types/tokens, and their environments of occurrence (e.g., during periods of constructed action, with or without contiguous pointing signs, or with reference to the sequential order of related nominal arguments). The focus is on the analysis of the grammatical use of space in Auslan in terms of semantic roles and grammatical relations. 1 1 An Australian Research Council project grant awarded to Louise de Beuzeville and Trevor Johnston-#DP0665254 The linguistic use of space in Auslan: semantic roles and grammatical relations in three dimensions. For initial data on indicating verbs see Johnston et al (2007) and de Beuzeville et al (submitted). For details of the controlled vocabularies and codes (tags) used in these annotated texts the reader is referred to annotation guidelines produced as part of the above mentioned project (see Johnston and de Beuzeville, 2008). ). The guidelines do not attempt to set specific annotation protocols for all signed language corpora. Provided each signed language corpus is internally consistent in its annotation conven-What is being claimed in this paper is that there are two principles which all signed language corpora should adhere to in order to facilitate their optimal use: prioritise annotation above transcription, and identify signs uniquely using gloss-based annotations. Without this the whole rationale for corpus-creation is undermined. The essential characteristic of a signed language corpus is that it has been annotated , and not, contrary to the practice of many signed language researchers, that it has been transcribed. Annotations are necessary for corpus-based investigations of signed or spoken languages. Multi-media annotation software can now be used to transform a recording into a machine-readable text without it first being necessary to transcribe the text, provided that linguistic units are uniquely identified and annotations subsequently appended to these units. These unique identifiers are here referred to as ID-glosses. The use of ID-glosses is only possible if a reference lexical database (i.e., dictionary) exists as the result of prior foundation research into the lexicon. In short, the creators of signed language corpora should prioritize annotation above transcription, and ensure that signs are identified using unique gloss-based annotations. Without this the whole rationale for corpus-creation is undermined .
Scalar Implicatures: Pragmatic Inferences or Grammar? *  This talk discusses the nature of different kinds of scales and controversies over issues on the generation of scalar implicatures, particularly those in complex sentences involving disjunction and another operator in its scope, and so on. The pragmatic position based on Gricean reasoning in opposition to the grammatical position based on alternative semantics and LF syntax employing the exhaustivity (Exh) operator will be examined. The context-driven view and the default view largely still within the pragmatic position will also be discussed. In doing so, the talk will offer my position that scalar implicatures are motivated by Gricean pragmatic reasoning but that they are deeply and crucially rooted in the grammatical devices of Contrastive Topic (CT), overt or covert. CT requires PA (pero/aber) conjunction, i.e. &apos;concessive But&apos; and that&apos;s why scalar implicatures begin with but and its equivalents cross-linguistically. The CT operator rather than the exhaustivity (Exh) operator must be represented to be related to the previous discourse and the forward concessive conjunction.
NIST 2007 Language Recognition Evaluation: From the Perspective of IIR * Automatic spoken language recognition (SLR) is a process of determining the identity of the language in a spoken document. As multilingual applications are demanded by the emerging need for globalization and the growing international business interflow, SLR has become an enabling technology in many applications such as multilingual conversational systems (Zue and Glass, 2000), multilingual speech recognition and translation ( Waibel et al., 2000), and spoken document retrieval ( Dai et al. 2003). It is also a topic of great importance in the areas of intelligence and security, where the language identities of recorded messages and archived materials need to be established before any information can be extracted. SLR technology also facilitates massive on-line language routing for voice surveillance over telephone network.The National Institute of Standards and Technology (NIST) has conducted a series of evaluations of SLR technology in 1996in , 2003in , 2005in (NIST, 2007. The language recognition evaluations (LREs) focus on language and dialect detection in the context of conversational telephony speech. They are conducted to foster research progress, with the goals of exploring promising new ideas in language recognition, developing advanced technology incorporating these ideas, and measuring the performance of this technology. The Institute for demonstrated the state-of-the-art technologies.One of the fundamental issues in SLR is to explore the discriminative cues for spoken languages. In the state-of-the-art language recognition systems, these cues mainly come from the acoustic features (Sugiyama, 1991;Torres-Carassquilo et al., 2002;Burget et al., 2006;Campbell et al., 2006) and phonotactic representations (Hazen and Zue, 1994;Zissman, 1996;Berkling and Barnard, 1994;Corredor-Ardoy et al., 1997;Li and Ma, 2005;Ma, Li, and Tong, 2007), which reflect different aspects of spoken language characteristics. Another issue is how to effectively organize and exploit these language cues obtained from multiple sources in the recognition system design for the best performance.Significant improvements in automatic speech recognition (ASR) have been achieved through exploiting the acoustic features representing the temporal properties of speech spectrum. These acoustic features, such as Mel-frequency Cepstral Coefficients (MFCCs), are also good choices to be the front-ends in language recognition systems. Gaussian mixture model (GMM), which can be seen as a one-state hidden Markov model (HMM) (Rabiner, 1989), is a simple modeling method to provide a multimodal density and is reasonably accurate when speech data are generated from a set of Gaussian distributions. It has demonstrated a great success in textindependent speaker recognition (Reynolds, Quatieri, and Dunn, 2000). In language recognition, GMM is also an effective method to model the unique characteristics among languages (Torres-Carassquilo et al., 2002). The support vector machine (SVM) has proven to be a powerful classifier in many pattern classification tasks. It is a discriminative classifier to separate two classes with a hyperplane in a high-dimensional space. The generalized linear discriminant sequence kernel (GLDS) has been proposed to apply SVM for speaker and language recognition (Campbell et al., 2006). The cepstral feature vectors extracted from an utterance are expanded to a high-dimensional space by calculating all the monomials.In recent years, phonotactic features have been shown to provide effective cues for language recognition. The phonotactic features are extracted from an utterance to represent phonetic constraints in a language. Although common sounds are shared considerably across spoken languages, the statistics of these sounds, such as phone n-gram, can differ considerably from one language to another. Parallel Phone Recognizers followed by Language Models (PPR-LM) (Zissman, 1996) uses multiple parallel phone recognizers to convert the input utterance into a phone token sequence. It is followed by a set of n-gram phone language models that imposes constraints on phone decoding and provides language scores. Instead of n-gram phone language models, vector space modeling (VSM) was proposed as the classifier (Li, Ma, and Lee, 2007), called PPR-VSM. For each phone sequence generated from the multiple phone recognizers, the occurrences of phone n-grams are counted. A phone sequence is then represented as a highdimensional vector of n-gram occurrence. SVM is used as the classifier on the concatenated ngram occurrence vectors.It is generally agreed upon that the integration with different cues of discriminative information can improve the performance of language recognition (Adda-Decker et al., 2003). The information extraction and organization of multiple sources has been critical to a successful language recognition system ( Singer et al., 2003;Tong et al., 2006). In this paper, we will report our language recognition system submitted to the 2007 NIST LRE. The system is based on the fusion of multiple classifiers, each providing unique discriminative cue for language classification. In order to avoid a spoiled classifier in the submitted fusion system, we have designed a pseudo key analysis approach to check the integrity of each individual classifier before the system fusion.The remainder of this paper is organized as follows. The evaluation data and evaluation metric of the 2007 NIST LRE will be introduced in Section 2. The system structure together with the phonotactic and acoustic language classifiers will be presented in Section 3. The fusion of multiple language classifiers and language recognition results on the 2007 NIST LRE evaluation data will be described in Section 4. The pseudo key analysis will be shown in Section 5. Finally in Section 6, we summarize our findings in language recognition. This paper describes the Institute for Infocomm Research (IIR) system for the 2007 Language Recognition Evaluation (LRE) conducted by the National Institute of Standards and Technology (NIST). The submitted system is a fusion of multiple state-of-the-art language classifiers using diversified discriminative language cues. We implemented several state-of-the-art algorithms using both phonotactic and acoustic features. We also investigated the system fusion and score calibration strategy to improve the performance of language recognition, and worked out a pseudo-key analysis approach to cross-validate the performance of the individual classifiers on the evaluation data. We achieve an equal-error-rate (EER) of 1.67 % on the close-set general language recognition test.
A Call for Executable Linguistics Research * Talks that state the obvious, or review well-known research are boring and risk losing an audience. Talks that give controversial positions often have the same result. But I'd rather take the dangerous route, in hopes of spurring some new ideas and new research. A further risk is that I'm a computer scientist by training, not a linguist. I may have substantial blind spots in computational linguistics, and there are undoubtedly people I'm not aware of already working in the direction I will advocate, but that provides a big opportunity for me to learn from your feedback on this talk. I'll focus on a broad goal of language understanding or language processing in Artificial Intelligence. I'll define this as a set of techniques for processing human language that show evidence of the same competencies or behaviors as human language processing. Fundamental to this is the ability to accept statements in language that affect future responses, and the ability to respond to questions that demonstrates prior assimilation of knowledge. I don't believe that a "tabula rasa" approach is feasible in this context; I will take it as a given that a great deal of knowledge must already reside in a practical language understanding system.There has been considerable research in computational linguistics that takes particular linguistic features and subjects them to semantic analysis, with the goal of specifying a formal semantic interpretation derived from syntactic features. This entire area of research has waned however, in part because it was so difficult to combine these sorts of analyses into a single semantic theory. The need for providing some interpretation of all text has moved the computational linguistics field to robust shallow interpretations, rather than brittle and deep ones. A contributing factor has been the need for some very large resources to make it possible to have non-toy implementations of deep linguistic semantic processing. Another issue is that because the scope of linguistic semantics is so large, it's hard to tell if different component theories result in a harmonious total interpretation. We're now at the threshold of being able to address these issues. This is why I call for a new direction of Executable Linguistics Research.Note that I'm not proposing an exclusive alternative to statistical linguistic methods. There are portions of this general problem that benefit from the marriage of statistical and logical approaches, most notably, word sense disambiguation. I'm proposing a shift in emphasis, recommending that more people concentrate on an approach to linguistics that has been somewhat neglected -a shift to focusing on a more difficult and longer term approach, because the utility of current robust and arguably shallow approaches to understanding are yielding less substantial incremental improvements as time goes on.I'll first sketch an outline of the products that I think are needed. Then I'll discuss existing resources that can meet some of those needs. Next I'll provide some concrete examples to show how this all might work. This paper mirrors my invited talk at PACLIC-22. It describes a call for a renewed emphasis in work on the logical semantics of languages. It lists some of the computational components needed for symbolic interpretations of language, and of automated reasoning within those semantics. It details existing components that meet those needs and provides short examples of how they might work. It also touches on how open source products can support collaboration, which is needed on a project that has the scope of creating a full semantics of language.
Some Challenges of Advanced Question-Answering: an Experiment with How-to Questions * Question answering (QA) is an area that operates on top of search engines (such as Google, Exalead, Yahoo, etc.) in order to provide users with more accurate and elaborated responses where search engine response outputs remain punctual, difficult to understand, and sometimes incoherent. QA builds on top of search engine responses via language processing tools, reasoning mechanisms and response production techniques. QA has been recognized as an essential component of man-machine communication since it greatly improves communication with the Web or textual databases, allowing users to communicate in their own language, in order to get much more precise, accurate and user-tailored responses. Factoid questions (questions about facts, e.g.: what is the capital of X ?) have been largely investigated (witness, e.g. the TREC competition), but the other types of questions, frequently encountered such as : procedural (how to), causal (why), evaluative and comparative questions (what is the cheapest air ticket to go to Cebu from Singapore ?) have received little attention so far. However, they correspond to the majority of the 'real' questions asked either by the large public or by professionals to the web or to textual databases. Questions cover a large number of everyday life (health, employment, administrative life, tourism, etc.) as well as technical areas (investments, competitive intelligence, etc.). Complex questions require language processing (to process the question and on the outputs of search engines), reasoning (e.g. data fusion, incoherence solving, summarizing, etc.) and response production in natural language (in a way accessible and user-friendly). These complex questions are in fact those users ask most of the time, these users being professionals or from the large public. Complex questions are traditionally classified according to the type of response which is induced, e.g.:-definition questions, -how-to questions for procedures, -why questions for causes/consequence (possibly involving chains of events), -evaluative and comparative questions (asking for comparisons between two or more elements such as fares).Considering the technology which is required, it is clear that search engines will not be able in the near future or even in the mid-term future to correctly process these questions due to the complexity of the task, the need of some domain knowledge and the necessity to provide natural language responses, beyond text extracts. The current trend in TREC Question Answering is towards the processing of large volumes of open-domain texts (e.g. documents extracted from the World Wide Web). Open domain QA is a hard task because no restriction is imposed either on the question type or on the user's vocabulary. This is why most of the efforts (in evaluation campaigns such as TREC) are focused on answering factoid style questions, and, to a little extend, definition questions, using shallow text processing which is roughly based on pattern extraction or information retrieval techniques. However, QA should integrate more complex techniques, in order to provide, for example, deep semantic analysis of NL questions such as anaphora resolution, context and ambiguity detection, complex question processing, better answer ranking and answer justification, responses to unanticipated questions, response fusion or integration, and response summarization to cite just the main topics. It is also important to be able to resolve situations in which no answer is found or when the answer is incomplete or fuzzy in the data sources. This can be resolved for example via dialogue or interactive QA scenarios. In order to address these future challenges and to guide research in this field, a number of QA roadmaps have been proposed: in 2001, Advanced Question and Answering for Intelligence, the (AQUAINT) program initiative, was created by ARDA. It was then revised in 2002 by the participants of the LREC (Language Resources and Evaluation Conference) QA workshop and further refined at the AAAI (American Association of Artificial Intelligence) Spring Symposium in 2004 and by the KRAQ series 2004-2008. In these roadmaps, the research community states that QA systems should support the integration of deeper modes of language understanding as well as more elaborated reasoning schemas in order to boost the performances of current QA systems as well as the quality and the relevance of the produced answers. Advanced QA systems can be viewed as an enhancement, rather than a rival to retrieval based approaches.In this paper, we present the main principles that we followed to identify titles, instructions, instructional compounds and arguments. From this point of view, our work is a contribution to the semantics of texts, including the recognition of some rhetorical relations, which is known to be a hard problem. This work is applied to French; we are developing a similar study for Thai. A priori, it should be possible to transpose it to a number of other languages. It is interesting to note how useful text semantics can be for question-answering. This paper is a contribution to text semantics processing and its application to advanced question-answering where a significant portion of a well-formed text is required as a response. We focus on procedural texts of various domains, and show how titles, instructions, instructional compounds and arguments can be extracted.
How Even Revises Expectation in a Scalar Model: Analogy with Japanese Mo *  
Toward a Global Infrastructure for the Sustainability of Language Resources * Sustainability has become a byword of our times. In fact, The Global Language Monitor recognized sustainable as the Top Word of 2006. 1 Behind all the buzz there is an important concept that has significance even for our language resources community. Focus on the sustainability of the planet in the news media is making us increasingly aware that unless we mend our wasteful ways, we could squander the world's natural resources along with the opportunity of future generations to enjoy the same quality of life that we do.The language resources community is no stranger to waste. Waste happens when the resources resulting from prior work are no longer available due to the deterioration of the media that store them or the obsolescence of the formats that encode them. Waste also happens when a new project redoes work that has already been done by someone else, because the new project does not know about the prior work or because the prior work was not made available in a form they could use. Even more pervasive is the waste that happens when ordinary users, who would have no ability to create the needed resources, miss out entirely on the opportunity to benefit from resources that already exist because they are not able to discover or access or use them. Rather, we should be seeking to create an environment in which language resources thrive through regular use by all who can benefit from them.This paper describes the work the Open Language Archives Community (OLAC) is doing to address issues like these. First, the paper defines the scope of OLAC by offering a definition of language resource ( §2). Next, it identifies the conditions that are necessary for the sustainable use of language resources ( §3) and defines the roles of the four sets of players-creators, archives, aggregators, and users-that are involved in making such sustainability a reality ( §4). With this background, the paper is then able to describe what OLAC is doing to contribute to a global infrastructure for supporting the sustainable use of language resources ( §5). The concluding section probes the broader issue of sustainable development to consider the sustainability of language resources in the context of the sustainability of language development and of languages themselves ( §6). This paper describes work the Open Language Archives Community (OLAC) is doing to contribute to a global infrastructure for the sustainability of language resources. After offering a definition of language resource, it addresses the issue of what makes language resources sustainable by defining six necessary and sufficient conditions for their sustained use, then discusses what it takes to make such sustainability a reality by describing the roles of four key sets of players-creators, archives, aggregators, users. With this background, the paper describes the community infrastructure OLAC has developed for allowing its members to express consensus about best practices for digital archiving, plus the technical infrastructure it has developed to provide aggregation and search for the language resources community. The concluding section probes the broader issue of sustainable development to consider the sustainability of language resources in the context of the sustainability of language development and of languages themselves.
Constituent Structure for Filipino: Induction through Probabilistic Approaches * This paper discusses the algorithms used for the automatic induction of grammar for the Filipino language.The rationale for the study stems on the minimal work done on the development of a computational grammar for the Filipino language for the development of robust and industrialstrength natural language analysis and technologies. Existing Filipino grammars can only handle a subset of declarative type sentences. Considering the difficulty to manually construct a robust grammar capable of parsing a broad scope of sentences, automatic grammar induction is a consideration that can be used for learning language structure.Automated grammar induction systems deals with the generation of a grammar based from input corpora. Existing work for grammar induction fall under two categories based on their input constraints: Supervised and Unsupervised. Slightly supervised systems generate grammar rules from bracketed corpora or tree banks. Bracketed corpora are text documents that have been bracketed by a linguist to represent the skeletal syntactic structure of the sentences. Treebanks are large corpora that have been annotated with the part of speech tags, syntactic structure, and other functional attributes necessary. Unsupervised systems make use of nonbracketed corpora while applying searching and clustering algorithms to attempt to learn the language rules.Works on slightly supervised grammar induction, such as the works of Lari and Young(1991), Brill(1993), Sekine and Grishman(1995), and Charniak(1996), present different output formalisms to represent the grammar. However, Filipino is currently a resource-limited language and does not have the computational resources necessary for the algorithms presented. There are existing corpora available for the language, but these have not yet been bracketed. Osborne and Briscoe(1997), Clark(2001), Klein and Manning,(2001) attempt to learn the grammar formalism for the language through the use of statistical analysis methods applied to a tagged corpus. Klein and Manning(2002) applies a context-constituency model and achieved promising results, precision rate of 55% and recall rate of 48%. Existing systems are designed for the English language and have had modifications applied that are specific to the right-biased nature and Subject-Verb-Object structure. The Filipino language does not follow these structures; rather it has free word order patterns, and Predicate-Topic phenomena, wherein the focus of a sentence is referred to as Topic rather than as a Subject. Klein(2005) experimented with a combination of the said context-constituency model and a dependency model Klein and Manning(2004), which was then applied to English, German, and Chinese. The English language reached recall of 88% and precision of 69%. The German language had a corresponding recall value of 90%, but a considerably lower precision of 50%, caused by relatively flat gold standard corpora. The flat structure of the German language is attributed to free word order, a phenomena also identified in Filipino.The aim is to develop an automated grammar induction system using an unsupervised approach. The approach chosen is influenced by the limitation of computational grammar resources in Filipino. Existing induction algorithms, usually for the English language, are modified to handle Filipino language phenomena. The current state of Philippine linguistic resources, which includes formal grammars, electronic dictionaries and corpora are not yet significant to address industrial-strength language technologies. This paper discusses a computational approach in automatically estimating constituent structures from a corpus using unsupervised probabilistic approaches. Two models are presented and results show an F1 measure of greater than 69%. Issues and phenomena of the Filipino language are identified and discussed
Who&apos;s Missing in the Group? Argument Sharing in Core Cosubordinate Construction in Filipino * Cosubordination is defined as one type of nexus relation in which units of equivalent sizes are strung together in a coordinate-like relation with no marker of syntactic dependency is found between and among units (Olson, 1981). These units share some grammatical categories such as arguments, aspects, negation, and other operators. Argument sharing is a process in which one argument in the matrix core is the same with another argument found from the linked core. However, the problem lies in the syntactic representation of a cosubordinate clause in which there is a missing argument from the linked core. That seems to be a violation of a theory known as Completeness Constraint as it states that all arguments overtly expressed in the semantic representation of a clause must be realised in the syntax (Van Valin, 2005). Before we move on to our discussion of argument sharing, let us first clarify some terms in the literature. Control construction, as one theory applied in analysing argument sharing in cosubordination, refers to how the controller of the missing NP in the linked core is to be determined (Van Valin, 2005). In an ergative language like Filipino, subject control and object control cannot be used, for these terms will lead to some problems. Thus, non-subject actor, controller, or syntactic pivot will be used to avoid confusion. In sentence (1), the single argument of the matrix core realised by ko 'I' is in ergative case functioning as a non-subject actor in the construction and cannot be called either subject or object controller. The semantic role of the argument ko 'I' is assigned by its nucleus pinilit 'tried', a patient verb. The argument in the linked core, on the other hand, is missing but although this argument is not overtly expressed in the syntax, it is clear that this argument is the same with the syntactic argument in the matrix core making this non-subject actor the controller by default. This paper examines the relationship between two important arguments in core cosubordinate construction in Filipino: namely, controller found in the matrix core and the missing argument (controllee) in the linked core using the framework Role and Reference Grammar of Van Valin (2005). This paper has proved that there is really an argument sharing between two cores and each core plays a crucial role in the syntactic development of cosubordinate clauses. The first core assigns the juncture levels and nexus of relations of the units involved, whereas the second core determines the syntactic structure of the clause. The researcher also found out that regardless of the number of arguments found in the linked core, the matrix core argument which is also the controller is the same with the missing actor argument from the linked core.
Natural Language Generation of Museum Object Descriptions based on User Model With the emergence of the Internet, lack of information is no longer a major problem most users face. The vast amount of information available is overwhelming and it is important to hide information that is irrelevant to the users. One example that provides vast information to the people is museums. A museum is an institution providing services-acquiring, conserving, researching, communicating, and exhibiting objects for education and enjoyment (ICOM, 2001). Some museums are made available online through virtual museums so that more people can have access to the information provided by the museum.In order to improve the services that virtual museums provide, they should be able to determine which information is most relevant to a particular user and object information should be presented in such a way that the user can understand (Bandelli, 1999).Similar to internet users, visitors of museums have different expectations, needs and behaviors, and it is important to address these differences. Ambeth Ocampo, a columnist at a notable Philippine newspaper, says that, "While I appreciate the educational task of museums, I would like to think that all our best efforts are still not enough to get the youth into museums and keep them returning… The problem lies not with a museum but a child's first encounter with it" (Ocampo, 2007). Ocampo also says that for many college students who had to endure a grade school trip to the museum, going there a second or third time is considered a cruel and unusual punishment. This mind-set is not the fault of the museum; it is the fault of the teacher or museum guide who did not infect the students with a sense of discovery and appreciation of our past. Thus, improving the encounter with a museum through a web-based interactive virtual museum that generates descriptions based on user profile is the motivation for this research.The next section introduces the virtual museum we have created. Section 3 expounds more on the components of the virtual museum. In Section 4, we discuss how we modeled the user preference and how these are updated. Section 5 presents the rules created to generate the different types of description. Samples of generated descriptions based on user profile is also discussed in this section. In Section 6, we discuss the testing done for the system. Lastly, we give our conclusion and indicate some future work in Section 7. Natural Language Generation (NLG) techniques can be applied in generating virtual documents dynamically using information from a database (Dale et al, 1999). One of the applications of NLG techniques to generate documents dynamically is the web-based interactive virtual museum, VIGAN. NLG is used to generate the descriptions of the objects in a virtual museum dynamically based on the profile and interests of the visitor. The focus of the research is on incorporating user&apos;s interest, age group, and visit history in the generation of museum object descriptions. The descriptions do not vary only on user&apos;s profile but also in lexicalization. Facts are not only in describing the objects, but also in describing Ilocano personalities. User Acceptance Testing proved that object descriptions do vary based on age groups, category of interest, and lexicalization. They commented that the descriptions are easy to understand, the user interface is user friendly and the suggested objects are appropriate.
Sources of Individuation in Mandarin Chinese, a Classifier Language * Language allows us to express different perspectives towards things in the world. For example, a single object, like a wooden table, can be described both as a table (i.e., a kind of object), and as some wood (i.e., a kind of material). The ways in which these perspectives are expressed, however, differs from language to language, leading some to claim that speakers of different language may think differently about objects in the world (Lucy, 1992;Imai &amp; Gentner, 1997;Quine 1960). This paper contributes to this debate by probing the representation and development of syntactic cues to individuation in Mandarin Chinese.In English, a distinction can be made between count nouns and mass nouns. Typically, words like dog, table and idea are used as count nouns, and refer to kinds of things that have "atomic structure", with "atoms" or "individuals" that come in natural units for counting. When hearing one of these words (e.g., dogs), we know that it refers to a quantity of discrete, naturally bounded individuals, and not some arbitrary portions thereof (e.g., pieces of dog). In contrast, mass syntax does not specify individuation (see Bloom 1994;Gordon 1988;Link 1983). Mass nouns can refer to unindividuated stuff like water, wood, and fun, or to sets of individuals like footware, furniture, and ammunition. Ususally, words used in mass syntax do not refer to individuals. In English count nouns can occur directly with numerals (e.g., one dog), in singular or plural forms (a dog, some dogs), or with quasi-cardinal determiners (these dogs) that signal reference to sets of individuals. Mass nouns, in contrast, usually 1 cannot be used directly with numerals, with singular or plural morphology, or with quasi-cardinal determiners. Also, mass and count constructions selectively specify different quantifiers (many/*much table vs. *many wood/much wood).Not all languages, however, have such transparent syntactic cues to individuation. In classifier languages like Chinese and Japanese, there is no mass-count distinction at the level of the noun, regardless of what the noun refers to. Instead, nouns in classifier languages syntactically resemble mass nouns in English. For example, in Mandarin Chinese, nouns cannot co-occur directly with numerals, but require a discretizing unit (i.e., a "classifier") for counting, like English mass nouns (e.g. two pieces of toast). Classifiers encode information such as the shape, animacy, functionality, or the unit of measure of the referent noun. For example, to label three pens in Mandarin requires both the numeral san (three) and the classifier zhi (stick) as in san zhi bi (or "three stick pen"). Also, unlike English count nouns, which obligatorily specify number via singular-plural marking (e.g., a cat vs. some cats), classifier languages normally lack obligatory plural marking. As a result, bare nouns in classifier languages are unspecified for number. If a classifier language has a plural marker, its use is often optional, infrequent, and restricted (e.g., to animates). Finally, nouns in classifier languages, irrespective of whether they denote countable individuals or unindividuated stuff, typically permit the same quantifiers. 2 If count syntax specifies individuation in English, are there equivalent syntactic structures to encode individuation in languages that lack a mass-count distinction? According to Sybesma (1998, 1999) Mandarin Chinese may make an analogous distinction at the level of the classifier. They argue that Mandarin features two types of classifier, which they call "count classifiers" and "mass classifiers". Count classifiers form a closed-class and mark reference to individuals, whereas mass classifiers form an open-class and function as measure words that are used to denote portions of unindividuated stuff (e.g., a cup of sugar) or portions of objects (e.g., a cup of marbles).To test the hypothesis that classifiers are semantically analogous to mass-count syntax, Li, Barner and Huang (in press) examined how Mandarin speakers interpret them in a word extension task. Typically, when speakers of English learn new count nouns (e.g., "Look, this is a wug"), they assume that these words refer to kinds of objects that share a common form. When they learn mass nouns ("Look, this is some wug"), in contrast, they are less likely to assume that the word denotes a solid thing. Based on this, Li et al asked if Mandarin speakers would extend count classifiers, like gen (rod) and pian (slice) to solid things with matching shape (e.g., rod shapes or slice shapes), and whether they would do so less for mass classifiers like dui (pile) and tuan (wad). When asked to find "one CL something" (CL = classifier) among several choices, adults selected solid, shape-matched objects when presented count classifiers, but rejected things that were non-solid or that didn't match in shape. However, this distinction was less available to young children, who were willing at age four to accept portions of non-solid stuff when the experimenter requested something with a count classifier, so long as the substance matched the shape specified by the classifier (e.g., toothpaste that was shaped like a rod when the experimenter asked for gen). It was not until approximately six years of age that children began to extend classifiers like adults. Li and colleagues concluded that, before 6 years of age, most Mandarin-speaking children still have not learned that count classifiers are cues to individuation. As a result, syntactic cues to individuation may emerge much later in Mandarin than in English One problem in comparing classifiers to count syntax is that whereas count syntax specifies only individuation, classifiers also encode item-specific conceptual information, such as the shape, animacy, etc. Classifiers may emerge later in Mandarin acquisition in part because they pose a more difficult learning problem. In the present studies, we explored this question by testing children and adults with the generic or default classifier ge rather than on shape-based classifiers tested in Li et al's study. Ge is by far the most frequent count classifier in the language. As it provides no shape information, it may function mainly to encode individuation. Therefore, we might expect children to learn the role of this classifier in individuation earlier.In addition to ge, we also tested Mandarin speakers' sensitivity to another potential source to individuation -the diminutive suffix -zi (Doetjes, 1997;Sybesma, 2007). Linguists have noted that in several languages diminutive markers function much like unitizers in making mass nouns into count nouns (Wiltschko, 2006). Historically, -zi, (meaning "son" or "child") functioned as a diminutive marker in Chinese. However, simplification of the phonological system in the language during the Han Dynasty, and a movement away from monosyllabic nouns led to the adaptation of -zi as an ending for many nouns (Li &amp; Thompson, 1981). In contemporary Mandarin it is debateable whether the suffix is still productive (Nishimoto, 2003). When asked about the function of -zi, native speakers are often unable to state its contribution to the meaning of the noun. Nonetheless, linguists have observed that many nouns used with -zi often refer to "concrete, non-abstract things, that can be counted individually" (Dragunov, 1960, p. 81;Sybesma, 2007). It is possible that speakers of Mandarin are implicitly sensitive to this property of -zi and that -zi could function as a cue to individuation. However, no experiments have ever examined -zi and its relation to object individuation. We, therefore, included -zi in our study and compared it to the general classifier ge, in the domain of word learning.The present study investigated whether the general classifier ge and the diminutive suffix -zi are cues to individuation in Mandarin-speaking children and adults. Experiment 1 tested the hypothesis that -zi tends to occur with nouns that denote discrete individuals by asking whether this relation held true for the 256 most frequent nouns in Mandarin child-directed speech. Experiment 2 then examined how adult speakers interpret both -zi and ge by contrasting them with bare nouns in two word learning tasks. Experiment 3 extended Experiment 2 by testing Mandarin-speaking children's developing comprehension of the classifier ge, in order to establish the role of classifiers in children's emerging understanding of individuation. When presented with an entity (e.g., a wooden honey-dipper) labeled with a novel noun, how does a listener know that the noun refers to an instance of an object kind (honey-dipper) rather than to a substance kind (wood)? While English speakers draw upon count-mass syntax for clues to the noun&apos;s meaning, linguists have proposed that classifier languages, which lack count-mass syntax, provide other syntactic cues. Three experiments tested Mandarin-speakers&apos; sensitivity to the diminutive suffix-zi and the general classifier ge when interpreting novel nouns. Experiment 1 found that-zi occurs more frequently with nouns that denote object kinds. Experiment 2 demonstrated Mandarin-speaking adults&apos; sensitivity to ge and-zi when inferring novel word meanings. Experiment 3 tested Mandarin three-to six-year-olds&apos; sensitivity to ge. We discuss differences in the developmental course of these cues relative to cues in English, and the impact of this difference to children&apos;s understanding of individuation.
How to Overcome the Domain Barriers in Pattern-Based Machine Translation System * The use of on-line systems is the biggest growth area in the use of machine translation. People are translating web pages or very large documents by using machine translation system as the solution, as human translation of pages which need to be continually updated or are very large scale is not feasible (Mellebeek et.al., 2005).Electronics and Telecommunications Research Institute (ETRI, henceforth) in Korea has developed the web-based English-Korean machine translation system till 2004, under assumption that as the size of patterns grows, the performance of the system can be incrementally improved (Hong et. al., 2003). During 2 years (2005)(2006)) it implemented an English-Korean patent machine translation system on the basis of the web-based EnglishKorean machine translation system. The English-Korean patent machine translation system was installed in International Patent Assistance Center (IPAC, henceforth) under Ministry of Commerce, Industry and Energy in Korea and provides the patent attorneys and the patent examiners with the on-line English-Korean machine translation service for electro-electric patent documents (http://www.ipac.or.kr) because it helped them understand the existing English patent documents easier and more rapidly .It was due to the customization method ) that we could change successfully an existing machine translation system from general domain to patent domain. Figure 1 shows us an example of patent machine translation service at IPAC. ETRI had upgraded the customization method applied to the English-Korean patent machine translation system since 2007 and completed the practical level of English-Korean scientific paper machine translation system within 8 months. The English-Korean machine translation service for scientific paper translation is expected to be launched for students since September, 2008.This paper describes how we have resolved such barriers among domains as default target word of any domain, domain-specific patterns, and domain adaptation of engine modules in pattern-based machine translation system, especially English-Korean patterns-based machine translation system. Especially, we will describe a difference between the pure customization method for patent machine translation system and the upgraded customization method applied to scientific paper machine translation system. The construction of this paper is as follows: in section 2 the pure customization method will be sketched and its experiment will be showed. The limits of the pure customization method will be uncovered in the section 3. To deal with the problems found in the experiment, an upgraded customization method will be proposed in the section 4. In section 5 another experiment will be conducted to evaluate the proposed method. The discussion of the previous sections will be summarized in the concluding section 6. One of difficult issues in pattern-based machine translation system is maybe to find how to overcome the domain difference in adapting a system from one domain to other domain. This paper describes how we have resolved such barriers among domains as default target word of any domain, domain-specific patterns, and domain adaptation of engine modules in pattern-based machine translation system, especially English-Korean pattern-based machine translation system. For this, we will discuss two types of customization methods which mean a method adapting an existing system to new domain. One is the pure customization method introduced for patent machine translation system in 2006 and another is the upgraded customization method applied to scientific paper machine translation system in 2007. By introducing an upgraded customization method, we could implement a practical machine translation system for scientific paper translation within 8 months, in comparison with the patent machine translation system that was completed even in 24 months by the pure customization method. The translation accuracy of scientific paper machine translation system also rose 77.25% to 81.10% in spite of short term of 8 months.
Multi-Engine Approach for Named Entity Recognition in Bengali * Named Entity Recognition (NER) is an important tool in almost all Natural Language Processing (NLP) application areas including machine translation, question answering, information retrieval, information extraction, automatic summarization etc. The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN's IdentiFinder in (Bikel, 1999)), ME (New York University's MENE in (Borthwick, 1999)), CRFs ( Lafferty et al., 2001) and SVM ( Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in ( Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting.Named Entity (NE) identification in Indian languages in general and in Bengali in particular is difficult and challenging as: Unlike English and most of the European languages, Bengali lacks capitalization information, which plays a very important role in identifying NEs. Indian person names are more diverse and a lot of these words can be found in the dictionary with specific meanings.  Bengali is a highly inflectional language providing one of the richest and most challenging sets of linguistic and statistical features resulting in long and complex wordforms.  Bengali is a relatively free order language.  Bengali, like other Indian languages, is a resource poor language -annotated corpora, name dictionaries, good morphological analyzers, Part of Speech (POS) taggers etc.are not yet available in the required measure.  Although Indian languages have a very old and rich literary history, technological developments are of recent origin.  Web sources for name lists are available in English, but such lists are not available in Bengali forcing the use of transliteration. A pattern directed shallow parsing approach for NER in Bengali is reported in Ekbal and Bandyopadhyay (2007a). A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are kept for handling the unknown words. More recently, the related works in this area can be found in Ekbal et al. (2008a), Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Cucerzan and Yarowsky (1999) with a language independent method. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 2 . This paper reports about a multi-engine approach for the development of a NER system in Bengali by combining the classifiers such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) with the help of weighted voting approach. The training set consists of approximately 272K wordforms, out of which 150K wordforms have been manually annotated with the four major named entity (NE) tags such as Person, Location, Organization and Miscellaneous tags. An appropriate tag conversion routine has been defined in order to convert the 122K wordforms of the IJCNLP-08 NER shared task 1 , into the desired forms. The classifiers make use of the different contextual information of the words along with the variety of features that are helpful in predicting the various NE classes. Lexical context patterns, which are generated from an unlabeled corpus of 3 million wordforms in a semi-automatic way, have been used as the features of the classifiers in order to improve their performance. In addition, we have developed a number of techniques to post-process the output of each of the classifiers in order to reduce the errors and to improve the performance. Finally, we have applied weighted voting approach to combine the systems. Results show the effectiveness of the proposed approach with the overall average recall, precision, and f-score values of 93.98%, 90.63%, and 92.28%, respectively, which shows an improvement of 14.92% in f-score over the best performing baseline SVM based system and an improvement of 18.36% in f-score over the least performing baseline ME based system. The proposed system also outperforms the other existing Bengali NER system.
Incorporation of WordNet Features to n-gram Features in a Language Modeler * n-gram language modeling is a popular statistical language modeling (SLM) technique used to improve performance of various natural language processing (NLP) applications such as speech recognition (SR), machine translation (MT), and information retrieval (IR). However, it still faces the "curse of dimensionality" issue wherein word sequences (i.e. n-grams) on which the model will be tested are likely to be different from those seen during training (Bengio et al., 2003). This means that these sequences would always be assigned low probabilities.There have already been attempts to address this problem. A number of works made use of smoothing techniques such as the one presented in (Callison-Burch and Flournoy, 2001) which made use of linear interpolation of trigram, bigram and unigram probabilities. ( Bengio et al., 2003) presents an approach that combines neural networks and smoothing techniques to a trigram model while the work in (Brockett et al., 2001) made use of additional linguistic features, such as syntax trees, in order to construct a decision tree that will classify sentences.Another possible solution is to make use of other linguistic resources such as WordNet ( Fellbaum et al., 2006) and devise an approach that combines WordNet features (i.e. synsets and their relations) with the n-grams used in language modelers. By incorporating WordNet features, the language modeler can generate related sequences which can possibly give it a higher score even if there are no exact matches seen during training.( Hoberman and Rosenfeld, 2002) presents a study on the integration of WordNet features to address the data sparseness of nouns in bigrams. The study covered the IS-A relationship of nouns and reported improvement in the language model perplexity, although the improvement was below expectation.This paper presents an extension of the study in (Hoberman and Rosenfeld, 2002). A trigram language modeler has been developed that considers other parts of speech (i.e adjective, adverb, verb) and relationships (i.e. HAS-A, Synonymy/Antonymy), in addition to nouns and the IS-A relationship, to address the "curse of dimensionality" issue. Since there are many existing MT systems with different ways of producing translations, the language modeler was used as a tool to automatically rank parallel translations (i.e. translations produced by multiple MT systems). The ranking of translations was based on the method found in (Callison-Burch and Flournoy, 2001) where sentences are ranked based on their fluency, which are computed using the probabilities of their trigram components. Trigrams were used for the n-gram language modeler since the study in (Callison-Burch and Flournoy, 2001) showed that trigrams are effective when used in ranking parallel translations.Section 2 discusses the architecture of the language modeler while Section 3 presents an example on how the new approach works. Section 4 presents the evaluation results both in terms of addressing the "curse of dimensionality" issue and when applied to the ranking of parallel translations. Lastly, Section 5 contains the conclusions and the discussion of possible future works for this research. n-gram language modeling is a popular technique used to improve performance of various NLP applications. However, it still faces the &quot;curse of dimensionality&quot; issue wherein word sequences on which the model will be tested are likely to be different from those seen during training (Bengio et al., 2003). An approach that incorporates WordNet to a trigram language modeler has been developed to address this issue. WordNet was used to generate proxy trigrams that may be used to reinforce the fluency of the given trigrams. Evaluation results reported a significant decrease in model perplexity showing that the new method, evaluated using the English language in the business news domain, is capable of addressing the issue. The modeler was also used as a tool to rank parallel translations produced by multiple Machine Translation systems. Results showed a 6-7% improvement over the base approach (Callison-Burch and Flournoy, 2001) in correctly ranking parallel translations.
Korean Parsing Based on the Applicative Combinatory Categorial Grammar * In this paper, we propose a new approach to Categorial Grammars by introducing Curry's Combinatory Logic (Curry &amp; Feys 1958) in order to improve the parsing of Korean texts from a computational point of view.Since the introduction of simple Categorial Grammars, different propositions were made to improve this formalism by adopting applicative languages such as the calculus of syntactic types proposed by J. Lambek (1961), the lambda-calculus proposed by A. Church, the combinatory logic created by the mathematician H.-B. Curry (1958), some attempts by the logician W. V. O. Quine, etc. These works are based on the mechanism of the application of an operator to an operand. Combinatory logic and lambda-calculus were applied to the analysis of grammatical and lexical meaning in natural languages by S. K. Shaumyan (1987) with his model of the Universal Applicational Grammar using Curry's combinatory logic, which extends the simple Categorial Grammars: this model is easily implementable on computational tools using functional programming languages such as CAML, HASKELL and SCHEME. In the 80's, important extensions were given by R. Montague, M. Moortgat (1988), J. Lambek and M. Steedman (1989). Combinatory Categorial Grammar (CCG) developed by Steedman (1989Steedman ( , 2001) was most often quoted and studied for the analysis of Korean sentences.There exist several studies on Korean parsing based on the Categorial Grammar formalism. For example, the Korean Combinatory Categorial Grammar (KCCG) was developed by (Cha 2001 andCha &amp; Lee 2002) by extending the CCG of Steedman for the Korean parsing. The KCCG, having a purely computational approach, shows the ability to handle important linguistic phenomena of the Korean such as coordination, long distance scrambling, free word Copyright 2008 by Juyeon Kang, Jean-Pierre Desclés order, etc. Cho and Park (2000) tried also to improve the complexity in the coordination, and Lee and Park (2003) proposed a morphological analysis of the irregular conjugation of Korean in order to conceive a morphological parser.The studies presented above and most of the related works are based exclusively on the CCG formalism of Steedman and developed in the purpose of a computational realization. Thus they often ignore the linguistic aspect of language and cannot capture some fine points such as morphological cases in Korean.Compared to these works based on the CCG formalism, the ACCG formalism that we develop in this paper, is not only a computational but also a linguistic approach, namely it better reflects the linguistic aspect in the Korean natural language processing. Consequently, this advantage allows us to parse the Korean language in a more explicit way and to show clearly the morphosyntactic structure of the Korean through our calculations. Thus, the ACCG formalism is a new approach which is both linguistic and computational.This formalism allows us to scope the difficult characteristics of the Korean that we can often find during automatic processing. In particular, we are interested in the problem of cases in Korean including the phenomenon of double case. Despite of their importance in parsing texts, cases have not been well studied from a computational point of view. Once we analyze the cases in the ACCG formalism, we will use some of the results of these analyses to handle the problem of free word order structure and coordination structure. This formalism leads us to easily analyze the free word order structure by a simple application of the combinatory rules we developed. This approach allows us to handle even long distance scrambling in the coordination structure, which is one of the most difficult problems in Korean parsing and has not been completely analyzed in other works (e.g. Cha 2001). The Applicative Combinatory Categorial Grammar (ACCG) is a new approach to Categorial Grammars by using exclusively the Combinatory Logic. This extended categorial grammar that was originally developed by J.-P. Desclés and I. Biskri, allows us to tackle the problem of the Korean language parsing in which there exist many difficulties from a computational point of view. In this paper, we handle in particular some parsing problems in Korean such as the problem of case, the free word order phenomenon, the coordination structure and the long distance scrambling in the coordination structure. We will show throughout this work some new and robust solutions for the Korean parsing in the ACCG formalism by introducing combinators such as B, C*, Φ of Combinatory Logic developed by H.-B. Curry and R. Feys.
Stress Processing Sensitivity in Reading Korean and English Words * Spoken words can be characterized in terms of suprasegmental or prosodic features, markedly stress, determined by acoustic frequency, intensity, and/or duration. Such a stressed-syllable is relatively louder and longer than other syllables in the same word or phrase (Ladefoged, 2001). In some languages such as Korean, French, and Czech, stress pattern of words is somehow predictable and syllable-based. For example, in Korean, stress mostly falls on the first syllable, otherwise on the second syllable, displaying no significant linguistic difference (Lee, 1990;Park, 2004). In French, stress dominantly falls on the final syllable with a full vowel, with no distinctive minimal pairs of words differed by its stress pattern and in Czech, stress almost always falls on the first syllable of a word (Jannedy, Poletto and Weldon, 1994). These syllablebased languages do not show meaning and grammatical differences influenced by their stress patterns.In other languages such as English, although the placement of stress is less predictable, stress can involve in lexical contrasts, causing a difference in meaning (i.e., TRUsty-truSTEE). In addition, it can change grammatical functions of words. For example, the word progress functions as a noun when the stress is placed on the first syllable, whereas as a verb when the stress is placed on the second syllable. The other stress-based languages in which speech sounds is controlled by stress are Spanish and Dutch (Goetry, Wade-Woolley, Kolnsky and Mousty, 2006). These languages do also illustrate lexical contrasts defined by their stress patterns.Current understanding of the stress pattern across languages suggests that the linguistic role of stress pattern widely differs with respect to lexical and grammatical functions and the rule of stress assignment is language specific. Recent studies on first-language (L1) stress sensitivity have demonstrated that L1 speakers of stress-based languages (i.e., English, Spanish, and Dutch) are more sensitive to stress patterns than those of syllable-based languages (i.e, Korean and French). For example, given the discrepant lexical function of stress between Spanish and French, adult L1 speakers of French produced more errors in judging whether a string of pseudo-words differing in stress pattern display the same stress pattern or not than adult L1 speakers of Spanish (Dupoux, Pallier, Sebastian-Galles and Mehler, 1997). In another follow-up study of phoneme-and stress-contrast sensitivity, Dupoux and colleagues (Dupoux, Peperkamp and Sebastian-Galles, 2001) found that adult L1 speakers of Spanish and French performed similarly on the phoneme contrast judgment (i.e., /kypi/-/kyti/). However, the Spanish-native monolinguals significantly outperformed the French-native monolinguals on the stress contrast judgment (i.e, /'kipi/-/ki'pi/).In the meantime, the sensitivity to stress patterns has not received much attention in L2 reading research until recently because suprasegmental information is not manifested in most written systems. In a recent pioneering study of sensitivity of prosodic features among monolingual and bilingual first graders, Goetry and associates (Goetry, Wade-Woolley, Kolinsky and Mousty, 2006) compared the stress and phonemic awareness of French-native, Dutch-native monolingual, French-native children taught in Dutch, and Dutch-native children taught in French. The results showed that the four groups of first-graders performed similarly on phonemic awareness judgment (i.e., /'tepy/-/'tapy/) but differed on stress sensitivity judgment (i.e., /'tipy/-/ti'py/). The Dutch monolinguals notably outperformed the French monolinguals. The other bilingual first-graders had the intermediary performances. More importantly, Goetry et al. (2006) also suggest that early literacy development in a second stress-based language can be influenced by stress sensitivity by observing the correlation between stress awareness and word reading in the French monolinguals schooled in Dutch, not in the Dutch monolinguals schooled in French.Based on the findings of existing research on cross-linguistic comparisons of L1 and L2 stress awareness, stress processing ability may play a crucial role in learning to read in a second stress-based language such as English, Dutch, or Spanish, especially when students' L1 is not stress-based. Moreover, spoken word processing is largely related to written word identification (Morais, 2003). At this point, it is significant to explore the stress assignment sensitivity of word reading in Korean, a syllable-based language, compared with that in English, a stress-based language as there has been little exploitation of learning to concurrently speak and read in an L1 with transparent stress rule and an L2 with opaque stress rule. In addition, previous studies have mainly focused on the perceptive sensitivity of stress contrasts. Consequently, more attention needs to be raised in interpreting the productive sensitivity of stress assignment. As discussed earlier, the stress pattern of Korean is somewhat predictable and regular without distinguishing meanings and grammatical functions of words. On the other hand, because the stress rule in English varies depending on word classes (Roca and Johnson, 1999), to some degree, it is less predictable and irregular with lexical and grammatical contrasts. Indeed, if Korean-speaking English language learners (ELLs) face exclusively distinctive stress patterns of English, which is not relevant to their L1, they may confront drastic restructuring of their interlanguage stress assignments and deal with unstable prosodic representations across the two languages. As a result, they would be at risk for difficulties in learning to speak and read in English as a stress-sensitive L2 and displaying prosodic information of English words.Given the discrepant stress assignment between Korean and English, the primary goal of this current study is to investigate the stress processing sensitivity of Korean-speaking ELLs in terms of number of syllables in a word and syllable structure in the word. The three research questions addressed in this study are as follows:(1) Given the distinctive stress assignment between Korean and English, what are the stress processing abilities of Korean-speaking ELLs in the L1 (Korean) and in the L2 (English)? (2) Depending on the number of syllables, how does students' stress sensitivity differ between the two languages? (3) Within a syllable structure, how sensitive are they to stress patterns across the two languages?Investigating language-specific stress awareness engaged in L1 and L2 word reading can shed light on the degree of cross-language transfer and can furthermore suggest that dissimilar L2-specific prosodic features increase particular difficulties in L2 stress processing due to overgeneralized stress rules gained from L1. The present study explored the sensitivity to stress patterns of sixty-four ninth-graders learning to speak and read in Korean as a first language (L1) and English as a second language (L2) concurrently. Students&apos; productive stress processing abilities were assessed in reading Korean real words, English unfamiliar real words, and English pseudo-words. Results unveiled that the Korean-speaking English language learners (ELLs) performed differently between Korean and English in terms of number of syllables and stress placement within a syllable structure. More specifically, their stress processing performances between the two languages clearly differed when the number of syllables increases and their stress assignment differences across the two languages were much larger on dissimilar stress patterns than similar ones. These findings suggest that unfamiliar L2-specific prosodic information such as stress may present additional challenges to L2 learners, especially when their L1 is not stress-based.
Constructing an Ontology of Coherence Relations: An example of &apos;causal relation&apos; *  The goal of this paper is to present the methodology of constructing a language ontology of &apos;coherent relations&apos;. We construct an ontology of more abstract concepts by combining an upper-level ontology and a middle-level ontology. At the former we use theoretical considerations following Sanders et al. (1992, 1993), and at the latter we use lexical items through the substitutability test of Knott and Dale (1994).
An Improved Corpus Comparison Approach to Domain Specific Term Recognition * Automatic term recognition (ATR) plays an important role in many natural language processing applications, e.g., information retrieval (IR) (Chowdhury, 1999;Zhou and Nie, 2005), information extraction ( Yangarber et al., 2000), domain specific lexicon construction (Hull, 2001), and topic extraction (Lin, 2004). Its technological advancements can facilitate all these applications for performance enhancement.Despite the large volume of literature on ATR, further significant success in the field still relies heavily on a sound resolution of two basic issues, namely, the unithood and termhood of a term candidate, as identified in Kageura and Umino (1996). The former quantifies the unity of a candidate (especially, a multi-word candidate), indicating how likely a candidate is to be an atomic linguistic unit. It works more like a filter to exclude non-atomic (thus unqualified) term candidates but has little authority to determine which atomic language unit is a true term. The latter measures how likely a qualified candidate is to be a true term in a subject field. It plays a decisive role in licensing a term.Regardless of the previous progress in term extraction, termhood measurement remains the most critical problem to be solved. Novel technologies and methodologies are needed in order to bring up new insights into our understanding of this problem. This paper is intended to pre-sent a novel statistical approach to domain specific term extraction from a collection of thematic documents following the basic ideas of corpus comparison and emerging pattern. It measures the termhood of a term candidate in a subject domain in terms of its peculiarity to this subject domain via comparison to several background domains.In order to avoid unexpected interference from unithood issues, our study focuses on investigating the termhood measurement for mono-word terms. Nevertheless, this is by no means to imply that mono-word ATR can be any easier than multi-word ATR in any sense. It is pointed out in Daille (1994) that the automatic identification of mono-word terms is possibly more complex than that of multi-word ones. One of the reasons is that all structural information that can be utilized for multi-word ATR is not available for mono-word ATR. In this sense, the latter needs to tackle a fundamental issue, that is, how to differentiate between terms and non-terms without resorting to structure information.The rest of the paper is organized as follows. Section 2 presents a brief review of previous work on ATR, to give a background for our research. Section 3 formulates our approach and the working procedure involved. A series of experiments are then reported in Section 4 for the purpose of evaluation. Section 5 concludes the paper with a highlight on the advantages of our approach. Domain specific terms are words carrying special conceptual meanings in a subject field. Automatic term recognition plays an important role in many natural language processing and knowledge engineering applications such as information retrieval and knowledge mining. This paper explores a novel approach to automatic term extraction based on the basic ideas of corpus comparison and emerging pattern with significant elaboration. It measures the termhood of a term candidate in terms of its peculiarity to a given domain via comparison to several background domains. Our experiments confirm its out-performance against other approaches, achieving an average precision of 83% on the top 10% candidates in terms of their termhood.
Extending an Indonesian Semantic Analysis-based Question Answering System with Linguistic and World Knowledge Axioms * A question anwering (QA) system seeks to provide answers to questions expressed in natural language, where the answers are to be found in a given collection of documents. QA systems typically require more sophisticated linguistic analysis than conventional information retrieval, as they need to reason about various other factors, among others the types of questions, predicate argument structure, and result aggregation.In the work presented in this paper, we start from a unification-based grammar augmented with lambda-calculus rules that constructs semantic representations of Indonesian declarative sentences. To these representations we subsequently combine a suite of axioms designed to encode linguistic and world knowledge, and assert them into a knowledge base. A separate QA module answers queries by unifying the question semantic representation with the augmented set knowledge base.In Sections 2 and 3 we first discuss some relevant past work. Section 4 presents the overall framework of our system, and Section 5 discusses the semantic representation underlying our approach, arguing for some form of axiomatic post-processing. Finally, Sections 6 and 7 present the axioms themselves, along with some examples of how they contribute to the QA process. We adopt a previously developed model of deep syntactic and semantic processing to support question answering for Bahasa Indonesia, and extend it by adding a number of axioms designed to encode useful knowledge for answering questions, thus increasing the inferential power of the QA system. We believe this approach can increase the robustness of semantic analysis-based QA systems, whilst simultaneously lightening the burden of complexity in designing semantic attachment rules that transduce logical forms from syntactic structures. We show how these added axioms enable the system to answer questions which previously could not have been answered.
An Implementation of a Flexible Author-Reviewer Model of Generation using Genetic Algorithms *  This paper proposes performing natural language generation using genetic algorithms to address two issues: the difficulty in controlling global textual features which arise from a large number of interdependent local decisions, and the difficulty in applying conventional NLG wisdom in domains where the communicative goal lacks sufficient detail. It presents details of an implemented system that embodies the aforementioned proposal, and discusses the results of an empirical study conducted using the system.
Semantic Change of the Selected Cebuano Words* "Earlier grammars and dictionaries reveal that Cebuano-Bisaya has undergone rapid changes in the course of the last four centuries." (Wollf, 2001, p. 121). There are words listed in the earlier dictionaries that seem to be unknown in the current time. In fact, some of the young Cebuano speakers who are more exposed to other languages such as English do not understand the speech of their elders who used old Cebuano terms. Trask (1994) also pointed out that the young generation had a hard time in reading what the ancestors wrote 400 years ago. In the same manner, the descendants would surely find reading what the current generation of speakers is writing presently or the future generation could even hardly understand the tape recordings and films shown these days.Because of the cultural influences, the language was also affected. There are many Cebuano terms that are currently used by the native speakers that were originated in English and Spanish. In fact, "when the Spaniards left the Philippines in 1898 after almost 350 years of colonization…" as reported by Gonzalez (2004, p. 73), ten percent of the Filipinos spoke in Spanish. This study attempted to determine the types of semantic change of the selected Cebuano words from the written texts, specifically the Bisaya magazine and spoken language of Cebuano speakers aged 15-40 years old living in Cebu province. Twenty (20) Cebuano words were analyzed using Campbell&apos;s (1998) and Crowley&apos;s (1997) classifications of the semantic change. The results revealed that metaphor was the dominant type of semantic change in the written text and broadening was frequently-used in the spoken language.
On Japanese Desiderative Constructions * Ranging across a number of differing expressions in differing languages, there are various constructions described as complex predicates. Japanese also abounds in such predicates, which consist of a stem verb or gerundive expression followed by another morpheme. Passives and causatives, for example, have been a focus of attention in many linguistic studies. However, only few attempts have so far been made at desideratives by comparison. In this paper we conduct a detailed examination of desiderative constructions in Japanese with the main focus on the suffix, ta(i) 'want'.Ta(i) 'want' is suffixed to a stem verb and forms an adjective as exemplified in (1):The active counter part for (1) is the following (2):As shown in (1), the object argument of the stem verb can be marked with either nominative ga or accusative wo, though the stem verb originally marks its object by only accusative as in (2).We call the type with nominative object ga-desiderative, and the type with accusative object wo-desiderative.One question here is why the object of desiderative construction can be marked with either nominative or accusative. Under the previous approaches based on series of transformations or movements, the desideratives have been claimed to have different complex structures at some abstract level of derivation or representation. However, there are some data that cannot be accounted for in terms of the only structural distinction.In this paper we discuss the syntactic and semantic properties of desideratives with the main focus on ta(i) 'want', show the lexical representation of the suffix, and answer to the question. This paper describes desiderative constructions in Japanese with the main focus on ta(i) &apos;want&apos; desideratives. In spite of the morphological one-word status, desiderative constructions have been claimed to have a complex structure at some abstract level of representation. We claim that there are two types of desideratives, and that their predicates have different lexical representations within the framework of Combinatory Categorial Grammar. Building on the proposed analysis, we also discuss the difference between the two types of desideratives in terms of adverbial modification and passivizability.
Integrating Prosodics into a Language Model for Spoken Language Understanding of Thai * In the realm of artificial intelligence, there is no denying that the ultimate goal for man-machine communication is to use natural human languages. Humans aspire to build a machine capable of understanding and responding to commands issued in the form of spoken language. Research towards this goal encompasses various research disciplines, and the fruits of such labor include machine translation, speech synthesis and recognition, and spoken language understanding.Spoken language understanding has been a challenge for scientists and engineers for many decades. Early efforts were primarily aimed at simply recognizing speech, and extensive research has been conducted to improve performance of speech recognition systems. Much of the improvements can be attributed to combining various research disciplines including linguistics, psychology, computer science, and signal processing, as well as improvements in modeling techniques, such as statistical and symbolic pattern recognition. Speech models now incorporate linguistic modeling to account for coarticulation and grammatical constraints to reduce the search space for the correct utterance. Also, steady advances in the computational efficiency of computers and hardware have enabled researchers to implement computationally intensive models that were deemed nearly impossible in the past.With the remarkable advances in speech recognition, the focus has now shifted toward the development of spoken language understanding systems. The goal is to build a machine capable of understanding naturally spoken language by combining together speech recognition and natural language processing technologies. Such systems must be capable of recognizing a large number of words (on the order of tens of thousand), and they also need to make better use of additional information present in the speech signal, such as prosody. It is well known that prosodic information, such as pause, stress, intonation, etc., facilitates human speech communication by helping disambiguate utterances.Prosody is an important aspect of speech that needs to be fully explored and utilized as a disambiguating knowledge source in spoken language understanding systems. Statistical modeling of prosodic information has the potential to be used as an additional knowledge source for both lowlevel processing (recognizing) and high-level processing (parsing) of spoken sentences. A knowledge source may be thought of as a module in spoken language understanding system, which contributes to the understanding of a spoken sentence.Low-level use of prosodic information potentially allows for more accurate recognition at the phonetic and phonological levels. It may also prove to be useful at the stage of lexical access during word hypothesization. The most relevant prosodic information at this level is intra-word and syllable-level stress. A lexicon containing word stress patterns may help the recognizer discard word hypotheses with poorly matched stress patterns. A syllable is considered stressed if it is pronounced more prominently than adjacent syllables. Stressed syllables are usually louder, longer, and higher in pitch.High-level use of prosodic information can help resolve ambiguities inherent in natural language independent of contextual information since computers do not currently utilize all the knowledge sources that humans do. Relevant prosodic information at this level includes pauses, sentential stress, and intonation contours. Sentential or inter-word stress information can be used in speech recognizers to resolve lexical and syntactic ambiguities. For example, the degree of stress among words in a spoken sentence provides an acoustic cue for distinguishing between content (noun, verb) and function (auxiliary, preposition, etc.) words. Furthermore, the marking of prosodic phrases may improve parsing performance by reducing syntactic ambiguities since prosodic groupings may rule out some of the syntactic groupings for a syntactically ambiguous sentence. Also, the identification of sentence mood (i.e., declarative, interrogative, command, etc.) from intonation contours may reduce syntactic and pragmatic ambiguity.The overall objective of this research is to study the role of prosody in the implementation of a spoken language understanding system for Thai since prosodic information has the potential to be used as a pruning mechanism at both the low and high levels of spoken language processing. In particular, we specifically examine how salient prosodic features of Thai (e.g., stress) can be integrated with the overall language modeling scheme. Thai is the official language of Thailand, a country in the Southeast Asia region. The language is spoken by approximately 65 million people throughout different parts of the country. The written form is used in school and all official forms of communication.Language modeling is one of the many important aspects in natural (both written and spoken) language processing by computer. For example, in a spoken language understanding system, a good language model not only improves the accuracy of low-level acoustic models of speech, but also reduces task perplexity (the average number of choices at any decision point) by making better use of high-level knowledge sources including prosodic, syntactic, semantic, and pragmatic knowledge sources. A language model often consists of a grammar written using some formalism which is applied to a sentence by utilizing some sort of parsing algorithm.To overcome the difficulties in parsing Thai, we believe that a constraint dependency grammar (CDG) parser proposed by Potisuk (1996) appears to be an attractive choice for analyzing Thai sentences, considering vantage points from both written and spoken language processing aspects of an automatic system. CDG parsers rule out ungrammatical sentences by propagating constraints. Constraints are developed based on a dependency-based representation of syntax. The motivation for our choice of dependency grammar, instead of phrase-structure grammar, stems from the fact that it appears that Thai syntax might be better described by the former representation.Our work in this paper extends the adopted CDG parsing approach by incorporating prosodic constraints into a grammar for Thai. Prosodic information is specifically used as part of a disambiguation process. Disambiguation is accomplished through the use of prosodic constraints which identify the strength of association between prosodic and syntactic structures of the sentence hypotheses. We next describe the proposed basic framework for a Thai spoken language understanding system and detail the necessary steps for achieving the goal of integrating prosodic information with other aspects of the language model. This paper describes a preliminary work on prosody modeling aspect of a spoken language understanding system for Thai. Specifically, the model is designed to integrate prosodics into a language model based on constraint dependency grammar. There are two steps involved, namely the prosodic annotation process and the prosodic disambiguation process. The annotation process uses prosodic information (stress, pause and syllable duration) obtained from the speech signal during low-level acoustic processing to encode each word in the parse graph with a prosodic feature called strength dynamic. The goal of the annotation process is to capture the correspondence between the phonological and phonetic attributes of the prosodic structure of utterances and their intended meanings. The prosodic disambiguation process involves the propagation of prosodic constraints designed to eliminate syntactic ambiguities of the input utterance. It is considered a pruning mechanism for the parser by using prosodic information. The aforementioned process has been tested on a set of ambiguous sentences, which represents various structural ambiguities involving five types of compounds in Thai.
Using a Word-Space Model to Determine the Relevance of Messages in Anchored Asynchronous Online Discussions * Asynchronous Online Discussions (AOD), also known as forums or threaded discussion boards, are a popular form of web-based computer-mediated communication (CMC). More recently, this form of communication has come into focus in education as an alternative form of assessment that can be used to supplement traditional classroom discussions. The advantage of this communication medium lies in its anytime and anywhere accessibility which provides logistic flexibility for students to communicate with each other and for teachers to assess their interaction. A disadvantage of this medium however is that the focus of discussion is prone to topic drifting; this drifting is primarily due to the time-lags inherent in the asynchronous mode of discussion. Another problem is that the manual monitoring and assessment of message contributions are time-consuming and often tedious, especially for teachers. For these reasons, automated analysis for discussion understanding and better methods of topic-focus monitoring to enable more efficient information assessment are much sought for (Ravi &amp; Kim, 2007).In our study, we aim to investigate the feasibility of utilizing the functionality of word-space models for the task of analyzing online discussion transcripts. More particularly, we want to determine how word-space models can be used to detect the relevance of individual message posts Copyright 2008 by Rodolfo Raga, Jennifer Raga, Eric Bonus, and Raymund Sison relative to the overall topic of discussion. Several methods for generating word-space models are available and have been successfully tested on detecting similarity between terms in groups of texts, but to our knowledge very few have been applied and tested on analyzing online discussion transcripts, a rare example is that of (McArthur and Bruza, 2003) which applied Latent Semantic Analysis on a small dataset of email correspondence to detect conversational implicatures. In our experiments, we chose to use the Random Indexing (RI) technique. RI is a word-space modeling technique primarily designed to measure similarity between terms and not between documents. In our work, we extended the capability of RI to work both at the document and message size levels.The rest of the paper has the following organization: The second section endeavored to put our study into context by providing a brief background on the problem of topic drifting in online discussions and cites a recent approach used to address it. The third section discussed the concept of word-space models and provides a brief introduction on how it can be implemented using RI. The fourth section presented a description of the experiments we conducted. In the fifth section we presented the results of the experiments and provided some discussion. We ended the paper with our conclusion and expected future directions. This paper presents results of the first phase of our study aimed at investigating the applicability of word-space models, particularly those generated using Random Indexing (RI) technique, for the task of determining the relevance of messages posted in anchored asynchronous online discussion forums. In this phase, we addressed several questions intended to establish baseline figures: How efficient will the word-space model perform in this task? How much of its classification decisions align with the decisions of the human annotators? More importantly, does the paradigmatic and syntagmatic contexts of words have a direct effect on its output and which produces the best results? Using Cohen&apos;s Kappa and Holsti&apos;s Coefficient Reliability measure, our experiments generated an initial reliability performance (K=0.41, CR=0.72). It further indicated that the syntagmatic context is more applicable for this task. We concluded with a discussion of the weaknesses identified and possible means of improving the level of performance.
Trend-based Document Clustering for Sensitive and Stable Topic Detection * Due to the information explosion on the WWW, the cost of catching up with the latest trends has risen. Consumer Generated Media (CGM), such as weblogs and social networking service (SNS), are only accelerating this explosion. The best approach to recognizing trends from among the huge number of documents being created is to analyze the topics in them.The goal of Topic Detection and Tracking (TDT) is to find state-of-the-art events in a stream of broadcast news stories ( Allan et al., 1998). The study defines segmentation, new event detection, and event tracking as the major tasks. Segmentation proceeds by automatically dividing a text stream into topically homogeneous blocks. New event detection identifies stories in several continuous news streams that pertain to new or previously unidentified events. Event tracking identifies any and all subsequent stories describing the same event as sample instances of stories describing the event. Document clustering is an efficient approach to find topics in many documents.In the tasks, new event detection is intimately related to clustering, and involves the functions of retrospective detection and on-line detection. The input to retrospective detection task is the entire corpus, and it is desired to divide them into event-specific groups. The input to on-line detection is a chronologically ordered document stream, and the change point of topics should be found.On the WWW, where documents are numerous and increasing hourly, our goal is to provide an environment that supports users on finding and tracking the topics. In particular, sensitive detection of new topics is needed there. Then, our research is categorized as both on-line event detection and event tracking in TDT. As a matter of fact, both aspects are essential for adequately grasping the topics. This paper introduces a trend-based document clustering algorithm that enables the detection of topic occurrence at the earliest possible stage and the observation of topic transition.The remainder of this paper is organized as follows: Section 2 describes related work; Section 3 describes our clustering algorithm; Section 4 describes our experiments and their results; and we conclude in Section 5. The ability to detect new topics and track them is important given the huge amounts of documents. This paper introduces a trend-based document clustering algorithm for analyzing them. Its key characteristic is that it gives scores to words on the basis of the fluctuation in word frequency. The algorithm generates clusters in a practical time, with O(n) processing cost due to preliminary calculation of document distances. The attribute allows the user to settle on the best level of granularity for identifying topics. Experiments prove that our algorithm can gather relevant documents with F measure of 63.0% on average from the beginning to the end of topic lifetime and it largely surpasses other algorithms.
Sentiment Sentence Extraction Using a Hierarchical Directed Acyclic Graph Structure and a Bootstrap Approach * As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. Finding information relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user's opinions for products. Buying products, users usually survey the product reviews. More precise and effective methods for evaluating the products are useful for users. To classify the opinions is one of the hottest topics in natural language processing. Many researchers have recently studied extraction and classification of opinions ( Hatzivassiloglou and McKeown 1997, Kobayashi et al. 2005, Pang et al. 2002, Wiebe and Riloff 2005.There are many research areas for sentiment analysis; extraction of sentiment expressions, identification of sentiment polarity of sentences, classification of review documents and so on. The goal of our study is to easily construct a corpus for sentiment information for particular domains that users want. For the purpose we need to extract sentiment sentences from documents as the 1st step of the corpus construction. Extraction of sentiment expressions or sentiment sentences is one of the most important tasks in the sentiment analysis because classification tasks usually need a large amount of training data to generate a high accuracy classifier. There are several reports for classification of sentences ( Kudo andMatsumoto 2004, Osajima et al .2005). However, the purpose of these studies is to classify sentences into positive and negative opinions. Our purpose in this paper is to classify sentences into opinions and non-opinions. Touge et al. (2004) and Kawaguchi et al. (2006) have proposed methods for opinion extraction. However, these approaches essentially need a large amount of training data for the process. Construction of training data by hand is costly. Kaji and Kitsuregara (2006) have reported a method of acquisition of sentiment sentences in HTML documents. The method required only several rules by hand and obtained high accuracy. Also they have proposed a method for building lexicon for sentiment analysis (Kaji and Kitsuregawa 2007). The knowledge extracted from the Web by using the proposed methods contains the huge quantities of words and sentences. Takamura et al. (2005) also have reported a method for extracting polarity of words. These dictionaries are versatile and valuable for users because they do not depend on a specific domain. Here, assume that we need to construct a system for a domain. In that case, we often desire domain-specific knowledge for the system. Therefore, we need to efficiently extract sentiment sentences, which depend on a particular domain or topic.In this paper, we propose a method of sentiment sentence extraction. It uses several sample sentences for the extraction process. In the process, we compute a similarity between the sample sentences and target sentences. Yu and Hatzivassiloglou (2003) have reported a similarity based method using words, phrases and WordNet synsets for sentiment sentence extraction. However, word-level features are not always suitable for the extraction process because of lack of relations between words. For the similarity calculation we, therefore, employ the graph-based approach, called Hierarchical Directed Acyclic Graph (HDAG), which has been proposed by Suzuki et al. (2006). Furthermore, we apply a bootstrap approach into the sentiment sentence extraction process. The number of extracted sentiment sentences increases with the bootstrap approach. As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. We obtain a huge number of review documents that include user&apos;s opinions for products. To classify the opinions is one of the hottest topics in natural language processing. In general, we need a large amount of training data for the classification process. However, construction of training data by hand is costly. The goal of our study is to construct a sentiment tagging tool for particular domains. In this paper, we propose a method of sentiment sentence extraction for the 1st step of the system. For the task, we use a Hierarchical Directed Acyclic Graph (HDAG) structure. We obtained high accuracy with the graph based approach. Furthermore, we apply a bootstrap approach to the sentiment sentence extraction process. The experimental result shows the effectiveness of the method.
An Effective Speech Understanding Method with a Multiple Speech Recognizer based on Output Selection using Edit Distance * Speech understanding and dialogue systems have been developed for practical use recently. These systems often recognize user utterances incorrectly. It is important to deal with speech recognition errors for speech understanding systems. Extracting keywords and understanding an utterance using them reduce speech recognition errors ( Bouwman et al. 1999, Komatani andKawahara 2000). Another approach is to use domain-specific grammars and linguistic models. However these methods can not handle out of domain and spontaneous utterances. One approach for the improvement is to repair recognition errors by users. There are many studies on detection of recognition errors in a speech output. Goto et al. (2005) have proposed some systems with nonverbal speech information, such as "SPEECH STARTER" and "SPEECH SPOTTER". Ogata and Goto (2005) have proposed a speech input interface with a speech-repair function. Although repairing recognition errors by humans is effective in terms of development of a speech understanding system with high recognition accuracy, it is costly for users.Combining some recognizers is one of the best approaches to improve the accuracy of speech understanding systems ( Isobe et al. 2007, Utsuro et al. 2004). Utsuro et al. (2004) have obtained high accuracy by using some speech recognizers' outputs. However they dealt with word error reduction only. Although Isobe et al. (2007) have proposed a multi-domain speech recognition system by using some domain-specific recognizers, their system cannot treat out-of-domain utterances such as a chat between users. However the chat utterances often include a significant role as the context of the dialogue.In this paper we propose a simple and effective speech understanding method based on a large vocabulary continuous speech recognizer (LVCSR) and some domain-specific speech recognizers (DSSR). The task of this system is speech understanding for a livelihood support robot. The DSSRs recognize particular utterances about orders; e.g., order utterances from elders who need care and order utterances from nurses. Figure 1 shows the outline of the proposed method. We construct the grammar-based DDSR for order utterances with small vocabulary and high accuracy for each order type. We use the LVCSR for recognition of utterances that the DDSR can not recognize, such as a chat between users. The information recognized by the LVCSR is of assistance for context construction of a dialogue. If we handle these different speech recognizers selectively and integratively, we realize a flexible and robust speech understanding method. Figure 2 shows the effectiveness of the proposed multiple recognizer. The DDSR achieves the order recognition with high accuracy and the LVCSR supplies lack of information in the order utterances.In this paper we use two recognizers, a large vocabulary continuous speech recognizer and a domain-specific speech recognizer for user's order utterance understanding. In the experiment we focus on the selective usage of the multiple speech recognizer. In other words, it is to select outputs from each recognizer. For example, with respect to the utterance "Please pick up the remote" in Figure 2, it is important which result to select. For the selection we propose the One Commoner and Some Specialists (OCSS) model. In our system, the LVCSR is the commoner, namely domain-independent, and the DSSRs are specialists, namely domain-dependent. We focus on the difference between outputs generated from the commoner and specialists. For the method, we compare several features to judge whether an input utterance is an order to the robot or not. In this paper, we propose a simple and effective method for speech understanding. The method incorporates some speech recognizers. We use two recognizers, a large vocabulary continuous speech recognizer and a domain-specific speech recognizer. The integrated recognizer is a robust and flexible method for speech understanding. For the integration process, we use a simple edit distance measure of each output sentence from each recognizer. Our method has high scalability and accuracy. The experimental results show the effectiveness of the proposed method.
The Relationship between Semantic Similarity and Subcategorization Frames in English: A Stochastic Test Using ICE-GB and WordNet * The following examples taken from Lasnik and Uriagereka (2005) bring out an issue concerning the relationship between semantic similarity of verbs and the corresponding subcategorization frames.(1) a. She asked what time it was. b. She asked the time. c. She wondered what time it was. b. *She wondered the time.Sentences in (1) show that verbs like ask and wonder, which are similar in meaning, show different ways of argument realization. Although it is true that arbitrariness is at the core of the form-meaning mapping in any language, there are other aspects of language where formal or grammatical realization seems to be largely determined by meaning. Levin (1993) provides many examples of semantic classes of English verbs that lead to the same or similar pattern of argument realizations. For example, the cut verbs (e.g. chip, cut, scrape, snip, etc.), a semantically identifiable group of words in the sense that they all involve notions of motion, contact, and effect, do not allow causative alternations. That is, their subcategorization frames are affected by their meaning.(2) a. Carol cut the bread.b. *The bread cut.The question we raise in this paper is if the meaning restriction applies in a significant way to the realization of argument structure, if it does, to what extent it holds. In other words, how much is a syntactic pattern tied up with the semantic properties of the English verbs? In order to test this research question in a rather comprehensive way, we make use of large sets of language resources, in particular, ICE-GB and WordNet. In the process, we propose a more articulated methodology to take advantage of the language resources statistically and computationally. Also introduced in this paper are a software package to measure similarity of lexicons, an algorithm to cluster words in natural languages, and a kind of stochastic model to calculate selectional preference strength. In this paper, we test a working hypothesis that there is a significant relationship between semantic similarity and subcategorization frames in English. This paper is under the assumption that if a group of verbs form a cluster sharing a similar meaning, they tend to share subcategorization frames. In the process, we propose a statistical method to test this assumption, making use of two language resources, namely, ICE-GB and WordNet. We come to the conclusion that the proposed hypothesis holds true to the degrees of 42~73%, based on our stochastic analysis.
Automatic Bilingual Lexicon Extraction for a Minority Target Language * Automatic bilingual lexicon extraction is the automated process of acquiring bilingual word pairs from corpora in order to construct a lexicon. The product of this process, a bilingual lexicon (or a dictionary), is commonly used for Machine Translation, Natural Language Processing, and other linguistic usages. Since acquiring lexicon from parallel corpora already yields 99% accuracy (Rapp, 1999), this study focused into processing non-parallel corpora, specifically on comparable corpora.In addition, a lexicon is more helpful if it has the feature of grouping similar senses together. This feature is called word sense discrimination. In this study, the syntax and senses of the word are the contributing factors to discriminate words that have several meanings.Throughout the years, linguists and translators compile different types of lexicons manually from experts and automatically from corpora, which are collections of texts and other forms of writings. Parallel corpora refer to a source text and its translation into one or more target languages (Ahrenberg, et al., 1999). Parallel corpora are used for lexicon extraction due to the following characteristics (Fung, 1998): (1) Words have one sense per corpus; (2) Words have single translation per corpus; (3) No missing translations in the target document; (4) Frequencies of bilingual word occurrences are comparable; and (5) Positions of bilingual word occurrences are comparable.However, acquiring parallel corpora is labor intensive and time consuming. It is also unlikely that one can find parallel corpora in any given domain in electronic form especially for minority languages. This problem can be solved by using non-parallel corpora. Non-parallel corpora come in two forms: comparable and non-comparable corpora. Comparable corpora are collections of documents that have common domains or topics but different authors and published dates, while non-comparable corpora have totally different domains, authors, and published dates. This factor is crucial for languages with minimal language resources, and the utilization of existing language resource (such as non-comparable corpora) should be maximized to automatically generate other language resources (such as a bilingual lexicon).Several methods have been developed to utilize these types of corpora to automatically construct a lexicon such as the co-occurrence model (Rapp, 1999), the Convec algorithm (Fung, 1998), the exploration of other clues ( Koehn and Knight, 2002), the dependency grammar approach (Zhou, 2001), and the word filtering approach ( Sadat et al., 2003). Others utilize these types of resources for word sense disambiguation (WSD) (Kikui, 1999 andKaji, H et al., 2005). Rapp (1999) yielded 65% accuracy when the first word in the ranked list was considered and 72% when other senses were considered. Convec (Fung, 1998) yielded a 30-76% precision when the top 20 ranks of the translations were considered. Zhou et al. (2001) achieved a 70.8% accuracy for high frequency verbs and 81.48% for low frequency words. Koehn et al. (2002) achieved 39% noun translation accuracy. The average precision of Sadat et al. (2003) was 41.7%.The main idea behind these techniques is to collect words that co-occur with the source word in the corpora and to establish the correlation between the patterns of word co-occurrences in the corpora of another language. Words that occur in a certain context window in one language have translations that are likely to occur in a similar context window in another language ( Koehn et al., 2002).Nevertheless, relying on context alone is not sufficient to handle ambiguity. For example, the word "find" in the sentence "How did you find the Philippines?" may be interpreted automatically as either "discovery" or "observe". The process of assigning correct senses to an ambiguous word, called disambiguation, is addressed by the following word sense disambiguation approaches (Kikui, 1999 andKaji, H et al., 2005).On the other hand, discrimination is a subtask of disambiguation that clusters (or groups) similar senses of a word into a number of classes (Schutze, 1998). Part-of-speech (POS) provides syntactic information and serves as a useful clue to sense disambiguation ( Stevenson et al., 2001), and imply its usefulness to sense discrimination. However, current studies on discrimination are applied to monolingual corpora and syntactic information is not included in classifying word senses.This study used the basic concepts introduced by Rapp (1999) to build a lexicon from two comparable, non-parallel corpora of the same domain. Thus, it assumes the existence of a bilingual lexicon as its initial seed lexicon. The application domain of this study is on tagged English and Tagalog corpora. Also, a small scale set of tagged English and Tagalog corpora is used to simulate the presented approach.The research did not support phrasal translations, and bi-directional translations such as Tagalog to English.The evaluation based on Rapp (1999) and Zhou et al. (2001) used 50 high and 50 low word frequency occurrences from the corpora and manually validated by human experts. The criteria of their judgment were based on the acceptability of the outputs. Section 2 discusses the approach taken in this study, and the various modules of the developed system. Section 3 presents the evaluation results. And finally, Section 4 presents some recommendations and conclusions. An automated approach of extracting bilingual lexicon from comparable, non-parallel corpora was developed for a target language with limited linguistic resources. We combined approaches from previous researches which only concentrated on context extraction, clustering techniques, or usage of part of speech tags for defining the different senses of a word. The domain-specific corpora for the source language contain 381,553 English words, while the target language with minimal language resources contain 92,610 Tagalog word, with 4,817 and 3,421 distinct root words, respectively. Despite the use of limited amount of corpora (400k vs Sadat&apos;s (2003) 39M word corpora) and seed lexicon (9,026 entries vs Rapp&apos;s (1999) 16,380 entries), the evaluation yielded promising results. The 50 high and 50 low frequency words yielded 50.29% and 31.37% recall values, and 56.12% and 21.98% precision values, respectively, which are within the range of values from previous studies, 39-84.45% (Koehn et al., 2002 and Zhou et al., 2001). Ranking showed an improvement to overall F-measure from 7.32% to 10.65%.
Using &apos;Low-cost&apos; Learning Features for Pronoun Resolution The computational resolution of anaphoric expressions lies at the heart of a variety of NLP applications, including text understanding, Machine Translation, text summarization and many others. Although it has received a great deal of attention for many years now, anaphora resolution remains a computational problem yet to be overcome (Mitkov, 2002), and a challenge that is considerably increased if we speak of languages for which basic NLP resources (such as parsers, taggers or large corpora) are still under development, or may have only recently become available. This is the case, for instance, of Portuguese, one of the most widely-spoken languages in the world, and which still lacks somewhat behind as a relatively resource-poor language in NLP.In this work we extend our previous investigation on learning approaches to Portuguese personal pronoun resolution in (Cuevas et. al., 2008.) In doing so, we focus on so-called 'lowcost' learning features, that is, we will limit the proposed solution to the knowledge readily obtainable from basic NLP tools such as part-of-speech taggers, and we will largely bypass deep syntactic or semantic analysis. In this sense, our work resembles the knowledge-poor approach in Kennedy &amp; Boguraev (1996), which consists of a re-interpretation of the 'classic' algorithm proposed in Lappin &amp; Leass (1994) using shallow rather than in-depth analysis. In addition to that, as we do not intend to explicitly write any anaphora resolution algorithms or rules (but rather induce them automatically) our work is mainly related to machine learning approaches such as Soon et. al. (2001), McCarthy and Lehnert (1995) and Ng &amp; Cardie (2002). However, in discussing a possible 'low-cost' learning approach to Portuguese third person plural pronouns ("Eles/Elas"), we will focus more on the choice of learning features, and less on the results of a particular machine learning approach, which are to be discussed elsewhere.The rest of this paper is structured as follows. Section 2 reviews previous work taken as the basis for our present investigation. Section 3 proposed an extended set of features for the problem at hand. Results of a standard decision-tree induction algorithm using the new features are presented in Section 4. Finally, Section 5 draws a number of comparisons with related work in Portuguese pronoun resolution and Section 6 describes our future work. We investigate a machine learning approach to Portuguese pronoun resolution. We presently focus on so-called &apos;low-cost&apos; learning features readily obtainable from the output of a part-of-speech tagger, and we largely bypass deep syntactic and semantic analysis. Preliminary results show significant improvement in resolution precision and recall, and are comparable to existing rule-based approaches for the Portuguese language spoken in Brazil.
Natural Language Database Interface for the Community Based Monitoring System * CBMS is a poverty monitoring system that is now used in Pasay City. It tracks poverty by surveying the people living in a certain area. Once this is done, they input them using the programs Census Professional (CSPro) and CMS-Natural Resource Database (NRDB), which are customized free software used to encode the data and to digitize spot maps. The output of CSPro is a text file, which is then used as an input to the program STATA, which they use to actually statistically monitor poverty. Since STATA needs technical skills to be used effectively, the data is very hard to access. Thus, people who actually must have direct access to these pieces of information must first ask the people/person who are/is capable of using the software to get the data for them, which might take some time. To address this problem, a Natural Language Database Interface (NLDBI) was developed for CBMS that will allow users to access the CBMS data without having to learn STATA. A NLDBI allows users to access a database using natural language query. It accepts a user query, extracts pertinent data from the query, converts the extracted data to SQL, then retrieves the data from the database. AlLaDIn is domain-dependent, database-dependent natural language interface for CBMS-Pasay. In most information systems, databases are accessed and manipulated typically through systems developed to tailor-fit the company&apos;s needs. The usual problem in these cases is the limitation on data accessibility because the users are constrained to the forms created for the system. Another way of accessing the database is through Structured Query Language (SQL), a language that is not familiar to end users, thus still limiting the access to the data. Natural Language Database Interfaces were developed to address limited database accessibility for end users. This paper presents AlLaDIn, a web-based natural language interface to the Community-Based Monitoring System of a city in the Philippines.
Controlled Korean for Korean-English MT Controlled Language is a sublanguage of a natural language designed to improve the readability and the translatability of a text. Originally, the concept of a controlled language was introduced in the field of technical documentations to prevent the misunderstanding of texts. 1 In the last couple of years the necessity of a controlled language has increased significantly, especially in the technical documentation domain, as the number of pages to be translated has increased enormously.The increase of the volume of the documents to be translated made it necessary to employ a full automatic translation system like an MT or a machine-aided translation system like translation memory. In Korea, however, MT systems have not been widely welcomed by the experts in the localization business, because the quality of the translations failed to match their expectations.The recent researches on the controlled languages show that the use of a controlled language can generally lead to the improvement of machine translation quality, thus reducing the cost of post-editing. The general assumption behind the idea of the controlled language is that the cost of the use of a controlled language that can be paraphrased as 'pre-editing' is lower than that of 'post-editing' in using an MT system for localization.Previous works on the controlled language in the context of MT have mainly focused on the impact of the controlled language on MT. Aikawa et al. (2007), for example, addressed the impact of the controlled English for MSR-MT. Lehrndorfer (1996), Lehrndorfer&amp;Schachtl (1998) dealt with the controlled German for German-English MT.In this paper, we will introduce the concept of the 'Controlled Korean' for Korean-English MT. We will not only show the impact of the Controlled Korean on Korean-English MT, but also share our experience in designing the Controlled Korean. This paper addresses the issues in designing the so-called &apos;Controlled Korean&apos; for a Korean-English MT system. Controlled Language is a sublanguage of a natural language which is supposed to improve the readability and the translatability of a text. Much effort has been made to design a controlled language for major international languages such as English, German, Spanish and etc. However, little effort has been made yet to design a Controlled Korean in the context of machine translation. In this paper we introduce the concept of the Controlled Korean we have developed for a Korean-English MT system and compare the Controlled Korean with the Controlled English and Controlled German from the perspective of the translatability. The result of our experiments shows that in designing a Controlled Language, not only the linguistic characteristics of the language but also the characteristics of an MT-System must be taken into account.
Contrastive Approach towards Text Source Classification based on Top-Bag-of-Word Similarity Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant ( Barzilay and Elhadad, 2003;Munteanu et al., 2004;Shao and Ng, 2004;Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC's Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word similarity for comparing the language variants contained in Chinese GigaWord corpus. Texts from the same period of time from Central News Agency (Taiwan), Xinhua News Agency (PRC) and Lianhe Zaobao (Singapore) are extracted and compared in our study. By comparing these three varieties of Mandarin Chinese, we hope to find the language significant lexical contrasts and meaning variations. We also propose a constrative approach towards automatic text source classification based on co-occurence similarity measures with documents from the same time period of Chinese Gigaword. Experimental resultes indicated that our proposed constrastive approach is reliable and robust.The rest of this paper is organized as follows. Section 2 investigates related literature in word similarity measures in comparable corpus and a brief introduction to Chinese Gigaword. Section 3 describe the text source classification based on co-occurence similarity. Section 4 presnts experimental results and further discussion. Finally, Section 5 concludes this study. This paper proposes a method to automatically classify texts from different varieties of the same language. We show that similarity measure is a robust tool for studying comparable corpora of language variations. We take LDC&apos;s Chinese Gigaword Corpus composed of three varieties of Chinese from Mainland China, Singapore, and Taiwan, as the comparable corpora. Top-bag-of-word similarity measures reflect distances among the three varieties of the same language. A Top-bag-of-word similarity based contrastive approach was taken to solve the text source classification problem. Our results show that a contrastive approach using similarity to rule out identity of source and to arrive actual source by inference is more robust that directly confirmation of source by similarity. We show that this approach is robust when applied to other texts.
Semantic Structures of Polysemous Psych-adjectives in Korean: A Conceptual Semantics Approach The idiosyncratic syntactic behaviors of psych-predicates in many languages have attracted linguists around the world since 1960s. Korean psych-adjectives, however, are also interesting from the semantic perspective; that is, some of them seem to be polysemous between the meanings of one's purely psychological state and the objective property of an entity. Contrary to the arguments for the polysemy (e.g. S Kim 1994, H-K Yoo, J-N Kim 2005), it has not been recognized by many syntacticians and all the psych-adjectives have been treated as monosemous with the meaning of psychological state (e.g. Y-J Kim 1990, J-H Han 1999, Gerdts &amp; Yoon 2001.Although researches have been conducted on the polysemous nature of some Korean psychadjectives, no consensus has been made on the criteria used for evaluating the polysemy. Furthermore, few formalizations (semantic structures) have been proposed for the polysemous phenomena. The purpose of this paper is twofold: 1) to propose new criteria for distinguishing polysemous psych-adjectives from monosemous ones, and 2) to provide exact semantic structures for the polysemous psych-adjectives. For the second goal in particular, I will work under the framework of Jackendoff's Conceptual Semantics. Although researches have been conducted on the polysemous nature of some Korean psych-adjectives, no consensus has been made on the criteria used for evaluating the polysemy. Furthermore, few formalizations (semantic structures) have been proposed for the polysemous phenomena. The purpose of this paper is twofold: 1) to propose new criteria for distinguishing polysemous psych-adjectives from monosemous ones, and 2) to provide exact semantic structures for the polysemous psych-adjectives. For the second goal in particular, I will work under the framework of Jackendoff&apos;s Conceptual Semantics.
Sign Language and Computing in a Developing Country: A Research Roadmap for the Next Two Decades in the Philippines  This paper presents the current situation of Filipino Sign Language in the Deaf community and milestones in sign linguistics research. It highlights key computing research, particularly in Asia as well as enumerates research attempts to date of Philippine academic institutions which have applications in sign language recognition. This paper also touches on technical considerations, economic feasibility, partnerships and other sociocultural considerations appropriate for a developing country such as the Philippines.
Unsupervised Approach for Dialogue Act Classification Recognizing the intentions of a user in a dialogue system is very important. So far, many methods have been developed to infer a user's intention in a dialog situation. To infer the user's intention in an utterance, the utterance can be categorized into given classes. Therefore, many studies have designed the classes called dialogue act (DA) labels that approximate a speaker's intention. They annotated the labels on a corpus to analyze the phenomena for DA interaction or to develop a DA tagger in order to infer the DA label from a speech segment (e.g. some utterances, an utterance, or a part of an utterance). Most studies on DA taggers were based on a supervised method (e.g., ( Stolcke et al., 2000;Tanaka and Yokoo, 1999)). The labels used in a DA tagger have to be predefined, and supervised methods require a corpus that is manually annotated by the labels. On the other hand, it is difficult to design a tag set (labels) that can be used to annotate a corpus because the design of a tag set depends on the domain and the task. Therefore, we have to redesign the tag set and construct a corpus annotated with a new tag set if we apply our system to different domains or tasks. In addition, designing a tag set that can be used in any domain or task is very difficult. However, we have to annotate DA tags on a corpus, because many applications require predefined DA tags. This paper discusses an unsupervised approach to infer the user's intention in a situation by using a dialog system. Unsupervised approach may not achieve highly accurate results when compared to the supervised approach. However, in any domain or task, the unsupervised approach can yield human DA annotators with machine judgments of the DA classification that may be useful to keep the consistency of DA annotation results for a corpus.In addition, annotating a corpus with given labels is very time-consuming. An unsupervised method is independent of annotation and designing the tag set.In order to achieve an unsupervised method, we need an unsupervised clustering method. So far, many clustering methods have been proposed and discussed for applications in natural language processing (NLP), such as works by Zhao and Karypis (Zhao and Karypis, 2005). However, an utterance is very short against a document that is used in a common NLP application. In addition, the feature space that is used to express any natural language expression is extremely large and an utterance is expressed by a very sparse vector in the feature space. Therefore, it is very important to handle a sparse feature vector of an utterance in the huge feature space. This paper presents an unsupervised approach for dialogue act (DA) classification. We used a latent variable model to compress the dimensions of the feature vector. We introduced a paraphraser to reduce the variety of expressions and to solve the pragmatic problem for DA classification. The paraphraser seemed to work well on some DA classifications in the unsupervised approach. The results obtained by the unsupervised approach were compared with the manually annotated labels. A preliminary experiment for semi-supervised tagging was also carried out, and we discuss these results.
Automatically Extracting Templates from Examples for NLP Tasks * Templates have been used in IE as extraction patterns to retrieve relevant information from documents (Muslea, 1999), and in NLG as forms that can be filled in to generate syntactically correct and coherent text for human readers. They have also been used in machine translation (Cicekli and Guvenir, 2003) and (McTait, 2001), and in pun generation ( Ritchie et al, 2006).In this paper, we present two NLP systems. TExt ( Go et al, 2006 andNunez, 2008), a bidirectional English-Filipino machine translator, extracts translation templates from a bilingual corpus, and together with a bilingual lexicon, uses these templates to translate an input text to another language. T-Peg (Hong, 2008) utilizes semantic and phonetic knowledge to capture the wordplay used in a training set of human jokes, resulting in templates that contain variables, tags, and word relationships that are used to generate punning riddles.Because manually creating templates can be tedious and time consuming, several researches have worked on automatically extracting templates from training examples that have been preprocessed. In our previous example-based MT, SalinWika ( Bautista et al, 2005), templates are extracted from a bilingual corpus that has been pre-tagged and manually annotated with word features, resulting in a long training process. Its successor, TExt ( Go et al, 2006), did away with a tagger and instead requires a parallel bilingual corpus and an English-Filipino lexicon to align pairs of untagged sentences to extract translation templates.Our pun generator, T-Peg (Hong, 2008), on the other hand, subjects the training examples through a pre-processing stage to identify nouns, verbs and adjectives. Instead of manually annotating the example set, the training algorithm relies on existing linguistic resources and tools to perform its task. In this paper, we present the approaches used by our NLP systems to automatically extract templates for example-based machine translation and pun generation. Our translation system is able to extract an average of 73.25% correct translation templates, resulting in a translation quality that has a low word error rate of 18% when the test document contains sentence patterns matching the training set, to a high 85% when the test document is different from the training corpus. Our pun generator is able to extract 69.2% usable templates, resulting in computer-generated puns that received an average score of 2.13 as compared to 2.7 for human-generated puns from user feedback.
Recognizing Coordinate Structures for Machine Translation of English Patent Documents * Patent machine translation is one of main target areas of current practical MT systems such as the English-Korean patent machine translation system (Kwon, 2007). Patent documents have their own description style and there have been some studies on the analysis of patent documents (Sheremetyeva, 2003;Shinmori and Okumura, 2003;Shinmori and Okumura, 2002). Especially, abstracts or claims in the patent documents are notorious for their long and complex syntactic structures, which are usually caused by coordination or relative clauses. Long sentences formed by relative clauses can be handled by segmentation (Kim et al, 2001). On the other hand, in case of long sentences formed by coordination, segmentation can cause syntactic analysis errors, because a segment resulting from segmentation can be dependent on the other constituents in the parse tree. Also, coordinate structures in patent documents have usually a large number of coordinate conjuncts (which we will call nodes) and can cause syntactic ambiguity explosion and parsing failure at the worst case in practical MT systems. So, syntactic analysis of patent documents requires special treatment for coordination.There have been many computational researches about coordinate structures (Kaplan and Maxwell, 1988;Kosy, 1986). But, it is unrealistic to apply most of them to large-scale MT systems as mentioned in (Okumura and Muraki, 1994;Kurohashi and Nagao,. 1994). The more practical approaches about analyzing coordinate structures such as ( Okumura and Muraki, 1994; * This work was funded by the Ministry of Information and Communication of Korean government.Copyright 2008 by Yoon-Hyung Roh, Ki-Young Lee, Sung-Kwon Choi, Oh-Woog Kwon, and YoungGil Kim Kurohashi and Nagao, 1994;Agarwal and Boggess, 1992) all analyze coordinate structures using parallelism between conjuncts, but they are mainly targeted to recognizing two conjuncts (i.e., pre-conjuct and post-conjuct). Out of them, (Kurohashi and Nagao,. 1994) is one of the most practical approaches. They recognize coordinate structures in a Japanese sentence by constructing a similarity matrix between bunsetsus and searching a path with the highest parallelism in the similarity matrix using a dynamic programming method. However, that method is inadequate to apply to patent documents which have usually a large number of coordinate nodes and sometimes complex modification such as an inserted clause. We devised an appropriate method to recognize coordinate structures for patent documents using a similarity table. Although our method seems similar to that method in appearance, it is considerably different from that in the manner of constructing a similarity table and finding coordinate structures. Our method is simpler but more effective in patent documents.In the next section, we outline the characteristics of coordinate structures in patent documents. And in the section 3, we present a method to recognize coordinate structures. In the section 4, we show experimental results and some analysis of the erroneous results, and then conclude our paper with some future works. Patent machine translation is one of main target areas of current practical MT systems. Patent documents have their own peculiar description style. Especially, abstracts or claims in patent documents are characterized by their long and complex syntactic structures, which are often caused by coordination. So, syntactic analysis of patent documents requires special treatment for coordination. This paper describes a method to deal with long sentences in patent documents by recognizing coordinate structures. Coordinate structures are recognized using a similarity table which reflects parallelism between conjuncts. Our method is applied to a practical MT system and improves its quality and efficiency.
A Morphological Analyzer for Filipino Verbs *  This paper presents a morphological analyzer that accepts Filipino verbs conjugated in different forms as inputs and analyzes them to produce the affixes used, the infinitive forms, and the tenses of the original input verbs. A prototype system was implemented and was fed with a file containing 1,050 Filipino verbs conjugated in various tenses using different types of affixes. The preliminary result showed that the accuracy rate was high in three expected outputs, i.e., tenses, infinitive forms, and affixes used.
PACLIC 23 Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation Volume 1 Edited by Olivia Kwong Preface: Program Committee Chair  The number and breadth of submissions to PACLIC conferences as well as the diversity of authorship have been on an upward trend since the inauguration of the series in 1995. However it expands, PACLIC is certainly proud to remain one of the few which manage to maintain a healthily balanced and symbiotic coexistence of formal linguists and computational linguists, and continue to emphasize the synergy of theoretical analysis and computer processing of language among researchers in the Pacific Asia region. Such strengths are evident from the 145 papers submitted to PACLIC 23 from 20 countries or regions: 82.8% from 9 regions in Asia, 8.3% from 7 regions in Europe, 5.5% from the United States and Canada, 2.8% from the Middle East, and 0.7% from Australia. We would like to thank all authors for their submissions. Of these, 40% were accepted for regular paper presentations and an additional 25% for poster presentations, constituting a diverse technical program. I must say a big thankyou to our professional Program Committee, including the six regional Co-Chairs:, 73 supportive committee members and 34 helpful additional reviewers, for their competence and hard work, and most importantly, prompt feedback. PACLIC 23 will span three days from 3rd to 5th December 2009, featuring invited talks, plenary talks, regular paper presentations, and a poster session. We would like to thank the invited and plenary speakers, who will enlighten us on a wide range of topics: Dr.
Gesture Theory is Linguistics: On Modelling Multimodality as Prosody * * * *  Prosody and gesture have, with few exceptions, not appealed to computational linguists, and there is even less awareness of the parallels between them, though in modelling sign languages of the hearing-impaired, the linguistic metaphor of gesture patterns as &apos;phonology&apos; has been very fruitful. On the other hand, many disciplines from anthropological linguistics to robotics are currently occupied very productively with conversational gesture studies, but nevertheless the field is still highly fragmented. Starting from (computational) linguistic models as metaphors for the forms and functions of gestural signs, it can be shown that similarities between gesture and speech (specifically: prosody), go much further than metaphors, opening up avenues for integrating speech and gesture theory and developing a shared ontology for speech and gesture resource creation and retrieval. Gestures are an essential part of communication-not only the gesticulatory body language of everyday face-to-face communication and the signing of deaf communicators, but also in the production of speech and in the production of acts of writing, typing, manual morse code transmission, semaphoring and &apos;talking drums&apos; and many other varieties of communication. In the broadest sense, music performance can also be seen as non-propositional gestural communication, though with such a generalisation about gestural communication one rapidly becomes overwhelmed with the dimensionality of the concept. First, a note on gesture. Gestural communication, with few exceptions, remained the province of psychologists, sociologists, and of experts in the gestural sign languages of those with restricted hearing until relatively recently. In speech technology, the similarities between acoustic speech and sign language patterning, were recognised, and the technological development of multimodal systems for screen avatars and humanoid robots with speech and gesture communication developed. But computational linguistics has largely avoided the issue of gesture modelling, and until relatively recently gestures have not been the subject of computational linguistic investigations. This is one of the issues which will be taken up here. Second, a note on prosody. The gestures involved in the production of prosody, the &apos;music&apos; (rhythm and melody) of speech, are an intimate and essential part of verbal communication. The domain concerns the overall temporal coordination of all the gestures of speech, but it is the gestures of the larynx (which control the vocal cords) which are the key contributors to the prosodic domain. Linguistics (particularly applied linguistics, both in the domain of foreign language teaching and clinical linguistics) has been concerned with prosody for many decades, * Supported in part by DAAD grant to ModeLex project 2001-2004 (FG &apos;Task-oriented communication&apos;).
Automatic Lexical Classification -Balancing between Machine Learning and Linguistics Verb classifications have attracted a great deal of interest in both linguistics and natural language processing (NLP). They have proved useful for various important tasks and applications, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation ( Swier and Stevenson, 2004;Dang, 2004;Shi and Mihalcea, 2005;Kipper et al., 2008;Zapirain et al., 2008).Particularly useful are classes which capture generalizations over a range of (cross-)linguistic properties, such as the ones proposed by Levin (1993). Being defined in terms of similar meaning and (morpho-)syntactic behaviour of words, these classes generally incorporate a wider range of properties than e.g. classes defined solely on semantic grounds (Miller, 1995).For example, verbs which share the meaning component of 'manner of motion' (e.g. travel, run, walk), behave similarly in terms of subcategorization (e.g. I travelled/ran/walked, I travelled/ran/walked to London, I travelled/ran/walked five miles) and usually have zero-related nominals (e.g. a run, a walk) can be grouped to the same lexical class. Such verb classes can be identified across the entire lexicon and they can also apply across languages, since the basic meaning components they are comprised of are cross-linguistically applicable or overlapping.While the classes do not provide means for full semantic inferencing, they can offer a powerful tool for generalization, abstraction and prediction which is beneficial for practical tasks. Fundamentally, the classes are a critical component of any system which needs mapping from surface realization of arguments to predicate-argument structure. As the classes capture higher level abstractions they can be used as a principled means to abstract away from individual words when required. For example, they can be utilized to organize a default inheritance hierarchy which effectively captures generalizations over words and predicts much of the syntactic/semantic behaviour of a new word simply by associating it with an appropriate class. The predictive power of the classes can help compensate for lack of sufficient data. In addition, the classes have theoretical benefits. For example, classified data can be used to evaluate empirical claims of different linguistic and psycholinguistic theories.Although lexical classes have proved helpful for a number of (multilingual) tasks, their largescale exploitation in real-world or highly domain-sensitive tasks has been limited because no fully accurate or comprehensive lexical classification is available. There is no such resource because manual classification of large numbers of words has proved very time-consuming. Class-based differences are typically manifested in differences in the statistics over usages of syntactic-semantic features. This statistical information is difficult collect by hand as it is highly domain-sensitive, i.e. it varies with predominant word senses, which change across corpora and domains.In recent years, automatic induction of verb classes from corpus data has become increasingly popular (Merlo and Stevenson, 2001;Schulte im Walde, 2006;Joanis et al., 2008;Sun et al., 2008;Li and Brew, 2008;Korhonen et al., 2008;´ O Séaghdha and Copestake, 2008;Vlachos et al., 2009). This work is important as it opens up the opportunity of learning and tuning classifications for the application and domain in question. Automatic classification is not only cost-effective but it also gathers the important statistical information as side effect of the acquisition process and can easily be applied to new domains and usage patterns provided relevant corpus data is available.To date, a variety of approaches have been proposed for verb classification and applied to general English and other languages. Both supervised and unsupervised machine learning (ML) methods have been used to classify a variety of features extracted from raw, tagged and/or parsed corpus data. Although the results have been generally encouraging, the accuracy of automatic classification shows room for improvement. After providing a short introduction to the basic principles of manual verb classification, this paper reviews recent research in automatic classification -particularly focussing on work conducted in English -and discusses then the various current challenges that need to be met for substantial further advances. Meeting these challenges requires solid expertise in both machine learning and (computational) linguistics. Verb classifications have been used to support a number of practical tasks and applications , such as parsing, information extraction, question-answering, and machine translation. However, large-scale exploitation of verb classes in real-world or domain-sensitive tasks has not been possible because existing manually built classifications are incomprehen-sive. This paper describes recent and ongoing research on extending and acquiring lexical classifications automatically. The automatic approach is attractive since it is cost-effective and opens up the opportunity of learning and tuning lexical classifications for the application and domain in question. However, the development of an optimal approach is challenging, and requires not only expertise in machine learning but also a good understanding of the linguistic principles of lexical classification.
Resultatives as Causal Relations between Events *  This paper investigates some resultative constructions in English, Korean, Chinese, and Japanese. It will be argued that the Direct Object Restriction (DOR) used in previous syntactic accounts is not correct. This paper examines the typical resultative constructions in English and Korean and, within the framework of event semantics, offers a proper semantic account of the relevant data on the basis of the semantic relationships of the lexical items. The analysis makes use of the notions such as &apos;event,&apos; &apos;appropriate result,&apos; etc. In addition, it will be shown that the proposed account is also useful for explaining the so-called resultative Verb-Verb Compounds (VVCs) in Korean, Chinese, and Japanese. 1 Resultative vs. Depictive Sentences Jespersen (1909-49, Part V: 23-28) discusses the type of sentence in (1) under the term &apos;object of result&apos;. Later Halliday (1967:63-65) discusses a similar type of sentence in (2a) using the term &apos;resultative,&apos; which he compares with another sentence pattern &apos;depictive&apos; in (2b). (1) He pushed the gate open. (2) a. She washes them clean. (resultative) = She washes them and, because she washes them, they become clean. b. She sells them cheap. (depictive) = She sells them and, when she sells them, they are cheap. Simpson (1983:143) recognizes four types of resultatives, and she claims that the matrix verb and the resultative phrase form a complex verb. The resultative phrases are diverse and they are collectively termed result &apos;XPs&apos; (Hovav and Levin, 2001:766) or &apos;RPs&apos; (Goldberg and Jackendoff, 2004:536). I will use the abbreviation RP in this paper. collected data of various resultative constructions and focused on the typical sentences listed in (3). Following the general convention (in particular, Wechsler, 1997), the result phrase (RP) is italicized and the NP of which the RP is predicated is underlined. (3) a. He hammered the metal flat. (RP=Adjective Phrase) [Object-predicated] b. He painted the car a pale shade of red. (RP=Noun Phrase) [Object-predicated] c. The brook froze solid. (RP=Adjective Phrase) [Subject-predicated] d. He yelled himself hoarse. (RP=Adjective Phrase) [Object-predicated, but actually Subject-predicated] * I would like to thank my colleague Minhaeng Lee at Yonsei University for providing me with important papers on resultatives and helpful comments on the contents of this paper. Any remaining mistakes, however, are all my own. In this paper the following abbreviations are used: Acc=Accusative particle, Asp=Aspect marker,
Developing Speech Recognition and Synthesis Technologies to Support Computer-Aided Pronunciation Training for Chinese Learners of English * English is the lingua franca of our world. It is the second language (L2) most actively studied across Asia, as well as an official language or working language used in education, government, media, etc. in many regions. Hence, acquiring communicative competence in English is of prime importance. It has been estimated that by 2010 there will be 2 billion English learners worldwide, and the proportion in Asia alone will exceed the number of native speakers (Asia Economic News, 2006). Second language learning, specifically pronunciation learning, involves correct perception and production of sounds in the target language. The learning process tends to be influenced by well established perceptions of sounds and articulatory motions in the primary language (L1). This cross-linguistic influence is often referred as language transfer. Negative transfer of L1 features causes inaccuracies and errors in L2 speech productions, which impede intelligibility. Consequently, the study of L2 speech productions (i.e. the "interlanguage" of learners who have not acquired native-like proficiency) is of great interest to phoneticians, linguists, language educators, as well as technologists engaged in the development of CAPT (computer-aided pronunciation training) systems. These applications can complement classroom teaching and provide unique benefits to the learner in terms of accessibility, reduced anxiety and individualized instructions.Effective pronunciation training tools need to provide learners with detailed mispronunciation detection, diagnosis and corrective feedback (Ehsani and Knodt, 1998). Previous work has shown that automatic pronunciation scores at the word-level or sentencelevel correlate highly with human raters but fail to lead to measurable improvement in learner's overall pronunciation ( Precoda et al., 2000). However, locating mispronunciations at the phone-level to learners has been shown to lead to statistically significant improvement for the production of those targeted phones (Neri et al., 2006). Moreover, diagnostic feedback to learners (e.g. "you inserted a vowel at the end of the word") has also been shown to lead to significant improvements in pronunciation training (Kim et al., 2004). We describe ongoing research in the development of speech technologies that strives to raise the efficacy of computer-aided pronunciation training, especially for Chinese learners of English. Our approach is grounded on the theory of language transfer and involves a systematic phonological comparison between the primary language (L1 being Chinese) and secondary language (L2 being English) to predict possible segmental and suprasegmental realizations that constitute mispronunciations in L2 English. The predictions are validated based on a specially designed corpus that consists of several hundred hours of L2 English speech. The speech data supports the development of automatic speech recognition technologies that can detect and diagnose mispronunciations. The diagnosis aims to support the design of pedagogical and remedial instructions, which involves text-to-speech synthesis technologies in audiovisual forms.
Capturing Lexical Variation in MT Evaluation Using Automatically Built Sense-Cluster Inventories The majority of the existing Machine Translation (MT) evaluation metrics look for exact surface correspondences between the compared translations. They are mostly based on the strict precision criterion, which does not account for the semantic similarity of words found in the hypothesis and the reference. Given that evaluation scores often do not reflect the quality of translation, there is a growing tendency towards increasing the correlation of the metrics with human judgments of translation quality. An important factor determining this correlation is the identification of sense correspondences between the hypothesis and the reference, which may exist even if the words used in the translations differ. Capturing this type of correspondence would also allow a more conclusive estimation of the impact of WSD techniques on MT systems than is possible with the current evaluation metrics (Callison-Burch et al., 2006;Carpuat and Wu, 2005;Chan et al., 2007).In this paper, we show how variation at the unigram level can be captured during evaluation using information induced from parallel corpora by an unsupervised sense induction method. This method generates bilingual semantic inventories, where the senses of the words of one language are described by clusters of their semantically similar translation equivalents (TEs) in another language. These sense-clusters, which are similar to WordNet 1 synsets, can serve to capture correspondences between synonymous words found in the compared translations.The structure of the paper is as follows: in the next section, we present how lexical variation is dealt with in existing MT evaluation metrics. In section 3, we describe the sense induction method and the training data used. In section 4, we explain how the automatically built sense inventory is integrated into METEOR. In section 5, we present the experiments carried out in English and in French and we analyze the obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation. 2 METEOR (Banerjee and Lavie, 2005;Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting WordNet information: a translation is considered to be correct not only if it exactly corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same WordNet synset. Nevertheless, predefined semantic resources like WordNet present some limitations. They cannot be easily updated and adapted to the domains of the processed texts and, most importantly, they are not publicly available for languages other than English. This is an important issue concerning METEOR as, when it is used for evaluation in languages other than English, only the exact and stemming matching modules are used, while the synonymy module is not operational and is omitted. This explains why Lavie and Agarwal (2007) propose to develop new synonymy modules for languages other than English, that would be based on alternative methods and could be used in the place of WordNet. The strict character of most of the existing Machine Translation (MT) evaluation metrics does not permit them to capture lexical variation in translation. However, a central issue in MT evaluation is the high correlation that the metrics should have with human judgments of translation quality. In order to achieve a higher correlation, the identification of sense correspondences between the compared translations becomes really important. Given that most metrics are looking for exact correspondences, the evaluation results are often misleading concerning translation quality. Apart from that, existing metrics do not permit one to make a conclusive estimation of the impact of Word Sense Disambiguation techniques into MT systems. In this paper, we show how information acquired by an unsupervised semantic analysis method can be used to render MT evaluation more sensitive to lexical semantics. The sense inventories built by this data-driven method are incorporated into METEOR: they replace WordNet for evaluation in English and render METEOR&apos;s synonymy module operable in French. The evaluation results demonstrate that the use of these inventories gives rise to an increase in the number of matches and the correlation with human judgments of translation quality, compared to precision-based metrics.
Dependency Grammar Based English Subject-Verb Agreement Evaluation1 Subject-verb agreement error is the most common type of mistakes made in translating other languages to English text, and affects the quality of the generated text considerably. By making a detailed analysis on 300,000 error-noted English patent texts, we found that the subject-verb agreement errors comprise 21.7% of all the translation errors. It is obviously indicated that subject-verb agreement is one of the common problems translators would encounter. Due to the complicate grammar and flexible usage of sentence types, especially the complicated relationship between subjects and predicate verbs, the subject-verb agreement evaluation is a difficult mission to tackle.Currently, manual proofreading is still the main approach widely applied in detecting subject-verb agreement errors made by translators. However, it costs too much while in low efficiency, and manual work is not capable of reuse. To solve this problem, a computational approach is proposed in this paper to automatically recognize the subject-verb pairs and evaluate their agreement by obtaining the dependency relationship between the subjects and its predicate verbs. Phrase syntactic parsing and sentence simplification are used and proved to be effective in our routine.The rest of the paper is organized as follows: a concise survey of related works is presented in the next section; section 3 is the description of our method; section 4 illustrates the procedure of our experiments; and the experimental results with analysis are presented in section 5; section 6 is the conclusion. As a key factor in English grammar checking, subject-verb agreement evaluation plays an important part in assessing translated English texts. In this paper, we propose a hybrid method for subject-verb agreement evaluation on dependency grammars with the processing of phrase syntactic parsing and sentence simplification for subject-verb discovery. Experimental results on patent text show that we achieve an F-score of 91.98% for subject-verb pair recognition, and a precision rate of 97.93% for subject-verb agreement evaluation on correctly recognized pairs in the previous stage.
WikiSense: Supersense Tagging of Wikipedia Named Entities Based WordNet1 Machine readable natural language resources, like ontologies or semantic categories, are crucial in natural language processing or information retrieval tasks that demands world knowledge. Such tasks include question classification and answering, knowledge mining and semantic search. In these tasks, lexical databases such as WordNet or Suggested Upper Merged Ontology (SUMO) are widely used to provide meaning representation and semantic relations. These handcrafted ontologies rely on manual compilation and maintenance of small groups of experts over long periods of time. New words and phrases are added to the vocabulary progressively over the years through each new release. With the scale close to that of a dictionary, these resources consists mostly of common nouns, verbs, adjectives, adverbs, and a small amount of named entities. Only highly well-known people, places, organizations, and events, are included. Although these resources are very useful for many natural language tasks, they sometimes suffer severely from the out-of-vocabulary problem due to limited coverage, most notably for lack of NEs. Furthermore, for NEs they cover, there are also issues with consistency and relevance. For example, Charles Dickens is found in WordNet, but none of the books he wrote (e.g., A Tale of Two Cities). The generally accepted lexicographer guidelines called for using the powerful tool of frequency to make informed linguistic decisions about creating and framing an entry. A typical Web search (e.g., Google, http://www.google.com) show Celine Dion, a contemporary Canadian pop singer, has slightly higher visibility on the Web than Johnny Cash, the late American country singer. (13,100,000 v.s. 11,600,000 page counts) However, Celine Dion is not listed in WordNet, while Johnny Cash is listed as an instance of singer#n#1. Such Copyright 2009 by Joseph Chang, Richard Tzong-Han Tsai, and Jason S. Chang inconsistent and poor coverage could be problematic for building robust language systems for practical applications.In effort to automatically produce broad and general semantic categories of NEs, we turn to Wikipedia, an online encyclopedia contributed by millions of volunteers all around the world. Wikipedia consists millions of articles of all kinds, including rich information of constantly emerging NEs not found in existing dictionaries and ontologies. Owing to the tremendously active participation from its contributor community, Wikipedia is constantly infused with the newly created words and phrases, especially NEs, such as names of movies, books, celebrities, and events, both new and old. For instance, Wikipedia includes all four NEs mentioned above: Charles Dickens, A Tale of Two Cities, Johnny Cash, and Celine Dion. Intuitively, to extend WordNet, a feasible approach is to classify Wikipedia NE titles into semantic categories in WordNet, thus greatly extending WordNet's coverage of NEs. Including Johnny Cash, there are 21 singer instances found in WordNet, all of them are also included in Wikipedia. The Wikipedia entries for these and similar NEs (e.g., Celine Dion) contains information (e.g., "singer", "song-writer"), which is indicative of the relevant semantic categories (e.g., PERSON). See Examples (1) and (2), for more details. Additional information in the form of categories (e.g., American composers) and dominating pronoun type (e.g., personal pronouns, such as "his" and "he") are all very indicative of the semantic class (e.g., PERSON Intuitively, by using Wikipedia entries (e.g., Johnny Cash, Bob Dylan, Judy Garland, and Lena Horne) listed under a certain WordNet semantic class (e.g., PERSON), we can extract such features and train a classifier capable of predicting that Celine Dion is a PERSON, or more specifically a SINGER.We present a novel system, WikiSense, that automatically learns to classify titles in an encyclopedia (e.g., Wikipedia) into broad semantic categories in an ontology (e.g., WordNet). An example of WikiSense classifying the entry of Celine Dion in Wikipedia is shown in Table 1. WikiSense has determined the indicative features for the entry and goes on to classify Celine Dion as a PERSON. WikiSense automatically learns the relationships between features and semantic categories by using WordNet and Wikipedia named entities with minimally supervision. We describe the training process of WikiSense in more detail in Section 4.In our prototype, WikiSense return a set of Wikipedia NE titles annotated with WordNet supersenses. By combining WikiSense and the traditional handcrafted ontology, WordNet, we can provide a very large scale gazetteer, a potentially useful resource for many NLP tasks.The rest of the paper is organized as follows. In the next section, we describe Wikipedia and WordNet, the two main resources used in this paper. In Section 3, we survey previous work. In Section 4, We explain in detail the problem statement and proposed methods. Finally in Section 5, we report the evaluation results and error analysis, and conclude in Section 6. In this paper, we introduce a minimally supervised method for learning to classify named-entity titles in a given encyclopedia into broad semantic categories in an existing ontology. Our main idea involves using overlapping entries in the encyclopedia and ontology and a small set of 30 handed tagged parenthetic explanations to automatically generate the training data. The proposed method involves automatically recognizing whether a title is a named entity, automatically generating two sets of training data, and automatically building a classification model for training a classification model based on textual and non-textual features. We present WikiSense, an implementation of the proposed method for extending the named entity coverage of WordNet by sense tagging Wikipedia titles. Experimental results show WikiSense achieves accuracy of over 95% and near 80% applicability for all NE titles in Wikipedia. WikiSense cleanly produces over 1.2 million of NEs tagged with broad categories, based on the lexicographers&apos; files of WordNet, effectively extending WordNet to form a very large scale semantic category, a potentially useful resource for many natural language related tasks.
An Integrated Approach to Heterogeneous Data for Information Extraction Semantic web, which collects and formats different kinds of web knowledge, plays an important role in the development of a new generation of web. One important component of semantic web is to automatically extract different relations existing in web data. Information extraction (IE) can provide such a technology to solve this problem, particularly for a specific named entity. For example, Web People Search 1 (WePS) 2009 evaluation (Sekine &amp; Artiles, 2009) tries to extract some personal information, and TREC Entity Track 2 plans to find some related information for a product.Web IE is particularly challenging because web data is heterogeneous in nature. Complete or comprehensive IE information necessarily come from many different sources with different formats. For example, "affiliation" and "email" are so different in their own expressions so that they need different extraction approaches. Hence, a homogeneous IE model, regardless of statistical model or rule-based model, often cannot perform effectively for web IE. To overcome this problem, some previous systems ( Culotta et al., 2004;Lan et al., 2009;Watanabe et al., 2009) have tried to combine different IE approaches, which are often homogeneous IE, to extract different types of information in web data. Nevertheless, few of them have explored how to effectively utilize or integrate different IE tools for web data.In this paper, we propose a framework to integrate heterogeneous IE approaches for web IE. In this framework, we first segment web data according to the expression format. Similar to the genre categories -"formal text" and "informal text" -which were defined in Minkov et al. (2005), a text is either in "formal style" or "informal style." A "formal style" text obeys prescribed writing standards, i.e. a complete sentence usually with a subject and an object. On the contrary, "informal style" has few limitations on writing format and can mix various representation levels. In order to do so, we develop a novel algorithm to segment a webpage into fragments according to their expression format: formal style and informal style. This segmentation allows an existing IE system, which often was developed for a specific type of text, to be applied to its similar text fragments. For example, most statistical IE systems are developed for a news corpus, therefore it is better to apply them to formal-style fragments.In addition, web data also have their ways of conveying certain information. For example, it is common that occupation and affiliation information is expressed in a homepage in the format of "Name, Position, Affiliation," such as "Anita Coleman, Assistant Professor, University of Arizona." As this kind of web-specific expression is often multi-lines, some existing IE patterns (Mann &amp; Yarowsky, 2003;Mann, 2006;Rosenfeld &amp; Feldman, 2006), which were limited to one sentence or were designed for formal style text, cannot be directly applied. To identify this web expression property, we develop web-specific patterns, which take into account of different kinds of information, such as webpage type information (i.e. homepage and biographical webpage), and text expression style (formal style and informal style). The experiment shows that those patterns could achieve high precision, which is very important for real applications.Instead of presenting a totally new IE solution for web data, the goal of this paper aims at providing a flexible framework, which is able to effectively reuse and integrate different existing well-developed IE technologies for web data, and tries to collect some web-specific information to further help IE. We test our IE framework on a small scale data provided by the WePS 2009 evaluation 3 , one of whose tasks is to extract some personal information for a given person, and the experiment shows a promising result. Moreover, the comparatively-high precision of our system indicates the strong capability of our framework to integrate IE technologies. Finally, according to the personal information distribution in web data, we discuss the practical problems of IE systems for further improvement. In this paper, we concentrate only on IE for a focus person. The terms "attribute" and "relation" are interchangeable here. The paper proposes an integrated framework for web personal information extraction, such as biographical information and occupation, and those kinds of information are necessary to further construct a social network (a kind of semantic web) for a person. As web data is heterogeneous in nature, most of IE systems, regardless of named entity recognition (NER) or relation detection and recognition (RDR) systems, fail to get reliably robust results. We propose a flexible framework, which can effectively complement state-of-the-art statistical IE systems with rule-based IE systems for web data, and achieves substantial improvement over other existing systems. In particular, in our current experiment, both the rule-based IE system, which is designed according to some web specific expression patterns, and the statistical IE systems, which are developed for some homogeneous corpora, are sensitive only to specific information types. Hence we argue that our system performance can be incrementally improved when new and effective IE systems are added into our framework.
Are Emotions Enumerable or Decomposable? And its Implications for Emotion Processing* Emotions represent one of the most fundamental set of shared human experience, while the recognition and identification of emotions is one of the most crucial human cognitive ability. It is probably not an exaggeration to claim that most human activities are motivated by or designed to excite some emotion. And most events do activate emotion, regardless of whether they are designed to do so. Given the critical roles emotions play in human activities, it is not surprising that sentiment analysis, as coarse-grained account of emotional tendencies (positive, negative, and neutral) became one of the most popular topics in NLP and IE. What is surprising is that there were few studies on emotion computing, which would offer finer-grained information and will be universally applicable regardless of domain and product types.With regard to emotion processing, some works ( Tokuhisa et al., 2008;Mihalcea and Liu, 2006) have been done on text, and most of them use the resource from web, i.e. web blog and analysis that can be explored, such as emotion detection ( Tokuhisa et al., 2008), emotion classification ( Mishne, 2005;Mihalcea and Liu, 2006), and emotion trend prediction (Mishne &amp; Rijke, 2005;Balog &amp; Rijke, 2006). In this paper, we discuss a basic yet important question in emotion analysis: How to classify and represent emotions?Although scientific study of emotion can be traced all the way back to early philosophers, both in the West and in the East, we still lack a standard theory of emotion classification today. In terms of emotion classification, the most urgent issue is the nature of emotion taxonomy. Should human emotions be treated as an enumerable, albeit rather large, set of atomic emotions? Or should human emotions be treated as decomposable as a set of primary emotions and their combinations? For example, in Turner's taxonomy (Turner, 2000), "pride" is decomposed into "happiness + fear". This indicates that the two emotions: "happiness" and "fear", are more basic and can be combined to form complex emotions. However, for emotion classification in NLP, this compositional representation changes the content of classification (detect a vector, not one label), and a different classification technology is required. In this paper, we choose multi-label classification (each instance can have more than one label) to handle the vector detection task. We also discuss the trade-off between single-label classification (e.g. the detection of "pride" only) and multi-label classification (e.g. the detection of both "happiness" and "fear").The paper is organized as follows. Section 2 gives some related work about emotion processing on text, and provides some background for multi-label classification. In Section 3, we first explain the objective of emotion processing in formal text, and then discuss the two emotion representations, namely the enumerative representation and the compositional representation. Section 4 describes our emotion system and our Chinese emotion corpus, and Section 5 explains the experiments of our study. Finally, a conclusion is made in Section 6. Emotion is a complicated concept, and can be represented in different ways. In this paper, we discuss two kinds of emotion representations: the enumerative representation and the compositional representation. Compared to the enumerative representation, the compositional representation is the less rigid description of an emotion. However, from the perspective of emotion classification and detection, different representations often correspond to different emotion processing task. In the enumerative representation, emotion processing can be considered as single-label classification (detecting one and only one label); in the compositional representation, the task turns into the detection of a vector. In this paper, we explore the impact of these emotion representations in emotion processing, including the trade-off of these representations and the selection of technologies to process emotion.
Scope and Anaphoric Links in Dynamic Discourse Representation Theory * * * *  This paper analyzes anaphoric links in conditional and quantificational structures in which more than one operator occurs. To this end, we adopt the indexing system and the basic principles of Dynamic Discourse Representation Theory proposed by Chung (2008a, 2008b, 2009) and extend them to the structures in question. We also propose that a restrictor discourse representation structure of a quantification or conditional operator is an island which blocks any quantification or conditional operator inside it from introducing its own restrictor DRS out of it. This implies that a quantifier phrase can have wide scope over another quantifier or conditional operator only when the first occurs in the scope DRS of the latter.
Voted Approach for Part of Speech Tagging in Bengali * Part of Speech (POS) tagging is the task of labeling each word in a sentence with its appropriate syntactic category called part of speech. POS tagging is a very important preprocessing task for various language processing activities. This helps in doing deep parsing of text and in developing information extraction systems, semantic processing etc. POS tagging for natural language texts are developed using linguistic rules, stochastic models or a combination of both. Stochastic models ( Cutting et al., 1992;Merialdo, 1994;Brants, 2000) have been widely used in POS tagging task for simplicity and language independence of the models. Among stochastic models, Hidden Markov Models (HMMs) are quite popular. Development of a stochastic tagger requires large amount of annotated data. Stochastic taggers with more than 95% word-level accuracy have been developed for English, German and other European languages, for which large labeled data is available. The problem is difficult for Indian languages (ILs) due to the lack of such annotated large corpus. Simple HMMs do not work well when small amount of labeled data are used to estimate the model parameters.Incorporating diverse features in an HMM-based tagger is difficult and complicates the smoothing typically used in such taggers. In contrast, a Maximum Entropy (ME) based method (Ratnaparkhi, 1996) or a Conditional Random Field (CRF) based method ( Lafferty et al., 2001) or a SVM based system ( Kudo and Matsumoto, 2001) can deal with diverse and overlapping features.The International Institute of Information Technology (IIIT), Hyderabad, India initiated a POS tagging contest, NLPAI ML-Contest06 1 for the Indian languages in 2006. Several teams came up with various approaches and the highest accuracies were 82.22% for Hindi, 84.34% for Bengali and 81.59% for Telugu. As part of the SPSAL2007 2 workshop in IJCAI-07, a competition on POS tagging and chunking for south Asian languages was conducted by IIIT, Hyderabad. The best POS tagging accuracies reported were 78.66% for Hindi (Karthik, 2007), 77.37% for Telugu (Karthik, 2007) and 77.61% for Bengali (Dandapat, 2007). An HMM based POS tagger has been reported in  that make use of the additional context dependent information along with word suffixes, Named Entity Recognition (NER) system and lexicon for handling of unknown words. Further, the POS taggers for Bengali can be found in  with ME,  with CRF and in Ekbal and Bandyopadhyay (2008a) with a SVM based approach. Part of Speech (POS) tagging is the task of labeling each word in a sentence with its appropriate syntactic category called part of speech. POS tagging is a very important preprocessing task for language processing activities. In this paper, we report about our work on POS tagging for Bengali by combining different POS tagging systems using three weighted voting techniques. The individual POS taggers are based on Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) frameworks. The POS taggers use a tag set of 27 POS tags, defined for the Indian languages. The individual system makes use of the different contextual information of the words along with the variety of word-level features that are helpful in predicting the various POS classes. The POS tagger has been trained and tested with 57,341 and 35K tokens, respectively. It has been experimentally verified that the lexicon, named entity recognizer and different word suffixes are effective in handling the unknown word problems and improve the accuracy of the POS tagger significantly. Experimental results show the effectiveness of the proposed voted POS tagger with an accuracy of 92.35%, which is an improvement of 5.29% over the least performing ME based system and 2.23% over the best performing SVM based system.
Adjective Density as a Text Formality Characteristic for Automatic Text Classification: A Study Based on the British National Corpus * * * *  In this article, we report significant findings resulting from an investigation into the correlation between adjective density, calculated as the proportion of adjectives in word tokens, and degrees of text formality as part of an attempt to examine the potential application of adjectives in automatic text classification and identification. Correlations obtained from the training corpus will be compared with human ranking of the text categories concerned in the study and then adapted to unseen data in the test set. A linear regression analysis suggests a strong correlation between degrees of text formality and adjective density. With a weighted average F-measure of 0.606 achieved by a Naïve Bayes classifier, the research establishes adjectives as a powerful differentia of text categories amongst the open word classes, an important feature that has been generally ignored by past studies in automatic text categorization. The empirical findings suggest that the use of adjective density will lead to enhanced practical systems for automatic text classification.
Correcting Errors Using the Framework of Argumentation: Towards Generating Argumentative Correction Propositions from Error Annotation Schemas  This paper presents a first step towards the automatic generation of argumentative responses to accompany the corrections proposed by a correction and writing-aid system. This system focuses on pairs of languages (e.g. French speakers writing in English), and incorporates a strong didactic orientation. We show how, in case several corrections are available, error annotations can be used to design argumentations weighing the pros and cons of each correction. Argumentation is paired with decision theory in order to help the user pick out the most appropriate correction. Argumentative responses produced manually are used to create the generation schemas required to implement the automatic generation of such texts in the future. 1 Aims and Challenges 1.1 The Context Our1 project aims at designing a didactic tool targeted at non-native speakers of a language (L2) who have to produce written documents in that language (e.g. French speakers writing in English). The project emerges from the simple observation that these writers often encounter lexical, grammatical, and stylistic difficulties which might hinder the comprehension of their message, as well as undermine their credibility and professionalism (Ellis, 1994). Our main objective is to develop procedures for the correction of those errors which are not (and will not in the near future) be treated by the most advanced text processing systems such as the Office Suite, Open Office and the like (Lee and Seneff, 2006). We also aim at correcting style and text-level errors in the user&apos;s native language, since those are very frequent. Research for this project is conducted on the basis of language pairs, as a large number of errors seem to be specific to a community of speakers of a L1, which is imputable to the influence of L1 structures and lexicon on the production of texts in L2 (Chan, 2004; Han et al., 2005). The present paper focuses on the pair French to English, but other language pairs are also being investigated in the project (e.g. Thai to English). One of the fundamental aspects of this project is the inclusion of a didactic approach into the task of correcting errors. The resulting tool (also called an assistant) should be able to interact with the user in order to explain errors and provide grammatical, lexical or stylistic guidelines and information, as well as to produce argumentative responses where several corrections are possible. In contrast with text editors, but in the spirit of tutoring systems, we want to leave
Shallow Semantic Parsing of Persian Sentences * Semantic role labeling (SRL), also called shallow semantic parsing, involves identifying which groups of words (phrases) act as the arguments to a given predicate. These arguments must be labeled with the role they play in relation to the predicate (verb), indicating how the proposition should be semantically interpreted (Hacioglu, 2004).A number of algorithms have been proposed for automatically assigning such shallow semantic structure to English sentences. But little is understood about how these algorithms may perform in other languages, and in general the role of language-specific idiosyncrasies in the extraction of semantic content, and how to train these algorithms when large hand-labeled training sets are not available ( Sun and Jurafsky, 2004).So, to design an optimal model for a Persian SRL system we should take into account specific linguistic aspects of the language. Regarding the remarkable amount of research that has already been done in English, we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology for English and verify if it is suitable for Persian.Our proposed SRL system implements a two-phase architecture to first identify the arguments by a shallow syntactic parser or chunker, and then to label them with appropriate semantic role, with respect to the predicate of the sentence. We treat both phases as a multiclass classification problem, where the classifier is trained in a supervised manner, from human-annotated data, using memory-based learning. To our knowledge it is the first corpus based SRL system for Persian.Memory-based language processing is based on the idea that NLP problems can be solved by storing solved examples of the problem in their literal form in memory, and applying similarity-based reasoning on these examples in order to solve new ones. Keeping literal forms in memory has been argued to provide a key advantage over abstracting methods in NLP that ignore exceptions and subregularities (Morante and Busser, 2007).MBL works best when the features have been carefully selected and weighted (Hammerton et al., 2002). We have used some syntactic properties of arguments for the feature set. But since no automatic parser exists to syntactically parse Persian sentences, we decided to develop a system for shallow parsing of Persian sentences in the first phase of the system.Shallow parsing (also called partial parsing) most often refers to the task of chunking and has become an interesting alternative to full parsing. It is a natural language processing technique that attempts to determine the constituents' boundaries in the sentence, but without parsing it fully into a parsed tree form ( Marquez et al., 2008). Shallow parsing is easily trainable, fast, robust and much less ambiguous. Such properties make it a good choice over full parsing ( Hammerton et al., 2002).The rest of this paper is organized as follow: We first describe our creation of a small 2000-sentence Persian corpus labeled with 12 selected thematic roles in section 2. Section 3 introduces the general architecture of our model and describes its components in details. The experimental results are shown in section 4. Finally, conclusion of this study is presented in section 5.In all examples throughout this paper, we will show Persian sentences by their transliteration in italic between quotes followed by their translation to English between parentheses. Extracting semantic roles is one of the major steps in representing text meaning. It refers to finding the semantic relations between a predicate and syntactic constituents in a sentence. In this paper we present a semantic role labeling system for Persian, using memory-based learning model and standard features. We show that good semantic parsing results can be achieved with a small 1300-sentence training set. In order to extract features, we developed a shallow syntactic parser which divides the sentence into segments with certain syntactic units. The input data for both systems is drawn from Hamshahri corpus which is hand-labeled with required syntactic and semantic information. The results show an F-score of 90.3% on argument boundary detection task and an F-score of 87.4% on semantic role labeling task using Gold-standard parses. An overall system performance shows an F-score of 83.8% on complete semantic role labeling system i.e. boundary plus classification.
Dependency Relations as Source Context in Phrase-Based SMT  The Phrase-Based Statistical Machine Translation (PB-SMT) model has recently begun to include source context modeling, under the assumption that the proper lexical choice of an ambiguous word can be determined from the context in which it appears. Various types of lexical and syntactic features such as words, parts-of-speech, and supertags have been explored as effective source context in SMT. In this paper, we show that position-independent syntactic dependency relations of the head of a source phrase can be modeled as useful source context to improve target phrase selection and thereby improve overall performance of PB-SMT. On a Dutch-English translation task, by combining dependency relations and syntactic contextual features (part-of-speech), we achieved a 1.0 BLEU (Papineni et al., 2002) point improvement (3.1% relative) over the baseline.
Multi-Task Learning in Conditional Random Fields for Chunking in Shallow Semantic Parsing Semantic parsing of text corpora is needed to support tasks such as information extraction and question-answering. In particular, shallow semantic parsing focuses on identifying the semantic roles of the arguments of a verb (or any predicate) rather than parsing the whole sentence in detail.Traditional shallow semantic parsing systems for chunk analysis (Kudoh and Matsumoto, 2001;Zhan etal., 2002), which employ machine learning method, focus on the selection of features and their variety combinations to train a statistical model (Carreras and Marquez, 2005). However, we usually have no idea of what a good model is like, neither do we know which features to select. Fortunately, Multi-task Learning (MTL) provides us with a substantial solution to work out this nontrivial problem (Caruana, 1997;Ben-David and Schuller, 2003;Evgeniou and Pontil, 2004;Miccheli and Pontil, 2005;Maurer, 2006). Multi-task Learning is an approach to inductive transfer that emphasizes learning multiple tasks in parallel while using a shared representation, so that what is learned by all tasks is available to the target task.Alternating Structure Optimization (ASO) algorithm ) is a linear method based on such idea with the shared representation projected as a low-dimensional projection matrix shared by all related tasks. In supervised learning applications, large amount of unlabeled data is readily available, while labeled data are costly to obtain. Therefore, ( Ando and Zhang, 2005b) employs ASO in semi-supervised method and seeks to discover shared predictive structures through jointly learning multiple classification problems on unlabeled data. As adequate unlabeled data is involved, promising result achieved.Based on the consideration of huge resource consumption in ( Ando and Zhang, 2005b) and the availability of large human-labeled corpora such as PropBank (Palmer et al., 2005) and FrameNet ( Baker et al., 1998), ( Liu and Ng, 2007) utilizes labeled data to train ASO classifier, reasonably promising performance is obtained with comparably less additional labeled corpora. Alternating Structure Optimization (ASO) is a recently proposed linear Multi-task Learning algorithm. Although its effective has been verified in both semi-supervised as well as supervised methods, yet they necessitate taking external resource as a prerequisite. Therefore, feasibility of employing ASO to further improve the performance merely rests on the labeled data on hand proves to be a task deserving close scrutiny. Catering to this challenging while untapped problem, this paper presents a novel application of ASO to the subtask of Shallow Semantic Parsing: Chunking. Our experiments on Chinese Treebank 5.0 present promising result in chunk analysis, and the error rate is reduced by 5.72%, proposing a profound way to further improve the performance.
Query-Focused Multi-Document Summarization Using Co-Training Based Semi-Supervised Learning * * * * Query-focused multi-document summarization has attracted much attention in recent years. Different from generic summarization, it aims to provide more personalized information for a given query. By automatically capturing relevant and salient content from a large amount of searching results and showing them in a concise way, the sort of summarization can aid people to quickly access and digest their interested information. It also provides an effective means to diverse applications such as question answering system, personalized information retrieval, personalized news recommender, etc. To date, the most influential annual evaluation workshop for automatic summarization research is the Document Understanding Conference (DUC or now TAC), which provides a large-scale test benchmark as well as common evaluation procedures for researchers to share their ideas and experiences.The critical issues in query-focused multi-document summarization are as follows: The first one is that the information contained in the generated summary should be highly related to the given query. The second issue is how to take salience and diversity into account when selecting a batch of sentences or other textual units as representatives for a summary. As the allowed capacity in a summary is usually limited, it will be inappropriate to put all the informative sentences from different documents into the summary for they may convey the similar meanings. The intuition behind a good query-focused summary is to preserve the information biased to the query as much as possible, and remain the most representative and salient information that have the least duplicate contents to the information selected previously.In this study, we propose a novel sentence-based extractive approach. Since it is not enough to select relevant sentences from a single point of view and no labeled relevant or irrelevant sentences are available in advance, the proposed approach first makes full use of the co-training algorithm to identify the relevant sentences on two abundant feature views with a small number of pseudo-labeled sentences. Then it employs a ranking algorithm to sort the relevant sentences and choose a certain number of representatives with highest content salience and information diversity for the final summary. Experimental results on DUC2007 main task dataset show that the proposed approach significantly outperforms the baseline approaches and achieves comparable performance to the state-of-the-art systems.The remainder of the paper is organized as follows. Section 2 discusses related work. The proposed summarization approach is described in Section 3. The details of the experimentation are shown in Section 4. Section 5 presents our conclusion and future work. This paper presents a novel approach to query-focused multi-document summarization. As a good biased summary is expected to keep a balance among query relevance, content salience and information diversity, the approach first makes use of both the content feature and the relationship feature to select a number of sentences via the co-training based semi-supervised learning, which can identify the query relevant sentences beyond a single point of view. Then the ranking algorithm based on Markov chain random walks is employed on the relevant sentences by encouraging content salience and information diversity in a unified framework. The final summary focusing on the integration of relevance, salience and diversity is created after several sentences with the highest overall ranking scores are extracted. We performed experiments on DUC2007 dataset and the evaluation results show that the proposed approach can achieve significant improvement over standard baseline approaches and gain comparable performance to the state-of-the-art systems.
Gender and Animacy Knowledge Discovery from Web-Scale N-Grams for Unsupervised Person Mention Detection * The task of detecting entity mentions (references to entities) is very important to the downstream processing of information extraction such as coreference resolution and event extraction. Entity mentions can be divided into name mentions (e.g. "John Smith"), nominal mentions (e.g. "president") and pronouns (e.g. "he", "she"). Typical mention detection systems are based on supervised learning (Boschee et al., 2005;Zitouni and Florian, 2008) or semisupervised learning (Ji and Grishman, 2006). Achieving really high performance for mention detection requires deep semantic knowledge and large costly hand-labeled data. Many systems also exploited lexical gazetteers such as census data with gender information. However, such knowledge is relatively static (it is not updated during the extraction process), expensive to construct, and doesn't include any probabilistic information.Mention detection is by definition a semantic task: for example, a phrase is a person mention if it refers to a real-world person entity. We should thus expect a successful mention detection system to exploit world knowledge, in order to resolve hard cases. For example, if a reflexive pronoun (e.g. "himself") is bound by a phrase in its governing category (Haegeman, 1994), then this phrase is likely to be a person mention (masculine or feminine). In addition, a person mention usually has a life and therefore is likely to be animate (Cobuild, 1995). Therefore, if we could automatically discover a large knowledge base of gender and animacy properties for all possible noun phrases, it will be a valuable resource for person mention detection.In this paper we will glean these two powerful lexical properties -gender and animacy -for person mention detection. Further progress will likely be aided by flexible frameworks for representing and using the information provided by this kind of properties. We shall discover these properties from web-scale Google n-gram data and use them to detect person mentions in an unsupervised learning fashion. Such methods allow us to compensate for the absence of annotated training data and semantic resources. The derived properties may include a lot of noise, and thus we will introduce several confidence estimation methods and experiment with various patterns for knowledge discovery. The contributions of this paper are two-fold: (1) the first attempt to discover gender and animacy knowledge from web-scale n-grams; (2) the first work on detecting entity mentions based on unsupervised knowledge discovery.The rest of this paper is structured as follows. Section 2 describes our main research task and experimental setting. Section 3 motivates our approach based with error analysis of traditional supervised learning. Section 4 and Section 5 then present the detailed knowledge discovery process from n-grams and using them for mention detection. Section 6 presents experimental results. Section 7 briefly reviews the previous research on the discovery and the use of gender and animacy knowledge. Section 8 then concludes the paper and sketches our future work. In this paper we present a simple approach to discover gender and animacy knowledge for person mention detection. We learn noun-gender and noun-animacy pair counts from web-scale n-grams using specific lexical patterns, and then apply confidence estimation metrics to filter noise. The selected informative pairs are then used to detect person mentions from raw texts in an unsupervised learning framework. Experiments showed that this approach can achieve high performance comparable to state-of-the-art supervised learning methods which require manually annotated corpora and gazetteers.
Layer-Based Dependency Parsing* Graph-based models (McDonald et al., 2005;McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003;Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors.Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence (the whole dependency graph) both in training and inference procedures, which results in a cubic computational complexity (in projective case). By contrast, transition-based approaches train a classifier to greedily choose the best parsing action under the current parser state. They make decisions at a configuration which is usually composed by a couple of focus tokens and the parsing contexts. Therefore, these two kinds of dependency parsing methods represent the two extremes when they seek the best dependency structure of the input sentence. In this paper, we adopt a moderate structural granularity to calculate the parser: a dependency layer.The dependency layer we mean here is a set of tokens whose dependency depth (the depth of the dependency tree) is at most one. Inside the layer the dependency graphs can be searched exhaustively while between the layers the parser state transfers deterministically. On one hand, this design will decrease the computational cost for searching the whole tree like graph-based models do; on the other hand, it may alleviate the error propagation resulting from the complete no "search" outside the parsing configuration in transition-based models.It is well known that chunking, which is deemed to be a useful and tractable precursor to full parsing, has been successfully handled by sequence labeling techniques ( Kudo and Matsumoto, 2001;Sha and Pereira, 2003). Inspired by this scheme, we adopt the globally optimal sequence labeling to search the best depth-one sub-graph in the dependency layer. We believe that the line-typed sequential models are potent complementarities to the tree-typed hierarchical ones or even the latent substitutes.The experiments show that our layer-based parser yields comparable dependency attachment accuracies to the state-of-the-art dependency parsers on both English and Chinese datasets. Especially, it is quite efficient due to the layer-based search and sequence typed analysis. The remainder of the paper is organized as follows: Section 2 describes the details of the algorithm and feature set. Section 3 presents the experimental results. The related work is discussed in Section 4. Conclusion and future work comprise Section 5. Wu et al. (2007) designed a neighbor parser to identify the neighboring parent-child relations between two consecutive tokens in the input sentence. Following their framework we label the dependency relations in our parsing layer. An example is shown in Figure 1(a). The first and second columns represent the words and part-of-speech (POS) tags respectively. The third column implies whether the token modifies its left neighbor (LH, left-headed) or right neighbor (RH, right-headed) or neither (O). The string behind the character "_" indicates the dependency type of the neighboring link. Wu et al. (2007) employed linear chain conditional random fields (CRFs) as the labeling algorithm to capture the higher order features and avoid the greedy search when labeling with sequential classifiers ( Cheng et al., 2006). To prevent the error propagation, they regarded the labeling results as features of the subsequent parsing stage instead of reducing the child words. However, this weakens the strength that neighboring parsing can provide. In our approach, besides the CRF-based relation labeler, an additional tagger is introduced to examine whether a dependent child can be reduced, i.e., whether it has found its head and has already been a complete sub-tree. The reduce tagger tries to guarantee safe reductions and ensures the parsed structures can be formed into a tree after several passes of analysis. In Figure 1(b), the letter "r" in the rightmost column implies that the corresponding token will be reduced while others are reserved for the next stage. In this paper, a layer-based projective dependency parsing approach is presented. This novel approach works layer by layer from the bottom up. Inside the layer the dependency graphs are searched exhaustively while between the layers the parser state transfers deterministically. Taking the dependency layer as the parsing unit, the proposed parser has a lower computational complexity than graph-based models which search for a whole dependency graph and alleviates the error propagation that transition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed.
Mediatory Summary Generation: Summary-Passage Extraction for Information Credibility on the Web Many pages on the Web contain incorrect or unverifiable information. Therefore, there is a growing demand for technologies that enable us to obtain reliable information. However, it would be almost impossible to automatically judge the accuracy of information presented on the Web. In this case, the second-best approach is to develop a supporting method that judges the credibility of information on the Web.At present, when we would like to judge the credibility of information on the Web, we often read some relevant Web documents retrieved via Web search engines. However, if the content described in some of the documents conflicts with the content in some other documents, Web search engines do not give users any suggestions on how to interpret the conflict. Furthermore, the amount of retrieved documents is too large to read, and the documents may not be ranked in the order of the credibility of information. Therefore, information retrieval is not sufficient to support a user's judgment on the credibility of information, and therefore, additional techniques are required.Such techniques for supporting a user's judgment on the credibility of information are classified into three types. The first type involves the extraction of significant descriptions such as statements that may be relevant to a user and the sender of the statements. In this paper, a statement is defined as text such as an opinion, evaluation, or objective fact. The term sender refers to a person or an £ This research is partially supported by National Institute of Information and Communications Technology, Japan and a research project of Graduate School of Environment and Information Sciences, Yokohama National University.  Figure 1: Example of survey report organization providing a description of certain information on the Web. The second type involves the analysis of semantic relations such as AGREEMENT, CONFLICT, or EVIDENCE between statements ( Murakami et al., 2009). The third type involves the summarization and presentation of the extracted and analyzed information to enable the users to easily judge the credibility of statements. In this study, we attempt to develop a system that automatically generates a survey report for verifying the credibility of information ( Ando et al., 2008). The rest of this paper is organized as follows. In section 2, we discuss the different points of the summarization for information credibility and the summarization for general purpose, and describe a survey report for verifying the credibility of information. In section 3, we explain mediatory summarization that is one of the most significant concepts in our summarization. In section 4, we describe the outline of the summarization system for supporting the user's judgment on the credibility of information on the Web. In section 5, we conduct examinations on elementary techniques of our summarization, and discuss the results. In section 6, we provide our conclusion. In this paper, we discuss the summarization for supporting a user&apos;s judgment on the credibility of information on the Web. In general, if a statement contradicts another statement, the credibility of either of the statements decreases. However, these opposing statements may coexist under certain situations, and presenting such situations is helpful for a user&apos;s judgment. Therefore, we focus on the coexistence of these opposing statements, and attempt to develop a system to generate survey reports that contain mediatory summaries, which are defined as passages extracted from Web documents in order to present situations in which these opposing statements can coexist. We describe the outline of the summarization system and describe how to improve the TextRank algorithm from the viewpoint of passage extraction for the system. From experimental results, we confirmed that the methods based on the improved TextRank algorithm can extract significant passages, which are actually considered as significant by human assessors, with higher precision than baseline methods.
An Experimental Syntactic Study of Binding: A Case Study of Korean Long-Distance Anaphor caki * * * * Pollard and Sag (1992) and Reinhart and Reuland (1993) argued that local anaphors can be LDbound or be unbound in certain contexts as shown in (1). In sentences shown below, the English local anaphor himself is bound outside the minimal GC (cf. 1a, 1b) or unbound (cf. 1c) and yet the sentences are acceptable. The above researchers posit a distinction between core or grammatical binding on the one hand and exempt or logophoric binding on the other to explicate how local anaphors can sometimes occur in structures where the core constraints on anaphor binding-locality and ccommand-are seemingly violated. The proposal that these researchers make is that not all anaphors are licensed grammar-internally. Anaphors that are licensed grammar-internally are called core/grammatical anaphors, while anaphors licensed by extra-grammatical mechanisms are categorized as exempt anaphors/logophors. In the theory of Pollard &amp; Sag (1992), an anaphor is exempt when it does not have a more prominent co-argument in its argument structure. However, in the presence of such a co-argument, the anaphor must be bound grammar-internally as a core anaphor. The bracketed NPs within which anaphors occur in (1)ad above do not contain a more prominent co-argument (i.e., the Possessor), and hence, the anaphor is licensed as an exempt anaphor, freed from the constraints on core binding.More generally, exempt anaphors display a cluster of properties that distinguish them from core anaphors, which include the following:(2) a. Exempt anaphors may be unbound or discourse-bound (cf. 1c).b. Exempt anaphors do not need c-commanding antecedents (cf. 1d). c. Exempt anaphors may take antecedents outside the local domain (the Governing Category) for local binding (cf. 1a,b). d. Exempt anaphors allow strict readings in VP ellipsis contexts.The sloppy-strict ambiguity (cf. 2d) was employed as another diagnostic of the core-exempt distinction in Huang &amp; Liu (2001). 1 This is predicated on the observation that elliptical VPs 2 containing core anaphors are predominantly interpreted sloppily, whereas those containing exempt anaphors allow strict readings. For example, in (3a), the anaphor himself is a core anaphor since it is bound within the local GC. The elliptical VP 'did so, too' is interpreted sloppily (i.e. as meaning 'Bill defended Bill'). The strict reading is highly marginal, if available at all . On the other hand, in case of exempt binding shown in (3b), the strict reading (i.e. Bill thinks that an article written by John…) is much easier to obtain for the elliptical VP, and may indeed be more prominent than the sloppy reading.(3) a. John defended himself against the committee's accusations.Bill did so, too (=Bill defended Bill &gt;*?John…). b. John thinks that an article written by himself caused the uproar.Bill does so, too (= Bill thinks that an article written by John &gt;Bill…).While exempt anaphors can escape the strictures of syntactic conditions that constrain core anaphors, their licensing is nevertheless subject to discourse-pragmatic conditions known as logophoricity ( Sells 1987, Huang andLiu 2001). Antecedents of exempt anaphors are optimal if they can be associated with a logophoric role. In Sells (1987), three logophoric roles are introduced--Source, Self, and Pivot--with the following characterizations.(4) Logophoric roles (Sells 1987): SOURCE: the agent communicating the propositional content SELF: one whose mental state or attitude the content of the proposition describes PIVOT: one with respect to whose (space-time) location the content of the proposition is evaluated Researchers investigating logophoricity have furthermore argued that there is canonical hierarchy of logophoric roles. SOURCE is more canonical than the other two roles, while PIVOT is less canonical than SELF or SOURCE ( Sells 1987, Huang &amp; Liu 2001. This is reflected in the ease with which antecedents of logophoric anaphors can be licensed. For example, exempt anaphors are more easily licensed by SOURCE antecedents than by PIVOTs.Since exempt binding has been investigated mostly for English, a language that does not have genuine LDAs, questions such as whether a language like Korean, which possesses multiple anaphors (i.e., the long-distance anaphor 'caki' as well as local anaphors such as 'caki-casin' -Kang 1998), still allows local anaphors to be licensed as exempt anaphors naturally arise. Kim &amp; Yoon (2006 investigated this question with the Korean local anaphor 'caki-casin' and found that native speakers of Korean allow the local anaphor 'caki-casin' to be LD-bound in contexts known to license exempt anaphors in English. When LD bound, 'caki-casin' showed a strong preference for the strict reading in VP-ellipsis contexts, implying that the speakers treated LD-bound 'caki-casin' as an exempt anaphor. Finally, the degree of well-formedness of the exempt binding reflected the Sells (1987)'s canonical hierarchy of logophoric roles, in that binding of 'caki-casin' by a logophoric SOURCE got the highest acceptability rating, while that by a PIVOT antecedent got a much lower rating.The results of Kim and Yoon (2009) invite further questions. If the local anaphor 'caki-casin' can be bound as LD an exempt anaphor, satisfying logophoric conditions, what are the properties of genuine LDAs (such as 'caki') when they are LD bound? Do the anaphors which have been treated as LDA's behave differently from the local anaphor 'caki-casin' when the latter is LD-bound? Are genuine LDAs also sensitive to logophoric factors when they are bound by LD antecedents?The present study seeks to investigate whether the interpretations that speakers assign to this anaphor differs systematically from those that native speakers assign to 'caki-casin' when it is bound LD as an exempt anaphor. 3 The specific research questions addressed in the present study are the following: 1) Does LD-bound 'caki' behave differently from LD-bound exempt anaphor 'caki-casin'? 2) Does LD-bound 'caki' show sensitivity to logophoric factors? This study investigates the binding behavior of the Korean anaphor &apos;caki&apos;, which has been regarded thus far as a long-distance anaphor (LDA). However, given that even local anaphors can be bound long-distance when they function as exempt anaphors in certain languages (Pollard and Sag 1992; Kim and Yoon 2009), we investigated the binding behavior of LD-bound &apos;caki&apos;, in order to determine whether LD-bound &apos;caki&apos; differs from LD-bound &apos;caki-casin&apos;. In the experiment, subjects were required to rate the acceptability of Korean sentences representing various types of LD binding of &apos;caki&apos; and to determine whether the sloppy or the strict reading was more prominent in elliptical VPs containing the anaphor. The results are discussed with respect to the typology of LDAs proposed by Cole, Hermon and Huang (2001).
On the Scope Interaction of Japanese Indefinites An Epsilon Calculus Approach In languages like English, the distinction between definite and indefinite NPs are explicitly indicated by syntactic devices such as determiners, demonstratives and articles, while such distinctions are blurred in languages like Japanese, with the exception of NPs containing demonstratives, and both of them simply occur as bare nominals (without determiners). English indefinites have been treated as existentially quantified expressions, but they are quite different from universally quantified ones like any N or all Ns in that the former often do not obey the island conditions and can take arbitrarily wide scopes and/or function as referential terms. As for the construal of indefinites, several different approaches have been proposed. The first one sticks to the parallel treatment of universal and existential quantifiers, both of which are treated as generalized quantifiers, and some new mechanism of assignments deals with peculiar properties of indefinites. The second approach posits two different categories for indefinites (referential nominal and existential quantifier), as proposed by Fodor and Sag (1982). The third one deals with indefinites as discourse referents, suggested by so-called Discourse Representation Theories ( Kamp and Reyle, 1993), arguing that indefinites do not express existential force, but introduce new discourse referents to the contexts. The last one, which we will adopt in this paper, is to regard indefinites as choice functions. Among them, we will explore the use of epsilon terms as a syntactic counterpart of choice function, as proposed by Meyer Viol et al. (1999), Kempson et al. (2001), Cann et al. (2005),We would like to thank anonymous reviewers for their comments. This study explores the scope properties of some indefinites in Japanese in terms of epsilon calculus. Different from ordinary noun phrases, quantificational noun phrases like indefinites are assigned higher-order categories and/or types in syntax, and taken to denote functions from sets to truth values in semantics, which results in great difficulty in deriving proper interpretations of sentences with quantified expressions, given the tight syntax-semantics relation built into theories of grammar. We simply deal with all quantified expressions as terms of type e, and treat indefinites as choice functions, i.e., functions that apply to sets and arbitrarily select one of their members. Some indefinites can take arbitrarily wide scopes, depending on contextual information, whereas others have limitations on the freedom of scope taking. We adopt Dynamic Syntax to implement this idea, making it possible for the scope of indefinites to be left unspecified and fixed in a later stage of parsing.
Pattern Lattice as a Model for Linguistic Knowledge and Performance  This paper outlines a theoretical model, called the Pattern Lattice Model (PLM) of human linguistic knowledge and performance, and presents a simple implementation of this model. Any expressions found in a natural language L are structured in some ways, and linguists are willing to assume that those expressions are the products of what they call the grammar of L. In contrast, the PLM embodies a &quot;radically memory-based&quot; view of L, and provides a viable alternative to the traditional &quot;grammar-based&quot; model of L. The PLM is also expected to lay the theoretical foundations for the so-called &quot;usage-based model&quot; of language, which lacks solid foundations.
Sentiment Classification Considering Negation and Contrast Transition * * * * Sentiment classification is a task to classify text according to sentimental polarities of opinions they contain (e.g., favorable or unfavorable). This task has received considerable interests in computational linguistic community due to its wide applications.In the latest studies of this task, machine learning techniques become the state-of-the-art approach and have achieved much better results than some rule-based approaches ( Kennedy and Inkpen, 2006;Pang et al., 2002) . In machine learning approach, a document (text) is usually modeled as a bag-of-words, a set of words without any word order or syntactic relation information. Therefore, the whole sentimental orientation is highly influenced by the sentiment polarity of each word. Notice that although each word takes a fixed sentiment polarity itself, its polarity contributed to the whole sentence or document might be completely the opposite. Negation and contrast transition are exactly the two kinds of linguistic phenomena which are able to reverse the sentiment polarity. For example, see a sentence containing negation "this movie is not good" and another sentence containing contrast transition "this mouse is good looking, but it works terribly". The sentiment polarity of the word good in these two sentences is positive but the whole sentences are negative. Therefore, we can see that the whole sentiment is not necessarily the sum of the parts (Turney, 2002). This phenomenon is one main reason why machine learning often fails to classify some testing samples (Dredze et al., 2008).Fortunately, a language usually has some special words which indicate the possible polarity shift of a word or even a sentence. These words are called contextual valence shifters (CVSs) which can cause the valence of a lexical item to shift from one pole to the other or, less forcefully, even to modify the valence towards a more neutral position ( Polanyi and Zaenen, 2006). Generally speaking, CVSs are classified into two categories: sentence-based and discourse-based ( Polanyi and Zaenen, 2006). Sentence-based CVSs are responsible for shifting valence of some words in a sentence. The most obvious shifters are negatives, such as not, none, never, nothing, and hardly. These shifts usually reverse the sentiment polarity of some words. Other sentence-based shifters can be intensifiers (e.g., rather, very), modal operators (e.g., if), etc. Discourse-based CVSs often indicate the valence shifting in the context. Some connectives, such as however, but, and notwithstanding, belong to this type.In this paper, we mainly focus on sentiment shifting including negation and contrast transition because this kind of shifting often fully reverses the sentiment polarity and thus mostly reflects the weakness of those machine learning approaches based on one-bag-of-words modeling. Other types of shifting, for instance, intensification with intensifiers (e.g., rather, very) is capable of changing the intension of some words but would not reverse their polarities.Note that contrast transition is one special type of transition and is used to express contradiction or contrast when connecting one paragraph, sentence, clause or word with the other. It is distinguished from other types of transitions by different connectives. For contrast transitions, the connectives are some CVSs like however, but, and notwithstanding while others use some different connectives, e.g., conclusion transition takes the connectives like therefore, in a word, in summary, and in brief.To incorporate sentiment reversing information into a machine learning approach, we first segment the whole document into sub-sentences. We then partition them into two groups: one includes those called sentiment-reversed sentences and the other includes those called sentiment-non-reversed sentences. As a result, each document is represented as two-bags-ofwords rather than traditional one-bag-of-words. Finally, we propose the classification algorithm to do the classification on the text with two-bags-of-words modeling.The remainder of this paper is organized as follows. Section 2 introduces the related work on CVS applications in sentiment classification. Section 3 presents our approach in detail. Experimental results are presented and analyzed in Section 4. Finally, Section 5 draws our conclusions and outlines the future work. Negation and contrast transition are two kinds of linguistic phenomena which are popularly used to reverse the sentiment polarity of some words and sentences. In this paper, we propose an approach to incorporate their classification information into our sentiment classification system: First, we classify sentences into sentiment reversed and non-reversed parts. Then, represent them as two different bags-of-words. Third, present three general strategies to do classification with two-bag-of-words modeling. We collect a large-scale product reviews involving five domains and conduct our experiments on them. The experimental results show that incorporating both negation and contrast transition information is effective and performs robustly better than traditional machine learning approach (based on one-bag-of-words modeling) across five different domains.
Modal Verbs for the Advice Move in Advice Columns * * * * Giving advice has been recognized as a common discourse function. Advice givers express their opinion to influence addresses' behaviors or decisions. According to Morrow (2006), to achieve discourse function, advice-givers have employed strategies in various contexts to make advice acceptable. Previous research has focused on oral discourse, including clinical interaction (Heritage and Sefi, 1992), radio call-in programs (Green and Kupferberg, 2000), or daily interaction (Jefferson and Lee, 1992). However, the written discourse has been less discussed.For the written discourse, advice columns in magazines have been the major concern. The research has focused on underlying social norms or values (Currie, 2001;Mutongi, 2000;Stoll, 1998) or linguistic strategies (Thibault, 1988). Though the research has provided insights for psychology and sociology, van Dijk (1997) stated that the study was not suitable for linguistics. The reason was that different writers might view the world differently and appeal to different readers. Thus in linguistics, underlying social norms/values is not the major focus but complex structures, interaction, social practices, the functions in context, society, and culture. Furthermore, there are few studies on the strategies employed by different columnists for different issues. This present study would investigate use of modal verbs as one linguistic strategy applied by different columnists. The reason for this focus was that language users could usually deliver information more than the literal meaning of modals. Modals have varieties of communicative functions. Since advice columns have provided social norms and values, modal uses also need to be explored for better understanding of their grammatical, pragmatic, and contextual functions in the advice columns.Therefore, this paper aims to identify the moves and realization of modal verbs. To achieve the research purpose, ten discursive moves proposed by Locher's (2006) for advice columns was adopted to examine the corpus. In addition, to deal with the modal verbs for lexical strategy, the three discourse functions of modal verbs proposed by Biber et al. (1999) and Leech's (2005) were based on. To achieve the research purpose, two research questions were raised to answer: (1) What are the move sequences in different advice columns? (2) How is modal verb as linguistic strategy realized in the move of advice? This research hypothesized that advice writers of different topics apply different move sequences and modal verbs to achieve discourse function and the differences imply writers' intentions, emotion and expectancy of the effects on readers to interpret. The present study investigates how advice writers employ move sequences and modal verbs to achieve intended discourse functions. This paper aims to testify two research hypotheses: 1. Advice writers of different topics employ different moves and modal verbs to achieve discourse function, and 2. The differences may imply writers&apos; intentions, emotion and expectancy of effects on the readers to interpret. The corpus of five advice columns for investigation is collected from the website Creators.com. Locher&apos;s (2006) ten moves for advice columns and Leech (2005)&apos;s proposal of discourse function of modals are the frameworks for data analysis. The results indicate four frequent moves: advice, assessment, explanation, and general information. In addition, the columnists use different modal verbs in dealing with different issues. This study has shed light on language learning about the discourse function realized by moves and modals.
Approach to Selecting Best Development Set for Phrase-Based Statistical Machine Translation * * * * In recent years, the phrase-based statistical machine translation model obtains more attention and achieves good translation performance. But the quality of the translation results is greatly influenced by the model's parameters. How to get a group of parameters which can make good translation results becomes a problem which we focus on. In general, we could get the optimized parameters though minimum error rate training (MERT) on the development set (Och, 2003). The MERT will continue running until the BLEU score on the development set convergences and then we translate the test set to the target language using this group of parameters.Since we get the optimized parameters on the development set, it becomes an important factor to the quality of the final translation results. Usually, we run the MERT on all of the development set, but when the development set is in large-scale and there are many long sentences included in it, the MERT will consume too long time on translation and parameters' adjusting. Nevertheless we can not sure whether the parameters trained on it are optimal. So what we are interested in are: 1) How many sentences are adequate for the MERT and what kind of sentences contribute more to the MERT? 2) How can we select such development set to obtain more effective and robust parameters with less time and without performance losing?Based on the analysis above, our aim is to find a method to select sentences from the whole development set, and use the new development set we can save time on MERT process as well as improve the performance of the translation system. In intuition, if the sentences in the development set are more similar to the ones in the test set, the parameters trained on them will be more appropriate for the test set. So the intuitive way is to select the part of sentences from the whole development set base on some similarity measures. There are many similarity measure have been proposed by the former researchers, so which measure is suit for our task is the key problem we should focus on.The remainder of the paper is organized as follows. Section 2 will discuss the related work. We present our methods in Section 3 and discuss the experimental results in Section 4. Finally, we give our conclusions in Section 5. In phrase-based statistical machine translation system, the parameters of model are usually obtained from minimum error rate training (MERT) on the development set. So the development set has a great influence on the performance of the translation system. Generally, more development set will achieve more effective and robust parameters but consume much more time on MERT process. In this paper, we propose two methods to select sentences from the large development set, based on the phrase and the sentence structure respectively. The experimental results show that our methods can get better translation performance than the baseline system on the compact development set by using a state-of-the-art SMT system.
Using Extra-Linguistic Material for Mandarin-French Verbal Constructions Comparison The study reported in this paper is part of a broader project (M3: Model and Measurement of Meaning) which investigates lexical organization of French and Mandarin. The project combine psycho-linguistic and computational approaches for investigating the verb lexical organization for both languages. We therefore deal with two kinds of data: On the one hand the productions obtained through psycho-linguistic experiments realized both with French and Mandarin Chinese speakers; On the other hand we systematically exploit existing electronic resources. However, this paper does not address on the later aspect of this work. We focus here on crossing the results of both methodologies: psycho-linguistics experiments and analyses with computational and statistical methods. As suggested by (Biber, 1988), particular attention is given to extra-linguistic material that is used here as a context for applying a distributional approach. Distributional approaches are usually applied to linguistic material. However, in this study we attempt to use the videos themselves for unraveling some aspects of the mental lexicon. The experiments proposed in the following sections demonstrate the validity of such an approach. Our work can be situated in a growing body of semantic studies that approximate semantic objects with distributional information. The core idea is that similar semantic objects have similar distributional properties. The meaning of a word is modelled as a vector in a multidimensional space. Although our approach is rooted into empirical evidence, we do not discard more theoretical insights of lexical organisation. We rather argue for a combination of distributional semantics for content units such as nouns or verbs and of more traditional formal semantics for more functional units like closed-class words. Distributional semantics are well equipped for studying the paradigmatic organisation of openclass words. Once our empirical investigation achieved, we will relate it to existing theoretical frameworks dealing with verb semantics such as ( Ahrens et al., 2003).The paper is structured as follows. Section 2 introduces the protocol used for eliciting the linguistic material of this study. Section 3 details how extra-linguistic material can be used as a space for projecting linguistic data from both languages. Then we introduce our bilingual alignment in section 4 and we use it for evaluating and analyzing our statistical results (in section 5). The approach raises some issues about the bias that the extra-linguistic material may introduce. We will try to address this concern in section 6 before concluding and discussing the impact of this study on our longer term objectives. Systematic cross-linguistic studies of verbs syntactic-semantic behaviors for ty-pologically distant languages such as Mandarin Chinese and French are difficult to conduct. Such studies are nevertheless necessary due to the crucial role that verbal constructions play in the mental lexicon. This paper addresses the problem by combining psycho-linguistics and computational methods. Psycho-linguistics provides us with a bilingual corpus that features verbal construction associated with carefully built extra-linguistic material (short video clips). Computational approaches bring us distributional semantic models (DSM) to measure the distance between linguistic elements in the extra-linguistic space. These models allows for cross-linguistic measures that we evaluate against manually annotated data. In this paper , we discuss the results, potential shortcomings involving cultural variability and how to measure such bias.
Improving Unsegmented Dialogue Turns Annotation with N-gram Transducers A dialogue system is usually defined as a computer system that interacts with a human user to fulfil a task (Dybkjaer and Minker, 2008). These systems are of particular interest in many applications, like information systems that are accessed by telephone (Seneff and Polifroni, 2000;Aust et al., 1995) or assistant systems for people with special necessities (Wilks, 2006). All the systems define the way they react to user inputs with the so-called dialogue strategy. This strategy can be rule-based ( Gorin et al., 1997) or data-based (Young, 2000).In any case, the strategies are based on the interpretation of the user input in terms of dialogue semantic units. These semantic units are usually coded in terms of Dialogue Acts (DA) (Bunt, 1994), which model the intention of the current user interaction along with its associated information. This concept can be extended to system responses. In an interaction, several dialogue meaningful units can be distinguished. These units are called segments (or utterances according to authors such as ( Stolcke et al., 2000)), and each segment has associated only one DA label.The annotation of a dialogue corpus in terms of DA is an interesting problem for both the development of data-based dialogue systems and the study of discourse and dialogue structure. In the first case, the statistical models that implement the dialogue manager (Williams and Young, 2007;Meng et al., 2003;Stolcke et al., 2000) rely on annotated dialogues to estimate their parameters. This annotation process is developed by human experts and it is a hard and time-consuming task. The use of probabilistic models can provide a draft annotation of the corpus ( Stolcke et al., 2000) that can make the manual annotation process faster.Most of the previous works on the use of probabilistic models for DA annotation use segmented dialogue turns ( Stolcke et al., 2000;Webb and Wilks, 2005;Rangarajan et al., 2007). However, this segmentation is not usually available in the initial transcription of a dialogue corpus. Other works propose a decouple segmentation-annotation scheme ( Ang et al., 2005), but the ideal option is the use of models that can annotate unsegmented dialogue turns, giving the correct segments and labels. This option has been explored in a few previous works ( Zimmermann et al., 2005;Martínez-Hinarejos et al., 2008), giving in any case (as could be expected) poorer results than when the segmentation is available.The classical model for this task is based on Hidden Markov Models (HMM) ( Stolcke et al., 2000). In this work we present an enhanced version of an alternative model, the N-Gram Transducer (NGT) model. This enhancement provides competitive results with respect to the classical HMM approach in the annotation and segmentation accuracy for unsegmented dialogues, even in two dialogue corpora of very different nature. This paper is organised as follows: in Section 2 we present the two statistical models that are compared, in Section 3 we detail the corpora for the experiments, in Section 4 we describe the experiments and show their results, in Section 5 we draw conclusions and future lines of work. The statistical models used for dialogue systems need annotated data (dialogues) to infer their statistical parameters. Dialogues are usually annotated in terms of Dialogue Acts (DA). The annotation problem can be attacked with statistical models, that avoid annotating the dialogues from scratch. Most previous works on automatic statistical annotation assume that the dialogue turns are segmented into the corresponding meaningful units. However, this segmentation is not usually available. Most recent works tried the annotation with unseg-mented turns using an extension of the models used in the segmented case, but they showed a dramatical decrease in their performance. In this work we propose an enhanced annotation technique based on N-gram transducers that outperforms the accuracy of the classical HMM-based model for annotation and segmentation of unsegmented turns.
Using Tree Kernels for Classifying Temporal Relations between Events * * * * In recent years, many progresses have been made in natural language processing (NLP). Combining statistical and symbolic methods plays a significant role in these advances. Tasks such as part-of-speech tagging, morphological analysis, parsing, and named entity recognition have been addressed with satisfactory results ( Mani et al., 2006). Problems such as temporal information processing that requires a deep semantic analysis are yet to be addressed.Lately, the increasing attention in practical NLP applications such as question answering, information extraction, and summarization have resulted in an increasing demand for temporal information processing ( Tatu and Srikanth, 2008). In question answering, one would expect the system to answer questions such as "when an event occurred", or "what is the chronological order between some desired events". In text summarization, especially in multi-document type, knowing the order of events is a useful source for merging related information correctly. It is also the case that in some information extraction applications, the temporal information between events can be very useful and effective (Alonso, 2009).Temporal information is usually encoded in the textual description of some events. For a given ordered pair of components ( ) 2 1 , x x , where 1 x and 2 x are times or events, a temporal information processing system tries to identify the type of relation i r that temporally links 1 x to 2x . The type of relation i r can be one of the 13 types proposed by James Allen (Allen, 1984). For example, in the sentence "Ocean Drilling said (e22) it will offer (e23) 15% to 20% of the contract-drilling business through an initial public offering (e25) in the near future (t67). (wsj_313), there are some relations between pairs (e23, e25), and (e25, t67). The task is to automatically tag these pairs with relations INCLUDES and BEFORE, respectively.With recent construction of the Timebank corpus ( Pustejovsky et al, 2003), the efficiency of different machine learning methods can now be compared. The recent work with Timebank has disclosed that six-class classification of temporal relations is a very complicated task, even for human annotators. In this paper, we propose an improved way of classifying temporal relations, using a machine learning approach. Support vector classification using effective kernel functions are specifically applied to two types of features: corpus gold-standard event features and underlying syntactic features of the contextual sentence. To capture either type of features, we apply an event-kernel to the gold-standard event features, and a convolution tree-kernel to syntactic features. The event kernel has been implemented according to (Mani et al., 2006) and some novel tree kernels have been employed as our syntactic tree kernel. Experimental results on Timebank validate the proposed method by showing 6% improvement over the state of the art method that merely uses gold-standard features.The remainder of the paper is organized as follows: section 2 is about previous approaches to temporal relation classification. Section 3 explains our proposed method. Section 4 briefly presents characteristic of the corpus that we have used. Section 5 demonstrates evaluation of the proposed algorithm. Finally, paper is concluded in section 6. The ability to accurately classify temporal relations between events is an important task in a large number of natural language processing and text mining applications such as question answering, summarization, and language specific information retrieval. In this paper, we propose an improved way of classifying temporal relations, using support vector machines (SVM). Along with gold-standard corpus features, the proposed method aims at exploiting useful syntactic features, which are automatically generated, to improve accuracy of the SVM classification method. Accordingly, a number of novel kernel functions are introduced and evaluated for temporal relation classification. Our evaluations clearly demonstrate that adding syntactic features results in a considerable performance improvement over the state of the art method, which merely employs gold-standard features.
Extended GL and Japanese Postposition No  This paper proposes elaboration of the Generative Lexicon (GL) in Pustejovsky (1995) and the Extended Generative Lexicon theory (Lenci et al., 2000). My proposal is based on the Japanese genitive postposition no 1. The Japanese NP 1-no NP 2 &quot;NP 1-GEN NP 2 &quot; construction expresses a wider range of relations between two entities than the English possessive NP 1 &apos;s NP 2 , such that neither selective binding (Pustejovsky, 1995) nor type-shifting based on qualia roles in NP 2 (Vikner and Jensen, 2002) captures the necessary relations-time, location, manner, and others of temporary nature. The disambiguation of possessive relations requires that lexical entries be augmented by incorporating a Referential Module comprising subcategories such as LOCATION, TIME, and MANNER. 1 Inherent Problems with Selective Binding GL proposed in Pustejovsky (1995) encodes four qualia roles which originate in Aristotle&apos;s concept of matters and represent four inherent properties. CONSTITUTIVE quale represents part-whole relation, FORMAL role indicates shape, ontological category, and so forth, TELIC role represents purpose and AGENTIVE role expresses origin. Pustejovsky (1995) further suggests selective binding when computing the meaning of the noun phrases modified by non-intersective adjectives. For example, fast in a fast typist does not denote a typist who is also generally fast apart from typing, but specifically a typist who is fast at typing. In other words, fast does not modify the typist himself, but it does modify the way that the typist types, i.e., fast modifies the event argument of the TELIC (purpose) quale of the noun typist-to type. (1) [[f ast typist]] = λx[typist(x) ∧ ...[TELIC = λe[type(e) ∧ agent(e) = x ∧ fast(e)]]...] Selective binding works for some of the prenominal possessive modification in Japanese when NP 1-no phrases modify one of the qualia of NP 2 , that is, selectively bind an event contained in the quale. However, I will show that there are many examples in which selective binding does not apply.
Note on Japanese Epistemic Verb Constructions: A Surface-Compositional Analysis In some languages, an argument that belongs semantically to an embedded clause is realized syntactically as an object of a matrix clause, this "raising to object" (RTO) is schematized as follows:(1) [ matrix subject . . . object i . . . [ embedded t i . . . ] . . . ]The term "raising" has its origin in the transformational analysis of such constructions in which the subject of the lower clause is "raised" to become the object of the matrix verb.English has many examples of RTO verbs (consider, believe, think, etc.), which can be classified together as "epistemic verbs" (verbs of thinking, feeling, perceiving, etc.) in semantic terms. The RTO epistemic verbs take three kinds of complement clause, a fully-fledged clause as in (2a), an infinitive clause as in (2b), and a small clause as in (2c) below:(2) a. I considered [ (that) she was intelligent.] b. I considered [ her to be intelligent.] c. I considered [ her intelligent.] Although her in (2b) and (2c) is understood semantically to be the subject of to be intelligent referring to the equivalence of propositional contents against (2a), they are both syntactic objects of consider. In this analysis, the argument her is "raised" from its initial position as the subject of the embedded clause to its final position as the main clause object as illustrated in (1).There has been much debate about the derivation of RTO constructions such as that of (2b). In English, Postal (1974), Lasnik and Saito (1991), and others argue that (2b) involves the RTO movement illustrated in (1). Chomsky, on the other hand, argues that the derivation is either by the Exceptional Case-marking (ECM) process under S-deletion (Chomsky, 1981) or by IPcomplementation (Chomsky, 1986). However, his motivation for not accepting the RTO analyses is mainly theoretical.In Japanese, it has often been noted in the literature on transformational syntax that examples such as (3) below share syntactic properties with the example sentences listed in (2). The propositional contents of (3a), (3b), and (3c) are virtually equivalent and the differences between them lie in the case of the argument kanozyo 'she/her': nominative in (3a) and accusative in both (3b) and (3c). As those glosses indicate, (3) shows the same case alternation patterns that English exhibits in (2). It has been argued that (3b) is also derived by the RTO movement ( Kuno (1976), Tanaka (2002), and others) or the ECM process (Kaneko (1988) and Ueda (1988)).In this paper, we firstly examine Japanese data and their analysis for RTO leaving aside theoretical concerns about the transformation. Secondly we discuss the semantic and pragmatic properties of epistemic verb construction considering some facts which have been ignored by previous studies. Lastly, we propose an alternative analysis of the construction whose accusative object should be treated as a surface-compositional object rather than as a raised argument. This paper offers a new analysis of the raising to object construction in Japanese. This has been extensively discussed following Kuno (1976) for the case where the matrix predicate is an epistemic verb. Under CCG analysis an o-marked phrase is a surface-compositional object rather than a raised argument. This new approach correctly predicts the thetic and categorical judgments of epistemic verb constructions, which have hitherto only been accounted for by the studies which emphasize only the syntactic aspects of the construction.
On the So-Called Thematic Use of Wa: Reconsideration and Reconciliation This paper develops a semantic analysis of (some major functions of) the particle wa in Japanese, which integrates and reconciles two major lines of analyses in the existing literature: the topichood-based approach and the groundhood-based approach.There have been a great deal of studies of information packaging strategies in Japanese, including and especially the use of the putative 'topic-marker' wa (see Noda 1996, Fry 2003, and Heycock 2008 for literature surveys). Most previous studies of wa start from the assumption that, although wa is often (and indeed, most frequently, according to Fry's (2003) corpus-based survey) attached to a subject, wa is not a subject-marker. This assumption is based on the fact that wa can be attached to constituents other than subjects, such as (direct or indirect) objects, (temporal or locative) modifiers, and even modifiers of nominal constituents; the sentences in (1) are adapted from Noda (1996:2), and the one in (2) from Mikami (1963:13). 1 (1) a. Kodomo-tachi-wa child-Pl-wa karê-o curry-Acc tsukutte-i-mas-u. make-Asp-Polite-Pres 'The kids are making curry.' (wa occurring on a subject) b. Karê-wa curry-wa kodomo-tachi-ga child-Pl-Nom tsukutte-i-mas-u. make-Asp-Polite-Pres 'As for the curry, the kids are making it.' (wa occurring on a direct object) c. Kawara-de-wa riverside-Loc-wa kodomo-tachi-ga child-Pl-Nom karê-o curry-Acc tsukutte-i-mas-u. make-Asp-Polite-Pres 'On the riverside, the kids are making curry.' (wa occurring on a locative modifier) ( In past studies, there have been two major lines of analyses of the func-tion/meaning of the particle wa in Japanese in its thematic use. Some scholars argue that it marks a topic, while others claim that it marks ground (old, presupposed, or backgrounded information). I demonstrate, testing their predictions against empirical data, that neither the topichood-based approach nor the groundhood-based approach constitutes a complete theory of wa in its thematic use, and argue that the two approaches need to be combined in an adequate fashion. Namely, I propose that wa indicates topichood only when it is associated with certain grammatical functions, such as direct object, while it merely indicates groundhood when it is associated with others, including subject.
Chinese Semantic Class Learning from Web Based on Concept-Level Characteristics Semantic class learning is an important and well-studied task, which takes in a semantic class name as input (e.g. fruits) and automatically outputs its instances (e.g. apple, banana, orange, etc.). Although there are some existing semantic dictionaries, such as WordNet ( Miller et al., 1990), they lack the coverage to handle large open domains or rapidly changing categories: Vieira and Poesio (2000) found that, only 56% of antecedent/anaphoric coreferent pairs in hyponymy relations in the WSJ were in WordNet. So, automatic semantic class learning has been the motivating force underlying many applications in lexical acquisition, information extraction, and the construction of semantic taxonomies.Many methods have been developed for automatic semantic class learning, under the rubrics of lexical acquisition, hyponym acquisition, semantic class identification, and web-based information extraction. Almost all of these approaches face the same crucial problem: how to validate whether a extracted instances is a true class member. Currently, the validation methods mainly are based on the co-occurrence between candidates (or true instances) ( Kozareva et al., 2008;Kozareva et al., 2009; Wang and Cohen, 2009).This kind of validation has three shortages: 1) once some error instances are introduced in the bootstrapping process for some unavoidable reason, they possibly bring more and more error instances, which makes they get a high score in a common re-ranking algorithm; 2) can not reject Copyright 2009 by Wenbo Pang, Xiaozhong Fan, Jiangde Yu, and Yuxiang Jia the error instances caused by people's usage habits or misunderstanding. For example, because (European Union) often appears with (America) and (Japan) in the hyponym pattern of countries, although is not a country, it is also accepted by cooccurrence-based validation; and 3) miss the instances that cooccur only with some specific instances, such as (Neon fish), which only cooccurs with other kind of lamp fish. Because of these shortages, the systems employing cooccurrence-based validation would add other extra constraint to the candidate instances in order to improve system's accuracy, which will reduce the system's recall rate. For example, in Kozareva et al. (2008), the golden fish are 1102, but the maximum evaluated fish are 116.What should a correct instance satisfy? Why do the error instances cooccur with the right instances? We answer these two questions from the viewpoint of concept characteristics. Then utilize three kind of concept characteristics, including the category features that characterizes the usage environment of a candidate instance or a semantic class, the interference semantic classes that are so close to the goal semantic class at the level of concept that people often use them together, and collective instance that is a collective of some correct instances, to validate the candidate instances. The automatic extraction of semantic class instance is a foundational work for many natural language processing applications. One of its crucial problems is how to validate whether a candidate instances is a true class member. Different from the common validation approaches based on the cooccurrence between instances, we present a novel approach based on concept characteristics, including category features, interference semantic classes, and collective instance. Firstly, we analyze the common error instances produced by cooccurrence-based validation from the perspective of concept, and then utilize the concept characteristics to validate the candidate instances. We conduct experiments on eight semantic classes and achieved high accuracies and recall rates, especially on open semantic classes, such as fish and singer.
On Pseudogapping in HPSG* English has ellipsis like these examples.(1) a. Sluicing:She read something, but she won't say what [  Mary has read more books than Bill has [ vP ]. (Johnson, 2008) Sentences in (1) have a certain phenomenon in common. In those sentences except (1e), reduplicated elements of the right clause are elided, remaining their antecedent in the left clause. vP-ellipsis is referred to phenomenon that the vP in the right clause is elided, except an auxiliary verb. On the other hand, gapping involves the deletion of finite verb, remaining its arguments. Pseudogapping shares its characteristics with gapping and vP ellipsis. Generally, pseudogapping occurs in coordination structures, such as (1c). However, it is related to not only coordination structures, but also subordination structures and comparative structures.(2) If you don't believe me, you will [ vP ] the weatherman.( Levin, 1978) (3) John gave Bill a lot more money than Bill will [ vP ] Susan.( Bowers, 1998) In HPSG, studies related to gapping have not flourished and even there is no schema which can account for pseudogapping. In this paper, I will examine some previouse studies of pseudogapping within the Minimalist Program. And then, a gapping schema in HPSG will be modified in order to explain pseudogapping in a proper way. So a new schema will be introduced that can capture the characteristics of pseudogapping in subordination and comparative structures as well as coordination structures. This study investigates the constraints imposed on the pseudogapping in the framework of Head-Driven Phrase Structure Grammar (HPSG). Based on the existing schema to account for coordination and gapping, a new pseudogapping schema in coordination structures is proposed in this paper. In the process of capturing the constraints, new DOM lists are added and an existing DOM list is divided into two DOM lists depending on the feature of elements in each domain. Furthermore, new features SEP and INC are introduced. SEP is used for distinguishing prepositions which should be located in the same domain with the following NPs from those which can be separated from the following NPs. INC feature determines whether overlapping adverbs are in non-empty lists or not. Pseudogapping occurs not only in coordination structures, but in comparative or subordination structures. Thus, this paper introduces a pseudogapping schema that can be applied to all structures mentioned above.
PACLIC 23 Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation Volume 2 Edited by Olivia Kwong  
Incorporate Web Search Technology to Solve Out-of-Vocabulary Words in Chinese Word Segmentation Chinese word segmentation plays an important role in many Chinese language processing tasks. In the past decade it has drawn a large body of research in Chinese language processing community. A variety of methods have been exploited ranging from rule-based (Palmer, 1997;Cheng et al., 1999) to statistics-based (Sproat et al., 1996), word-based ( Sun et al., 1998) to character-based (Xue, 2003), supervised learning-based ( Peng et al., 2004;Low et al., 2005) to unsupervised learning-based ( Goldwater et al., 2006;Zhao and Kit, 2008), as well as their hybrid ( Gao et al., 2005). It is reported in SIGHAN Bakeoff-2005(Emerson, 2005) and SIGHAN Bakeoff-2006 ( Levow, 2006) that the highest F1-measure achieved on open tracks is 97.9% while the OOV recall rate is only 84%. This performance is achieved on the test sets of which OOV rates only ranging from 2% to 8%. When facing Chinese running text with much higher OOV rate, the performance will drop dramatically. It is reported that performance loss caused by out-of-vocabulary (OOV) words is at least five times greater than that of segmentation ambiguities (Huang and Zhao, 2007). So, OOV problem is the main factor which extremely influences the performance of CWS system and there still has some room to improve.Recent studies in CWS focus on statistic machine learning methods. Regarding CWS task as sequence labeling problem (Xue, 2003;Goh et al., 2005), various machine learning methods can be adopted to do this task. Features derived from labeled corpora are taken to train the model. The performance of this kind of method much depends on the size and the quality of the training data. As labeled corpus is usually limited in size and unbalanced in content, it can not provide enough knowledge to train a model which is robust enough when facing large scaled running text which contains large majority of OOV words.Nowadays the number of web pages grows very fast. The web text can be considered as a very large scaled knowledge database which seldom has OOV problem. So, one way that can supplement the knowledge is to incorporate web knowledge database. There already have some works which are motivated by this idea. The most related one is ( Wang et al., 2007), they proposed a search-based CWS method which is entirely unsupervised. They perform word segmentation as a search procedure by using search engine to directly find answer on web. First, sub-sentences are extracted from sentences using punctuation as delimiters. Second, these sub-sentences are directly sent to search engine as user queries. At last, the highlight parts in the returned snippets are used to construct the final word segmentation. Experimental result shows performance improvement on OOV recall rate but the reported F-measure is only about 87% which is much worse than supervised machine learning method. Motivated by taking both advantages of web-search method and supervised machine learning method, a new framework combines using web search and CRF model is proposed. For every sentence, segmentation candidates are collected and organized as lattice. Instead of sending sub-sentences as queries, specific small segments derived from the lattice are sent to the search engine. Search based segmentation is constructed using the highlighted parts of returned snippets. Final decision is made by measuring the distance of the search-based segmentation with the CRF segmentation candidates.The rest of the paper is organized as follows. We introduce our specific implementation of linear-chain CRF model based word segmenter in Section 2. In Section 3, we propose the new segmentation framework which combines using search technology and supervised machine learning method. Experimental results are given in Section 4 and in Section 5, we conclude our work. Chinese word segmentation (CWS) is the fundamental technology for many NLP-related applications. It is reported that more than 60% of segmentation errors is caused by the out-of-vocabulary (OOV) words. Recent studies in CWS show that, statistical machine learning method is, to some extent, effective on solving OOV words. But labeled data is limited in size and unbalanced in content which makes it impossible to obtain all the required knowledge to recognize OOV words. In this paper, large scaled web data is incorporated as knowledge supplement. A framework which combines using web search technology and machine learning method is proposed. For each sentence, basic segmentation is performed using linear-chain Conditional Random Fields (CRF) model. Substrings which CRF model gives low confidence decisions are extracted and sent to search engine to perform web search based word segmentation. Final decision is made by considering both CRF model based seg-mentation result and that of web search based result. Evaluations are conducted on SIGHAN Bakeoff 2005 and 2006 datasets, showing the effectiveness of the proposed framework on dealing with OOV words.
 In this paper, we confine personalized document summarization to the extraction from a given document a few sentence which can best represent the whole content of the document in the point of view of a given user. While generic document summarization without personalization has been extensively researched, the trend of personalization is attracting more and more focus since the outgrowth of the technology of Web 2.0. For example, personalized search engine and personalized e-commence(movies, commodities) recommendation are two successful applications.Collaborative filtering is a widely used technology for personalized recommendation. Suppose some users have rated a set of books, the key idea of collaborative filtering is that similar users should share similar rating behaviours in the future, where the similarity between users itself is defined by the similarity of their past rating behaviours. The advantage of collaborative filtering is that it harvests the computing power of individuals in a population to process the large amount of complex information. Taking the example of book recommendation, there is no model competitive with a human brain with regards to understanding a book. We can treat each individual reader as a super computational power which is reluctant to give feedback. Then the task of collaborative filtering is to harvest these human power with the least workload on each individual.Fortunately, due to the growth of internet and web 2.0 technology, a large amount of tags data on webpages have been generated by human users. For example, people can tag their favourite Thanks for all the people who help collecting the dataset. We propose a new way of generating personalized single document summary by combining two complementary methods: collaborative filtering for tag recommendation and graph-based affinity propagation. The proposed method, named by Collaborative Summa-rization, consists of two steps iteratively repeated until convergence. In the first step, the possible tags of one user on a new document are predicted using collaborative filtering which bases on tagging histories of all users. The predicted tags of the new document are supposed to represent both the key idea of the document itself and the special content of interest to that specific user. In the second step, the predicted tags are used to guide graph-based affinity propagation algorithm to generate personalized summarization. The generated summary is in turn used to fine tune the prediction of tags in the first step. The most intriguing advantage of collaborative summarization is that it harvests human intelligence which is in the form of existing tag annotations of webpages, such as delicious.com bookmark tags, to tackle a complex NLP task which is very difficult for artificial intelligence alone. Experiment on summariza-tion of wikipedia documents based on delicious.com bookmark tags shows the potential of this method.
Finding Answers to Definition Questions Using Web Knowledge Bases * * * * Current researches on Question Answering (QA) mainly concern more complex questions than factoid ones. In TREC2007, ciQA track (Dang et al., 2007) focuses on 'Relationship' questions, which is defined as the ability of one entity to influence another, including both the means to influence and the motivation for doing so. In NTCIR-7, complex question ( Mitamura et al., 2008) are taxonomically defined as four types (Event, Definition, Biography and Relationship). Sentences that contain correct responses are extracted as answer candidates. In most cases, the correct answer for a complex question is composed of multiple sentences.Complex questions refer to complex relations between semantic concepts or synthesizing processes of deep knowledge; they implicate rich information need. Take a question-answer pair in Table 1 as an example. The question is a definition one; the information need is supposed to be the consequence between the greenhouse gas and the greenhouse effect, and names of gases that cause the greenhouse effect. Therefore, Answer 1 and Answer 2 are both correct answers for the question. Current researches on Question Answering concern more complex questions than factoid ones. Since definition questions are investigated by many researches, how to acquire accurate answers still becomes a core problem for definition QA. Although some systems use web knowledge bases to improve answer acquisition, we propose an approach that leverage them in an effective way. After summarizing definitions from web knowledge bases and merge them to a definition set, a two-stage retrieval model based on Probabilistic Latent Semantic Analysis is produced to seek documents and sentences in which the topic is similar to those in definition set. Then, an answer ranking model is employed to select both statistically and semantically similar sentences between sentences retrieved and sentences in definition set. Finally, sentences are ranked as answer candidates according to their scores. Experiments indicate following conclusions: 1) specific summarization technologies improves definition QA systems to a better performance; 2) topic based models can be more helpful than centroid-based models for definition QA systems in solving synonym and data sparse problems; 3) shallow semantic analysis is effective to find discriminative characteristics of definitions automatically.
Incorporating Statistical Information of Lexical Dependency into a Rule-Based Parser * * * * While it is easy to develop a rule-based parser and to improve the performance in the early developing stage, it becomes more and more difficult to resolve the conflicts between the rules as the number of rules increases. Because the rule-based parser has the limit in its ability to resolve syntactic ambiguity, syntactic ambiguity is the challenging problem especially for the rule-based parser using CFG as its grammar. One way to solve the problem is lexicalization.Many recent parsing technologies have taken statistical approaches as we can get more linguistic data such as the Penn Treebank (Collins, 1999;Collins, 2000;Charniak and Johnson, 2005). They also show encouraging performance. But practically the statistical parsing has the efficiency problem and the scalability problem. The scalability problem means the difficulty in incorporating other types of syntactic information such as lexical patterns or semantic patterns. Also it is not easy to tune the parser minutely with respect to each sentence. So, we want to use a PCFG parser as a base parsing system and use statistical information for syntactic ambiguity.There are many researches about resolving syntactic ambiguity using statistical information. The representative case is PP attachment disambiguation (Stetina and Nagao, 1997;Olteanu and Moldovan, 2005;Foth and Menzel 2006). Most of them simplified the problem into selecting an attachment site between a noun and a verb. However, in the real parsing the situation is more complicated. There can be more attachment sites and the impact of PP attachment on the other part has to be considered. In Foth and Menzel (2006), more comprehensive disambiguation method was presented using Lexical Attraction, a sort of mutual information.Another related research area is dependency parsing technology (Kudo and Matsumoto, 2000;McDonald et al., 2006). But for the present, we want to add statistical information without significant influence to the current weighting mechanism for the parsed tree selection.In the next section we introduce our base parsing system, and in the section 3, we present the way to apply statistical information for syntactic ambiguity. In the section 4, by analyzing the performance variation according to the types of statistical information, we improve the efficiency by applying the statistical information selectively. Then, we conclude this paper with several remarks on future works. This paper presents a method to incorporate statistical information into a rule-based parser to resolve syntactic ambiguities. We extract the statistical information from the Penn Treebank, and apply the information to the rule-based parser. For the extraction of the statistical information the tag conversion is needed because of the disagreement of the tags and the bracketing style. We will show the effect of the tag conversion with experiments. The final result shows about 7% error rate reduction in the dependency evaluation. We will also show how much each type of statistical information affects the parsing performance.
An Automated Thematic Role Labeler and Generalizer for Filipino Verb Arguments * * * * According to Rohwer and Freitag (2004), a lexicon is an essential resource in the Natural Language Processing research area. It provides the link between the terms of a language and the semantic and syntactic properties they are associated with. It can be of use in various tasks such as information extraction, text simplification, and machine translation ( Litkowski, 2005).Ideally, lexicons contain semantic, syntactic, morphological, and phonological information. However, not all of them contain all four of these as they are designed according to the specific needs of their applications. For example, a thesaurus-like lexicon contains information such as synonyms and antonyms, while a bilingual lexicon has translations of a term from one language to another (Litkowski, 2005). Some Natural Language Processing applications, on the other hand, require more complex lexicons -those that keep information on the lexical relations of terms such as thematic roles, which are the relations of verbs and their arguments. Thematic roles are useful as they serve as cues to the senses of the terms ( Gildea and Jurafsky, 2002). Furthermore, they could allow systems to check whether the required arguments are present in the sentence.For the English language, VerbNet -the largest online verb lexicon -keeps track of thematic roles as one of its verb class descriptions ( Schuler, 2005). For the Filipino language, most (if not all) currently existing lexicons are simply online bilingual or multilingual lexicons. They do not have essential information such as the thematic roles.In this research, the authors explored automatic learning of thematic roles to augment manual encoding of entries. The following sections are organized as follows: Section 2 gives an overview of the Filipino sentence structure; Section 3 discusses the resources needed and the processes; Section 4 discusses the results; Section 5 gives the conclusion. A lexicon is an essential resource in the Natural Language Processing research. It provides the link between the terms of a language and the semantic and syntactic properties they are associated with. For the Filipino language, only bilingual and multilingual lexicons are available electronically. Generally, the only information they contain are the translations of a term from one language to another. They do not have information on thematic roles, which are the relations of verbs and their arguments. These relations are useful because they could allow systems to check whether the required arguments are present in the sentence. To augment manual entries of the thematic roles into the lexicon, automatic learning of thematic roles of verb arguments is explored. This paper presents the resources needed, the processes, and the results.
Hybrid N-gram Probability Estimation in Morphologically Rich Languages * * * * An N-gram model is usually n-token sequence of words and is essential in natural language processing and speech processing. In morphologically simple language, like English, a bigram is a two-word sequence like machine translation, and machine learning. N-gram probabilities are computed in terms of the maximum likelihood estimation (MLE). One usually gets the MLE for the parameters of an N-gram model by normalizing counts from a corpus. The MLE, however, confronts major problems of sparse data and eventually zero probability. The longer n-grams one has, the higher chances of sparse data one has. Thus smoothing techniques have been developed to get better estimates for zero or low frequency sequences.In morphologically rich languages or in agglutinative languages like Korean, sparse data in language modeling are even more serious. Since words are formed by combining lemmas and various affixes together. For example, the bigram haksaying-i ka-nta 'student-subject marker, go-sentence final ending' has different count from another bigram haksaying-i ka-ss-ta 'studentsubject marker, go-past-sentence' 1 , even though there is only one morpheme difference. In typical N-gram modeling, bigrams with the same lemmas but with slightly different affixes do not get the same counts.Due to the agglutinative characteristics in Korean, most N-gram modeling is based on morphemes not words. Thus, instead of the bigram, haksayng-i kata 'student go', three bigram sequences such as 'haksaying i', 'i ka', and 'ka ta' are considered. Since the word forms were broken into morphemes, the sequence of morphemes would have higher counts than in the word bigram. Morpheme-based bigrams, however, cannot directly compute the MLE between two lemmas, except when two words have no affixes, like haksayng yokeum 'student fare'. If one uses a trigram in haksayng-i ka-nta, instead of a bigram, the 'haksayng i ka' sequence is considered. But the sequence would have a lower frequency than that of bigrams. Another problem originates from that morpheme-based N-grams inevitably generates an unnecessary morpheme sequence like 'i ka' in the example above. The sequence may assign an improper amount of probability to unseen N-grams.Instead of the prevalent N-gram modeling used in Korean, we suggest a new hybrid method of word and morpheme-based N-grams. The method takes advantage of the agglutinative nature of the Korean language and utilizes class-based N-gram modeling. We make use of a variablelength N-gram model in accordance with the structure of word sequences. We focus on lemmas in word sequences and get probability estimates from lemma bigrams or functional morphemelemma combinations. This method also works well with unknown words, since probabilities of unseen words are also approximated by variable-length N-grams. N-gram language modeling is essential in natural language processing and speech processing. In morphologically rich languages such as Korean, a word usually consists of at least one lemma (content morpheme) and functional morphemes which represent various grammatical. Most word forms in Korean, however, have problems of sparse data and zero probability, because of quite complex morpheme combinations. Thus morpheme-based N-gram modeling is widely used instead of a word sequence modeling. In this paper, we contend that a morpheme-based N-gram is inefficient language modeling in that it inevitably approximates the probability of unnecessary morpheme sequences, so the longer sequences we have, the lower probability estimates we get. We suggest a hybrid method that joins word-based and morpheme-based language modeling. The new method can also be regarded as an extension of a class-based measurement. Our experimental results show that the method produces better probability estimation than the morpheme-based measurement.
Summarization Approaches Based on Document Probability Distributions With the rapid growth of electronic information services, information is becoming available at an incredible rate. The phenomenon of information overload has meant that access to consistent and correctly-developed summaries is very important. As access to data has increased so has the interest in automatic summarization.Most current automatic summarization systems use sentence extraction, where key sentences in the input documents are selected to form the summary. Sentence scoring methods utilize both purely statistical and purely semantic features, for example as in (Nenkova et al., 2006;Vander- wende et al., 2006;Yih et al., 2007). Systems that go beyond sentence extraction, reformulating or simplifying the text of the original articles, must decide which sentences should be simplified, compressed, fused together or rewritten ( Barzilay and McKeown, 2005;Daumé III and Marcu, 2005;Jing and McKeown, 2000;Knight and Marcu, 2002;Vanderwende et al., 2004). Common approaches for identifying important sentences to include in the summary include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights ( Lin and Hovy, 2002;Schiffman et al., 2002).In this paper, we show that a very simple approach to multi-document summarization can yield results that are comparable to the best systems. We present two effective and simple fully automatic summary generation techniques which were designed keeping in mind the hypothesis Work done at Multilingual Systems group at Microsoft Research Lab, India.Copyright 2009 by Sandeep Sripada and Jagadeesh Jagarlamudi that the summaries would be effective if the summary document models are similar to that of the original documents. Despite the simplicity of the techniques, our results are among the best in multi-document summarization. Our approaches attempt to use subject-independent techniques, based mainly on fast, statistical processing and also explicitly deal with the issue of reducing redundancy.We compare our results against the SumBasic system (Nenkova et al., 2006; Nenkova and Vanderwende, 2005) and also the system which generates summaries based on maximizing Informative Content-Words (ICW) (Yih et al., 2007). SumBasic is extremely simple, but its performance is within statistical noise of the best system of DUC-2004. SumBasic first computes the probability of each content-word (i.e., verbs, nouns, adjectives and numbers) by simply counting its frequency in the document set. Each sentence is scored as the average of the probabilities of the words in it. The summary is then generated through a simple greedy search algorithm: it iteratively selects the sentence with the highest-scoring content-word, breaking ties by using the average score of the sentences. This continues until the maximum summary length has been reached. In order not to select the same or similar sentence multiple times, SumBasic updates probabilities of the words in the selected sentence by squaring them, modeling the likelihood of a word occurring twice in a summary. The ICW system improves on the SumBasic system in three ways. Firstly, they consider both the frequency and position information in their approach. Secondly, they use a discriminative, machine-learning based algorithm to combine these information sources. Finally, instead of applying the greedy heuristic they proceed by using an optimization technique that searches for the best summary.We first compare the results of our first approach (which is motivated by the SumBasic system but designed to incorporate our hypothesis) with the above mentioned systems and then show how a simple, light technique can produce summaries that are comparable with the best. We then move on to our second approach (which improves on the first one) and show the difference in the results and also the effectiveness of the algorithm when compared to the latter system described above.Our paper is organized as follows. Section 2 describes our main hypothesis which forms the basis for our paper. Initial experiments and observations that were conducted are described in Section 3. Our summary generation techniques are outlined in Section 4. We present our results on summarization in Section 5. We conclude and discuss future work in Section 6. One of the main purposes of a summary is to be able to replace the original document. We assume that a summary would be able to replace or act as a substitute for the document if its probability distribution is similar to that of the original document. With this hypothesis in mind, we carry out our analysis using datasets from Document Understanding Conference (DUC), studying the results of unigram probability models and also looking at Kullback-Leibler Divergence (KLD) as a measure of &apos;closeness&apos; between probability models of the documents and their summaries. In this paper, we also discuss two summary generation approaches that were designed based on the above hypothesis (a) Summary generation by extraction of sentences based on its coverage and (b) Minimum KLD Summary Generation Method (uses a metric for reducing redundancy). Our research shows that the above summarizer, which is light and simple, can deliver good summaries comparable to other state-of-the-art systems.
What L2 Processing Strategy Reveals about the Prototypical Relationship: A Case of Japanese Modal Markers of Possibility* Grammaticalization deals with a diachronic process whereby a lexical word evolves into a grammatical word. Modality in the area of general linguistics is defined as "a grammatical/semantic category expressing speakers' psychological attitude" (Narrog 2005:165). The auxiliary verbs representing modality is defined as modal markers (Palmer 2001:4). It is cross-linguistically not uncommon for a single modal marker to represent several subcategories of modality. This phenomenon is defined as 'polysemy', which is considered as a result of semantic change on accordance with grammaticalization. According to the typological study of grammaticalization by Bybee et al.. (1994), the common path of grammaticalization of modality among 75 languages is recognized. As such, it has often been analyzed from the perspective of universal human cognition in the areas of cognitive linguistics and grammaticalization .assertions and indicates the extent to which the speaker is committed to the truth of the proposition. Epistemic possibility indicates that the proposition may possibility be true (ibid: 179). We will explain the each usage of modal markers of possibility using the example of can.(1) He can run a mile in five minutes.(he has the ability) Ability (2) He can escape.(the door is not locked) Root Possibility (3) He can go now.(I give permission) Permission (4) It can take me up to four hours to get there. Two common directionalities in grammaticalization of modal markers of possibility, (5) and (6), are reported by the cross-linguistic survey by Bybee et al. (1994).(5) ability &gt; root possibility &gt; permission (6) ability&gt; root possibility &gt; epistemic possibility (5) is explained as flows. The transition from the generalization from ability to root possibility can be seen as the loss of a specific component of the meaning, the component that requires that the enabling conditions reside in an agent. This generalization resembles the one just described: since the enabling conditions for an agent to perform an act do not lie entirely in the agent, but also depend on the external world, can would also be used in cases in which the enabling conditions are both in the agent and outside the agent. Permission is that an agent is permitted to do something. The general enabling conditions expressed by root possibility include both physical conditions and social conditions, and permission is simply is the presence of social enabling conditions. Therefore, it is natural to regard that the permission use developed out of the root possibility sense (Bybee and Pagliuca 1985).A shift from root possibility to epistemic meaning in (6) involves a change in scope. Root possibility modal is part of the propositional content of the clause and serves to relate the agent to the main predicate. The epistemic modal, on the other hand, is external to the propositional content of the clause and has the whole proposition in its scope (Bybee et al., 1994: 197). Grammaticalization deals with a diachronic process whereby a lexical word evolves into a grammatical word. The typological study recognized the common directionality of grammaticalization among different languages. Two common directionalities in grammaticalization are recognized in the case of the modal markers of possibility: (1) ability &gt; root possibility &gt; permission 1 and (2) ability&gt; root possibility &gt; epistemic possibility. On the contrary to this, the modality of possibility in Japanese is encoded by different modal markers. The aim of this study is to analogize the relationship among them based on the data of second language acquisition, especially based on the processing strategy adopted by learners. This survey suggests that both permission and epistemic possibility seem to develop out from root possibility in Japanese.
HMM Expanded to Multiple Interleaved Chains as a Model for Word Sense Disambiguation A variety of text analysis tasks essentially rely on word sense disambiguation (WSD) for retrieving semantics from text. Recently, Wikipedia has become an important external knowledge base for performing text analysis tasks, and WSD based on information derived from Wikipedia is currently the area of active research (Grineva et al., 2009;Medelyan et al., 2008;Milne and Witten, 2008;Turdakov and Velikhov, 2008).The structure of Wikipedia differs (Zlatic et al., 2006) from the structure of the thesaurus WordNet that is traditionally used for WSD. Wikipedia consists of more than 2.5 million articles; each article describes some real world concept. Each Wikipedia article has a title that serves as the main representation for the concept. The body of an article in particular contains links to other conceptually related articles. Formally, each link has two parts: (i) an article in the encyclopedia the link points to and (ii) a caption that is displayed to readers of Wikipedia. Besides regular articles, Wikipedia contains several types of special pages. Redirect pages provide synonyms of the main representation. Another type of special pages important for this work is a disambiguation page, containing a list of articles with a similar ambiguous representation.Some knowledge-based WSD algorithms (Medelyan et al., 2008;Turdakov and Velikhov, 2008;Milne and Witten, 2008) rely on an assumption that text contains enough unambiguous terms, and these are used as the basis for disambiguation. However, the number of ambiguous representations increases as Wikipedia grows, and additional meanings appear for most commonly used terms. This tendency brings to texts that have a relatively small number of unambiguous terms that weakly correlate with the main subject of text. As a consequence, precision of WSD algorithms based on unambiguous content tends to decrease as Wikipedia continues to evolve.From this perspective, a WSD algorithm has to be durable to the proportion of ambiguous terms in text. Several research papers thus represent WSD as the maximization problem using the Hidden Markov Model formalism (De Loupy et al., 1998;Molina et al., 2004). Denoting the set of terms in text by T and the set of meanings by M , the input for a WSD algorithm is the sequence of terms τ = t 1 , . . . , t n found in text; t i ∈ T . The maximization problem is finding the most Copyright 2009 by Denis Turdakov and Dmitry Lizorkin probable sequence of meanings µ = m 1 , . . . , m n , where m i ∈ M , in accordance with the model, i.e. ˆ µ = arg max µ P (µ | τ ) = arg max µ P (µ)P (τ |µ) P (τ ) . Since the probability P (τ ) is constant in the maximization process, the problem is reduced to maximizing the numerator of the equation. For making this equation solvable, the problem is simplified using Markov assumptions. Thus the problem is reduced to solving the equationˆµequationˆ equationˆµ = arg max µ (, where h is the order of the model. The parameters of the last equation define the HMM of order h, where P (m i | m i−h:i−1 ) represents transition probabilities between states, and P (t i | m i ) represents the probability of emitting a term t i in state m i , i.e. the probability of emitting a particular term representing the meaning.Although the WSD task can be easily represented as a maximization problem using the HMM formalism this way, applying HMM to WSD is closely related to the natural language sparseness problem. That is, learning a transition model for WSD requires tremendous amount of training examples-at least several for each pair of concepts. Additionally, the classical Markov assumption itself seems to be not quite suitable for the WSD task; Section 3 gives more detailed considerations on this issue.To solve the problems outlined above, we propose a model that combines lexical chaining and WSD algorithms and show a possible way to estimating parameters of the presented model. In summary, the main contributions of this paper are the following:• We observe that information in natural language text constitutes multiple interleaved chains, and propose an expanded HMM to formalize this observation.• Based on the proposed model, we present a WSD algorithm and describe a method for estimating parameters of the model using statistical and link information from Wikipedia.Experiments show that the proposed WSD algorithm produces systematically better results than the state-of-the-art knowledge-based WSD algorithms, thus verifying that the proposed model is appropriate for locating concepts implied in natural language texts. The rest of the paper is organized as follows. Related work is discussed in Section 2. The proposed approach to word sense disambiguation is motivated by examples in Section 3. The model and the algorithm for the proposed approach are formally described in Sections 4 and 5. Estimation of parameters in order to apply model to WSD is given in Section 6. Results of experimental evaluation are presented in Section 7. We outline future work and conclude in Section 8. The paper proposes a method for Word Sense Disambiguation based on an expanded Hidden Markov Model. The method is based on our observation that natural language text typically traces multiple interleaved chains consisting of semantically related terms. The observation confirms that the classical HMM is too restricted for the WSD task. We thus propose the expansion of HMM to support multiple interleaved chains. The paper presents an algorithm for computing the most probable sequence of meanings for terms in text and proposes a technique for estimating parameters of the model with the aid of structure and content of Wikipedia. Experiments indicate that the presented method produces systematically better WSD results than the existing state-of-the-art knowledge-based WSD methods.
Passage Retrieval Using Answer Type Profiles in Question Answering Question Answering (QA) aims at finding exact answers to natural language questions in a large collection of documents (such as World Wide Web). Compared to a standard document retrieval framework, which just returns relevant documents to a query, a QA system has to respond with an adequate answer to a natural language question. Typically, a QA system has the following four components: 1) Question Analysis, 2) Document Retrieval, 3) Passage Retrieval, and 4) Answer Extraction. The question analysis component analyzes the question to determine its answer type, and to produce a list of keywords. Using these keywords document retrieval searches for a set of potentially relevant documents from the collection. From these documents, passage retrieval selects passages that are likely to contain the answer. Finally, answer extraction searches these passages for the final answer.Passage Retrieval is considered as one of the key components in a QA system. It reduces the search space for finding the answer from a massive collection of documents to a fixed number of passages (say top 20). Questions which do not have answers in the set of passages considered for answer extraction, cannot be answered correctly by any QA system. So, high performance of passage retrieval is desired to improve the success rate of a QA system. Most often passage retrieval suffers from terminological gap i.e., passages holding the answer to a question have semantic alterations of original terms in the question. Moldovan et al. (2003) showed that their system failed to answer 25.7% of questions solely because of terminological gap. This problem is normally addressed by the use of query expansion techniques. These techniques can be broadly classified into two categories; explicit query expansion and implicit query expansion techniques.In explicit query expansion, new terms are added to the original query to bridge terminological gap between the question and answer containing passages. Different methodologies have been Copyright 2009 by Surya Ganesh Veeravalli and Vasudeva Varma proposed to expand queries by utilizing top N ranked passages (pseudo-relevance feedback) (Gong et al., 2005) or utilizing external knowledge sources like WordNet, Encyclopedias or Web ( Yang et al., 2003). In implicit query expansion, the original query remains unchanged but during the process of retrieval semantic variants of original query terms like their stems (Bilotti et al., 2004) or morphological root forms are considered.The Statistical Machine Translation (SMT) framework which expands the query implicitly, has been used in several areas of information retrieval (IR). This model was first proposed by Berger and Lafferty (1999) for monolingual document retrieval. In this paper, we describe a passage retrieval methodology leveraging this framework. Our approach includes two phases: one is offline phase and the other is an on-line phase. The off-line phase constructs answer type profiles (ATPs) from question-answer sentence pairs parallel corpus using a statistical alignment model. Construction of ATPs includes: semantic categorization of questions based on their answer types, and building a distinct translation model for each category (answer type) of questions using a statistical alignment algorithm. These translation models are termed as ATPs. The on-line phase uses ATPs within the SMT framework to retrieve a ranked set of relevant passages given a question.The rest of this paper is organized as follows: Section 2 describes the related work; Section 3 describes the statistical machine translation model for information retrieval; Section 4 describes our passage retrieval methodology; Section 5 describes the experiments conducted and their results; Section 6 discusses the observations made in experiments and Section 7 concludes the paper. Retrieving answer containing passages is a challenging task in Question Answering. In this paper, we describe a novel passage retrieval methodology using answer type profiles. Our methodology includes two steps: estimation and ranking. In the estimation step, answer type profiles are constructed from question-answer sentence pairs parallel corpus using a statistical alignment model. Each answer type profile consists of triples: the query word, the answering sentence word and the probability of translation. In the ranking step, answer type profiles are incorporated into the Language Modeling framework called Statistical Machine Translation models for Information Retrieval. Using this framework a set of relevant passages are retrieved, given a question. We conducted experiments on FACTOID questions from TREC 2002 to 2006 QA tracks. The experimental results showed significant improvements over different retrieval models including TFIDF, Okapi BM25, Indri and KL-divergence.
A Continuum-Based Approach for Tightness Analysis of Chinese Semantic Units Many people are working on acquisition of multi-gram semantic units, although the terminology varies. "Gram" here means "sociological word," which is the familiar "word" in English, and the "character" in Chinese (Packard, 2000). Whether the goal is collocation extraction (Lin, 1998), multiword expression extraction ( Sag et al., 2002), or Chinese word extraction (Feng et al., 2004;Xu and Lu, 2006), they all try to extract multi-gram semantic units for which the meaning as a whole can not be predicted from the meaning of the gram units, and called "limited compositional" or "non-compositional." Multi-gram extraction identifies strings like "kick the bucket," "at gunpoint," or "make out" in English, and strings like "" (peanut), "" (match maker), or "" (Urumchi, name of a city) in Chinese.Multi-gram extraction is important for many natural language processing (NLP) applications. For example, it can be used for lexicon acquisition from corpora, extracting new words like " " (metaphor for cheating), which appeared in the Internet after 2008; it can be used for a word-based indexing of an information retrieval (IR) system; furthermore, it can be beneficial for word-based machine translation (MT). The impact of Chinese word extraction (or segmentation) on the last two tasks has been intensively analyzed ( Nie et al., 2000;Foo and Li, 2004;Peng et al., 2002;Chang et al., 2008). The results suggest the relationship is not monotonic, better segmentation does not always yield better MT or IR performance.Our hypothesis for this phenomenon is that independent Chinese semantic units (also referred to as "Chinese strings" in the following) as observed in a text do not fall cleanly into the binary classes of compositional or non-compositional, but into a continuum of tightness and looseness, where tightness is considered as a degree of compositionality. Intuitively, this continuum also exits Copyright 2009 by Ying Xu, Christoph Ringlstetter, and Randy Goebel in naturally segmented languages such as English (Halpern, 2000). This tightness characteristic of strings determines their linguistic nature as well as their preferred treatment in different NLP applications, e.g., for two consecutive nouns, whether to index two nouns or one nominal compound in IR, or to translate them as a unit or separately. For different NLP applications, the threshold for how tight a Chinese string need to be so that we keep it as a word will be different, but binary classification of semantic units is not enough.On this tightness continuum, at one extreme are non-compositional semantic units, such as idioms, non-compositional compounds, and transliterated names; at the other end are purely consecutive words which means there is no dependency relation between those words, with compositional compounds and phrases in between. Figure 1 shows some examples of English and Chinese multi-gram semantic units along this tightness continuum, where the left end is tightest and the right end is loosest. For English, "going Dutch" is a non-compositional idiomatic expression as its meaning has nothing to do with combination of the literal meanings of "going" and "Dutch"; the same holds for "milky way," a non-compositional compound; "machine learning" is a compositional compound but a tight one as compared to "plum pie" which is significantly looser; "last year" is a common sense phrase with "last" as a modifier of "year"; "CPU can't" is a phrase in a text with an arbitrary nominal CPU preceding the very general modal "can." For Chinese, " " (match maker) is a non-compositional idiomatic expression since its meaning has nothing to do with combination of the literal meaning of "" (under the moon) and "" (old people); "" (Urumchi) is a non-compositional transliterated proper noun; "" (machine learning) is a compositional compound; "" (legitimate income) is a phrase; and " " (Shanghai where) are two consecutive words. In our work, we exploit corpus data and propose a method to locate a Chinese character string in the continuum of tightness and looseness. The input of our approach are document frequencies of segmentation patterns for strings in corpora, i.e. number of documents that contain a specific segmentation pattern. A pattern is a potential segmentation, which here means that a character string of length n has 2 n−1 different patterns. For example, "||", "|||" and "| ||" are possible segmentation candidates for "" (machine learning). Note that every pattern contains all the characters of the string. The intuition of using document frequency is that a document that contains all the units of a string provides a stronger basis for the semantics of that string than a document that, for example, contains only one unit.We confirm the value of our approach with two experiments. First, we use our method to rank 300 Chinese strings according to their tightness and compare that result with a manually created gold standard ranking. The evaluation shows the automatic ranking is comparable to the manual ranking. Second, we extract non-compositional semantic units from the Chinese Gigaword corpus and compare the result with a dictionary. The precision is promising, which further supports the value of our tightness continuum measure.Our paper is organized as follows. Section 2 introduces the related work on tightness measures and Chinese word extraction. Section 3 presents our approach to measure the tightness of strings built from consecutive Chinese characters. In Section 4, we present the evaluation procedure and results. A brief conclusion summarizes our findings and anticipates future work. Chinese semantic units fall into a continuum of connection tightness, ranging from very tight, non-compositional expressions, tight compositional words, phrases, and then to loose more or less arbitrary combinations of words. We propose an approach to measure tightness connection within this continuum, based on document frequency of segmenta-tion patterns in a reference corpus. A variety of corpora, including search engine snippets, search engine results derived from query logs, as well as standard corpora have been investigated. Our tightness ranking on 300 phrases is quite close to their manual ranking, and non-compositional compound extraction can achieve a precision as high as 94.3% on the top 1,000 4-grams extracted from the Chinese Gigaword corpus.
A Bootstrapping Method for Finer-Grained Opinion Mining Using Graph Model * * * * In recent years, there has been a wealth of opinion source in the web. These opinions have attracted some special groups of people. For example, the government and politicians are interested in the response to governmental policies, merchants and potential customers are interested in the feedback of and comment on commercial products. Therefore, how to analyze and monitor the tide of prevalent attitudes on the web, how to extricate people from wading through a large number of opinions to find their interest, have received considerable attention in research community.A series of topic symposiums and evaluation sessions on opinion mining have appeared in TREC and NTCIR. For the opinion mining on document and sentence level, the task is to classify either positively or negatively in a review. However, the sentiment orientation of a review is not sufficient for many applications. Opinion mining begins to focus on the finergrained features level mining. The task is to find not only the sentiment orientation but also the commented features. This information could be used to deeply analyze prevalent attitudes or generate various types of opinion summaries.In feature level opinion mining, most researches are oriented on product reviews. The task is typically divided into three main subtasks: (i) identifying product features, (ii) identifying opinions regarding the product features, and (iii) determining the sentiment orientation of the opinions. In this paper, we focus on the first two steps to find the product features and opinion words in product reviews. Different from the previous work, we unify these two separate tasks into one process. A bootstrapping method is proposed to iteratively find both of them. A graph is built by linking pairs of product features and opinion words. The short length reviews are separately analyzed by rule-based method to find the initial seeds used in the bootstrapping.The remainder of the paper is organized as follows: section 2 describes the related work on feature level opinion analysis. Section 3 gives our approach of bootstrapping. Section 4 presents the graph model. Section 5 describes the techniques of initial seeds selection and candidates extraction. Section 6 gives the experiments and results. Finally section 7 summarizes this paper. Pursuing on the analysis of product reviews, a bootstrapping method is proposed to find the product features and opinion words in iterative steps. Different from conventional methods, a graph model is built to link and measure the relationship between the pairs of product features and opinion words. A rule-based method is presented to get the initial seeds of product features and opinion words automatically. Our experimental results on electronic product reviews are encouraging, which prove the proposed method and techniques are effective in performing this task of feature level opinion mining.
SESS: A Self-Supervised and Syntax-Based Method for Sentiment Classification* The task of sentiment classification is: given an opinionated piece of text, classify the opinion as falling under one of two opposing sentiment polarities (positive or negative) (Pang and Lee, 2008). The "piece of text" can refer to either a sentence or a document. In this paper, it refers to document, e.g., a movie review or a product review.Generally, there are two types of approaches tackling the sentiment classification task: supervised (Dave et al., 2003;Yu and Hatzivassiloglou, 2003;Aue and Gamon, 2005;Read, 2005) and unsupervised (Pang, 2002;Turney, 2002;Gamon and Aue, 2005;Zagibalov and Carroll, 2008a). Supervised approaches usually employ machine learning methods to train a model based on some human-labeled data, and then apply the acquired model on the new data. On the contrary, unsupervised approaches usually employ a list of sentiment words, e.g., a sentiment dictionary or some seed words, to help decide the sentiment polarity of documents. Supervised approaches generally achieve better performance than unsupervised ones, because methods such as SVM or Naïve Bayes have been deeply studied in machine learning area, and the human-labeled data reveal a lot of clues about human classification. However, as a doubleedged sword, human-labeled data also bring the disadvantage of domain-dependence. Although some researches have been done on domain adaptation (Aue and Blitzer et al., 2007), the problem is far from resolved.In this paper, a self-supervised method is proposed to share both the power of machine learning methods and the domain-independence property. The method is referred to as SESS (SElf-Supervised and Syntax-Based method). SESS takes three steps. Firstly, an unsupervised method is used on the data to label some documents (i.e., decide their sentiment polarities). Secondly, a machine learning method applies on these labeled documents to train a model. Thirdly, the model is used to label all the documents (notice that a document may change its label acquired in the first step to another one after the third step). SESS makes use of machine learning methods without need of any human-labeled data.To ensure that the machine learning method can achieve good performance in the third step, the unsupervised method must provide accurately labeled documents in the first step. To satisfy that requirement, SESS makes two special designs in the first step. First, an iterative procedure is used to decide the polarities of documents and words. A general method may use a sentiment dictionary to decide the document polarity. However, the words in the dictionary may not be comprehensive and the sentiment of those words may not fit for current data set. The iterative method can find new sentiment words that are not in the dictionary and revises the polarity of words according to current data set. Second, the polarities of documents and words are revised by analyzing the relation of clauses of compound and complex sentences in documents. Particularly, seven types of compound and complex sentences are analyzed, while three of them, i.e., coordination (discourse markers such as and or in addition), concession (discourse markers such as but or however) and condition (discourse markers such as if) sentences, take effect on sentiment of clauses. The detailed effects of these sentences are examined.The experiments show that SESS achieves an overall F 1 -score of 81.7% on data sets of four domains, which is comparative to 83.3%, the best result of the supervised approach in previous studies (Li and Zong, 2008) on the same data set.The rest of this paper is organized as follows. Section 2 surveys related work. The overview of our approach is presented in Section 3. Section 4, 5 and 6 describe the details of the SESS model. Experiments are shown in Section 7. The final section gives conclusions and proposes future work. This paper presents a method for sentiment classification, called SESS (SElf-Supervised and Syntax-Based method). SESS includes three phases. Firstly, some documents are initially classified based on a sentiment dictionary, and then the sentiments of phrases and documents are iteratively revised. This phase provides some accurately labeled data for the second phase. Secondly, a machine learning model is trained with the labeled data. Thirdly, the acquired model applies on the whole data set to get the final classification result. Moreover, to improve the quality of labeled data, the affect of compound and complex sentences on clause sentiment is examined. For three types of compound and complex sentences, i.e., coordination, concession or condition sentence, the clause sentiment is revised accordingly. Experiments show that, as an unsupervised method, SESS achieves comparative performance to state-of-the-art supervised methods on the same data.
Summarizing Opinions in Blog Threads Recent years have brought about an important shift in the way objective and subjective information are regarded and impact society and its individuals. It is no longer only the factual content (of news), but rather what people feel about it, that influences decisions taken daily. Supported by the fast development of the Internet and the Web 2.0 technologies, with the predominant presence of social networks, forums, "blogging" and reviewing as world-wide phenomena, exchanging views and debating on real-life related issue has reached a global scale. People express and search for opinions on blogs, forums, in reviews and comments -leading to the creation of extensive quantities of data that cannot be manually processed, although their analysis (discovery of opinions, their classification into positive and negative), could be useful to a high diversity of entities (potential customers, companies, public figures and institutions etc.), for a large variety of tasks (opinion analysis for marketing, sociological or political studies, decision support etc.). Automatic systems are thus needed to help resolve the issue of large-scale data analysis. Example of such a system would be one that is analyzing subjective data expressed on the Web on a certain topic and presenting the potential users with short summaries of the main points of view expressed, depending on whether or not they are in favor or against the topic. This paper presents an approach to building such a system that is able to output summaries of the points of view expressed in blog threads. The rest of the paper is organized as follows: in section 2 we briefly discuss related work; in section 3 we present our approach to opinion summarization and give details of the corpus we used to develop and evaluate our system; next, we present our experimental results and discuss the main issues (sec. 4); and finally, we conclude the paper and give pointers to future work.Whilst there is abundant literature on text summarization (Kabadjov et al., 2009;Hovy, 2005;Erkan and Radev, 2004;Gong and Liu, 2002) and sentiment analysis (Balahur et al., 2009a;Pang and Lee, 2008;Riloff et al., 2005), there is still limited work at the intersection of these two areas (Stoyanov and Cardie, 2006).Initial research in opinion mining concentrated on news texts. Wiebe (1994) defines subjectivity based on Quirks idea of "private states" (states that are not open to verification) and distinguishes between objectivity and subjectivity on this criteria. Consequently, based on this definition, the Multi-Perspective Question Answering (MPQA) annotation schema and corpus were created over news texts, distinguishing between the subjective/objective speech, as well as the polarity of text spans ). Subsequently, different authors show that this initial discrimination is crucial for the sentiment task, improving results obtained when using only polarity classification for sentence-level opinion mining ( Pang and Lee, 2004), as part of Opinion Information Retrieval (last three editions of the TREC Blog tracks, the TAC 2008 competition), Information Extraction ( Riloff et al., 2005) and Question Answering (QA) (Stoyanov et al., 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. In this paper we present an approach to summarizing positive and negative opinions in blog threads. We first run a sentiment analysis system and consequently pass its output through a standard LSA-based text summarization system. Further on, we evaluate our approach and present the results obtained, which we believe are promising in the context of multi-document text summarization. Finally, we discuss the main issues in applying standard text summarization techniques to the slightly different task of summarizing opinions in blog threads.
Constraint Based Hybrid Approach to Parsing Indian Languages Due to the availability of annotated corpora for various languages since the past decade, data driven parsing has proved to be immensely successful. Unlike English, however, most of the parsers for morphologically rich free word order (MoR-FWO) languages (such as Czech, Turkish, Hindi, etc.) have adopted the dependency grammatical framework. It is well known that for MoR-FWO languages, dependency framework provides ease of linguistic analysis and is much better suited to account for their various structures (Shieber, 1975;Mel'Cuk, 1988;Bharati et al., 1995). The state of the art parsing accuracy for many MoR-FWO languages is still low compared to that of English. Parsing experiments ( Nivre et al., 2007;Hall et al., 2007) for these languages have pointed towards various reasons for this low performance. For Hindi 1 , (a) difficulty in extracting relevant linguistic cues, (b) non-projectivity, (c) lack of explicit cues, (d) long distance dependencies, (e) complex linguistic phenomena, and (f) less corpus size, have been suggested ( Bharati et al., 2008) for low performance. The approach proposed in this paper shows how one can minimize these adverse effects and argues that a hybrid approach can prove to be a better option to parsing such languages. There have been, in the past, many attempts to parsing using constraint based approaches. Some of the constraint based parsers known in the literature are Karlsson et al. (1995), Maruyama (1990), Bharati et al. (1993Bharati et al. ( , 2002), Tapanainen and Järvinen (1998), Schröder (2002), and more recently, Debusmann et al. (2004). Some attempts at parsing Hindi using data driven approach have been ( Bharati et al., 2008b;Husain et al., 2009). Later in Section 4, we'll compare the results of data-driven Hindi parsing with that of our approach.The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of Copyright 2009 by Akshar Bharati, Samar Husain, Meher Vijay, Kalyan Deepak, Dipti Misra Sharma, and Rajeev Sangal specific dependency relations at two different stages. Furthermore, we show how the use of hard constraints (H-constraints) and soft constraints (S-constraints) helps us build an efficient and robust hybrid parser. Specifically, H-constraints incorporate the knowledge base of the language and S-constraints are used as weights that are automatically learnt from an annotated treebank. Finally, we evaluate the implemented parser on Hindi and compare the results with that of two data driven dependency parsers.The paper is arranged as follows: Section 2 describes in detail the proposed approach for parsing free word order languages. Section 3 discusses the types of constraints used. We evaluate the parser and compare the results with that of two data-driven parsers in Section 4. Section 5 gives certain observations on the approach. We conclude the paper in Section 7. The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of specific dependency relations at the two stages. Furthermore, we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser. Finally, we evaluate the implemented parser on Hindi and compare the results with that of two data driven dependency parsers.
Building Online Corpora of Philippine Languages* With 168 languages spoken natively in the archipelago (Gordon 2005), Philippine linguistics has persistently become a fertile ground for investigation. Foreign linguists exhibit a remarkable interest on the richness of Philippine languages. The studies of Constantino (1971), McKaughan (1971), Reid (1981), Quakenbush (2005), and Liao (2006) have outlined the development of Philippine linguistics in the last 25 years. With all the researches that have been done, Liao (2006) underscores the pressing need to document major and minor languages in the Philippines. And since the vast majority of studies in Philippine languages are done by non-Filipinos, Liao emphasizes the demand for Filipino linguists to be involved in the documentation of Philippine languages. Additionally, she highlights that in the last 25 years, there had been 14 M.A. theses and 16 Ph.D. dissertations written about Philippine-type languages, but only one of them was written by a Filipino (i.e., Daguman 2004).Researchers from all parts of the world have continually visited the Philippines to gather data for their studies. Others, however, have to resort to other means to obtain the needed data. For instance, Davis, Baker, Spitz and Baek (1998) came up with a grammar of Yogad (spoken in northern Luzon) in which the basis was just a native speaker of the language who now resides in Texas. Some would use published texts as corpus for their study (cf. Liao 2004). Others (cf. Ruffolo 2005; Rubino 1997; among others) would have a trip or two only to the country for data gathering due to travel nuisances. Thus, the accessibility of databank on Philippine languages is a major concern for those interested to further explore issues concerning Philippine linguistics.The online corpora of Philippine languages project poses several benefits. It is especially of great importance to those interested in Philippine languages, both locally and internationally. For the local researchers, the availability of Philippine data will enthuse linguists to delve into their own languages. For foreign researchers, the databank will expedite their studies on the one hand, and will pave way for other areas of studies, on the other. With this project, those interested with Philippine languages need not come personally to the Philippines to gather necessary data. Hence, easy access to Philippine data, as Dita (2007) conjectures, will pave the way to more researches on various fields such as syntax, semantics, pragmatics, sociolinguistics, and others. The online data on Philippine languages is then a valuable source to any linguist, or researcher, for that matter. As Meyer (2002:11) puts it, "linguists of all persuasions have discovered that corpora can be very useful resources for pursuing various linguistic agendas."Thus, a language corpus is an indispensable component in researches on various aspects of the study of the nature and functions of natural language, and its multi-faceted applications such as language education, lexicography, and natural language processing. There have been various applications being developed across the country which are an English-Filipino machine translation system (which includes part-of-speech taggers, morphological analyzers and generators, English-Filipino lexicon for translation), and spell checkers for Tagalog, to name a few.A corpus is a collection of documents in a particular language or languages that are to be stored, managed and analyzed in digital form. Francis (1982:7) defines corpus as "a collection of texts assumed to be representative of a given language, dialect, or other subset of language, to be used for linguistic analysis." In addition, Engwall (1992:167) gives another definition of corpus: "a closed set of texts in machine-readable form established for general or specific purposes by previously defined criteria." It is indispensable in researches on various aspects of the study of the nature and function of natural language, and its multi-faceted applications such as language education, lexicography, and natural language processing. In the last three decades or so, investigations of language use have focused on corpus-base approaches as these "provide a means of handling large amounts of language and keeping track of many contextual factors at the same time" (Biber, Conrad, &amp; Reppen 1998).The International Corpus of English (ICE) which started in mid-1990s has been one of the seminal works of corpus linguistics. With almost 20 countries participating in this project, the ICE has provided a comparative study of the different Englishes of the world. Greenbaum (1996) provides a comprehensive discussion of the common design of ICE corpora in which all countries should prescribe to. Although the majority of the member countries have completed the project, some are still working for the completion of these corpora. This fact clearly illustrates that corpus building is no mean feat.The availability and accessibility of the Philippine component of the International Corpus of English (ICE-PHI) paved the way to various researches by linguists worldwide. The countless possibilities of corpus linguistics and the detailed description of the creation and completion of the ICE-PHI, as provided by Bautista (2004), have been the impetus for the online corpora of Philippine languages. Corpora have been developed since then on other languages, both major and minor languages such as Spanish, German, Chinese, Japanese, Malay and Thai. The Malay corpus which is focused on the study of classical Malay literature features more than 4 million words and 95 texts, including 80,000 verses. The LOTUS (Large vOcabulary Thai continuous Speech Corpus) Thai corpus has 5,000 vocabularies for speech recognition. Another Thai corpus has 44,000 images of handwritten characters.With the existing infrastructure as provided by the Internet that virtually connects people from various physical locations, contributing to development of such collection of documents is now a reality. This paper aims at describing the building of the online corpora on Philippine languages as part of the online repository system called Palito. There are five components of the corpora: the top four major Philippine languages which are Tagalog, Cebuano, Ilocano and Hiligaynon and the Filipino Sign Language (FSL). The four languages are composed of 250,000-word written texts each, whereas the FSL is composed of seven thousand signs in video format. Categories of the written texts include creative writing (such as novels and stories) and religious texts (such as the Bible). Automated tools are provided for language analysis such as word count, collocates, and others. This is part of a bigger corpora building project for Philippine languages that would consider text, speech and video forms, and the corresponding development of automated tools for language analysis of these various forms.
Machine Translation Using Automatically Inferred Construction-Based Correspondence and Language Models Machine translation (MT) is the one application domain of computational linguistics where it would seem to be impossible to achieve any progress without giving some thought to semantics. For instance, the claim that the Spanish un perro mordí o a un hombre should be translated into English as a dog bit a man is merely a way of stating that these two utterances mean the same thing. The meaning of "meaning" is, however, notoriously elusive (Putnam, 1975;Pietroski, 2003), and so practical approaches to MT wisely skirt theoretical semantics even as they embrace various ways of defining meaning in computational terms.In computational linguistics, including MT, both context and use serve to operationalize meaning. For instance, a set of contexts inherent in a corpus can be distilled into a generative language model, while information about the use of corresponding phrases that is implicit in a parallel corpus can assist in the translation of new source-language utterances into the target language. In the present paper, we capitalize on recent advances in structural language model inference and on some cognitively motivated heuristics concerning cross-language construction mapping to derive a simple yet viable MT method. We discuss the problem of translation in the wider context of the problem of meaning in cognition and describe a structural statistical machine translation (MT) method motivated by philosophical, cognitive, and computational considerations. Our approach relies on a recently published algorithm capable of learning from a raw corpus a limited yet effective grammar that can be used to construct probabilistic parsers and language models, and on cognitively motivated heuristics for learning construction-based translation models. A pilot system has been implemented and tested successfully on simple English to Hebrew and Spanish to English translation tasks.
Latin Etymologies as Features on BNC Text Categorization * * * * Text Categorization (TC) is the task of classifying natural language texts into a predefined set of semantic categories (Lan et al., 2006). Features selection is always a bottleneck in the tasks of TC, especially the common used BoW introduces a large features space, some of the features are redundant, some of them bring noise. The current TC studies are based on features like words/phrases frequencies (Olsson and Douglas, 2006), therefore, need (1) features selection algorithms such as Information Gain ( Wang et al., 2004;Lee and Lee, 2006;Olsson and Douglas, 2006;Shang et al., 2007), Mutual Information ( Wang et al., 2004;Pei et al., 2007), χ 2 ( Wang et al., 2004;Olsson and Douglas, 2006;Shang et al., 2007), Maximum Entropy (Nigam et al., 1999;B. Chen et al., 2008) etc. A good review on the state-of-art feature selection techniques can be found in (Liu and Yu, 2005). However, as stated by (Mukras et al., 2007;Shang et al., 2007), these routine feature selection methods may fail to identify discriminatory features, particularly when they are distributed over multiple ordinal classes or especially like χ 2 ( Olsson and Douglas, 2006) are known to be misled by infrequent terms; (2) features transformation technique like Term clustering ( Lin and Kondadadi, 2001;Beil et al., 2002), Latent Semantic Indexing (LSI) ( Wu and Gunopulos, 2002;Kontostathis and Pottenger, 2006) so that the texts can be represented in features vectors; (3) because of this kind of representation of document, usually need to employ computationally expensive learning algorithms from machine learning like Naïve Bayes Classification ( Wang et al., 2004;J. Chen et al., 2008), Support Vector Machine Classification ( Wang et al., 2004;Lan et al., 2006;Shang et al., 2007), linear classification (T. Zhang and Oles, 2001; J. Zhang and Y. Yang, 2003), KNN (Lan et al., 2006;Olsson and Douglas, 2006;Shang et al., 2007), Neural Network ( Yu et al., 2008) etc. A thorough survey can be found in (Sebastiani, 2002). As stated by (D. Zhang and W. S. Lee, 2006), the learning algorithms even the verified most de facto SVM algorithm can be neither effective nor efficient to take all selected features straightforwardly. This study wishes to explore and verify discriminative features beyond words/phrases frequencies based on linguistic analysis and have not been reached yet up to now and limit the efficient features set as small as it can be. As stated by (Rogati and Yang, 2002), "The results we obtained using only 3% of the available features are among the best reported, including results obtained with the full feature set". In addition, this study provides a base step for the future work in which we do not want to deem classified texts as simple as a feature vectors of "bag-of-words", but as different levels of linguistic information, such as the investigated one in this study, which is the probabilities of the words having Latin-etymologies in the classified texts.The rest of this paper is organized as follows. Section 2 describes the approach proposed. Section 3 evaluates the proposed method. And Section 4 opens a discussion and presents the outline of the future works. This paper presents an early experimental work on BNC Text Categorization (TC) with Latin etymologies as features, emphasis on spoken and written texts. Two aims achieved in this study: (1) to explore discriminative new linguistic features rather than lots of noise-bringing &quot;bag-of-words&quot; (BoW). (2) to build up a base step to represent texts in distinct types of linguistic features with different weighting scheme rather than a plain feature vectors of BoW. The experiments disclose a notable distinct distribution pattern of Latin etymologies in spoken and written BNC texts. The performance of a home-made classifier based on the probability distribution ranges of Latin etymologies reaches a precision of 72.31% and recall of 73.22% on BNC spoken texts and precision of 73.31% and recall of 69.98% on BNC written texts.
Experiments on Domain Adaptation for English-Hindi SMT * In general, SMT models are trained on large corpora which may include quite heterogeneous topics. These topics usually define a set of terminological lexicons. Terminologies need to be translated taking into account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains.Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses ( ) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data's match with respect to domain, topic, and style ). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008).In the present work, we conduct experiments on the English-Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings.For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data.We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain training data. We apply adaptation techniques to combine sub-corpora with indomain small-scale training data into a unified framework.The remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 describes experimental results using our baseline SMT model. In section 4 we describe the domain adaptation techniques which are employed to combine multiple models. Section 5 presents the results obtained, together with some analysis. Section 6 concludes, and provides avenues for further work. Statistical Machine Translation (SMT) systems are usually trained on large amounts of bilingual text and monolingual target language text. If a significant amount of out-of-domain data is added to the training data, the quality of translation can drop. On the other hand, training an SMT system on a small amount of training material for given in-domain data leads to narrow lexical coverage which again results in a low translation quality. In this paper, (i) we explore domain-adaptation techniques to combine large out-of-domain training data with small-scale in-domain training data for English-Hindi statistical machine translation and (ii) we cluster large out-of-domain training data to extract sentences similar to in-domain sentences and apply adaptation techniques to combine clustered sub-corpora with in-domain training data into a unified framework, achieving a 0.44 absolute corresponding to a 4.03% relative improvement in terms of BLEU over the baseline.
Extraction of English Ditransitive Constructions Using Formal Concept Analysis In recent linguistic theories based on the symbolic thesis of language, constructions are thought of as fundamental units that cover virtually every level of form and meaning in language, and their importance is greatly emphasized in various areas of syntactic and semantic study (Goldberg, 1995;¨ Ostman and Fried, 2008;Fillmore, 2009). There is, however, some obscurity in exactly what meaning is paired with what form of language and, accordingly, what it means when linguists say a construction, such as the ditransitive construction, has various sub-constructions derived from a prototype (cf. Goldberg, 1995). Their argument appears valid, but there is no way to formally substantiate the theoretical entity referred to as a construction. Seemingly, it is this lack of a formal definition that impedes the practical application of construction grammar in natural language processing and related fields.This paper seeks to treat constructions as entities that can be computationally processed, attempting to prove the validity and usefulness of the proposed method by applying it to actual linguistic data. Our method makes it possible to extract the network of constructions from semantically tagged linguistic data without relying on probabilistic analysis. Instead, it utilizes the mathematical technique of formal concept analysis (FCA), originally devised to substantiate con-cepts, by regarding them as being composed of the extension and the intension. 1 An extension is defined as the set of objects, and an intension as the set of their attributes. It is possible to apply this to language analysis, taking sub-patterns of a construction as objects and the semantic frames represented by actual instances of that pattern as attributes. The network of constructions thus extracted may be suitable for both the verification of previous theoretical works and further computational processing and application. This paper proposes a method to extract constructions in a formal and mathematically rigid way using the technique of formal concept analysis (FCA). Looking at lemmas of core components of constructions as objects and semantic frames of the construction instances as attributes, in terms of the FCA, a complete lattice that represents the network structure of constructions can be obtained. We conducted the preliminary experiment of extracting the network of sub-patterns of the English ditransitive construction using a relatively small-sized corpus that was semantically tagged. The result displays the potential capability of this method, both for verifying and substantiating previous theoretical works on construction grammar and for enabling the application of such works to more practical enterprises in the field of natural language processing.
Towards Conceptual Indexing of the Blogosphere through Wikipedia Topic Hierarchy Weblogs or blogs are considered to be one of personal journals, market or product commentaries. While traditional search engines continue to discover and index blogs, the blogosphere has produced custom blog search and analysis engines, systems that employ specialized information retrieval techniques. With respect to blog analysis services on the Internet, there are several commercial and non-commercial services such as Technorati, BlogPulse (Glance et al., 2004), and kizasi.jp (in Japanese). With respect to multilingual blog services, Globe of Blogs provides a retrieval function of blog articles across languages. Best Blogs in Asia Directory also provides a retrieval function for Asian language blogs. Blogwise also analyzes multilingual blog articles.In terms of conceptual indexing of the blogosphere, existing services for blog retrieval can be roughly divided into two types. The first type is that of keyword based blog search function of search engines such as Yahoo! blog search (in Japanese) and Google blog search (in Japanese). In this type of indexing, not only keywords, but also subjective expressions as well as time series changes are used for indexing. This type of indexing is too fine-grained compared to actual needs for indexing the blogosphere. Since the number of indices is extremely huge, it is definitely impossible for users to grasp the whole structure of the index hierarchy. Therefore, unless each user comes up with queries appropriate for their search needs in the blogosphere, it is difficult for them to easily access the blogosphere with certain information needs. The second type is that of manually indexing the blogosphere through a directory of manually created categories such as Technorati. This type of indexing is, on the other hand, too coarse-grained compared to actual needs for indexing the blogosphere, and such indexing lacks coverage in the whole blogosphere. It is also quite difficult to manually update such a directory of categories when blog feeds of new topics are created in the blogosphere.Based on this observation, this paper takes an approach of conceptually indexing the blogosphere through the whole hierarchy of Wikipedia entries. In our approach, we regard Wikipedia Copyright 2009 by Mariko Kawaba, Daisuke Yokomoto, Hiroyuki Nakasaki, Takehito Utsuro, and Tomohiro Fukuhara as a large scale ontological knowledge base for conceptually indexing the blogosphere. We regard Wikipedia also as a large scale encyclopedic knowledge base which includes well known facts and relatively neutral opinions. In its Japanese version, about 623,000 entries are included (checked in October, 2009). For the purpose of conceptually indexing the blogosphere, Wikipedia has an advantage over any other ontological knowledge resource. Although many blog feeds with new topics keep being created rapidly, in Wikipedia, new entries for describing those new topics are also rapidly created, and existing entries also keeps being updated rapidly.More specifically, this paper proposes how to link Wikipedia entries to blog feeds in the Japanese blogosphere, where about 300,000 Wikipedia entries are used for representing a hierarchy of topics. Furthermore, based on the results of judging whether each blog feed is relevant to a given Wikipedia entry, this paper also examines how to judge whether there exist blog feeds to be linked from the given entry. In the following sections, first, we examine correlation between the number of hits of a Wikipedia entry title and existence of blog feeds to be linked from the entry. We empirically examine the range of the number of hits and conclude that the entries with the range over 10,000 tend to have relevant blog feeds. Actually, according to our manual evaluation of this range, about 80% of Wikipedia entries have at least one relevant blog feed. Second, we apply SVMs to the task of judging whether a blog feed is relevant to a given Wikipedia entry. Based on the learned SVMs model, we automatically judge whether there exists at least one blog feed which is relevant to the given Wikipedia entry. In our experimental evaluation, we achieved over 90% precision in this task. This paper studies the issue of conceptually indexing the blogosphere through the whole hierarchy of Wikipedia entries. About 300,000 Wikipedia entries are used for representing a hierarchy of topics. Based on the results of judging whether each blog feed is relevant to a given Wikipedia entry, this paper proposes how to judge whether there exist blog feeds to be linked from the given entry. In our experimental evaluation, we achieved over 90% precision in this task.
Syntactic Category Prediction for Improving Translation Quality in English-Korean Machine Translation * * * * Recent English-Korean machine translation systems generate good translation for the relatively short sentences. In the translation of long sentences, the translation results are bad, so the readers have difficulty in understanding the meaning of translated sentences. The difficulty in translating long sentences is syntactic one, while the problems in short sentence translation lie in the semantic area. That is, more accurate parsing helps improve the readability of the translation results for long sentences. Most long sentences consist of comma-separated phrases or clauses. The accurate and detailed analysis of the relationships among the comma-separated elements can improve the parsing accuracy, resulting in translation quality improvement. Of course, the semantic problems must be considered to improve the translation quality, but they are not scope of the paper.In (Kim, 2005;Kim et al., 2001), they proposed intra-sentence segmentation for speeding up the syntactic analysis of long sentences. In parsing using the segmentation, the input sentece is split into several shorter segments by commas and the above intra-sentence segmentation. The segments are parsed separately and the parsing results of segments are combined. After parsing each segment, a tree must be selected. So several selection decisions occur during parsing an input sentence. The wrong selection affects the translation result. As a result, the intra-sentence segmentation contributed to speeding up the parsing but may make little improvement of translation quality. Also, it is difficult to consider the long-distance dependencies amone segments, which can lead to additional translation errors.In order to improve translation quality by considering long-distance dependencies and selecting the correct parsing results for each segment, this paper proposes a syntactic category prediction of comma-separated segments. If we could know the syntactic category of a given A small TCL interpreter, which can be linked into the code, interprets the strings. Segment-1: A small TCL interpreter NP Segment-2: which can be linked into the code RLCL Segment-3: interprets the strings VP Translation-1:segment before parsing, we can guide parser in considering segment dependencies and selecting the correct parsing results. The prediction must be made before parsing using only information from lexical analysis. A sentence is split by commas, and then the long segments are again split. We try to predict the syntactic category of the comma-separated segments. This prevents predicting categories of the non-constituent phrases from the second segmentation step. In this paper, we construct rules and functions for the syntactic category prediction by the statistical and machine learning methods. We generate training data using the Penn Treebank corpus (Marcus et al., 1993). Section 2 describes the parsing method using sentence segmentation and syntactic category prediction. We explain the generation steps of rules and functions for the syntactic category prediction in section 3. Section 4 shows the comparison results of prediction accuracies and the degree of the translation quality improvement. Section 5 concludes the paper. This paper proposes the syntactic category prediction for improving translation quality. In parsing using sentence segmentation, the segments are separately parsed and then the parsing results of each segment are combined to generate a global sentence structure. The syntactic category prediction guides the parser to identify relationships among segments and to select the correct parsing results for each segment. We design features for predicting syntactic categories and generate decision trees for the prediction using training data from the Penn Treebank. In experiment, we show the prediction accuracy and comparison results with the prediction by human-built rules, heuristic probability function, and neural networks. Also, we present how much the category prediction contributes to improving translation quality.
Customizing an English-Korean Machine Translation System for Patent/Technical Documents Translation * * We often look for the foreign patents or technical documents for acquiring the current trends and new information. When we try to translate the foreign patents or documents in order to just acquire the information, we want to require the rapidity of the translation and the understandable translation quality, rather than the completeness of the translation quality. Such users' demand has become a hot research issue in the MT community.It is well known that sentence style and dominant translation for a word vary with domains. Therefore, if the domain to be translated is fixed to patents or technical documents, bilingual dictionary adaptation to the domain and customizing natural language analyzers to the linguistic specificity of the domain's style are effective ways to improve the translation quality of MT system. There have been studies concerned specifically with patent MT using these domainspecific advantages ( Shinmori et al., 2003;Hong et al., 2005).Though intensive research has been made on MT for the domain-specific advantages, there still remain many issues to be tackled. In this paper, we focus on the several issues: (1) domainspecific probabilities of POS tagger, (2) long and complex sentence analysis, and (3) target word selection.This paper addresses the customization of an E-K(English-Korean) MT system for patent and technical documents translation. The E-K patent MT system "FromTo-EK/PAT" and The E-K technical paper MT system "FromTo-EK/PAP" described in this paper is based on an E-K MT system developed for the web translation in a general domain. We first customized our general E-K MT system for patent translation, and then customized E-K patent MT system to technical document domain.Our E-K MT system belongs to basically the pattern-based methodology for machine translation. It has the formalism that does English sentence analysis in which English domainspecific patterns are used, matches the English domain-specific pattern with its Korean domainspecific pattern, and then generates a Korean sentence from it. E-K MT system consists of an English morphological analysis module based on lexicalized HMM, an English syntactic analysis module by pattern-based full parsing, a pattern-based transfer, and a Korean morphological generation. This paper addresses a method for customizing an English-Korean machine translation system from general domain to patent or technical document domain. The customizing method includes the followings: (1) adapting the probabilities of POS tagger trained from general domain to the specific domain, (2) syntactically analyzing long and complex sentences by recognizing coordinate structures, and (3) selecting a proper target word using domain-specific bilingual dictionary and collocation knowledge extracted from patent or technical document corpus. The translation accuracy of the customized English-Korean patent translation system is 82.43% on the average in 5 patent categories according to the evaluation of 7 professional patent translators. The translation accuracy of the customized English-Korean technical document translation system is 81.10% and its BLEU score is 0.5185 in the evaluation test set where the average BLEU score of cross-evaluation between references is 0.6615.
Extracting Keyphrases from Chinese News Articles Using TextRank and Query Log Knowledge Keyphrase extraction algorithms can be classified into supervised or unsupervised. When a large training corpus is available, a supervised learning algorithm is a possible solution. Statistical information, such as frequencies, positions and lengths of phrases, was proved to be important features to supervised algorithm. GenEx (Turney, 1999) used a hybrid genetic algorithm to evaluate the parameters of these features. These parameters were used to assign each phrase a score and the ones with higher scores were output. Frank et al. (1999) also developed a simple but efficient keyphrase extraction system based on naive Bayes training algorithm. Supervised training algorithms are limited to a specific field because of high dependency on the training corpus. Many works use academic papers as training datasets because authors are always required to provide keyphrases for their papers. But news articles, which are the most widely used application of keyphrase extraction, lack training corpora.For a great number of unlabelled news articles, an unsupervised algorithm is a good choice. Mihalcea et al. (2004) introduced a PageRank-based (Brin and Page, 1998) model called TextRank to extract keyphrases. But the TextRank does not make full use of the statistical information, such as the length and the position of the phrase. In this paper, we propose a method which incorporates various features into the TextRank which can improve the F-measure by 8.37%. Yang et al. (2008) also developed a PageRank-based algorithm which used average term frequency and proportional document frequency to extract Chinese keywords instead of keyphrases. A single word often fails to convey a certain meaning. For example, "7K/ˆÅ" (financial crisis) is a hot topic nowadays. However, neither "7K" (finance) nor "ˆÅ" (crisis) individually can express the meaning of the whole phrase "7KˆÅ" (financial crisis). The TextRank generates keyphrases by combining the adjacent keyword sequences after the PageRank iteration, while we use a more efficient method which generates phrases before the PageRank iteration. Furthermore, query logs are used to help determine the phrase boundary. Experimental results show that our phrase generation method improves the F-measure by 10.37%.We propose an efficient and practical algorithm to extract keyphrases from Chinese news articles. First, we apply several statistical criteria to generate phrases. Query logs are used to help determine the phrase boundary. Then, we propose a method which can integrate various features into the TextRank. Our test set covers various areas of news. All the news are manually assigned keyphrases by a third party. Experimental results show that our algorithm can improve the performance significantly.We introduce the framework of our algorithm in section 2. The phrase generation method is described in section 3. The features are introduced in section 4. The experimental results are shown in section 5. Finally, we give our conclusion and future work in section 6. Keyphrases extracted from articles are beneficial in helping people boost browsing speed, but unfortunately keyphrases are rarely available for news articles due to the high expense of labor and time for manual annotation. This paper proposes a practical approach to extracting keyphrases for Chinese news articles using the TextRank and query log knowledge. Previous work is word based, while our approach uses phrase as its basic element. We generate phrases by employing several statistical criteria with the huge amount of queries as a training corpus. We use TextRank, a graph-based learning algorithm, for extracting keyphrases from Chinese news articles. In addition, two instructive features, lengths and positions of phrases, are incorporated into the TextRank model. Experimental results demonstrate that our methods improve the performance significantly.
Modeling the Relationship among Linguistic Typological Features with Hierarchical Dirichlet Process Languages do not exhibit syntactic features, such as position of verbs, affixes, and nouns, at random. For instance, languages that have prepositions tend to have verbs precede nouns. Implicational universals, proposed by Greenberg (1966), model these phenomena with logical implications. These implications describe co-occurrence conditions between two features in the form "if a language has feature x, then it has feature y." For example, one of the universals regarding constituent order found by Greenberg states "If a language has VSO order feature, then it is prepositional."However, using logical implications to model the relationship among structural features has some problems. One problem is with the interpretation of implication: while implications take the form "feature x implies feature y", these are hypothesized implications induced from empirical observations, not logically deduced propositions. And thus using implications -which are empirically discovered under the implicational universals model -to explain the relationship among features is tautological (Cysouw, 2003). Additional knowledge is required to interpret them.One another problem is that implications are in nature asymmetric. But it is questionable that the relationship between features are uni-directional, as there do exist bi-directional implications, in which it's impossible to determine which feature of the two is more marked (Croft, 2002).Also there are implications that have exceptions -called universal tendencies (Rolf and Halvor, 2005). An example is "if a language has OV (object-verb) word order, then it has GenN (genitive-noun) word order," which has numerous counter-example languages. Both tendency and implication can be derived from a feature table (Croft, 2002). Even though many implications have been found, the existence of implications cannot explain why tendencies -which can be identified by the same method -also exist, with varying degrees of exception. This suggests Copyright 2009 by Chu-Cheng Lin, Yu-Chun Wang, and Richard Tzong-Han Tsai that implications is a special case where there are no exceptions, and a more general model is demanded.Last but not least, as Cysouw argues (Cysouw, 2003), implications found in a feature table may have little significance under statistical tests. On the other hand, meaningless feature relations, which cannot be appropriately modeled with implications, may be statistically significant. Cysouw further suggests that using statistical tests to examine the relationship between features is better than indicating the relationship with implications.As a result, we take a different approach here: we propose a plausible computational generative model: unlike the implications, under this model the structural features are generated by some highlevel concepts. The major work then is to infer the high-level concepts. And we can then judge the model's sanity by examining the inferred concepts.Topic models seem adequate for this purpose. They receives more and more attractions in document modeling (Steyvers and Griffiths, 2007), and are widely employed in various NLP applications. In general, a topic model first generates several topics of a corpus. Each topic is a distribution over all possible terms in the corpus. To generate a document, one first draws a weighted mixture of topics from some prior, say, Dirichlet distribution. With the common bag-of-words assumption, to generate each word in this document one first draws a topic from the previously drawn mixture; then one draws a word from this topic. Each word's topic assignment, each document's topic mixture, and each topic's distribution over terms can easily be inferred or estimated using Monte Carlo method, which we shall go into more details in Section 3.Under the assumption of independence between words, it is clear that terms that co-occur more frequently are more likely to belong to a same topic. Empirical results show that for natural language corpora, the inferred topics consist of semantically related terms ( Blei et al., 2003). In other words, semantically related words co-occur frequently, which largely adheres to the intuition.When applying the topic model to interpret the relations between language features, consider that (1) how implications are interpreted (2) what a topic represents. In the implication f x ⇒ f y , f x should be more significant than f y . Natually under a topic model we havewhere f x is a value of feature X and f y is a value of feature Y . For instance, the feature "word order" may have two values "VO" and "OV". The language cannot have multiple values of a same feature at the same time. m is the latent topic to which both f x and f y belong. We do not explicitly model the relation between two features; instead, each feature is an observation of the underlying topic. The less marked a feature is, the more probability it has in a topic. The asymmetric implications then correspond to the case where P( f y |m) P( f x |m). Therefore a continuum between uni-directional and bi-directional implications, and continuum between tendency -when there are mutually incompatible features in the same topic -and implication is allowed. Topic model also handles situations where multiple features are involved at the same time quite neatly. For example, the two object-verb orders OV and VO, which belong to the well-known word order types, often co-occur with several features. The OV feature often co-occurs with features such as postpositions, GenN order, SV order, sentence-final question particles, and so on. On the other hand, the VO order co-occurs with prepositions, NGen order, VS order, and sentence-initial question particles. However, it might have some same features co-occuring with the two features such as the two features OV and VO. For example, NAdj order was thought to co-occur with the VO word order. But later studies argue that the ordering of adjective and noun may not be related to verb-object ordering (Dryer, 1988), but to other features. With the introduction of latent topics, we can deal with the relationship of the features which co-occur with NAdj and AdjN. The detailed discussion is in Section 5.1.To our knowledge, there is no prior work which takes this view in the literature of linguistic typology. This is understandable since, until the advent of a computational approach (Daumé III and Campbell, 2007), implicational universals have been carried out by painstaking manual analysis; and it is hard to derive the underlying topics with a plain two-dimension feature table.We propose that a widely-used topic model, Hierarchical Dirichlet Process (HDP), can be used to model the latent topics, which represent relationship among typological features. We evaluate HDP on the WALS dataset ( Haspelmath et al., 2005). Then we discuss about implications of the topics we have discovered. We propose that topic models can be used to represent the relationship among linguistic typological features. Typological features are typically analyzed in terms of universal implications. We argue that topic models can better capture some phenomena, such as universal tendencies, which are hard to be explained by implications. We conduct experiments to evaluate the predictive accuracy of our Hierarchical Dirichlet Process (HDP) model on the WALS dataset. We discover some interesting findings. Topics regarding word order types are recognized. We also find a topic that regards areal tendency.
Document Re-ranking via Wikipedia Articles for Definition/Biography Type Questions * It is reported that most of the information system users are accustomed to browse the top returned search results only (iProspect, 2004), so they hope the top ranking documents are highly relevant. In order to meet the information need, it is necessary and significant to improve the precision of top retrieved documents. Currently, document re-ranking has become one of the main streams to improve the precision of top retrieved documents. After document reranking, it is expected that more relevant documents appear in the higher rankings.In information retrieval system, users often submit the query which is a short description by natural language, and they decides the relevance of document not based on existence of query terms, but semantics of query terms in documents. If the IR system just simply checks the existence of query terms in documents without considering the context of documents, it often causes term mismatch and declines the performance greatly (Salton and McGill, 1983). So the important problem in automatic document re-ranking is the relevance measure of document and query. The strategies, such as query expansion, latent semantic indexing (LSI) and mutual information, have been proposed to solve this problem. And it has been proved that these approaches can improve the performance of the retrieval system effectively. Geetha and Kannan (2007) put forward another approach to solve this problem. When the initial results are returned, the user can choose a document of interest as the seed document and initiate the re-ranking algorithm by which documents are re-ranked based on its similarity distance from the seed document. This algorithm helps users to re-rank documents based on seed document as a query. But it needs the interaction of users.In this paper, we make use of the related Wikipedia articles to calculate the Wiki-Document and Wiki-Cluster similarities to adjust the ranking score for document re-ranking to solve the problem mentioned above.As we know, Wikipedia 1 is a multilingual, web-based, free-content encyclopedia project, and each of its article provides information to explain the term of the article title. We can make use of the related Wikipedia article as the "seed" document of a query, without choosing the seed documents manually, mentioned by Geetha and Kannan. We calculate the similarity distance between the seed document and each document in the initial retrieved results, named Wiki-Document similarity.In the IR4QA task of 7 th NTCIR, we look on the question as the query. The whole Wikipedia article states like the definition of its title, so we just take the specific type of questions, the definition/biography type 2 into consideration. As the name implies, the definition/biography type questions are the ones about definitions of terms or biographies of persons, such as "What is the Nobel Prize?", "Who is Osama bin Laden?".Many researchers have made efforts on how to apply clustering to get better retrieved results. The document clustering based approach is now a typical one of document re-ordering, which is based on the assumption that cluster related documents should be more similar to each other and the content similar documents may appear in the same category with greater possibility (van Rijsbergen, 1979). Document clustering approach assigns documents to automatically created clusters, based on the degree of association between documents and clusters. But this is inappropriate to calculate the similarity of the query and the document since query consists of only a few terms for obtaining statistically meaningful frequency-vector and clusters.In order to apply the document clustering analysis for document re-ranking, we also make use of the related Wikipedia article as a question. When the initial ranking documents are divided into clusters, we can calculate the similarity between the question-related Wikipedia article and the document cluster, instead of the question and document cluster, named WikiCluster similarity, to regulate the ranking score.The remainder of this paper is organized as follows. Section 2 reviews related works. Section 3 introduces the proposed document re-ranking approach using Wikipedia article. Section 4 then presents experiment results. Finally, Section 5 concludes the paper. document re-ranking method that uses the distances between documents for modifying initial relevance weights. Yang et al. (2004Yang et al. ( , 2005) used query terms which occur in both query and top N (N&lt;=30) retrieved documents to re-rank documents.Many research efforts have been made on how to apply clustering to get better retrieved results. Lee et al. (2001) proposed a model of information retrieval system that is based on a document re-ranking method using document clusters mentioned above. Anick and Vaithyanathan (1997) exploited the synergy between document clustering and phrasal analysis for the purpose of automatically constructing a context-based retrieval system. In their system, a context consists of two components, cluster of logical related articles (its extension) and a small set of salient concepts, represented by words and phrases and organized by the cluster's key terms (its intension). The Scatter/Gather system (Hearst and Pedersen, 1996) was a clusterbased document to browsing method, as an alternative to ranked titles for the organization and viewing of retrieved results. All of their experiments show that the clustering methods can enhance the performance of the IR systems. In this paper, we propose a document re-ranking approach based on the Wikipedia articles related to the specific questions to reorder the initial retrieved documents to improve the precision of top retrieved documents in Chinese information retrieval for question answering (IR4QA) system where the questions are definition or biography type. On one hand, we compute the similarity between each document in the initial retrieved results and the related Wikipedia article. On the other hand, we do clustering analysis for the documents based on the K-Means clustering method and compute the similarity between each centroid of the clusters and the Wikipedia article. Then we integrate the two kinds of similarity with the initial ranking score as the last similarity value and re-rank the documents in descending order with this measure. Experiment results demonstrate that this approach can improve the precision of the top relevant documents effectively.
Factors Affecting Part-of-Speech Tagging for Tagalog * Part-of-speech (POS) tagging is one important phase in natural language processing (NLP). It is the process of labeling words in sentences with the part of speech. POS tagging is necessary in other NLP applications such as named entity recognition and syntactic analysis. Automated POS tagging approaches include rule-based approach (Brill, 1995) and probabilistic approaches such as Hidden Markov Models (HMM), Decision Trees ( Schmid, 1994), Maximum Entropy Model (Ratnaparkhi, 1996), and Conditional Random Fields ( Lafferty et al., 2001). Many POS tagging applications have been developed for different languages such as English, Chinese, and Japanese. A few work on POS tagging for Tagalog has been done, however, results were not sufficiently high for these systems. In the comparative study of POS taggers in Tagalog ( Miguel and Roxas, 2007), the tagger using Hidden Markov Model (HMM) has an accuracy rate of 78.3%, while a template-based, N-gram POS tagger reported only 70.0% accuracy. With the aim to improve POS tagging, this paper investigates the factors affecting POS tagging in Tagalog. Considering Tagalog word's morphological structure, morphological information is used as information in training the POS tagger. This paper investigates factors contributing to the performance of the POS Tagger for Tagalog language. Tagalog, a morphologically rich language, exhibits complex morphological structure, makes use of morphological information in determining parts of speech of the word, aspect and voice. As word feature information plays important role in efficient tagging, tag set definition capturing word information also contributes to the success or failure of the tagger. A refinement of tag set is defined to possibly improve tagging performance.
GuideLink: A Corpus Annotation System that Integrates the Management of Annotation Guidelines It is generally recognized that maintaining consistency is a key problem in manual corpus annotation. To maintain consistency in the annotation, the annotators have to share the common annotation policy throughout the annotation project. Some parts of the annotation policy are documented in the early stages of an annotation project, while other parts are documented during the annotation process. We will refer to the former parts as the annotation scheme and the latter parts as the annotation guidelines.The annotation scheme typically documents the core of the annotation policy, including the goal of the annotation work, the vocabulary of terms related to the annotations including labels, on the syntax of the annotation. For example, the MUC-7 named entity annotation scheme (Chinchor and Robinson, 1997) defines three labels for tagging text span: ENAMEX for named entities, TIMEX for temporal expressions, and NUMEX for number expressions. The goal of the annotation work is to find all mentions of the named entities and to tag them with their proper labels.The annotation guideline details how to treat some borderline cases that the annotator cannot decide how to treat with the annotation scheme. Although sometimes guidelines are prepared together with annotation scheme, it is often impossible to provide a complete set of guidelines beforehand. During the annotation process, it is typical for annotators to communicate in developing guidelines when difficult cases arise. Guidelines are consequently important not only for annotators to keep the consistency of annotation process, but also for the users to understand the annotation later (see Section 2.2). Nevertheless, there has only been a few studies done on the guideline production ( Lu et al., 2006).In this paper, we propose a framework in which the association between the guidelines and the corpus is important, and support the accessibility between the guideline and the corpus. In addition, we can systemically integrate the management of the annotation guideline into the annotation process in the proposed framework. We present GuideLink, an implementation of the annotation framework, which is integrated with the existing annotation tool, XConc Suite. 1 This paper presents an annotation framework wherein the management process of the annotation guidelines is integrated into the annotation process. Such an integration allows systematic management and reference of guidelines during annotation. For the evaluation of the proposed annotation system, we compare the conventional and proposed annotation frameworks, experiments using automatic guideline suggestion, and describe a unique feature of the integrated framework.
Using Wikipedia for Hierarchical Finer Categorization of Named Entities Ever-growing demand for classification of Named Entities is driven by open domain Question Answering and other search problems. The ability to perform fine-grained categorization of named entities is important due to the fact that it provides useful context information in Question Answering, Web Search and many other major Natural Language Processing (NLP) tasks. Very few works, however, aimed at broader and highly diverse categorization of named entities in opendomain. A popular named entity could be classified under a different class as the local context deems it.A named entity categorization model requires a large-scale open-domain tagged corpus for training. Of late, large-scale resources have been a choice of corpus. Therefore, we believe that a more structured and organized encyclopedic corpus like Wikipedia (www.wikipedia.org) is a suitable training corpus mainly for two reasons. Firstly, Wikipedia contains text on a wide range of topics that is continually created and edited by volunteers. Secondly, the structure of Wiki provides hyperlinks over important phrases linked to other articles from Wikipedia besides links to redirected and disambiguation pages. Therefore, it offers a natural way to harvest a large volume of "labelled" training data. In fact, some emerging works tried to exploit Wikipedia as a knowledge resource ( Ponzetto and Strube, 2006;Cucerzan, 2007).In this paper, as a first step towards our task, we discuss the usability of Wikipedia as training corpus and different facets of Wikipedia. One of these facets is exploiting structure of Wikipedia to create training corpus for training for use with different machine learning algorithms. Further, We would like to thank Chua Tat-Seng for his valuable guidance throughout this work and also thank anonymous reviewers for their useful feedback. Wikipedia is one of the largest growing structured resources on the Web and can be used as a training corpus in natural language processing applications. In this work, we present a method to categorize named entities under the hierarchical fine-grained categories provided by the Wikipedia taxonomy. Such a categorization can be further used to extract semantic relations among these named entities. More specifically, we examine instances of different kinds of Named Entities picked from Wikipedia articles categorized under 55 categories. We employ a Maximum Entropy based method to perform supervised learning that learns from local context of a named entity as well as a higher-level context such as hypernyms/hyponyms from Wikipedia and WordNet.
A Novel Method of Sentence Ordering Based on Support Vector Machine In extractive multi-document summarization tasks, how to extract sentences from source document is an important and major work. But it is not enough for a fluent and readable summary. Recent research indicates that research on summary should get more attention at sentence ordering. Barzilay has offered empirical evidence that proper order of extracted sentences would greatly improve the readability of a summary ( Barzilay et al., 2002).Sentence ordering is much easier in single-document summarization, because single document provides a natural order of sentences in summary based on source document. Differently, in multidocument summarization tasks, multi-documents contribute sentences with different authors and in different writing styles, which means source documents could not directly provide ordering criterion in multi-document summarization task.Obviously, sentence ordering in multi-document summarization task involves two fields, information provided by source documents and experiential knowledge of human. Neither of them can be easily got and handled, because both of them need semantic knowledge more or less. Fortunately, large raw corpus can afford opportunity for quantitative analysis of sentences ordering.Several methods of sentence ordering in multi-document summarization are presented in section 2. However, there is no ideal strategy to achieve coherent summaries. In this paper, we proposed a method based on information of source documents and experience of human to adjust sentence sequences, which discuss the relationship between sentences in multi-document summarization tasks. In this paper, we present a practical method of sentence ordering in extractive multi-document summarization tasks of Chinese language. By using Support Vector Machine (SVM), we classify the sentences of a summary into several groups in rough position according to the source documents. Then we adjust the sentence sequence of each group according to the estimation of directional relativity of adjacent sentences, and find the sequence of each group. Finally, we connect the sequences of different groups to generate the final order of the summary. Experimental results indicate that this method works better than most existing methods of sentence ordering.
Supertagging with Factorial Hidden Markov Models For many sequence prediction tasks in Natural Language Processing, modeling dependencies between individual predictions can be used to improve prediction accuracy of the sequence as a whole. For example, chunking involves identifying sequences of words in a sentence that are part of syntactically related non-overlapping, non-recursive phrases. An effective representation for this task involves assigning an individual part-of-speech (POS) tag and chunk tag to each word and deriving the actual chunks from these word specific labels. In these sequences, many of the POS and chunk tags are correlated, so joint inference can be quite useful.Supertagging ( Bangalore and Joshi, 1999), involves assigning lexical entries to words based on lexicalized grammatical theory such as Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2009). For example, the English verb join has the POS VB and the CCG category ((S b \NP)/PP)/NP in CCGbank (Hockenmaier and Steedman, 2007). This category indicates that join requires a noun phrase to its left, another to its right, and a prepositional phrase to the right of that. Every lexical item has as many supertags as the number of different syntactic contexts in which the item can appear, so supertags are far more detailed and numerous than POS tags. Recently there is increased interest on supertagging beyond their standard use as a pre-parsing step (Clark and Curran, 2007)-for example, they are being used as features in machine translation (Birch et al., 2007;Hassan et al., 2007).Chunking and supertagging can be modeled using a two-stage cascade of Hidden Markov Models (HMMs) (Rabiner, 1989). POS tags are first predicted from the observed words in the first stage; then the chunk tags or supertags are predicted from those POS tags in the next stage. Alternatively, both sequences can be jointly predicted with Factorial Hidden Markov Models (FHMMs) ( Ghahramani and Jordan, 1998), thereby preventing propagation of errors. Here, we applyWe would like to thank Sharon Goldwater and Ray Mooney for helpful feedback and suggestions. This work was supported by the Morris Memorial Grant from the New York Community Trust. Factorial Hidden Markov Models (FHMM) support joint inference for multiple sequence prediction tasks. Here, we use them to jointly predict part-of-speech tag and su-pertag sequences with varying levels of supervision. We show that supervised training of FHMM models improves performance compared to standard HMMs, especially when labeled training data is scarce. Secondly, we show that an FHMM and a maximum entropy Markov model in a single step co-training setup improves the performance of both models when there is limited labeled training data. Finally, we find that FHMMs trained from tag dictionaries rather than labeled examples also perform better than a standard HMM.
Identifying and Utilizing the Class of Monosemous Japanese Functional Expressions in Machine Translation The Japanese language has various types of functional expressions, which are very important for understanding their semantic contents. Those functional expressions are also problematic in further applications such as MT of Japanese sentences into English. This problem can be partially recognized by the fact that the Japanese language has a large number of variants of functional expressions, where their total number is recently counted as over 10,000 in Matsuyoshi et al. (2006). Based on those recent development in studies on lexicon for processing Japanese functional expressions ( Matsuyoshi et al., 2006), this paper studies issues on MT of Japanese functional expressions into English.More specifically, in order to solve the problem of a large number of variants of Japanese functional expressions, in this paper, we employ the "Sandglass" MT architecture (Yamamoto, 2002) 1 . In the "Sandglass" MT architecture, variant expressions in the source language are first paraphrased into representative expressions, and then, a small number of translation rules are applied to the representative expressions. In this paper, we apply this architecture to the task of translating Japanese functional expressions into English, where we introduce a recently compiled large scale hierarchical lexicon of Japanese functional expressions ( Matsuyoshi et al., 2006). We employ the semantic equivalence classes of the lexicon and examine each class whether it is monosemous or not. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain. In the &quot;Sandglass&quot; machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain.
Named Entity Recognition for Manipuri Using Support Vector Machine Named Entity Recognition (NER) is an important tool for several Natural Language Processing (NLP) application areas. The objective of NER is to identify and classify every word/term in a document into some predefined categories. The challenge in detection of Named Entities (NE) is that such expressions are hard to analyze using rule-based NLP techniques because they belong to the open class of expressions, i.e., there is an infinite variety and new expressions are constantly being invented. The development of an automatic NER system requires either a comprehensive set of linguistically motivated rules or a large amount of annotated corpora in order to achieve reasonable performance. But such rules or corpora have been developed for a few languages like English, most of the European languages and some of the Asian languages like Chinese, Japanese and Korean. NER systems for Indian languages are not readily available due to the unavailability of such handcrafted rules or annotated corpora.Rule based approaches focus on extracting names using a number of handcrafted rules. Generally, these systems consist of a set of patterns using grammatical (e.g., part of speech), syntactic (e.g., word precedence) and orthographic features (e.g., capitalization) in combination with dictionaries. One drawback of the rule-based approach is that it lacks the ability of coping with the problems of robustness and portability. Each new source of text requires significant Copyright 2009 by Thoudam Doren Singh, Kishorjit Nongmeikapam, Asif Ekbal, and Sivaji Bandyopadhyay tweaking of rules to maintain optimal performance and the maintenance costs could be quite high. While early studies are mostly based on handcrafted rules, most recent ones use machine learning (ML) models as a way to automatically induce rule based systems or sequence labeling algorithms starting from a collection of training examples. Some of the very effective ML approaches used in NER are HMM (Nymble in Bikel et al. (1999)), ME (Borthwick, 1999), CRFs ( Lafferty et al., 2001) and SVM ( Yamada et al., 2001). The SVM based NER system was proposed by Yamada et al. (2001) for Japanese.Manipuri is a Tibeto-Burman, scheduled Indian language and highly agglutinative in behavior, monosyllabic, influenced and enriched by the Indo-Aryan languages of Sanskrit origin and English. The affixes play the most important role in the structure of the language. A clearcut demarcation between morphology and syntax is not possible in this language. The majority of the roots found in the language are bound and the affixes are the determining factor for the class of the words in the language. Classification of words using the role of affix helps to implement the NE tagger for a resource poor language like Manipuri with high performance. There is no report of NER work prior to our experiment for Manipuri. NE identification in Indian languages as well as in Manipuri is difficult and challenging as:1 Unlike English and most of the European languages, Manipuri lacks capitalization information, which plays a very important role in identifying NEs. 2 A lot of NEs in Manipuri can appear in the dictionary with some other specific meanings. 3 Manipuri is a highly inflectional language providing one of the richest and most challenging sets of linguistic and statistical features resulting in long and complex wordforms. 4 Manipuri is a relatively free word order language. Thus NEs can appear in subject and object positions making the NER task more difficult compared to others. 5 Manipuri is a resource-constrained language. Annotated corpus, name dictionaries, sophisticated morphological analyzers, POS taggers etc.are not yet available. In Indian language context, a HMM based NER system for Bengali has been reported in Ekbal et al. (2007), where additional contextual information has been considered for emission probabilities and NE suffixes are used for handling the unknown words. Other works in Bengali NER can be found in , and  with the CRF, and SVM approaches, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF. Various works on NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL)2 1 using various techniques.In this paper, Manipuri NER systems have been developed using an active learning technique as well as SVM. We collected the data from http://www.thesangaiexpress.com/ , a popular Manipuri newspaper. Initially, a baseline system has been developed based on an active learning technique that generates lexical context patterns from the unlabeled corpus in a bootstrapping manner. This corpus has been manually annotated with the four major NE tags, namely Person name, Location name, Organization name and Miscellaneous name. Miscellaneous name includes the festival name, name of objects, name of building, date, time, measurement expression and percentage expression etc. The SVM based system makes use of the different contextual information of the words along with variety of orthographic word-level features that are helpful to identify the various NE classes. A number of experiments have been conducted to identify the best set of features for NER in Manipuri under the SVM framework. This paper reports about the development of a Manipuri NER system, a less computerized Indian language. Two different models, one using an active learning technique based on the context patterns generated from an unlabeled news corpus and the other based on the well known Support Vector Machine (SVM), have been developed. The active learning technique has been considered as the baseline system. The Manipuri news corpus has been manually annotated with the major NE tags, namely Person name, Location name, Organization name and Miscellaneous name to apply SVM. The SVM based system makes use of the different contextual information of the words along with the variety of orthographic word-level features which are helpful in predicting the NE classes. In addition, lexical context patterns generated using the active learning technique have been used as the features of SVM in order to improve performance. The system has been trained and tested with 28,629 and 4,763 wordforms, respectively. Experimental results show the effectiveness of the proposed approach with the overall average Recall, Precision and F-Score values of 93.91%, 95.32% and 94.59% respectively.
BaseNP Supersense Tagging for Japanese Texts  This paper describes baseNP supersense tagging for Japanese texts. The task extracts base noun phrases (baseNPs) from raw texts in Japanese, and labels their baseNPs with supersenses. This task has a number of applications including predicate argument structure analysis and question answering. While the definition of baseNP in English is relatively clear, its definition in Japanese has not yet been clear. In this paper, we defined Japanese baseNP analogous to English and defined Japanese supersenses using a broad-coverage Japanese thesaurus, Nihongo Goi Taikei (comprehensive outline of Japanese vocabulary). We then adopted a sequential tagging algorithm for the task, namely the averaged perceptron with HMM, and achieve high performance compared to a baseline.
Which is More Suitable for Chinese Word Segmentation, the Generative Model or the Discriminative One? F * * * *  Since the traditional word-based n-gram model, a generative approach, cannot handle those out-of-vocabulary (OOV) words in the testing-set, the character-based discriminative approach has been widely adopted recently. However, this discriminative model, though is more robust to OOV words, fails to deliver satisfactory performance for those in-vocabulary (IV) words that have been observed before. Having analyzed the word-based approach, its capability to handle the dependency between adjacent characters within a word, which is believed that the human adopts for doing segmentation, is found to account for its excellent performance for those IV words. To incorporate the intra-word characters dependency, a character-based approach with a generative model is thus proposed in this paper. The experiments conducted on the second SIGHAN Bakeoffs have shown that the proposed model not only achieves a good balance between those IV words and OOV words, but also outperforms the above-mentioned well-known approaches under the similar conditions. 1 Generative Model Versus Discriminative Model Unlike English and other western languages, there is no space delimiter between adjacent Chinese words. Therefore, for most Chinese NLP applications, Chinese word segmentation (CWS) is the first task, which aims to find the corresponding word sequence from the given Chinese character sequence. Among various approaches for CWS, statistical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the well-known character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word &quot;model&quot; will be loosely exchanged with the word &quot;approach&quot; when there is no confusion.
Discovery of Dependency Tree Patterns for Relation Extraction Relation extraction is the task that aims to identify relations between pairs of named entities from free text. From the perspective of methodologies employed to conduct relation extraction, current existing methods can be classified into three categories. The first category is rule-based methods, which identifies relations by utilizing inference rules. The rules could be extended regular expressions incorporating with pre-defined constraints. The second is to treat relation extraction as a classification problem with a kernel machine such as SVM. The problem is then how to define a kernel function between two relation instances. The third category of relation extraction method is regarding relation extraction as sequence labeling problem. Then the existing methods for sequence labeling, such as Maximum Entropy (ME) and Conditional Random Fields (CRFs), could be used to solve the relation extraction problem directly.Our algorithm proposed in this paper belongs to the rules learning framework. The reason we choose the rule-based method is that the rules could contain more semantic information than features used in statistical methods. A pattern is a sub dependency tree that indicates a relation instance. In other words, a pattern corresponds to a trimmed sentence, e.g. a sentence with all unrelated components removed. This is certainly an advantage to the flat patterns which will face significant problems if the sentences are very complex. Experiments show that our patterns are more effective used for relation extraction.The rest of the paper is organized as follows. Section 2 gives a brief introduction to the related work. Section 3 describes our new framework for relation extraction. In Section 4, we present our algorithm for automatic extraction of the dependency tree patterns. In Section 5, experiments are conducted to evaluate our new algorithm. Section 6 is the concluding remarks and future work. from the web pages. Agichtein (2000) proposed Snowball which improved DIPRE by introducing the notion of match degree, where only sentences, whose match degrees are higher than a pre-defined threshold, are used to generate new rules. Lin (2001) proposed DIRT which learns inference rules from the paths connecting pairs of arguments of the relations in dependency trees for question answering systems.For kernel methods, Zelenko (2002) propose the continuous and sparse tree kernels based on the syntactic parse tree for relation extraction. Culotta (2004) used this kernel on dependency trees to train a SVM( Cortes and Vapnik, 1995) classifier for relation extraction. Similar works include the shortest path dependency tree kernel( Bunescu and Mooney, 2005) and subsequence kernel( Bunescu and Mooney, 2006).For the sequence labeling methods, Kambhatla (2004) proposed an algorithm that makes use of features extracted from the dependency trees and syntactic parse trees, including the path from the first argument to the second argument, the parents of the first and second arguments of a relation in a dependency tree, the context words with their POS tags. All the features are used to train a ME model, which is then used to extract new relation instances. Culotta (2006) used a data mining algorithm to find implicit relations between different relation types. For example, a fatherof relation and a husband-wife relation may imply a mother-of relation. These global information, used in a CRFs framework, can effectively improve the performance of the relation extraction task. Bundschus (2008) adopted rich features except syntactic features to train a CRFs model for two different biological relations extraction: gene-disease and disease-treatment.The most related work is proposed by Dat in (2007). However, there are several differences between his algorithm and ours. First, Dat only considers subtrees whose leaf nodes are either arguments or the keyword that determines the relation type i.e., the subtrees are heuristically selected. But our algorithm treats the dependency trees as general subtrees with some extra constraints. Second, the subtrees in Dat's method are only used as features to train SVM models. Our algorithm uses the subtrees as patterns to directly extract new relation instances. The patterns themselves could perform well, although they are also able to be used as features. Third, the tree nodes are represented by original words in Dat's work, but our tree nodes are represented by several attributes including POS tags, high-level POS tags and dependency types. With our node representation, we can define any match function between two nodes. For example, we could define that if the POS tags of two nodes are same, then the two nodes are same. Relation extraction is to identify the relations between pairs of named entities. In this paper, we try to solve the problem of relation extraction by discovering dependency tree patterns (a pattern is an embedded sub dependency tree indicating a relation instance). Our approach is to find an optimal rule (pattern) set automatically based on the proposed dependency tree pattern mining algorithm. The experimental results show that the extracted patterns can achieve a high precision and a reasonable recall rate when used as rules to extract relation instances. Furthermore, an additional experiment shows that other machine learning based relation extraction methods can also benefit from the extracted patterns by using them as features.
Generalizable Features Help Semantic Role Labeling The task of semantic role labeling identifies the semantic argument(s) of a verb and assigns a semantic role label to each semantic argument. Since the seminal work on semantic role labeling (SRL) by Gildea and Jurafsky (2000), the community has seen much effort dedicated to the task, which generated dozens of published SRL systems, including the ones participated in the shared tasks on CoNLL-2004, CoNLL-2005, and CoNLL-2008 In the past eight years, although researchers approached the SRL problem from different perspectives, they all focused on determining the appropriate syntactic/semantic knowledge and machine learning system to tackle the challenges in SRL (Carreras and Marquez, 2005;Surdeanu et al., 2008).In terms of searching for the proper syntactic/semantic knowledge, the SRL researchers explored features based on two formalisms, namely constituency grammar and dependency grammar. The SRL systems constructed between the years of 2000 and 2006 investigated a variety of features that constituency grammar provides for. The main stream in this line of work had been developing specific features to cover diverse syntactic configurations that a predicate verb appears in ( Toutanova et al., 2005;Haghighi et al., 2005), in addition to the generic syntactic features for the verb (Gildea and Jurafsky, 2000;Pradhan et al., 2004). As a result, few researchers investigated a feature engineering that could generalize across different syntactic configurations.The lack of improvement in performance on the commonly used test data, such as Penn Treebank section 23, between 2005 and 2007 motivated SRL researchers to seek help from the grammatical relations between a word and its head within the framework of dependency grammar (Johansson and Nugues, 2007a,b;Surdeanu et al., 2008). However, the shift in feature representations, aiming to show that dependency grammar was more suitable for the feature design, was notThe author would like to thank the reviewers for their valuable comments. The author would also like to thank Professors Steven Abney and Richmond Thomason and Mr. Terry Szymanski for their help. In this paper, we take on the challenge of developing effective generalizable features for the task of semantic role labeling in the constituency grammar framework. Based on the knowledge of argument structure, on the constraint imposed by context dependence defined in the theory of argument realization, and on the knowledge of moved and displaced core arguments, we design the base argument configuration (BAC) feature that generalizes across four types of syntactic structures involving moved and displaced core arguments. As part of the effort to derive this base argument configuration feature, we also identify the core and non-core arguments in the system which is the first case in the field of semantic role labeling. Together with two levels of backoff features, the BAC feature effectively solve the argument classification task. However, as the experimental results show, our overall performance is affected by the argument identification module at present.
Utilizing Features of Verbs in Statistical Zero Pronoun Resolution for Japanese Speech Anaphora is a phenomenon whereby an expression (anaphor) can be clarified by binding it with an entity in its context. A typical example of an anaphor is a pronoun. In languages where the subject is often omitted, e.g. Japanese, the omitted subject can be regarded as an anaphor, and this is called a zero pronoun.For systems that need deep text processing, such as an automatic text summarization system, it is helpful to resolve such anaphora. The task of identifying the referent of an anaphor is called anaphora resolution and this has been the subject of many studies, including work on zero anaphora resolution ( Isozaki and Hirao, 2003;Iida et al., 2007a). Those state-of-the-art systems utilize statistical machine learning to achieve good levels of performance. They learn the weights of features extracted from corpora such as annotated newspaper articles to obtain rules for anaphora resolution.Although most such systems target written texts, anaphora resolution is also valuable for spoken text processing, such as speech summarization (Steinbergera et al., 2007) and interactive QA systems (van Schooten and op den Akker, 2005;Fukumoto, 2006). Recently, some projects have produced corpora of anaphora on spoken texts and used them to build anaphora resolution systems based on statistical machine learning. For example, Müller (2007) tagged anaphora on the ICSI Meeting Corpus and performed pronoun resolution using an empirical method that utilizes a logistic regression classifier.As regards zero pronouns in Japanese spoken texts, however, there have been few studies that adopt a state-of-the-art statistical machine learning approach with large corpora. Dohsaka (1990) proposed a zero pronoun resolution system with heuristic constraints, which is tested only on a small set of typed dialogues. Fukumoto (2006) introduced zero pronoun resolution into an interactive question answering system. However, the resolution method is only applicable to QA dialogues. It is necessary to develop an anaphora resolution system for Japanese spoken texts based on recent statistical learning methods to obtain adequate quality. This paper proposes a statistical zero pronoun resolution method that utilizes features of verbs. In Japanese speech, the subject is often omitted, especially when it is the first person. To resolve such zero pronouns, features related to the verbs such as functional expressions play important roles. However, recent state-of-the-art zero-pronoun resolution systems lack these features because they are mainly designed for written texts such as newspaper articles, in which first person subjects are rare. We show that a set of verbal features has the ability to distinguish first persons from others in monologue transcriptions, and this improves the accuracy of zero pronoun resolution with statistical machine learning.
Towards Establishing a Hierarchy in the Japanese Sentence Structure The purpose of this paper is to argue that the hierarchical theory of Japanese sentence structure, against which a set of linguistic data have been provided as counterevidence, is on the whole tenable. The result is presented in the form of Phrase Structure Grammar rules. By this we show that the hierachical view of sentence structure in the traditional Japanese grammatical studies can be developed into a semantics-based sentence processing system with multiple layers of localities in which scopes are manipulated and constraints enforced that guarantee sentence integrity. Minami (1964Minami ( , 1974 found that cooccurrence of sentence constituents within subordinate clauses is restricted by the type of the subordinate clause head, i.e., the conjunctive particle that introduces the subordinate clause. In (1), while the accusative case NP shinbun o is included in the subordinate clause indicated by the brackets, the nominative case NP Tar¯ o ga is not. This is evidenced by the fact that two separate subjects both for the matrix and subordinate clauses are not allowed in such complex sentences. A topic phrase marked by wa also cannot appear within this type of subordinate clause.  Furthermore, while a certain group of auxiliaries (e.g., the causative and passive auxiliaries (sa)seru and (ra)reru) and adverbials may occur within the subordinate clause headed by nagara, others (e.g., the negative nai, past ta, politeness masu, and epistemic/voluntative o and dar¯ o) may not. The Minami Hierarchy or the four-layers of embeddings within Japanese sentences has been known to give a convincing account of heterogeneous linguistic data in Japanese Grammar. However, the categorization of sentence constituents has faced serious problems. In this paper, we illustrate that the hierarchical sentential structure is on the whole tenable and attempt to represent it with Phrase Structure Grammar rules. The result is a realization of surface syntactic structure information that can serve as input to Scope Control Theory, a routine of interpretation or semantic evaluation that requires the generalizations of Minami&apos;s hypothesis to hold true for evaluation to successfully complete.
Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation 24 Organizing Committee and PACLIC Steering Committee  
PodCastle: A Spoken Document Retrieval Service Improved by Anonymous User Contributions Speech recognition researchers understand what sort of speech is easily recognized by speech recognizers and realize that speech recognizers perform best when dealing with clean speech. On the other hand, most end users of speech recognizers judge the effectiveness of speech recognition from their limited experiences and do not necessarily understand how useful state-of-the-art recognizers can be. Users sometimes do not adequately comprehend what sort of voices or recording conditions make recognition difficult. If they have previously had difficulty being understood by speech recognizers, they often doubt the usefulness of speech recognition and may stop using it.The first aim of this study is to address this problem by promoting the popularization and use of speech recognition by raising end user awareness of state-of-the-art speech recognition performance. For this purpose, we launched a podcast search web service called PodCastle (Goto et al., 2007;Ogata et al., 2007;Ogata and Goto, 2009b;Ogata and Goto, 2009a) in 2006 that allows anonymous users to search and read podcasts, and to share the full text of speech recognition results for podcasts. Podcasts are audio programs distributed on the web, like radio shows or audio blogs. They are becoming increasingly popular because updated podcasts (MP3 audio files) can be easily and frequently downloaded by using RSS syndication feeds. Since various contents have already been published as podcasts, users can grasp the current state of speech recognition technology just by seeing the results of speech recognition applied to published podcasts. This is Copyright 2010 by Masataka Goto and Jun Ogata important because when some users experience recognition errors while speaking into a microphone, they may become uncomfortable or frustrated and lose their motivation. Such problems do not occur for PodCastle because users do not have to provide their own speech input at all.However, even state-of-the-art speech recognizers cannot correctly transcribe all podcasts, because their contents and recording environments vary very widely. A typical approach to deal with speech contents that cannot be properly recognized is to create a speech corpus including such contents and prepare correct transcriptions to train speech recognizers. This approach, however, is impractical for PodCastle because advance preparation of a corpus covering diverse podcast contents will be too costly and time consuming.The second aim of this study is to dispense with the idea of using a pre-prepared corpus to address this problem, and instead employ the efforts of a large number of users to improve speech recognition and full-text search performance. Even if a state-of-the-art speech recognizer is used to recognize podcasts on the web, a number of errors will naturally occur. PodCastle therefore encourages users to cooperate by correcting these errors so that those podcasts can be searched more reliably. Furthermore, using the resulting corrections to train the speech recognizer, it implements a mechanism whereby the speech recognition performance is gradually improved. This approach can be described as collaborative training for speech recognition.In 2006, we coined the term Speech Recognition Research 2.0 ( Goto et al., 2007) to refer to the research approach where the current state of speech recognition technology is intentionally disclosed to users so that speech recognition performance can be improved through cooperative participation by users. This term was chosen to reflect the concept of Web 2.0 (O'Reilly), since this approach brings the benefits of Web 2.0 to speech recognition research. In Section 2 of this paper, we discuss the research approach that Speech Recognition Research 2.0 represents, and in Section 3 we describe the PodCastle web service as an instance of this approach. In Section 4, we summarize the contributions of this research. In this invited paper, we introduce a public web service, PodCastle, that provides full-text searching of speech data (Japanese podcasts) on the basis of automatic speech recognition technologies. This is an instance of our research approach, Speech Recognition Research 2.0, which is aimed at providing users with a web service based on Web 2.0 so that they can experience state-of-the-art speech recognition performance, and at promoting speech recognition technologies in cooperation with anonymous users. PodCastle enables users to find podcasts that include a search term, read full texts of their recognition results, and easily correct recognition errors by simply selecting from a list of candidates. Even if a state-of-the-art speech recognizer is used to recognize podcasts on the web, a number of errors will naturally occur. PodCastle therefore encourages users to cooperate by correcting these errors so that those podcasts can be searched more reliably. Furthermore, using the resulting corrections to train the speech recognizer, it implements a mechanism whereby the speech recognition performance is gradually improved. In our experiences from its practical use over the past 46 months (since December, 2006), we confirmed that the performance of PodCastle was improved by a number of anonymous user contributions.
The future role of language resources for natural language parsing (We won&apos;t be able to rely on Pierre Vinken forever... or will we have to?)  The transformation that natural language parsing has undergone since the nineties would have been impossible without the availability of syntactically annotated corpora such as the Penn Tree-bank and similar resources for other languages. By now, it has become increasingly difficult to increase parsing accuracy on our standard data sets. But as we move to other domains of text, or aim to recover richer representations that are required for natural language understanding, it is also clear that parsing is far from being a solved task. In this panel, I would like to initiate a discussion about the kind of language resources needed to advance natural language parsing. I will also reflect on what the translation of existing resources into other grammatical representations has taught us about treebank design.
The acquisition of word order in a topic-prominent language: Corpus findings and experimental investigation  It is well-known that Chinese has SVO as predominant word order, with variant orders OSV and SOV marking topic and focus, intimately linked to the topic-prominence of the language. Assuming early setting of the head parameter in syntactic acquisition and the peripheral positions of topic and focus in clausal structure, one might hypothesize that Chinese-speaking children will acquire the predominant SVO order early, but develop the variant orders later. Such acquisition findings, if true, would seemingly go against the idea of a topic prominence parameter. This paper explores the development of topic prominence by examining word order in early child Chinese, based on naturalistic longitudinal corpora as well as cross-sectional experiments that investigated the relative accessibility of different word orders. Our naturalistic data show that the word order of two-year-old Chinese children reflects adherence to canonical mapping of thematic roles to structural positions, as well as sensitivity to the unselectivity of subject and object. While sentences of OSV order appeared around two years of age, double nominative structures were virtually absent before three, suggesting that as a typological characteristic, topic-prominence is not acquired early. Our experimental results show that Mandarin-speaking children by three years of age have established SVO solidly as the dominant word order, on both comprehension and production, but still find the topicalized and fronting orders (OSV, SOV) difficult, indicating that the structures of the left periphery may be acquired at a later stage, and at different times. The implications of these acquisition findings for the topic prominence parameter will be explored. PACLIC 24 Proceedings 15
How and why two strangers can co-create a story: An application of the &apos;ba&apos;-theory based approach to discourse  This presentation addresses the question: how and why can a pair of teacher-student interactants co-create a story? In analyzing the discourse data taken while interactants try to achieve the task of making a coherent story by arranging cards, the &apos;ba (filed)&apos; theory is employed. &apos;Ba&apos; based approach is an innovating frame of thinking device that assumes 1) inside perspective of the subject, 2) dual mode thinking of the self, 3) a dynamic model like an improvised drama and 4) two modes of communication, i.e. overt and covert. The data have been analyzed into two phases of discourse: the dialogue discourse and the merging discourse. The former is indexed by the interpersonal modalities such as honorifics and sentence final particles, while the latter is characterized by dropping these linguistic features. The sudden drop of presupposed use of modalities by a teacher is obviously a deviation, but it would serve as a creative use (cf. Silverstein 1967). It is in this merging discourse that the discourse phenomena of repetition, simultaneous utterances, and chaining utterances occur. These phenomena would add no information to the conversation, but function to synchronize and to entrain the interactants. When synchronization and entrainment are maintained between the interactants, covert communication is to be maintained that would create a basis for smooth overt communication. I will argue that it is by the &apos;ba&apos; theory based approach to discourse that we can illuminate the dynamic processes of co-creating a story by interactants. References Brown, Penelope and Stephen Levinson. (1978) Universals in language usage: Politeness phenomena. In Esther Goody (ed.) Questions and politeness: strategies in social interaction, 56-289. Cambridge: Cambridge University Press. Condon, William S. (1980) The relation of interactional synchrony to cognitive and emotional process. In Key, M. Ritchie. (ed.) The relationship of verbal and nonverbal communication, 51-56. The Hague: Mouton Publishers. Duranti, Alessandro, and Charles Goodwin. (eds.) (1992) Rethinking Context: Language as an Interactive Phenomenon. Cambridge: Cambridge University Press. Eelen, Gino. (2001) A critique of politeness theories. Manchester, UK: St. Jerome Publishing.
Listening In *  Communicating agents are commonly thought of as intentionally addressing messages to other agents. A growing body of research exists on the interactive case: natural language dialogue. A somewhat different case, also important in many real life social and work settings, is a person overhearing or intentionally listening in on dialogue among a group of other people. Comparatively little research so far illuminates how, for example, a minute taker for a meeting can comprehend a discussion well enough to accurately record decisions, action items, and other such meeting outcomes, including ones that concern technical matters he does not understand. What prevents the small misunderstandings that frequently creep into discussions, even between active participants, from growing into a gross misunderstanding by the minute taker of the discussion to which he is listening? This talk will present some similarities and differences between participating in a conversation and listening in on one, with emphasis on how overhearers who lack opportunities to contribute to a discussion target their interpretive efforts in productive ways. Progress in creating artificial agents capable of similar listening feats will be surveyed and research directions assessed.
Basic Units of Lexicons and Ontologies: Words, Senses and Concepts * * * * Lexicons/Dictionaries have been one of the most important resources for linguistic research and applications. Ontologies are also becoming an indispensible resource not only for linguistics but also for other areas dealing with knowledge. In many cases, however, they fall short of our expectations. One reason for this under-expectation is that their basic units are not wellestablished. There are two kinds of basic units of dictionaries: head words and (word) senses. It is a truism that head words have to be words rather than affixes or phrases. However, it is not always true that only word units are registered as head words, especially in the dictionaries of agglutinative languages. In addition, the meaning of a word has to be carved into different senses on the basis of objective criteria. On the other hand, building blocks of ontologies have to be (simple and/or complex) concepts rather than senses.In section 2, we will evaluate the morpho-syntactic status of head words in a Korean dictionary. It will be shown that many head words are phrases and, hence, have to be removed from the list of head words. In addition, many elements that are treated as affixes are actually words and, hence, have to be registered as head words. We need to realize that agglutinative languages like Korean have many clitics, i.e. (syntactic) words which have some affixal properties as well. In section 3, we will consider issues related to polysemy. We need to distinguish between homophony and polysemy, on the one hand, and between polysemy and vagueness, on the other. Lastly, in section 4, we will consider basic units of ontologies. Some scholars argue that they have to be word senses rather than concepts. However, many scholars assume that they have to be concepts rather than senses. We will show, based on a variety of phenomena, that the building blocks of ontologies should be concepts. Dictionaries and ontologies are very important resources not only for linguistic research and applications but also for other areas dealing with knowledge. In general, however, they fall short of our expectations. One reason for this under-expectation is that their basic units are not well-established. Dictionary head words have to be words rather than affixes or phrases. The meaning of a (head) word has to be carved into different senses on the basis of objective criteria. In addition, building blocks of ontologies have to be (simple and/or complex) concepts rather than senses.
A morphosyntactic analysis of the pronominal system of Philippine languages With approximately 150 living languages in the Philippines (Headland, 2003), there are just as many structures of the pronominal system of these languages. The present paper attempts to compare the morphosyntactic features of personal pronouns of ten Philippine languages (henceforth, PL): Tagalog, Cebuano, Ilocano, Hiligaynon, Waray-waray, Kapampangan, Bikol, Pangasinan, Kinaray-a and Ibanag. The first nine are considered major languages, whereas the last is minor.Pronominals are a universal component of human languages and are considered basic vocabulary of any given language. Specifically, personal pronouns are generally closed-class and are unaffected by borrowing or code-switching. With these, it is hoped that a careful analysis of their features will shed light to the many controversies concerning PL (cf. Himmelman, 1991).Literature suggests that there is an obvious paucity of studies on the pronominal systems of PL. Early studies (Reid, 1975;Tharp, 1974) have dealt more with the reconstruction of prototypes and a few looked into deictics (e.g., MacFarland, 2006). It is this gap that the present research aims to address. Pronominal orientation is widely argued to be universal component of human languages. Meanwhile, the pronominal system of Philippine languages (henceforth, PL) has always been an obscure subject of investigation. With approximately 150 living languages, the structures of pronominals are just as many. This study attempts to explicate the grammatical functions, along with other known phenomena such as cliticization, homography, inclusivity/exclusivity, person-deixis interface, and hierarchy of some languages in the Philippines. Using an ergative-absolutive analysis, this cross-linguistic investigation of Philippine languages presents examples that illustrate the distinctive features of personal pronouns. Using a 100,000-word corpus for each language included, there are various similarities and differences revealed by the study: (1) some languages allow encliticization and some don&apos;t; (2) homography, as well as inclusivity/exclusivity, is a persistent feature of the languages; and (3) the strength of hierarchy poses semantic constraints, among others.
Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation  Syntax-based statistical translation model is proved to be better than phrase-based model, especially for language pairs with very different syntax structures, such as Chinese and English. In this talk I will introduce a serial of statistical translation models based on source syntax structure. The tree-based model uses the one best syntax tree for translation. The forest-based model uses a compact forest which encodes exponential number of syntax trees in a polynomial spaces and lead to better performance. The joint parsing and translation model produces source parse trees, using the source side of the translation rules instead of separate parsing rules, and generate translations on the target side simultaneously, which outperforms the forest-based model. Some extensions of these models are introduced also.
Benglish Verbs: a Case of Code-Mixing in Bengali* As in many other languages (Butt, 1995(Butt, , 2010Dasgupta, 1977Dasgupta, , 2003Hook, 1974;Masica, 1976;Mohanan, 1993Mohanan, , 1994Moravcsik, 1975, 1978and Wohlgemuth, 2009 among others) there exists a particular type of complex predicates in Bengali constituted of two items, one chosen from among various categories of words: noun, verbal forms, adjective, preposition, adverb, onomatopoeic word, etc., and the other, a duly inflected verb. The first item is usually called a pole and second one a vector. These predicates are generally put into two different groups on the basis of the syntactic category of their pole: i) Compound verbs (1) that categorically involve a verb (usually a non-inflectional verbal form such as participle, absolutive or past gerund), and ii) Conjunct verbs (2) that involve categories other than the verb. The main characteristic of compound and conjunct verbs is that they must denote one single action.1. Rik eshe poreche (Rik-having come-has fallen) 'Rik has just come.' 2. Rik bajar kore (Rik-market-does) 'Rik does shopping.' _______________________________ As in other South Asian languages there is a particular type of complex predicate in Bengali in which the pole is a word of English origin chosen from among various types of nouns (3-6), adverbs (7), adjectives (8), prepositions (9) and verbs (10)(11) while the vector is chosen from among a closed set of Bengali verbs consisting mainly of /kOra/ 'to do', /hOwa/ 'to be/to happen/to become', /dewa/ 'to give', /newa/ 'to take' and a few others. In this article, we will call these particular instances of code-mixing as Benglish verbs. As we can see in (3)(4)(5)(6)(7)(8)(9)(10)(11), some Benglish verbs are conjunct verbs (3-9), whereas some others (10-11) are compound verbs, which, unlike Bengali compound verbs, take a flexional verb as their pole (10)(11).3a. /EksiDenT kOra/ (accident-do) or 3b. EksiDenT hOwa (accident-be) 'to have an accident' 4. /ribhEnj newa/ (revenge-take) 'to take revenge' 5. /grup kOra/ (group-do) 'to put (things/persons) in a group' 6. /Ofish (or Ofis) kOra/ (office-do) 'to work in an office' 7. /slow kOra/ (slow-do) 'to make slow' 8. /kOmpaTibOl hOwa/ (compatible-be) 'to be compatible' 9. /in kOra/ (in-do) 'to get/come/put in' 10. /kOnfuz kOra/ (confuse-do) 'to confuse' 11. /jasTify kOra/ (justify-do) 'to justify'If one takes the idea of the headedness of words as valid, one can note that like other complex predicates, Benglish verbs also manifest the head-final syntactic pattern of Bengali, not the head initial pattern of English. Hence, the pole precedes the vector in (4), whereas in the English match of the verb: /take revenge/, the pole /revenge/ follows the vector /take/. As with any other simple or complex predicates in Bengali each Benglish verb has its own subcategorical features. For example, /EksiDenT kOra/ (3a) and /EksiDenT hOwa/ (3b)  the two Benglish matches of the English verb 'to have an accident' require their agent nouns to be case-marked differently, the former with nominative (marked with zero affix) (12), and the latter with genitive (13). We note that many Benglish verbs such as /ofish kOra/ (6) and /in kOra/ (9) have no verbatim English counterparts. In some cases, speakers can alternate between a conjunct verb (e.g. /obhijog kOra/ (14)) and its Benglish counterpart (e.g. /kOmplein kOra/ (15)). However, not all Benglish verbs (e.g. /ofish kOra/ (6) and /kOmpaTibOl hOwa/ (7)) can be replaced with a Bengali conjunct verb, and in many cases, the English pole cannot alternate with its Bengali counterpart. For example, */durghOTona kOra/ cannot replace (3a) although /accident/ is frequently used as a synonym of /durghOTona/. On the other hand, /Eksident howa/ (3b) and /durghOtona howa/ are synonyms. (Rik-Fahim-Gen-near-Gargi-Gen-against-complain-did) 'Rik has complained to Fahim against Gargi.' 15. Rik Fahimer kache Gargir biruddhe kOmplein koreche (Rik-Fahim-Gen-near-Gargi Gen-against-complain-did) 'Rik has complained to Fahim against Gargi.' In this article, we show how grammar can account for Benglish verbs, a particular type of complex predicate, which are constituted of an English word and a Bengali verb (e.g. /EksiDenT kOra/ &apos;to have an accident&apos;, /in kOra/ &apos;to get/come/put in&apos; or /kOnfuz kOra/ &apos;to confuse&apos;). We analyze these verbs in the light of a couple of models (e.g. Kageyama, 1991; Lieber, 1992; Matsumoto, 1996) which claim that complex predicates are necessarily formed in syntax. However, Benglish verbs like /in kOra/ or /kOnfuz kOra/ are problematic for these approaches because it is unclear how preposition in or flexional verb confuse can appear as the arguments of the verb /kOra/ &apos;to do&apos; in an underlying syntactic structure. We claim that all Benglish verbs can be satisfactorily handled in Morphology in the light of Whole Word Morphology (Ford et al., 1997 and Singh, 2006).
Enhanced Genre Classification through Linguistically Fine-Grained POS Tags * Text classification has been conventionally based on content matters and sentiment polarities. There are situations where genre classification is required for the identification of, for example, formal and informal sources of information. Genre classification of text is a process of classifying texts or documents according to the criterion of genre, such as style, form, or purpose, based on the assumption that "a document can be represented by the values of features that seem to express the attribute of a genre" ( Lim et al. 2005Lim et al. :1264. Part-of-speech (POS) tags have been employed in automatic genre classification in that they do not "reflect the topic of the document, but rather the type of text used in the document" (Finn and Kushmerick, 2003) and that their distribution has been observed to vary across different genres (e.g. Nakamura, 1993;Rayson et al., 2002). Nevertheless, a majority of past studies have included POS tags with other features to form a combined feature set. For example, Karlgren and Cutting (1994) included 6 POS tags (i.e. adverb, preposition, 2 nd person pronoun, 1 st person pronoun, noun and present verb) in classifying genres of the Brown Corpus. They carried out the classification tasks in terms of 2, 4 and 15 genre classes according to Brown categories. The combined feature set achieved an accuracy of 96%, 73% and 52% in the three classification tasks respectively. Dewdney et al. (2001) included POS tags of content words (i.e. noun, verb, adjective and adverb), where verbs were further defined in past, present and future tenses. Again, with a combined feature set, the performance of classifying 7 genre classes reached 92%. Eissen and Stein (2004) included 10 POS tags (i.e. noun, verb, relative pronouns, relative preposition, adverb, article, pronoun, modals, adjective and alphanumeric words) in classifying 8 genre classes. The performance of the combined feature set was 70%. Some other studies have not specified the POS tags, while they do report the performance using a combined feature set. For instance, Boese and Howe (2005) reported an accuracy of 79.6% when classifying 5 genre classes, and an accuracy of 74.8% for 7 genre classes. Lim et al. (2005) reported a much lower performance of about 38%. Still, some studies have treated POS tags as independent feature set for automatic genre classification. For example, Finn and Kushmerick (2003) used 36 POS features in subjectivity classification (3 genre classes) and review classification (2 genre classes), and achieved 84.7% and 61.3% accuracy respectively. More recently, Stein and Eissen (2008) used 10 POS tags to classify 8 genre classes and reported an accuracy of 74%. Santini (2004) further computed POS tags into unigram, bigram and trigram. When classifying 10 genre classes, POS trigram achieved the best performance with 82.6% accuracy, compared with 77.6% for bigram and 77.3% for unigram. The study also investigated 4 spoken and 6 written genre classes, and POS trigram again performed the best. To sum up, past studies have shown encouraging and suggestive results of using POS tags in genre classification, and yet there are some limitations. For example, it is difficult to evaluate whether POS tags are discriminatory features for a given classification task when they are included in a complex feature set. Limited studies have regarded POS tags as independent feature set. It is also noticeable that the number of genre classes is comparatively small.The current study introduces a new set of linguistically fine-grained POS tags generated by AUTASYS (Fang, 1996 and2007) for automatic genre classification. We will report in this paper an experiment designed to investigate the impact of the proposed feature set when compared and contrasted with word unigrams as a bag of words (BOW) and an impoverished POS tag set. Machine learning tools were used to evaluate the classification performance in terms of F-score. The British component of the International Corpus of English (ICE-GB; Greenbaum, 1996) was employed as a resource of different text genres. Ten different genre classification tasks were identified based on the existing ICE-GB categories, which are grouped according to different granularities. As our results will show, the use of linguistically rich POS tags as discriminative features produces superior accuracy when compared with BOW for finegrained genre classification. Our results will further demonstrate that the superior performance is due to the rich linguistic information since an impoverished tag set yielded worse classification results.The rest of the paper is organised as follows. Section 2 is a description of the methodology, covering the experimental setup, the genre resource, and machine learning tools. Section 3 explains the feature sets including the proposed linguistically fined-grained POS tags, bag of words and impoverished POS tags. Section 4 presents and discusses the experiment results from ten different genre classification tasks. Finally, section 5 draws some preliminary conclusions and suggests some future research. We propose the use of fine-grained part-of-speech (POS) tags as discriminatory attributes for automatic genre classification and report empirical results from an experiment that indicate substantial accuracy gain by such features over the conventional bag-of-words approach through word unigrams. In particular, this paper reports our research to investigate the performance of a fine-grained tag set when tested with the British component of the International Corpus of English. Ten different genre classification tasks were identified and the performance of the tags was evaluated in terms of F-score. Our results show that the use of linguistically fine-grained POS tags produces superior accuracy when compared with word unigrams, particularly for a rich set of 32 different genres with Naïve Bayes Multinominal Classifier. Through a comparison with an impoverished tag set, our results further demonstrate that the superior performance is due to the rich linguistic information embodied in the 400-strong different POS tags.
Decision Theory and Discourse Particles: A Case Study from a Large Japanese Sentiment Corpus There has been a recent surge of interest in the formal semantics and pragmatics literature on the topic of discourse particles (Zimmermann, to appear). Discourse particles straddle the border of semantics and pragmatics, and provide a perfect empirical domain for developing and challenging formal models of linguistic meaning. Discourse particles are, as the name implies, connected to the context of an entire discourse, and force the analyst to go above the sentence level and develop a theory of discourse contexts within which sentences and their associated particles are situated and interpreted.One problem for the development of formal theories of discourse particles is the fact that they typically make no truth-conditional contribution to the sentences in which they occur, and the contribution that they do make is typically very difficult to pin down. Most formal studies of particles rely on intuitionistic data from small sets of typically constructed examples. The ineffability and extreme context-sensitivity of discourse particles make it difficult to study them using corpora and other naturalistic data, in which the analysist is unable to control the discourse context and cannot probe the often subtle speaker intuitions that guide the use of these particles.Recently, a number of researchers have exploited large sentiment corpora to explore empirical regularities in the use of expressives and other emotionally-charged language (Potts and Schwarz, 2008;Constant et al., 2008;Davis and Potts, to appear). The structure of these corpora has allowed researchers to explore the use of these often ineffable items using large sets of naturalistic texts, on the basis of which empirical estimates of the expressive effects of this kind of language can be made. In this paper, I expand on this line of research by showing how sentiment corpora can be used as an empirical tool in the exploration of decision-theoretic analyses of the semantics and pragmatics of lexical items and constructions. I focus on a particular formal analysis of the Japanese sentence final discourse particle yo (Davis, 2009). This analysis builds on recent developments in decision/game-theoretic semantics and pragmatics (Parikh, 2001;van Rooy, 2003;Benz et al., 2005a). By testing the formal analysis with quantitative data from naturalistic texts, I demonstrate the utility of corpus methods for lexical pragmatics.In Section 2, I outline the decision-theoretic analysis of yo in terms of which the corpus data is analyzed. Section 3 introduces the sentiment corpus used in this paper, and explores the distribution of yo across ratings categories in this corpus. I show that yo occurs more frequently in more extreme reviews, and argue that this distribution falls out from the semantics presented in Section 2. The data is also consistent with other analyses, in particular ones in which yo contributes expressive meaning by indexing speaker emotionality. In Section 4 I present data suggesting that this alternative approach is insufficient. The discussion outlines ways in which the "expressive profiles" of lexical items in a sentiment corpus can emerge in several ways, so that the analyst must combine corpus data with other tools to arrive at the correct explanation for the distribution of a given item. In Section 5 I present evidence showing that yo tends to appear late in the text in which it is found, with a noticable bias toward text-final position. I argue that this fact falls out from the way that yo's denotation depends on the state of the post-update contextual common ground, rather than the information encoded by the sentence on which it appears. Section 6 concludes. The distribution and use of the Japanese particle yo is examined using a large annotated sentiment corpus. The data is shown to support a decision-theoretic account of yo&apos;s meaning (Davis, 2009). A decision-theoretic approach to the analysis of sentiment corpora is proposed, by which empirical predictions of decision-theoretic formal analyses can be tested using large sets of naturalistic data.
Finding Appropriate Subset of Votes Per Classifier Using Multiobjective Optimization: Application to Named Entity Recognition Named Entity Recognition (NER) is an important pipelined module in many Natural Language Processing (NLP) application areas that include machine translation, information retrieval, information extraction, question-answering, automatic summarization etc. Machine learning approaches are popularly being used for NER due to their flexible adaptation to new domains and languages. Most of the existing works in NER cover the languages such as English, European languages and some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. In India, there are 22 official languages that are inherited from almost all the existing linguistic families in the world. However, the works related to NER in Indian languages have started to emerge only very recently. Named Entity (NE) identification in Indian languages in general and Bengali in particular is more difficult and challenging compared to others due to facts such as: (i). missing of capitalization information, (ii). appearance of NEs in the dictionary with some other specific meanings, (iii). free word order nature of the languages, (iv). resource-constrained environment, i.e. non-availability of corpora, annotated corpora, name dictionaries, good morphological analyzers, part of speech (POS) taggers etc. Some of the recent works related to Bengali NER can be found in ( Ekbal and Bandyopad- hyay, 2009b;Ekbal and Bandyopadhyay, 2009a;Ekbal and Bandyopadhyay, 2008b). Other works related to Indian language NER are reported in the proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages (NERSSEAL) 1 .The concept of combining classifiers is a very emerging topic in the area of machine learning. The primary goal of classifier ensemble 2 is to improve the performance of the individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate of correctly classified individuals. But, the appropriate classifier selection for constructing an ensemble remains a difficult problem. Moreover, all the classifiers are not equally good to detect all types of output classes. Thus, in a voted system, a particular classifier should only be allowed to vote for that output class for which it performs good. Therefore, selection of appropriate votes per classifier is a very crucial issue. Some single objective optimization techniques like genetic algorithm (GA) has been used to determine the appropriate vote combinations per classifier (Ekbal et al., 2010). But, these single objective optimization techniques can only optimize a single quality measure, e.g. recall, precision or F-measure at a time. But sometimes, a single measure cannot capture the quality of a good ensembling reliably. A good voted classifier ensemble for NER should have its all the parameters optimized simultaneously. In order to achieve this, we use a multiobjective optimization (MOO) technique (Deb, 2001) that is capable of simultaneously optimizing more than one classification quality measures. Experimental results also justify our assumption that MOO can perform superior to the single objective approach for voting combination selection.The proposed MOO based voting combination selection technique is applied to solve the problem of Named Entity Recognition (NER). We use Maximum Entropy (ME) as a base classifier. Depending on the various feature combinations, several different versions of this classifier are made. The features include contextual information of the words, orthographic word-level features, semantically motivated feature and the various features extracted from the gazetteers. Thereafter, a MOO technique based on a popular multiobjective evolutionary algorithm (MOEA), nondominated sorting GA-II (NSGA-II) (Deb et al., 2002), is used to search for the appropriate voting combination selection. The proposed MOO based approach searches for an appropriate subset of predictions per classifier which are considered to be relevant enough in the process of final output selection.Our proposed technique is very general and can be applicable for any language and/or domain. Here, the technique is evaluated for a resource-constrained language, namely Bengali. In terms of native speakers, Bengali is the fifth popular language in the world, second in India and the national language in Bangladesh. Evaluation results show the effectiveness of the proposed approach with the recall, precision and F-measure values of 87.98%, 93.00%, and 90.42%, respectively. Results show the superiority of the proposed MOO based ensemble technique in comparison to the best individual classifier, two different baseline ensembles and a single objective GA based ensemble technique (Ekbal et al., 2010). These results are also supported by the sufficient statistical analysis.The remainder of the paper is organized as follows. The ME framework for NER is discussed briefly in Section 2. Section 3 describes in brief the definition of MOO and a popular way to solve this type of problem. The problem of vote based classifier ensemble is formulated under the MOO framework in Section 4. Section 5 describes different features that include contextual information of the words, several word-level orthographic features, semantic feature and various features extracted from the gazetteers. The proposed MOO based classifier ensemble selection approach is presented in Section 6. Section 7 reports the datasets, evaluation results and necessary discussions. Finally, Section 8 concludes the paper. In this paper, we report a multiobjective optimization (MOO) based technique to select the appropriate subset of votes per classifier in an ensemble system. We hypothesize that the reliability of prediction of each classifier differs among the various output classes. Thus, it is necessary to find out the subset of classes for which any particular classifier is most suitable. Rather than optimizing a single measure of classification quality, we simultaneously optimize two different measures of classification quality using the search capability of MOO. We use our proposed technique to solve the problem of Named Entity Recognition (NER). Maximum Entropy (ME) model is used as a base to build a number of classifiers depending upon the various representations of the contextual, orthographic word-level and semantically motivated features. Evaluation results with a resource constrained language like Bengali yield the recall, precision and F-measure values of 87.98%, 93.00%, and 90.42%, respectively. Experimental results suggest that the use of semantic feature can significantly improve the overall system performance. Results also reveal that the classifier ensemble identified by the proposed MOO based approach performs better in comparison to the individual classifiers, two different baseline ensembles and the classifier ensemble identified by a single objective genetic algorithm (GA) based approach.
A Computational Model of Language Generation Applied to Japanese Wh-questions Examination of a theory via a computational model can be an enlightening exercise. A computer requires precision. Thus, ambiguious and/or conflicting rules can become immediately apparent, since these cannot be processed by a computer. While we acknowledge that the ability to model a theory on a computer does not necessarily correlate with the correctness of the theory, awareness of problems for a computational implementation of a theory and development of solutions to these problems can lead to a more refined and accurate theory. To this end, we developed a computational model of language generation in which sentences are derived from an underlying set of Lexical Items (LIs), via a process of selection and Merge, following a version of Phase Theory (Chomsky, 1999(Chomsky, , 2000(Chomsky, , 2004(Chomsky, , 2006), the most recent implementation of work in the Minimalist Program (Chomsky, 1995b).In Phase Theory, a sentence is constructed via a bottom-up process whereby LIs are selected from a numeration, which consists of subnumerations, and Merged together. A derivation is broken up into phases -v* (transitive v) and C (also possibly D) are phase heads. A derivation is subject to the Phase Impenetrability Condition (PIC), which determines when chunks of a derivation are sent to Spell-Out (or to Transfer). Crucially, any elements contained within a chunk of a derivation that has been sent to Spell-Out are not accessible to higher operations. The PIC has been given (at least) two formulations (cf. Grewendorf and Kremers (2009)). In one version (Chomsky 1999(Chomsky , 2000), when v* is reached in (1), the complement of v*, which is VP, is sent to Spell-Out, and when C is reached, C's TP complement is sent to Spell-Out. In another version of the PIC (Chomsky 2004), in (1), when C is reached, VP, which is the complement of v*, is sent to Spell-Out. Within Phase Theory, a head with an uninterpretable feature functions as a probe that Agrees with a goal (if present) that has a matching interpretable feature. Crucially, the probe and goal must be accessible to each other -a probe cannot Agree with a goal that has been sent to Spell-Out.Various complex issues arise in Phase Theory. First of all, elements must be selected from a numeration. For example, it is assumed that v* selects for V and V selects for DP, but why this is the case is not necessarily clear. Also, issues arise with respect to the relationships between phases, subnumerations, and the timing of Spell-Out. A subnumeration can consist of LIs that form a phase, or it can consist of LIs that form a subject or adjunct, which can have a complex structure and must be formed outside the main spine of a derivation (cf. Johnson (2002)). Under the PIC, a phase edge remains visible to a higher phase. Thus, there is an apparent imperfection -the lack of a one-to-one correspondence between sending an element off to Spell-Out and phase-hood. Furthermore, the PIC creates problems for notions of movement. In (2a), the wh-phrase 'what' must move through an intervening v*P phase edge. Similarly, in the Japanese (3a), assuming that the Q particle 'no' undergoes movement (see section 2), it too must move through an intervening v*P phase edge. To deal with this issue, Chomsky (1999) suggests that a phase head can optionally have an EPP feature that attracts a wh-phrase. This means that an EPP feature must be present in every phase head that intervenes between the base and scope positions of a wh-phrase in constructions such as (2-3a). Thus, v* must have an EPP feature, as shown in (2b-3b), that attracts the wh/Q-element to the v*P edge (copies of moved phrases are italicized).However, if a derivation proceeds in phases, and a lower phase is not 'aware' of the contents of a higher phase, then it is not clear how an intervening phase head can 'know' that it requires an EPP feature (cf. Felser (2004), Bo˘ skovi´cskovi´c (2007)). Furthermore, even if v* were to have an EPP feature, the subject in Spec,v*P should eliminate this EPP feature when it is Merged, thus leaving no EPP to attract the wh/Q-element.In order to develop a more refined version of Phase Theory that eliminates the above mentioned imperfections and accounts for how sentences are generated, we created a computer model, implemented in the Python programming language, that applies a set of algorithms in order to generate a complete derivation of a sentence from an underlying numeration. Notably, we provide an explicit model for how LIs are selected from a numeration and Merged into a derivation. We also present a unique view of the timing of Spell-Out of phases in which a) there is a one-to-one correspondence between phase-hood and being sent to Spell-Out, and b) there is no need for optional EPP features that simply exist to bring a phrase to an edge of a phase. We explain how our model works with respect to a 'simple' wh-question in Japanese, and we demonstrate how our model leads to some interesting insights into how language is generated by the human mind. This paper discusses a computational model of language generation, based on work in Phase Theory, that attempts to shed light on how the human mind generates sentences. This model presents explicit algorithms that a) determine selection and merger of Lexical Items, b) determine the labels of Merged elements, c) account for movement of Lexical Items within a derivation, and d) account for when chunks of a sentence are sent to Spell-Out. We demonstrate how this model accounts for generation of a wh-question in Japanese.
Chains in Syntax and Morphology In current linguistic research, the vast majority of work assumes that constituency is the ordering principle of syntax and morphology. Items that belong together form constituents. If items are understood as belonging together, but do not appear together, one assumes displacement of (at least) one item. This is then explained by movement (internal merge) in syntax, or by lowering in morphology.Evidence from three areas in syntax, however, poses significant problems for a constituencybased analysis. These areas are ellipsis, idiom formation, and predicate verb complexes Examples of these phenomena are given in section 2.1.In morphology, bracketing paradoxes in particular have shown considerable resistance to constituency-based analyses. Further, multiple auxiliary constructions challenge the constituency position. These areas are taken up in section 2.2. This paper argues that languages need not necessarily build constituents, but rather chains. 1 The concept of the chain is introduced in section 3. In section 3.1, chains are applied to the syntactic phenomena introduced in section 2.1. Section 3.2 shows that chains equally apply in morphology. The bracketing paradox examples need to form proper chains, not proper constituents. Multiple auxiliary constructions also form chains, not constituents.The paper concludes that, based on the evidence presented, the constituent may not be the central structural unit of syntax and morphology, but rather the chain is. If the aforesaid holds, research in syntax and morphology, but also in language acquisition and computer-based language modeling, should start to at least ponder the possible existence of a more central unit, and what this could mean for theory formation. This paper argues for the existence of a deeper and more primitive structural unit of syntax and morphology than the constituent. Data from ellipsis, idiom formation, predicate complexes, bracketing paradoxes, and multiple auxiliary constructions challenge constituency-based analyses. In chain-based dependency grammar, however, constituents are seen as complete components. Components are units that are continuous both in the linear and in the dominance dimension. A unit continuous in the dominance dimension is called a chain. Evidence suggests that chains constitute the fundamental structural relationship between syntactic and morphological units, and that constituents are just a special subset of chains. If these assumptions are correct, linguistic research may need to change direction.
Feature Subset Selection Using Genetic Algorithm for Named Entity Recognition Named Entity Recognition (NER) is a well-established task that has immense importance in many Natural Language Processing (NLP) application areas such as Information Retrieval, Information Extraction, Machine Translation, Question Answering and Automatic Summarization ( Babych and Hartley, 2003;Nobata et al., 2002) etc. The objective of NER is to identify and classify every word/term in a document into some predefined categories like person name, location name, organization name, miscellaneous name (date, time, percentage and monetary expressions etc.) and "none-of-the-above".The main approaches to NER can be grouped into three main categories, namely rule-based, machine learning based and hybrid approach. Rule based approaches focus on extracting names using a number of handcrafted rules that yield better results for restricted domains; and are capable of detecting complex entities that are difficult with learning models. These types of systems are often domain dependent, language specific and do not necessarily adapt well to new domains and languages. Nowadays, researchers are popularly using machine learning approaches for NER because these are easily trainable, adaptable to different domains and languages as well as their maintenance are also less expensive. The main shortcoming of machine learning approach (particularly, supervised systems) is the requirement of large annotated corpus in order to achieve reasonable performance. Thus, building NER systems using machine learning approaches for the resource constrained languages is a great problem. In hybrid systems, the goal is to combine rulebased and machine learning based techniques, and develop new methods using strongest points from each one. Although, hybrid approaches can attain better result than some other approaches, but the weakness of rule-based system still exists when there is a need to change the domain and/or language of data. In this paper, genetic algorithm (GA) is utilized to search for the appropriate feature combination for constructing a maximum entropy (ME) based classifier for named entity recognition (NER). Features are encoded in the chromosomes. The ME classifier is evaluated for the 3-fold cross validation with the features, encoded in a particular chromosome, and its average F-measure value is used as the fitness value of the corresponding chromosome. The proposed technique is evaluated for determining the suitable feature combinations for NER in three resource-constrained languages, namely Bengali, Hindi and Telugu. Evaluation results show the effectiveness of the proposed approach with the overall recall, precision and F-measure values of 71.27%, 83.95% and 77.09%, respectively for Bengali, 74.72%, 87.15% and 80.46%, respectively for Hindi and 60.91%, 94.15% and 73.97%, respectively for Telugu.
Developing Punjabi Morphology, Corpus and Lexicon Punjabi is an Indo-Aryan language, widely spoken in the Punjab region of south Asia. It is spoken by 88 million people (Lewis 2009), which makes it approximately the 13th most widely spoken language in the world. Furthermore, it is the most widely spoken language of Pakistan as regards the number of native speakers.Punjabi is normally written in two scripts: Gurmukhi (in India) and Shahmukhi (in Pakistan). The resources reported in this paper are developed using Shahmukhi. Shahmukhi is a variant of the PersoArabic scripts. Therefore it possesses most of their inherent characteristics such as right to left writing, an optional use of diacritic marks and short vowels being not considered as letters of their own but applied above or below a consonant by using appropriate diacritics. It has 49 consonants, 16 diacritical marks and 16 vowels; see Malik (2006) for a detailed account of both scripts.In this paper, we report three important resources which are building blocks for various language technology tasks ranging from part of speech tagging to machine translation.First, we report an implementation of inflectional morphology for Punjabi which is described in Section 3. We use GF ( Grammatical Framework, Ranta 2004) which is a framework for developing multilingual grammar applications. GF also provides a built-in morphological analyzer and generator for the implemented languages. An overview of GF is given in Section 2.Second, we report the development of a corpus containing 0.9 million words (941,284), which is collected partly from Wikipedia, as described in section 4.Finally, we report a lexicon of 13,600 words (named entities:63%, lemmas of inflected words:37%; a lemma is also known as a dictionary form or a base form.). It is automatically extracted from the corpus and manually cleaned, as reported in Section 4. We have used a method proposed by (Forsberg et al., 2006) for this extraction. We describe our procedure in section 5.We release these resources as open-source 1 . Their importance is further increased by the fact that Punjabi is an under resourced language. For these components, our source of study is the Mahji dialect of Punjabi which is spoken in and around Lahore and is considered to be the standard form (Akhtar 1999:10). We used simple Roman 2 transcription for Punjabi. GF, Grammatical Framework (Ranta 2004), is a programming language for multilingual grammar applications. It is related to LKB 3 , XLE 4 as for its purpose, but based on functional programming and type theory. GF provides a built-in morphological analyzer and generator for the implemented languages. Since GF is a typed functional programming language, it uses similar techniques to those used by Zen toolkit (Huet 2005) and Functional Morphology (Forsberg and Ranta 2004) for its morphology component. We describe an implementation of morphology, development of a corpus and building of a lexicon for Punjabi language. Such resources are building blocks for various language technology tasks ranging from part of speech tagging to machine translation. Their importance is further increased by the fact that Punjabi is an under resourced language. We release these resources as open-source.
Word Order and NP Structure in Korean: A Constraint Based Approach It is well-known that prenominal expressions in Korean display a high flexibility in their distributional possibilities. For example, the prenominal expression 'honest' can either precede or follow a determiner:(1) a.ku chakha-n haksayng 'the honest student' the honest-MOD student b. chakha-n ku haksayng 'the honest student' honest-MOD the student Such prenominal expressions can be largely classified into two: deterministic ones and phrasalprenominal ones. 1 The deterministic prenominals, which we call determinants here, roughly correspond to determiners in English (cf. Jackendoff 1977, Huddleston andPullum 2002 for English). These lexical determinants can be classified as follows:(2) a.characteristic: say 'new', hun 'old', yes 'old', ttan 'other' ... The flexibility of Korean NP structures has been well-observed, but there have been few attempts to provide precise syntactic structures. This paper first reviews the basic distributional properties of Korean prenominal expressions as well as constraints in ordering, and then sketches a constraint-based, lexicalist analysis for Korean NP structures. Arguing for surface-based syntactic structures with more flexible subcategorization requirements, the paper shows that this lexicalist-based analysis can in a simple manner capture the flexible orderings of prenominal expressions as well as generate proper and precise NP structures, without resorting to functional projections.
Evidentials and Epistemic Modal in Korean -Evidence from their interactions * Recently the observational evidential -te in Korean became a hot issue particularly when J.  made the distinction clear between inferential evidential reading with PAST and direct evidential reading with PRES in tense. She offers a modal analysis of this typologically interesting evidential, on the basis of modal subordination evidence. But here I argue that this evidential category must be distinguished from an epistemic modal category of conjecture -keyss and of certainty thulim-eps-i 'certainly' (or thulim-eps-'certain').The perceiver of sensory and psychological observation involved (senses of sight, hearing, smell, taste, touch, weight and feelings of dizziness, etc.) in the evidential -te sentence is the speaker of the sentence. If the -te sentence ends with the reportative evidential -tay/-ray, however, the perceiver switches to the report source, not the speaker of the whole utterance.Strikingly, if the same -te co-occurs with the sequential/causal connective -(u)ni, the socalled non-equi subject constraint breaks down and the first person is required with PAST. The talk thus addresses various interactional aspects of -te with epistemic modal, with reportative evidential, and with a sequential/causal connective. Recently J. Lee (2010) made the distinction clear about the evidential-te in Korean between inferential evidential reading with PAST and direct evidential reading with PRES in tense. She offers a modal analysis of this typologically interesting evidential. I argue here that this evidential category, though with some modal force, must be distinguished from an epistemic modal category of conjecture-keyss. If both elements co-occur in one sentence the certainty of the event involved certainly decreases (Pi-ka o-ass-keyss-te-ra &apos;It might have rained, inferring from my observation&apos;) because of the doubly modalized situation. But evidentials primarily show information source rather than certainty/truth of propositions. The perceiver of sensory (visual) observation involved in the evidential-te sentence is its speaker but if the-te sentence ends with the reportative evidential-tay, then the perceiver is not equal to the speaker. Subject constraints and interactions are examined. If-te occurs in a non-final clause ending with the sequential/causal connective-(u)ni, the PRES clause denotes sequential relation with the third person subject, whereas the PAST clause denotes causal relation based on the internalized inferential result-experience as cause and the resulting consequence in the final clause with the first (and often third, but not second) person subject.
Implementation of Korean Syllable Structures in the Typed Feature Structure Formalism Since Bird &amp; Klein (1994), there have been many trials to implement phonological processes within the typed feature structure formalism. Most of them have been conducted within Headdriven Phrase Structure Grammar (HPSG;Pollard &amp; Sag, 1994;Sag &amp; Wasow, 1999;Sag et al. 2003).The goal of this paper is to provide computational implementations for Korean syllable structures within the typed feature structure formalism. The implementational system that we adopted in this paper is the Linguistic Knowledge Building (LKB) system (Copestake, 2002). In this system, we first implemented the type hierarchies for the two types segment and suprasegment. The types consonant and vowel were included under the type segment. Various different types were included under the type suprasegment for syllable structures. Then, we provided rules for syllable structures. It has been known that the syllable structures in Korean are different from those in English. Unlike English syllable structures, it has been known that onset and nucleus form a core, and core and coda form a syllable in Korean. We first provided the implementational rules for onset, nucleus, and coda first; and then we provided rules for core and syllable. Finally, we used the type phon-word in which all the syllables were combined within a phonological word. We also employed the type nf in order to solve the ambiguity problems during the parsing processes. It has been known that the syllable structures in Korean are different from those in English. The goal of this paper is to provide computational implementations for Korean syllable structures in the typed feature structure formalism. The system that we adopted in this paper is the Linguistic Knowledge Building system. We first implemented the type hierarchies and AVMs for segment and suprasegment. The types consonant and vowel were included under the type segment, and the various different types were included under the type suprasegment for syllable structures. Then, we provided the rules for syllable structures. Unlike English syllabification, it has been known that onset and nucleus form a unit in Korean, which is called core. Accordingly, we provided the rules for onset, nucleus, and coda; then, the rules for core and syllable to combine segments into syllable structures. This paper also employed the type nf to solve the ambiguity problems.
Focus Types and Subject-Object Asymmetry in Korean Case Ellipsis: A New Look at Focus Effects * Particle ellipsis is the phenomenon whereby speakers omit NP-final particles. One common type of particle ellipsis in Korean is case ellipsis, whereby case markers like -i/-ka and -(l)ul are omitted. An example of ellipsis of case markers is given in (1):(1) a. ecey Minswu-ka chinkwu-lul manna-ss-ta. yesterday Minsoo-Nom friend-Acc meet-Pst-Ind 'Minsoo met his friend yesterday.' b. ecey Minswu-ka chinkwu manna-ss-ta. yesterday Minsoo-Nom friend(-Acc) meet-Pst-Ind 'Minsoo met his friend yesterday.'In (1b), the object chinkwu 'friend' appears without the following accusative case marker -lul, which would normally indicate the object of the verb. While (1a) and (1b) are semantically equivalent, i.e., in both cases the agent is Minswu and the theme is chinkwu 'friend', they may differ in contextually determined meanings, pragmatic functions, and attitudes of interlocutors.It is often claimed that case ellipsis in Japanese and Korean is constrained by discourse and semantic factors such as focus and contrastiveness. A number of previous studies have suggested that case markers in Japanese and Korean cannot be dropped when the argument they mark is contrastively focused (Tsutsui, 1984;Masunaga, 1988;Yatabe, 1999;Ko 2000;D. Lee, 2002). However, these studies were not based on careful comparison of patterns of case ellipsis when subject and object are in focus. As observed by H. Lee (2009aLee ( , 2009b, the ellipsis of the case markers marking a focused direct object occurs much more naturally than that of the case markers marking a focused transitive subject, and in certain cases object case ellipsis is favored even though the object is contrastively focused.In this paper we will report a rating experiment which compared speakers' judgments of acceptability of sentences containing the case-marked or unmarked form of a focused subject and their judgments of sentences containing the case-marked or unmarked form of a focus object. We will first demonstrate that focus subjects exhibit a strong preference for explicit case marking over case ellipsis, whereas focus objects do not show such a preference. Our experiment further shows that focused objects show a stronger sensitivity to focus types, exhibiting a great difference in average ratings between sentences with three different subtypes of argument focus, whereas average ratings between sentences with the same three subtypes of focus did not show a statistically significant difference. We propose a new usage-based account of variable case marking that explains these asymmetries between focus subjects and focus objects in case ellipsis in terms of the usage probability of properties of arguments. In particular, it is shown that both the degree of acceptability of case ellipsis on focused argument NPs in Korean and the strength of the influence of focus types on case ellipsis correlate with the frequency in which the argument NP accommodates new information. Acceptability of case ellipsis on focused subjects and objects exhibits clear asymmetry which so far has not received a plausible explanation. Case ellipsis on focused direct objects occurs naturally, whereas case ellipsis on focused transitive subjects is unnatural whether the subject is contrastively focused or not. The main purpose of this paper is to provide experimental evidence that the degree of acceptability of case ellipsis on focused argument NPs in Korean is sensitive to the usage probability of their properties. Our experiment shows that the degree of acceptability of case ellipsis on focused argument NPs in Korean and the strength of the influence of focus types on case ellipsis both correlate with the likelihood for the argument&apos;s referent to be new information. We argue that this finding lends support to the view that language users&apos; intuitions of acceptability in context are probability-sensitive in that their preferences are affected by the usage probability of properties of argument NPs.
eSpaceML: An Event-Driven Spatial Annotation Framework *  This paper proposes eSpaceML as a representation scheme for annotating event-driven spatial expressions in natural language. It adopts SpatialML (MITRE, 2009) and ISO-Space (ISO, 2010) as a basis for the development of a novel, distributed spatial annotation scheme. SpatialML focuses on the annotation of spatial locations and their topological relations, while both ISO-Space and eSpaceML attempt to extend the scope beyond the treatment of toponyms. ISO-Space and eSpaceML also link space to events in various ways but with considerable differences. Unlike ISO-Space, which attempts to provide a self-contained framework for the various links, eSpaceML treats them in a distributed manner, operating as a pivotal system that refers to several other established annotation schemes such as MAF (ISO, 2008) and ISO-TimeML (ISO, 2009c) for morpho-syntactic as well as temporal-eventual annotations.
Domain-Independent Novel Event Discovery and Semi-Automatic Event Annotation Information Extraction (IE) techniques have been effectively applied to different domains (e.g. daily news, Wikipedia, biomedical reports, financial analysis and legal documents). A number of recent IE shared tasks (e.g. NIST Automatic Content Extraction Program (ACE 2005)) identify several common types of events. However, defining and identifying those types heavily rely on expert knowledge, and reaching an agreement among the experts or annotators requires a lot of human labor. Furthermore, annotating a high-quality event extraction corpus proves to be challenging because of highly-ambiguous and complicated event structures. The IE community has been aware of the limitation of this pre-defined event paradigm (e.g. Riloff, 1996; Yangarber et al., 2000;Grishman, 2001). Therefore a central track of IE research is the issue of portability -How can we automatically detect novel event types and rapidly annotate corpora for those types too, in order to alleviate the work load of human experts and annotators?Our hypothesis is that for each domain, there are typical event types that occur frequently, and thus should arouse our interest. For example, Transaction, Start-Organization events are most likely to appear in business domain; Justice events are likely to appear in law/criminal domain; Personnel (Start-Position and End-Position) event may appear frequently in politics and business domains according to the persons involved (President Obama: politics, Bill Gates: business). The general event types defined in the existing shared tasks are far from enough to satisfy the user needs in such domains.The first key problem is to automatically discover novel event types. In this paper we extract candidate event types based on event trigger clustering, and then rank these clusters based on their salience and novelty in the target unlabeled corpus (Section 3). After novel event types are discovered, it is beneficial to annotate arguments involved in such events. In this paper we demonstrate that such annotation can be realized in a semi-automatic way (Section 4). We take a new view of IE by considering it as a more fine-grained version of semantic role labeling (SRL). For each novel event type, we identify relevant and salient sentences involving new event triggers, and then correct errors based on uncertainty estimation, and finally map semantic roles into event argument roles based on semantic frame descriptions. We will demonstrate that our approach can make novel event discovery and annotation much more feasible (Section 5) by testing on three corpora from different domains: ACE, OntoNotes (Hovy et al., 2006) and a corpus of carbon sequestration literature ( Ji et al., 2010). Information Extraction (IE) is becoming increasingly useful, but it is a costly task to discover and annotate novel events, event arguments, and event types. We exploit both monolingual texts and bilingual sentence-aligned parallel texts to cluster event triggers and discover novel event types. We then generate event argument annotations semi-automatically, framed as a sentence ranking and semantic role labeling task. Experiments on three different corpora-ACE, OntoNotes and a collection of scientific literature-have demonstrated that our domain-independent methods can significantly speed up the entire event discovery and annotation process while maintaining high quality.
Developing an Online Indonesian Corpora Repository * Bahasa Indonesia, or just simply Indonesian, is spoken by well over 100 million people, and yet there is a proportionally small amount of available Indonesian language resources that would greatly support linguistics and language technology research. Recent work on Indonesian NLP resources and tools has started to bear results ( Adriani and Manurung, 2008), but to further advance research, there is a need for a comprehensive, balanced, and wide-coverage collection of corpora (Arka et al., 2007).Designing and building an online corpora repository is much more complex than simply uploading a set of text files onto a folder accessible over the Internet. For it to be of support to the research community, careful consideration must be paid to the design of standards, protocols, metadata, and architecture. In this paper we present two main desiderata that inform the design of our corpora repository design: the need to ensure sustainability and accessibility of the corpora (Section 2), and the enabling of open enrichment -through annotation-of the primary data (Section 3), before presenting our design and implemented prototype (Section 4). Simons and Bird (2008) state that for a language corpus to bring benefit, the design must take into consideration the following properties: This paper describes efforts to develop an online repository of Indonesian corpora-and its associated functions and services-that has been designed to support a wide variety of use cases and applications. Two design considerations are ensuring sustainability and accessibility of the corpora, and enabling open enrichment through annotation. The presented model supports OLAC-compliant metadata, is built atop an OAIS-compliant core repository, and exposes data and functionality via RESTful web services. A prototype implementation is presented, which allows users to upload, browse, and search the collection, whose extensible content model currently supports POS tagging. The future plan is for language-independent aspects of the system to be packaged up and released as an open-source package to aid the development of corpora repositories for other languages.
Topicalization and Truth Conditions: A Categorial Grammar Account * In Japanese linguistics, contrastive analyses of the topic and nominative markers has long received a great deal of attention, especially in terms of semantic or pragmatic notions like giveness/aboutness/contrastiveness/exhaustiveness, etc, whereas it seems that the truth conditional information they convey have been the subject of much less study. Though this paper examines truth conditional aspects of the use of the topic marker WA and nominative marker GA, I do not agree with the parallel treatment of WA and GA due to the fact that there are significant differences in syntax/semantics between these particles. Before presenting my analysis, let us review some properties of these markers.It has been agreed that sentences expressing categorical judgment (mostly, individual or kind-level sentences) strongly tend to have their subjects marked with WA (see papers in Kuroda 2003, among others), as illustrated in (1): (1) Saikin-no nihonjin-wa/*?nihonjin-ga se-ga takai. Recent-Gen Japanese-Top /Japanese-Nom height-Nom high. 'Recent Japanese are tall.' As often pointed out in the literature, WA marking is usually permitted only in matrix clauses (often called the root phenomena, see Heycock 2008), and even in matrix sentences expressing enduring properties of the subjects, question words answers corresponding to them can never be marked with WA (the complex form of which + N can be followed by WA, with contrastive connotations). (2) a. Dare-ga/*Dare-wa se-ga takai-no? Who-Nom/Who-Top height-Nom tall-Q 'Who is tall?' In this paper we will consider how the choice between the topic marker wa and nominative marker for subjects affects truth conditions of sentences. We derive the proper interpretations for sentences with topics and nominative subjects in terms of syntax-semantics interface. The categorial framework adopted here allows us to account for significant difference in meaning with respect to the choice of markers for subjects while maintaining the principle of strong compositionality.
Unsupervised Classification of Biomedical Abstracts using Lexical Association The RAMCORP project aims to design and construct a telephony-based dialogue system that provides interactive dissemination of knowledge in a variety of domains. In particular, it is intended for use by domain experts who are not native English speakers while engaged in dialogue with other domain experts. While the current focus is with respect to the translation of terminology (Webster et al., 2009), the supporting knowledge base is designed to be easily extensible and interoperable ( Fleissner et al., 2010); future iterations of the project will explore dialogue-based information retrieval and question answering in multiple domains. We aim to develop techniques that will automatically construct grammars for automatic speech recognition (ASR) that guide users through topics in various domains. Initially this will employ information automatically generated from articles and terminologies, but eventually we intend for dialogues to be driven by automatically constructed ontologies.A particular challenge faced by the project is the discrimination of a large number of terms through ASR (Fang et al., 2008); it is desirable to reduce the vocabulary size in order to improve recognition accuracy and promote user satisfaction. One approach to accomplish this is to organise terms according to topic and first query the user for their desired topic before proceeding with translation. An important step in this process is the classification of documents according to topic, so that the topic of terms may be inferred from the topic of documents in which they typically occur. This paper describes the acquisition of a corpus for use in topic classification experiments, our unsupervised approach to this task, and an evaluation of the efficacy of the approach.The current domain of experimentation and implementation is that of biomedicine, which is appealing due to the large potential user base and the extensive availability of electronic resources. Of particular interest is the Medical Subject Headings (MeSH), a controlled vocabulary thesaurus describing a hierarchy of concepts and related terminology in biomedicine. Section 2 describes both the MeSH and how we employed the hierarchy it describes to collect a corpus of biomedical article abstracts labelled with topic relevance scores.While the focus of this paper is with respect to the classification of biomedical articles, the project aims to be readily transferable to other domains. This requirement makes the use of supervised machine learning classifiers problematic and highlights the need for unsupervised learning, as obtaining and labelling training data for each new domain is costly. Instead, we explore an unsupervised method for text classification, wherein the relevance of a text to a particular topic is estimated using the constituent features' degree of association with a small number of manuallyselected prototypical features representing each topic. Section 4 presents our classification method in detail.Section 5 describes experiments that evaluate the performance of the unsupervised method using the biomedical abstract corpus, with respect to both classifying text according to their dominant topic and coring texts according to their relevance to topics. Section 6 discusses these results, presents conclusions and provides indications for future work. The task of text classification is the assignment of labels that describe texts&apos; characteristics , such as topic, genre or sentiment. Supervised machine learning techniques such as Support Vector Machines or the simple but effective Na¨ıveNa¨ıve Bayes have been successfully applied to this task. However, it is not always practical to acquire a sufficient corpus of labelled examples to train these methods. For these cases we describe an unsupervised method for text classification based on two hypotheses. Firstly, we propose that the class of a document may be determined by calculating its constituent features&apos; similarity with prototypical examples of each class. Secondly, we note the importance of class priors in Na¨ıveNa¨ıve Bayes classifiers, and hypothesize that class distributions might be estimated using the relative frequency of prototype words. Performing experiments on a corpus of biomedical abstracts with topic information derived from the Medical Subject Headings (MeSH), we investigate the characteristics of the method when used in conjunction with basic, linguistic and knowledge-based features, and find that the performance of the unsupervised method is approximately 80% that of Na¨ıveNa¨ıve Bayes. Our research is significant in that it highlights a candidate method with good potential for further improvement when training on unlabelled data.
Licensing Nominals in the Multiple Nominative Constructions in Korean -A Mereological Perspective One of the most controversial phenomena in Korean linguistics is the constructions in which two or more nominative case-marked NPs may occur in a clause headed by an intransitive predicate. They properly include the so-called Double Nominative Constructions (DNCs) and Multiple Nominative Constructions (MNCs). The former is characterized by the occurrences of two NPs marked with the nominative case-marker within a clause headed by an intransitive predicate. The latter is referred to the clauses containing three or more nominative case-marked consecutive NPs. Althought a large number of studies have been made on these constructions, there is still little agreement as to the nature of these constructions. One of the controversial issues is the issue why only a subset of the clauses containing three or more nominative case-marked consecutive NPs is grammatical and the issue whether or not the two constructions are grammatically related. If so, in what way? If not, why not? The questions raised above, in my opinion, may be answered by investigating the constraints on the licensing of nominals in the MNCs in Korean.The purpose of this paper is to tackle this licensing issue and to propose a set of licensing conditions from a mereological point of view. What I wish to show in this paper is that MNCs are cyclically formed only when the relationship between the two consecutive NPs satisfies one of the conceptual constraints including inclusion, possession and attribution. This paper investigates some important constraints on the licensing of nominals in the so-called Multiple Nominative Constructions (MNCs) in Korean from a mereological point of view, proposing a semantic relation hierarchy. The main idea advanced in this paper is that MNCs are cyclically formed only when the relationship between the two consecutive NPs satisfies one of the conceptual constraints including inclusion, possession and attribution. The inclusion constraints are further divided into meronymic relations, spatio-temporal relations and classificational relations. The meronymic relations integrate some essential ideas of the tradition of mereological thoughts. Some appealing consequences of this proposal include a new comprehensive classification of MNCs and a straightforward account of some long standing problems such as how the additional nominative NPs are licensed.
Combination of 3 Types of Speech Recognizers for Anaphora Resolution Speech understanding and dialogue systems have been developed for practical use recently. These systems often recognize user utterances incorrectly. It is important to deal with speech recognition errors for speech understanding systems. Extracting keywords and understanding an utterance using them reduce speech recognition errors (Bouwman et al., 1999;Komatani and Kawahara, 2000). Combining some recognizers is one of the best approaches to improve the accuracy of speech understanding systems (Isobe et al., 2007;Utsuro et al., 2004). Utsuro et. al. (2004) have obtained high accuracy by using some speech recognizers' outputs. However they dealt with word error reduction only. Although Isobe et. al. (2007) have proposed a multi-domain speech recognition system based on some domain-specific recognizers, their system cannot treat out-ofdomain utterances such as a chat between users. However chat utterances often include significant information as the context of the dialogue.In this paper we propose a simple and effective speech understanding method based on a large vocabulary continuous speech recognizer (LVCSR) and some domain-specific speech recognizers (DSSR). We call it "One Generalist and Some Specialists (OGSS) model". Figure 1 (a) shows the outline of the model. In our system, the LVCSR is the generalist, namely domain-independent, and the DSSRs are specialists, namely domain-dependent. We focus on the difference between outputs generated from the generalist and specialists. By using this method, we can recognize domain-dependent speech inputs with high accuracy and also handle context information in domain-independent speech inputs.The task of this system is speech understanding for a livelihood support robot. The DSSRs recognize particular utterances about orders; e.g., order utterances from elders who need care and order utterances from nurses. We construct the grammar-based DSSRs for order utterances with   a small vocabulary and high accuracy for each order type. We use the LVCSR for recognition of utterances that the DSSRs can not recognize, such as a chat between users. The information recognized by the LVCSR is of assistance for context construction of a dialogue. If we handle these different speech recognizers selectively and integratively, we realize a flexible and robust speech understanding method. Figure 1 (b) shows the effectiveness of the proposed multiple recognizer. The DSSR achieves the order recognition with high accuracy and the LVCSR supplies lack of information in the order utterances.In general, there are many anaphoric expressions in a dialogue. Anaphora resolution is one of the most important tasks for understanding the dialogue. In this paper, we also propose an anaphora resolution method in the multiple recognizer. By using previous outputs from the LVCSR and some DSSRs, we resolve an anaphora in the current output. For example, with respect to the utterance "Please pick it up" in Figure 1 (b), the system identifies that the word "it" in the utterance is the phrase "remote controller" which was recognized by the LVCSR in the previous utterance. The antecedent often appears in non-order utterances, that is outside of DSSRs. Therefore the target word is usually recognized by a LVCSR. However, the accuracy of the LVCSR is generally insufficient. The low accuracy of the detection of the antecedent in the speech recognition process leads to the decrease of the accuracy of the anaphora resolution process because the antecedent does not exist in the output of the speech recognizer. Here we apply a medium-scale DSSR to the multiple recognizer. It contains words of the target situation. In other words, the vocabulary of the medium-scale DSSR consists of the union of each small-scale DSSR, such as a nurse's order DSSR and a patient's order DSSR. By using the medium-scale DSSR, the accuracy of non-order utterances often improves. It leads to the improvement of the accuracy of the anaphora resolution method.In Section 2, we explain the basic idea of the multiple speech recognizer. In other words, it is to select an output from each recognizer. In Section 3, we describe an anaphora resolution method based on the combination of 3 types of speech recognizers. Then, we evaluate the method in terms of the output selection and anaphora resolution in Section 4. Finally we conclude this paper in Section 5. In this paper, we propose a method for anaphora resolution in speech understanding for a livelihood support robot. For robust speech recognition, we combine two types of speech recognizers; a large vocabulary continuous speech recognizer (LVCSR) and domain-specific speech recognizers (DSSR). One problem in the anaphora resolution is lack of the antecedent in the outputs. To solve the problem, we introduce 2 types of DSSRs; one medium-scale DSSR and several small DSSRs. In this paper, we describe the basic idea of our multiple speech recognizer first. The selection process in the recognizer is based on the similarity between the LVCSR and each DSSR. Then, by using the outputs from the LVCSR and the medium-scale DSSR, we resolve anaphoric expressions in the current output from a small-scale DSSR. The experimental result shows the effectiveness of our method.
Expanding Chinese sentiment dictionaries from large scale unlabeled corpus Sentiment analysis becomes more and more important as various user generated content(UGC) appears on the web, such as product reviews and personal blogs. Unsupervised sentiment classification has a great advantage that it does not need a large expensive labeled corpus but only a user defined sentiment dictionary. However, the current existing dictionaries(such as HowNet and NTUSD) are insufficient of their vocabularies.Many sentiment words are not included in the current Chinese sentiment dictionaries. For example, HowNet contains 3969 positive words and 3755 negative words; NUTSD contains 2648 positive words and 7742 negative words. Among them, only 669 positive words and 877 negative words are shared. Chinese idioms usually express strong sentiment, such as (to make great efforts to do sth.), (to win fame by cheating the world), however, they are not included in the dictionaries.Typical existing algorithms for sentiment dictionary extension use patterns to construct a graph that reflects the relation between words, clustering algorithm is then applied on this graph to get positive and negative clusters. The basic idea is that multiple sentiment words are usually used together at the same time in one sentence, and they express sentiment with same polarity. The intuition is that, if a word mostly occurred with positive/negative sentiment words based on the current dictionaries, then we can guess it is also a positive/negative sentiment word. For example, (tall and straight) is usually co-occurred with positive words: (tall), (style), (strong), (elegant), but rarely co-occurred with negative words, so we can guess that is also a positive word.In this work, we develop a new technique to deal with this problem. Similarly, by observing a large amount of sentiment word co-occurrences, we can construct a graph based on them, within which the polarity of some words that are defined in current sentiment dictionaries are known. So, our method is also based on a graph. However, we use syntactic analysis technology to get co-occurred word pairs. Meanwhile, we don't use clustering algorithm, because using patterns and clustering will both introduce errors. Then we compute a polarity strength for each word in the graph based on the link information and then rank all the words based on the strength. The top words that are mostly linked with positive words are likely to be positive words and bottom words that are mostly linked with negative words are likely to be negative words. The initial labels are given based on the current sentiment dictionary. The labels change during each iteration. Finally, the labels of the words will become stable.In all, compared with the existing methods of sentiment dictionary extension, our method first use dependency parsing to process the unlabeled corpus and extract coordinated word pairs to construct a word graph then use the link analysis techniques to compute a polarity strength for each word and rank the words rather than clustering them. Our experiments show that the dependency parsing contribute to construct a better sentiment dictionary and the polarity strength for each word is useful to improve the performance of sentiment classification task.The remainder of the paper is organized as follows. Section 2 is related work. Section 3 describes the main algorithm how to expand existing sentiment dictionaries. We conduct experiments to evaluate our technique in Section 4. Section 5 is the conclusion and future work. Unsupervised sentiment classification usually needs a user defined sentiment dictionary. However, the existing dictionaries in Chinese are insufficient, for example, the intersection rate of two popular Chinese sentiment dictionaries HowNet and NTUSD is less than 10%. In this paper, we present a method to help expand the dictionaries with more sentiment words by ranking them through link analysis based on a word graph constructed from a large unlabeled corpus. Meanwhile, our method could compute a sentiment polarity strength for each word in the new dictionaries. Manual evaluation has shown that our method has a high precision to expand the dictionaries. Experiments for sentiment classification have shown that the new dictionaries with the polarity strength for each word given by our algorithm are effective to improve the performance. As a byproduct, our algorithm could also discover the errors existing in current dictionaries.
Fault-Tolerant Learning for Term Extraction * Terms are the lexical units to represent the most fundamental knowledge of a domain. Term extraction aims to extract meaningful words or phrases representing domain specific meaning or concepts. Thus two issues are considered in term extraction. The first issue is to identify boundaries of meaningful words and phrases. The second issue is to verify terms by calculating domain specificity ( Kageura and Umino, 1996).Existing term extraction techniques can be divided into four main categories including statistics based measures, trigger words (or characters) based algorithms, domain knowledge based methods and supervised methods.The first category is statistics based measures which identify terms by their statistical significance. The most widely used statistical measurement is TF-IDF ( Salton and McGill, 1983;Frank, 1999), which is based on the hypothesis that "if a candidate occurs frequently in a few documents of a domain, it is likely a term". The co-occurrences between the target string and its components or context, referred to as Internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000), are used for term extraction. There are also studies evaluating the distribution of a term within a domain or across domains through different metrics, such as term representativeness ( Hisamitsu and Niwa, 2002), Inter-Domain Entropy ( Chang, 2005) and the Lexicon Set Algorithm ( Chen et al., 2006). Statistics based techniques can extract common used terms with statistical significance. However, the techniques are very sensitive to term frequency, and thus terms with low frequencies cannot be extracted.The second category is based on trigger words or characters. According to (Feng et al., 2004) and ( Yang et al., 2008), characters and words immediately before and after these terms are proven to be useful for term extraction. Accessor Variety Criteria proposed in (Feng et al., 2004) considers the characters that are directly before or after a string as important factors for determining the independence of the string. TE Del (delimiter based term extraction) proposed in ( Yang et al., 2008) identifies terms by finding their predecessors and successors as term boundary markers. Strings between delimiters are taken to be term candidates.The third category is based on some a priori domain knowledge such as a large domain lexicon. Nakagawa (2002) identified compound nouns as domain specific terms by measuring the domain specificity of the component words, which is determined by finding out whether they appear in the domain lexicon. But this method cannot deal with non-compound terms. TE Kno (knowledge based term extraction), proposed in (Ji and Lu, 2007) for Chinese, calculates the percentage of context words in a domain lexicon using both frequency information and semantic information. TE Kno also requires an existing domain lexicon for verification.Some supervised learning approaches have been applied to protein/gene name recognition ( Zhou et al., 2005) and Chinese new word identification ( Li et al., 2004) using Support Vector Machine (SVM) (Vapnik, 1995) which also require large domain corpora and annotations, and intensive training is needed for a new domain.As described before, different categories of term extraction techniques suffer from different problems. Statistics-based methods cannot identify terms without statistical significance since they are very sensitive to term frequency. Trigger word based algorithms, which use only limited features, are likely to extract certain kinds of terms but miss the others. Knowledge based algorithms and supervised methods rely heavily on both the size and the quality of domain knowledge or annotated training data which makes it difficult to be applied to a new domain.In this work, the Fault-Tolerant Learning (FTL) approach is proposed to overcome these problems. After automatically generating two sets of seeds based on two unsupervised algorithms, different classifiers are separately trained using different seed sets, followed by double checking for term verification. The proposed FTL approach extracts terms using automatically generated seeds instead of domain knowledge or annotated corpora. Thus it is applicable to any domain specific corpus. It is especially useful for knowledge-limited and resource-limited domains. Two classifiers are separately trained for prediction and verification which aims to improve the performance. Moreover, all the features are used in each classifier which makes it possible to cover more kinds of terms.The rest of the paper is organized as follows. Section 2 describes the proposed algorithms. Section 3 explains the experiments and the performance evaluation. Section 4 is the conclusion. This paper presents the Fault-Tolerant Learning approach for term extraction. The approach extracts terms using automatically generated seeds instead of prior domain knowledge or annotated corpora. Thus it is applicable to any domain specific corpus and it is especially useful for resource-limited domains. Two classifiers are separately trained for prediction and verification to ensure the performance of the proposed approach. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach.
Syntactically complex demonstratives and sortal inherency Usually when philosophers talk about "complex demonstratives" they talk about noun phrases of the form this/that CN, where CN stands for a common noun, possibly complex (Borg, 2000;Dever, 2001;King, 2001). There has been much debate recently on the semantic and pragmatic status of such constructions and their relation to other noun phrases in NLs. Some theorists (Braun,1994) claim that such expressions are directly referential in the sense of Kaplan. This means that strictly speaking they are not quantificational and are similar to other referential noun phrases. Other researchers take an opposite view: complex demonstratives are essentially quantificational and thus belong to the larger class of quantificational noun phrases (King, 2001). Arguments of King are criticised in (Altshuler, 2007). There are also views which although compatible with the demonstrative and quantificational positions, present complex demonstratives in a different light. Thus Roberts (2002) claims that they are just definites.Of course the discussion of demonstratives is unavoidably related to other problems in natural language semantics. One of the problems in the focus of the discussion concerning complex demonstratives is the semantic status of the common noun CN. In particular most researchers are concerned with the question of whether the sentence of the form This CN VP, completed by speaker demonstration, expresses a proposition if the object demonstrated does not have the property expressed by CN. Furthermore, is this situation similar to the more general situation of non-expressibility of proposition due to presupposition failure?I will leave aside many of the problems which the above debate brings up. The purpose of this article is twofold. First, I want to point out that from the empirical point of view the way philosophers talk about complex demonstratives is very restricted. I will show that natural languages use the usual means they have at their disposal to form syntactically complex expressions in forming syntactically complex demonstratives. It follows from this observation that the class of complex demonstratives that one has to consider is much larger than the class considered by philosophers.Secondly, I want to apply some results from the generalised quantifier theory to deal with the problem of the semantic status of the common noun in the subject NP in which a demonstrative occurs. In particular I will use the distinction between sortally inherent and not sortally inherent Copyright 2010 by Richard Zuber quantifiers to deal with this problem (cf. Keenan, 2000). The proposed analysis will apply to the whole class of syntactically complex demonstratives.In the next section I show why and how the class of complex demonstratives should be extended. Then, after presenting in the next section some formal tools from generalised quantifiers theory I apply some results from this theory to the discussion of the semantic status of the common noun. Two problems related to the analysis of noun phrases in which demonstratives occur are discussed: (1) it is shown that there exists an infinite number of syntactically complex demonstrative noun phrases and thus an infinite number of noun phrases which are neither purely referential not purely quantificational, (2) some problems concerning the semantic role of the common noun to which demonstrative apply can be treated within the generalised quantifier theory. This can be done using the distinction between sortally inherent and sortally not inherent quantifiers.
On Automated Hypernym Hierarchy Construction Using an Internet Search Engine Ontology is an ambiguous notion. In philosophy it goes back to Ancient Greeks and refers to the study of nature, or more generally, to the study of what might exist (Smith, 2004). In the context of computer science, terminological, information and knowledge anthologies (van Heijst et al., 1997) were introduced. Later two ontologies deal with concepts, while terminological ontologies are focused on natural language terms that may or may not be directly mapped into concepts. In contrast to information ontologies, knowledge ontologies use some formal languages to represent sematic relations between concepts.Terminological ontologies, such as WordNet, contain information about hyponym-hypernym (subtype-supertype, or IS-A relation), synonymy, and other semantic relations between terms. Such information can be used in variety of natural language processing applications, including query reformulation ( Jones et al., 2006;Chirita et al., 2007), text summarization ( Dang et al., 2008), categorization ( Li et al., 2009), query answering ( Lopez et al., 2007), and many others. Unfortunately, good quality ontologies are mostly manually created and they exist for a limited number of domains. Moreover, it is difficult to support an ontology for rapidly changing domains when new concepts or semantic relations frequently appear, so ontology construction and updating become a bottleneck for many applications. A possible solution to the above problem is an automatic or semi-automatic ontology learning.There is a lot of work has been done in the area of ontology learning during past two decades. While there exist many types of semantic relations between concepts and many different sources of information (e.g. databases, structured text, dictionaries) the problem of concept hierarchy learning from unstructured texts may be considered as the main topic in the area. An importance of this problem can justified by the fact that concept hierarchy forms a core of every ontology and that unstructured text is the most frequent data format.Automated ontology learning from text is a challenging task. It is well known that the set of concepts and terminology varies between different languages and cultures. For example, in many northen languages there exist a lot of terms describing specific conditions of snow, that can not be easily translated into other languages. Even in the same language some terms can be treated differently. For example, in the UK the word city means a settlement of high importance, differentiated from town or village by size, population density, or status. In the US almost all settlements are cities. One can find hundreds of Internet pages containing statements like "Canyon city is a small village".Most of the work on concept hierarchy learning can be divided into two classes. The first class consists of work that are based on Harris' distributional hypothesis (Harris, 1968) which states that semantically similar terms tend to occur in similar contexts. For instance, one can expect that contexts of words car and automobile will contain many common words. Although the semantic similarity can be measured by means of statistics it is hard to describe the reason of high similarity. An example of statistical approach application to ontology learning is (Sanderson and Croft, 1999) which based on subsumption hypothesis, stating that the set of documents where two terms co-occur is a subset of the set documents where their hypernyms co-occur.The second class of work, which the current work belongs to, uses lexical patterns (Hearst, 1992) that reflect specific semantic relations between terms. In particular, in order to estimate IS-A relation between terms A and B (e.g. hyponym-hypernym) one can search a corpus for "B is a A", "B is a kind of A", or "B, C and other As". If such expressions exist in a corpus then one can assume that term A is a hypernym of B and C. Similar patterns were found for PART-OF relation (Berland and Charniak, 1999;Girju et al., 2003). This approach is based on the assumption that in a sufficiently large corpus one can find a "definition" of term B written in one of above forms. The set of semantic pattern can be either manually coded, or automatically constructed by means of some machine learning technique. In contrast to statistical approaches, lexical patterns may deduce a hypernym from single occurrence. It is worth noticing that lexical patterns approach outperforms many statistical methods for semantic similarity measurement as well ( Bollegala et al., 2009). This is a surprising result because it is not evident what lexical patterns can reflect similarity between terms like nail and hammer, or group and homomorphism.Lexical pattern approach suffers from two problems, namely, sparsity of patterns in real life texts (which is also a strong point), and noisy output. There are hundred thousand pages containing the pattern "robbery is a", but only a hundred of pages containing the pattern "oroshi is a". In the later case there exist a page with the correct definition, but it is given in a different form: "Oroshi is a word with a very specific meaning: wind blowing down from the mountain". Such snippet will produce an erroneous hypernym oroshi -word, that can be considered as a noise. For more rare terms no required patterns could be found at all. A combined "pattern-statistical" approach was very recently proposed in (Drumond and Girardi, 2010).Our goal is development of a method for automatic hypernym learning from natural texts. Using lexical patterns approach and an Internet search engine as a mean for large corpus access we compute a set of candidate hypernyms of a given seeding term. For each candidate hypernym we compute the set of possible hypernyms (that are in some sense are neighbors of the seeding term) using the so-called double anchoring hyponyms discovery ( Kozareva et al., 2008). Finally we apply a filter that removes noisy seeding term's hypernyms. The filtering technique is based on the assumption that the set of hyponyms of a correct hypernym of the seeding term should contain tarms that are sematically similar to the seeding term. This is performed by means of statistical semantic similarity measurement and clustering. It is worth noticing that we do not require that all hyponyms should be semantically similar, so the noisy outut that can be generated by both double anchoring hyponym discovery and statistic similarity measure is not a problem.The structure of the paper is the following. In the next section we briefly recall some useful results on applications of lexical patterns to ontology learning and semantic similarity measurement. In Section 3 the proposed approach is described. Section 4 contains evaluation results and in the last section we discuss the results and possible directions of future work. In this paper we propose an approach for automatic construction of concept hierarchies from the snippets returned by Internet search engines using a number of well known techniques. We use surface lexical patterns to construct a set of candidate hypernyms of a given term and additional filtering that is based on both lexical patterns and distributional analysis. Preliminary experimental results for real life English examples are presented.
An Ontological Analysis of Japanese and Chinese Kinship Terms* In a previous work (S Baik and H-R Chae 2010), we tried to provide an ontological analysis of Korean kinship terms. We dealt with about 200 Korean kinship terms extracted from the Yonsei Korean Dictionary, under a framework comprising a simplified family tree and some features. We have shown that the framework is effective in defining Korean kinship terms explicitly and comprehensively. In addition, as the framework was designed to be language neutral, the possibility was open that it could be used in analyzing the kinship terms of other languages.In this paper, we will first introduce the framework of S Baik and H-R Chae (2010). Then, we will provide an ontological analysis of Japanese and Chinese kinship terms. We will deal with all the Japanese and Chinese kinship terms in Kodansha's Furigana Japanese Dictionary and A Chinese-English Dictionary (by Learning Express). We will see that the framework is very effective in dealing with the Japanese and Chinese data. This will show that the ontological framework has advantages over previous ones. Most of all, the framework makes it easy to capture similarities and differences between kinship terms in various languages.The organization of this paper is as follows. Section 2 provides an overview of some previous studies on kinship terms. In section 3, we will introduce an ontological framework for the analysis of kinship terms, together with actual analyses of Japanese and Chinese data. Section 4 concludes the paper. Most languages have some expressions to refer to family members (e.g., those referring to &apos;father,&apos; &apos;brother,&apos; &apos;uncle,&apos; etc.). In this paper, we will provide an analysis of Japanese and Chinese kinship terms, under a framework whose representational system has an ontological nature. It will be shown that this framework is effective not only in figuring out similarities and differences among the kinship terms of a particular language, but also in comparing the kinship terms of different languages.
GRASP: Grammar-and Syntax-based Pattern-Finder for Collocation and Phrase Learning Many language learners' queries (e.g., "play" and "role") are submitted to language-learning tools on the Web every day and an increasing number of services on the Web specifically target second language learning. For example, Word Sketch Engine (www.sketchengine.co.uk) is a concordancer that automatically summarizes a word's grammatical and collocational behavior while services such as TANGO 1 and MUST 2 provide a means of collocation finding (e.g., verbnoun and adjective-noun collocations) and collocation correcting.Language-learning tools such as Word Sketch and TANGO typically accept only one querying word and retrieve sentences with it or words it co-occurring probabilistically more frequently than usual. However, learners may attempt to learn the usage of a certain word sense of the query word.Consider the polysemy "play". In WordNet, it has a myriad of senses including 'participating in games or sport', 'act or having an effect in a specified way' and 'play on an instrument'. Learners may be aware of its senses but intend to acquire more knowledge on the context or usage of the word sense 'act or having an effect in a specified way'. Suggested by (Yarowsky, 1995), accompanying "play" with its collocate "role" is a good way to narrow down the senses of "play" (and vice versa). Therefore, multi-word query might be as important in language learning. However, the best response to the multi-word query "play role" is probably not an overwhelming set of sentences with it which may be returned by general-purpose concordancers and search engines (e.g., Google). A good response might indicate that the collocation "play role" is frequently followed by the grammatical part-of-speech (PoS) patterns 'preposition determiner' (e.g., "in the" and "in this"), 'preposition noun' (e.g., "in society" and "in relation") and 'preposition gerund' (e.g., "in determining" and "in shaping"), and preceded by the patterns 'noun auxiliary_verb' (e.g., "communication will" and "confidence will") and 'adjective noun' (e.g., "voluntary groups" and "foreign aid") and that "play" and "role" are usually separated by the grammatical patterns 'article adjective' (e.g., "an important" and "a major"), 'determiner gerund' (e.g., "the leading" and "the supporting") and 'adjective' (e.g., "significant" and "crucial"). Intuitively, these PoS patterns provide a general idea on how the querying terms are usually used in context. Figure 1. An example GRASP response to the query "play role". We present a new system, GRASP, that automatically learns to extract representative grammar-based patterns of the querying collocations/phrases. An example GRASP responses to is shown in Figure 1. GRASP has determined the sentences containing the query's words of a specific underlying corpus (e.g., British National Corpus). GRASP learns these word-tosentence mappings during corpus preprocessing. We describe the GRASP preprocessing process in more detail in Section 3.At run-time, GRASP starts with a collocational/phrasal query submitted by language learners (e.g., "play role"). GRASP then identifies the sentences with the words in the query within proximity and retrieves the grammatical patterns commonly occurring before, after, within the query. In our prototype, GRASP returns patterns together with statistical analyses to users directly (see Figure 1); alternatively, the statistics returned by GRASP can be used as reference to automatically extract possible phrases regarding the query words (e.g., "make up one's mind" concerning the query words "make up" or "make mind"). We introduce a method for learning to find the representative syntax-based context of a given collocation/phrase. In our approach, grammatical patterns are extracted for query terms aimed at accelerating lexicographers&apos; and language learners&apos; navigation through the word usage and learning process. The method involves automatically lemmatizing, part-of-speech tagging and shallowly parsing the sentences of a large-sized general corpus, and automatically constructing inverted files for quick search. At run-time, contextual grammar patterns are retrieved and presented to users with their corresponding statistical analyses. We present a prototype system, GRASP (grammar-and syntax-based pattern-finder), that applies the method to computer-assisted language learning. Preliminary results show that the extracted patterns not only resemble phrases in grammar books (e.g., make up one&apos;s mind) but help to assist the process of language learning and sentence composition/translation.
Simpler Is Better: Re-evaluation of Default Word Alignment Models in Statistical MT A majority of state-of-the-art statistical machine translation (SMT) systems operate with multiword units but still use word alignment as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of ( Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systemsto extract the synchronous grammar rules.The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model ( Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known implementation of the models, GIZA++, is derived from (Och and Ney, 2003) and involves training the following models in sequence: IBM model 1, HMM-based model, IBM model 3 and IBM model 4.In this work we show that a simpler alignment model introduced together with HMM-based alignment in ( Vogel et al., 1996), but discarded due to worse alignment error rate, results in essentially the same translation quality like HMM-based alignment in almost all cases -i.e. the relative-distortion IBM model 2. At the same time, it does not include a first-order dependency of the alignment, which means that it is much simpler to implement and train it. The experiment results also support the common knowledge that HMM-based alignment works just as well or sometimes better than IBM model 4, usually used by default.In the following section we review the related work on the link between the word alignment quality and translation quality. Section 3 consists of a theoretical overview of different aspects of the word alignment task in the models in question. In section 4 we present the experiments with simpler default and alternative models on translations from Chinese, Czech, Estonian, Finnish, German and Korean into English and back. Although several recent studies have shown that alignment quality is a poor indicator of the resulting translation quality, the word alignment models currently considered to be default (the so-called IBM models and HMM-based alignment) have been evaluated using the alignment error rate. We argue that from a machine translation perspective it makes sense to use simpler alignment models. Here we show that not only do the sequential models result in the same or better translation quality, but even from the set of sequential alignment models simpler ones can match the performance of the HMM-based model, whereas using computationally less expensive and faster algorithms to train and align new sentence pairs. Empirical evaluation is performed on a phrase-based and a parsing-based translation system.
The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme * Textometry is a textual data analysis methodology born in France in the 80s in relation with the data analysis framework designed by (Benzecri et al., 1973a,b). A first synthesis of the methodology can be found in ( Lebart et al, 1997). It somewhat differentiates from text mining by the fact that it tries to always combine various statistical analysis techniques, like factorial correspondence analysis or hierarchical ascendant classification, with full-text search techniques like kwic concordances to be able to always get back to the precise original editorial context of any textual event participating to the analysis. It tries to always relate golden nuggets found in corpora to the context of their original data source. Thus it involves more an interaction with corpora than a distillation process of them.In 2007 a project joining textometry teams from four French universities 1 started a four year project to build an original open-source software framework for a new generation of application software for textometry 2 .The main goals of the project are:• synthesize the best available algorithms for textometry analysis with the most up to date annotated corpora models • implement them with the best available and sustained open-source components • be compatible with Unicode (2006), XML and TEI (2008) standard encoded data sources • be able to efficiently analyze corpora of several hundred million tagged words • be compatible with usual NLP software (like taggers and lemmatizers)• distribute a framework toolbox for developers to build new applications • demonstrate a Windows and Linux prototype application for the end researcher users from the humanities and social sciences • demonstrate an equivalent web based client/server prototype application • package the applications for easy install and deployment • document the toolbox framework, the applications and the development process publicly to build a community driven development network This paper will first discuss the specification and the conception model of the new opensource textometry platform. It will then present the data workflow chosen to import corpora into the platform. After the presentation of the software design and its architecture it will detail the open-source software components chosen to implement the first alpha version of the core toolbox and the two first prototype applications: one local rich client and one web based. This paper describes the rationale and design of an XML-TEI encoded corpora compatible analysis platform for text mining called TXM. The design of this platform is based on a synthesis of the best available algorithms in existing textometry software. It also relies on identifying the most relevant open-source technologies for processing textual resources encoded in XML and Unicode, for efficient full-text search on annotated corpora and for statistical data analysis. The architecture is based on a Java toolbox articulating a full-text search engine component with a statistical computing environment and with an original import environment able to process a large variety of data sources, including XML-TEI, and to apply embedded NLP tools to them. The platform is distributed as an open-source Eclipse project for developers and in the form of two demonstrator applications for end users: a standard application to install on a workstation and an online web application framework.
Synthetic Compounds in Mandarin * * * * In Mandarin Chinese, the construction of synthetic compound is very productive. As for its derivation, the recent literature mainly has two different analyses. One discusses that the compounds are derived on the basis of argument structures of the root verbs and a number of changed argument structures of the root verbs ( Gu and Shen, 2001); the other argues that Chinese compounds are not the same as English synthetic compounds so that they should not be analyzed in the same way but can be generated directly without referring to VP structure (Shi, 2003;He, 2004). This paper takes Chinese five-syllable compounds as examples, and provides some phonological evidences to prove that Chinese synthetic compounds are derived in the interface of lexicon and syntax, based on argument structures of the root verbs. Duanmu (1990) claims that 'in a head-nonhead structure, stress the nonhead', which he calls 'non-head stress' (NHS). For example, in 'niu nai' (cow's milk), 'niu' (cow) modifies 'nai' (milk), so 'nai' is the head and 'niu' is the non-head. According to NHS, 'niu' obtains the main stress. In the verb phrase 'chao fan' (to fry rice), the noun 'fan' (rice) is a complement of the verb 'chao' (fry), so 'chao' is the head and 'fan' is the non-head which obtains the main stress. This paper takes Chinese five-syllable compounds as examples to reexamine the derivation of Chinese synthetic compounds. With phonological evidences which show that Chinese synthetic compounds are left stressed, the paper proves that Chinese synthetic compounds are derived in the interface of lexicon and syntax, based on argument structures of the root verbs. The paper assumes that Chinese synthetic compounds should have the same derivation process, although some seem to have different argument structures from others.
The Specialized Vocabulary of Modern Patent Language: Semantic Associations in Patent Lexis  In the knowledge economy age, the intellectual property rights (IPR) become the important assets to human beings. Especially to the knowledge industry, the IPR is the key measure of a company competing with others.The area of IPR includes patent law, copyright law, trademark law and trade secret law, together with some aspects of other branches of the law, such as licensing and unfair competition (American Bar Association, 2010). Besides, intellectual property lawyers are required interdisciplinary knowledge as new development in law generate needs for lawyers with specific backgrounds-patent law, technology law, business law, and economy law. The fact that even when the global markets have been affected by economic recession in the end of 2007, the demand for intellectual property lawyers remains unusually high (World Intellectual Property Organization, 2009) is worth mentioning. As long as novel inventions were created, there is a need for intellectual property law to be enforced to protect human rights and their invisible property for specific purposes.As globalization has resulted in greater economic growth rapidly, inevitably the challenges of interdisciplinary communication that concerned with intellectual property and other significant sector encounters has increased. This recognition of the importance has brought intellectual property to the limelight. Resulting from such recognition, the recent emphasis that has been placed on using English as the lingua franca to apply patents on an international level and the application of technical vocabulary for the writing of professional patents becomes an essential issue in applied linguistic research. This paper presents an analysis of the language of patents, as a contribution to the field of English for Specific Purposes (ESP). While there work appears to fill a niche in the ESP field (and particularly in the English for Occupational Legal Purposes), the present study insists that statistical approach is necessary for compiling patent technical word list for ESP. Since research studies on word associations of patent lexis have been relatively scarce, this paper reports the technique to select appropriate words from high frequency words is required for modern patent language. The research content and statistical investigations building up a patent technical word list which helps learners of modern patent language expand the vocabulary size for a better understanding of patent writing.
The KOLON System: Tools for Ontological Natural Language Processing in Korean In various fields related to Information Science, such as Artificial Intelligence and Natural Language Processing (NLP), ontologies, also called Knowledge Bases (KB), are formal representations of knowledge and the relations among its concepts, usually composed by individuals, classes, attributes, relations, events, rules and axioms (Nirenburg, 2004). Even though researchers had been building several types of ontologies for decades and have been using them as a formal representation of knowledge within certain domains, ontologies gained momentum with the creation and propagation of the World Wide Web, and, more recently, with the idea of the Semantic Web, introduced by Tim Berners-Lee's homonymous article (Berners-Lee, 2000a;Nirenburg, 2004;Hirst,relevant elements of each. KOLON Ontology's structure is then thoroughly discussed and details about the project are presented, together with a comparison of this work with WordNet and FrameNet. The following section mentions the main components of the software library, focusing on pyKOLON and webKOLON. Finally, this paper describes an application of the KOLON Ontology's data to a previously performed SA experiment showing an improvement in the results, which shows that, even though the KOLON Project is still under development, it can already be used for Korean Natural Language Processing experimentation with improved results. This also implies that upon its completion, KOLON will enable a significant increase in accuracy in Korean Natural Language Processing experiments. This paper presents and analyzes the KOLON System, created to facilitate Korean Natural Language Processing (KNLP) and to improve experimental results through the KOLON Ontology. It is currently under development at our Computational Linguistics Laboratory , and is based on previous works, namely the Mikrokosmos Ontology and 21 st Century Sejong Project. The KOLON System is also extended with software tools to simplify the handling and visualization of the data, as well as the creation of new programs. The mapping of words onto ontological concepts was performed automatically, with faulty information being corrected manually. In order to examine the effectiveness of using KOLON&apos;s data, we have rerun a previous sentiment analysis (SA) experiment, changing the approach to include data from the ontology. This new experiment obtained improved results, which is a strong indication that the project will be of use after its completion.
An Approach toward Register Classification of Book Samples in the Balanced Corpus of Contemporary Written Japanese As the volume and the variations of texts compiled in language corpora are greatly increasing in recent years, providing an appropriate and informative classification code is becoming more and more important. Among many classification measures that have been proposed and employed so far, register classification, or text type annotation, has attracted a particular attention of many researchers (Biber, 1988;Tambouratzis et al., 2001;Portele, 2002;Gamon 2004;Sato, Matsuyoshi and Kondoh 2008;Biber and Conrad, 2009; among others).With appropriate classification codes, users will be able to extract on a particular set of the texts according to their objectives. For example, in many cases, usage of a word tends to depend strongly on the register and/or genre, and therefore, it is important for language researchers or learners to refer to register information of the text. In addition, appropriate register information is also crucial for creating automatic text classification systems as a source of baselines. Hence, we propose a new method of text type annotation for corpus texts taken from books.The rest of this paper is organized as follows. Section 2 briefly reviews the style classification methods in the literature. Section 3 introduces the proposed method. Section 4 explains how our method is practically applied to a Japanese corpus called BCCWJ. Sections 5 and 6 give evaluation results and concluding remarks, respectively. Japanese books are usually classified into ten genres by Nippon Decimal Classification (NDC) based on their subject. However, this classification is sometimes insufficient for corpus studies which describe characteristics of the texts in the book. Here, we propose a method of classifying text samples taken from Japanese books into some registers and text types. Firstly, we discuss useful criteria to describe various characteristics of the texts and propose a two-step approach for stable annotation. We then apply our method to 161 book samples from the prerelease version of the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a balanced Japanese corpus comprising 100 million words developed by National Institute for Japanese Language and Linguistics. Finally, we evaluate our method in terms of stability of annotation using kappa coefficients and correlation coefficients.
A Computational Treatment of Korean Serial Verb Constructions Korean is one of the languages that employ the so-called serial verb construction (SVC) in which more than one verb occurs without any specific coordination or subordination markings:(1) a.Mia-ka hakkyo-ey kel-e ka-ss-ta Mia-NOM school-to walk-COMP go-PAST-DECL 'Mia walked to school.' b.Mia-ka cwul-ul cap-a tangki-ess-ta Mia-NOM rope-ACC draw-COMP pull-PAST-DECL 'Mia pulled a rope, drawing it.'Both sentences here, though including two serial verbs with their own predicate relations, semantically represent only a single event. These sentences display the canonical grammatical properties of SVCs in that the two successive verbs behave like a complex predicate, sharing one tense and subject value, and even the object value as in (1b).The generation of Korean SVCs is quite flexible as attested by the corpus examples of the verb mek-ta 'eat': (2) nanwu-e mek-ta 'divide and eat', kkulhi-e mek-ta 'boil and eat', mandul-e mek-ta 'make and eat', cap-a mek-ta 'catch and eat', cip-e mek-ta 'pick up and eat', ssip-e mek-ta 'chew and eat', kwu-e mek-ta 'broil and eat', ppal-a mek-ta 'suck and eat', etc.The examples, extracted from the Sejong POS-tagged Corpus, show us that the activity verb mekta 'eat' can combine with various types of verbs, forming an SVC. 1 Many thanks go to anonymous reviewers for helpful comments. Main theoretical aspects sketched here follow those in Kim (2010).In this paper, we try to explore the syntactic and semantic properties of the Korean SVCs and build a computationally feasible analysis that can parse the desired set of examples. We then test the analysis with implementing it in the LKB (Linguistic Knowledge Building) System within the preexisting computational grammar, KRG (Korean Resource Grammar). The so-called serial verb construction (SVC) is a complex predicate structure consisting of two or more verbal heads but denotes one single event. This paper first discusses the grammatical properties of Korean SVCs and provides a lexicalist, construction-based analysis couched upon a typed-feature structure grammar. We also show the results of implementing the grammar in the LKB (Linguistics Knowledge Building) system couched upon the existing the KRG (Korean Resource Grammar) which has been developed since 2003. The implementation results provides us with a feasible direction of expanding the analysis to cover a wider range of relevant data.
Arguments for Parallel Distributed Parsing Toward the integration of lexical and sublexical (semantic) parsings In the usual sense of syntactic parsing, the analyses of relationships among words and relationships among morphemes are separated. The former is called syntactic analysis (or syntactic parsing) and the latter is called morphological analysis (the term "morphological parsing" exists but it seems to denote a somewhat different notion). Sometimes, however, sublexical analysis is relevant. This is evident in shallow semantic parsing which is understood to be "labeling phrases of a sentence with semantic roles with respect to a target word. For example, the sentence (1) is labeled as (2):" 1)(1) Shaw Publishing offered Mr. Smith a reimbursement last March. The target of the labeling in (2) is offered. Note that the same kind of labeling should be available for the argument structure of reimbursement, which can be illustrated in (3): Clearly, semantic role labeling requires a high-precision predicate-argument analysis of a target predicate, whether the target is a word (e.g., offer) or a morpheme (e.g., reimburse) embedded in a word. The comparison of the two cases shows that a certain kind of parsing is necessary in effective semantic labeling, as Gildea and Palmer (2002) pointed out. But the problem is how to combine the lexical parse by which the predicate-argument structure of offer is recognized with the sublexical parse by which the predicateargument structure of reimburse is recognized. Integrating the two kinds of parses is not a trivial task. This paper presents the idea of Parallel Distributed Parsing (PDP) that is able to straightforwardly carry out the integration. The presentation, however, is theoretically oriented and the content is rather preliminary: no empirical results are presented other than a few sample parses. No parser implementation is available. The main purpose of this presentation is to illustrate a new model for parsing that integrates lexical and sublexical parsings, which I argue can be a remedy for the problem of data sparseness.Data sparseness is a serious problem in natural language processing (NLP) even now that computers can access more raw data than the average human does. The size of textual raw data automatically acquired from the Web exceeds that which a normal human can read in a lifetime. This suggests, however, that no human seems to suffer from data sparseness. What makes this more mysterious is that humans do also employ statistical information in their language processing. The difference between humans and machines, therefore, should lie in the difference in efficiency with which they acquire knowledge, be it is syntactic, semantic or morphological, from linguistic data given. Humans are certainly, at the present, able to acquire knowledge far more efficiently than computers. The crucial question is: How is this possible?I argue that data sparseness is a problem in NLP not only because distributional data itself is sparse, but also because parses availabe today are sparse and inefficient; otherwise, data sparseness should impact human language processing in the same way it does computers. The explanation I consider is that data contains enough information but current technologies fail to extract it due to inefficiency of available parses. PDP is proposed to make parses of linguistic data more efficient and less sparse. In what follows, I show how PDP can remedy the data sparseness. This paper illustrates the idea of parallel distributed parsing (PDP), which allows us to integrate lexical and sublexical analyses. PDP is proposed for providing a new model of efficient, information-rich parses that can remedy the data sparseness problem.
Scrambling of Wh-phrases in Japanese* An obvious difference between English and Japanese, among many others, is that wh-phrases in non-echo single-wh questions in English must appear in a left-peripheral position, while those in Japanese need not: 1 (1) a.John bought a book. b.What i did John buy t i ? c.John bought what? (echo question only)(2) a. John-ga hon-o katta (koto). -NOM book-ACC bought fact 'John bought a book.' b.John-ga nani-o katta no? -NOM book-ACC bought Q 'What did John buy?' c.Nani-o i John-ga t i katta no?One of the many issues that arise in the comparative study of English and Japanese is to what extent the two languages are alike, despite their superficial differences. Specifically, we might wonder whether the example in (2c) can be likened to the English example in (1b) in that the wh-phrase appears in the same SpecCP position. I argue that although Japanese and English wh-questions are alike in some respects, e.g., the scope of the wh-phrase is determined by the relation with a [+WH] C, the derivation and syntactic structure of Japanese wh-questions, including the one like (2c) that looks very much like the question in (1b), are not exactly like those in English. Specifically, I claim that overt syntactic movement of wh-phrases in Japanese is the result of scrambling, the same operation that applies to non-wh-phrases. The lack of scope reconstruction in certain cases is not due to a constraint on phrases undergoing wh-movement but to a combination of factors having to do with the computation of the available readings, the semantic effect and prosody. Despite the a priori desiratum of assimilating the derivation and interpretation of wh-questions in Japanese to that in English, empirical evidence shows that overt displacement of wh-phrases in Japanese is not the same as overt wh-movement in English. The syntax of wh-phrases in Japanese is essentially the same as that of non-wh-phrases. Failure of scope reconstruction in certain cases is not evidence for a constraint applying specifically to wh-movement but is due to the computation of the available readings, the semantic effect or prosody.
A Comparative Study on the Aspectual Classification of Korean and Japanese Verbs in Relation to &apos;-ess-&apos; &apos;-essess-&apos; in Korean and &apos;sudeni&apos; in Japanese This paper presents an aspectual classification of Korean and Japanese verbs in terms of the temporal properties which are considered inherent in the lexical meanings of the verbs, and also shows that the aspectual verbal classes provide an explicit explanation for some time relevant linguistic phenomena. We classify Korean and Japanese verbs into 12 distinct aspectual classes respectively on the basis of Vendler (1967).The temporal characteristics of the Korean verbs are closely related to a variety of the interpretations of the Korean past tense forms '-ess-' and '-essess-' which have been controversial, and also the semantic distinctions of 'te iru' form of Japanese verbs combined with 'sudeni' result from the internal time structure of the verbs. Of the temporal properties inherent in the verbs, telicity 1 plays a significant role in relation to linguistic phenomena. Tense and aspect are concerned with time, but in different ways. Tense is a deictic category which locates a situation in time, while aspect is a non-deictic category which is a different way of viewing the internal constituency of a situation (Comrie,1976).Two different definitions of aspect are given, i.e. one is the aspectual classes of verbs and the other is the aspectual forms of verbs. The first is called aktionsarten and the second is what are called the progressive, perfect, and resultative which the verbs show when combined with aspect markers. The verbs which have the same internal temporal structure form a certain aspectual class and the verbal behaviors of each aspectual class are differentiated syntactically and semantically. This paper deals with an aspectual classification of Korean and Japanese verbs based on Vendler (1967). Both of Korean and Japanese verbs are classified into 12 distinctive aspectual categories by their restrictions on time adverbials, progressive tenses and logical entailments. A well-established aspectual verb classes provide us not only with a better understanding of both languages, but also with an explicit explanation concerning some grammatical phenomena relevant to time. The aspectual classes of Korean verbs clarify the ambiguous semantic functions of so called Korean tense forms &apos;-ess-&apos; and &apos;-essess-&apos;, while those of Japanese verbs elucidate the semantic functions of &apos;sudeni&apos;, the equivalent of &apos;already&apos; in English, when it occurs with a variety of verbs in sentences.
A Modular Architecture for the Wide-Coverage Translation of Natural Language Texts into Predicate Logic Formulas Translating natural language expressions into logical formulas has been extensively studied for decades, attracting theoretical and practical interests. To date techniques have essentially involved adding methods of building a semantic expression piecewise to the process of identifying syntactic structure with a parser. In this paper we propose a departure from this tradition, with a system of semantic evaluation, Scope Control Theory or SCT (Butler, 2010), that accepts parsed syntactic structures as input. We employ semantic evaluation to generate explicitly semantic translations in the form of predicate logic formulas. It is also possible to have SCT evaluation generate the representations of alternative formalisms and offer different granularities for making semantic information explicit (e.g., generate encodings with more fine-grained presuppositional information, modal information, or tense information).In using semantic evaluation to generate semantic translations a notable contribution is the elimination of the need for enriched guidance information that is extra to a conventional parsed form. Notably the co-indexing of syntactic constituents is rendered unnecessary. Moreover sentence information and discourse information are dealt with equally at the time of evaluation as providing binding information, which allows for their seamless integration.The language formalism of SCT is supplemented by a compact grammar to determine the contribution of morphosyntactic information. This offers flexibility in what is acceptable as input, so there is no need to develop a SCT-specific parser from scratch. Here we adopt an HPSG-based syntactic parser (Miyao and Tsujii, 2008), because it offers wide coverage and high accuracy, and provides detailed syntactic information that is sufficient for SCT. Although the output of the parser is not directly compatible with the input assumed by SCT, a small conversion program can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas.A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for parsed syntactic structure conversion can achieve high coverage on real-world texts. This attests the portability of our method to other languages, as well as to other forms of semantic representation.To our knowledge, Boxer (Bos et al., 2004;Curran et al., 2007) is the only alternative to our system that translates unrestricted texts into logical formulas (Discourse Representation Structures) with resolved intra and inter argument dependencies and anaphoric dependencies. We discuss the relationship with Boxer in Section 6 and note some added benefits of our approach. We present a new method for translating unrestricted natural language texts into predicate logic formulas. This relies on the semantic evaluation procedure of Scope Control Theory (SCT), a variant of Dynamic Semantic formalisms. The key benefit is that parsed syntactic structures are shown to form sufficient input for semantic evaluation, eliminating the need to build distinct semantic expressions to feed semantic evaluation. To have parsed syntactic structures for SCT to evaluate we apply an existing wide-coverage syntactic parser by converting the parser output into a form SCT can receive. This modularity has led to the rapid attainment of a broad coverage on real text. An experiment revealed our system achieved 82.7% coverage on real-world sentences, generating representations that make explicit the scopes of quantifiers (e.g., ∃x), operators (e.g., negation), connectives (e.g., conjunction) and embedding predicates (e.g., thinks), while also capturing the inter and intra sentential dependencies and cross-sentential anaphoric dependencies that connect predicates.
Toward a Unified Computational Model of Quantificational Scope Readings* Quantification is one of the hottest topics in linguistic research where the issue of how syntax maps into semantics is invariably associated with the question of how constrained the syntax-semantics interface is. If quantification is a linguistic phenomenon which is operative at the syntax-semantics interface, the question that naturally arises is: how much contribution does syntax as opposed to semantics or vice versa make toward a heterogeneous set of scopal effects that we find in quantificational readings within and across languages? When do syntax and semantics interact for variable scope effects in quantificational readings? Issues of such kind are certainly convoluted given that quantificational interpretation is not a unified phenomenon (Ruys and Winter, 2010;Szabolcsi, 2010). Given the backdrop above, we find that quantification has been handled from a number of perspectives-set-theoretic, representational, derivational etc. (see for details, Ruys and Winter, 2010). Here in this paper, present derivational accounts of quantificational scope will first be considered. The principles proposed in those accounts have computational significance in the operational design of the language faculty. But they will be shown to be incompatible and inconsistent with each other. A unifying account that eliminates their inconsistencies but aligns them beautifully in a computational model so that the correct generalizations about quantificational scope readings across languages can be captured is therefore needed. This paper is aimed at uncovering a unifying computational grounding beneath a diverse range of cases of quantificational scope effects both within and across languages. Since quantificational scope readings are quite variable and interspersed with issues of modularity and interfaces of grammar, an underlying and universal generalization is certainly hard to come by. Research on quantification is not new at all; studies and research done on quantification have not yet been able to arrive at a useful but universally valid and satisfactorily unified account of how quantificational readings are derived at all, let alone computationally. Here in this paper, a preliminary sketch of a unified three-tier computational model will be drawn up to show how quantificational scope readings across languages can be computed and derived. For this purpose, principles drawn from recent derivational accounts of quantificational scope will be aligned properly to eliminate their incompatibilities with each other. Keywords: a unifying computational grounding, quantificational scope, computational model.
CCG of Japanese Sentence-final Particles  The aim of this paper is to provide formalization of Japanese sentence-final particles in the framework of Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000, Szabolcsi 1987). While certain amount of literature has discussed the descriptive meaning of Japanese sentence-final particles (Takubo and Kinsui 1997, Chino 2001), little formal account has been provided except for McCready (2007)&apos;s analysis from the viewpoint of dynamic semantics and relevance theory. I analyze particles such as yo and ne as verum focus operators (Höhle 1992, Romero and Han 2004).
A Multi-Dimensional Analysis of Japanese Benefactives: The Case of the Yaru-Construction In English, verbs such as bake are used ditransitively (Green, 1974;Oehrle, 1976;Levin, 1993):(1) a. Anna baked a cake (for Ken). b. Anna baked Ken a cake. (1b) can only mean that Anna baked a cake with the intention of giving it to Ken, and this construction contributes semantics not attributable to the lexical items involved (Goldberg, 1995).In Japanese, transitive verbs such as yaku 'bake' cannot be used ditransitively: (2) a. Anna-wa Anna-TOP bake-PAST However, verbs of giving such as ageru in (3) allow the verb yaku to be associated with the constructional meaning of (1b), which is exemplified in (4). In Japanese traditional grammar and in grammars for language learning, the unacceptability of (5) below is explained in terms of the mismatch between the benefactive interpretation construed by the construction and the concerned situation described by the object gomi 'garbage' (and the infinitive yaite 'bake'): give-PAST 'Intended: Anna gave to Ken the benefit of incinerating garbage for him.' That is, (Anna's) baking a cake rather than incinerating garbage provides the benefit for Ken. In this case, to describe the situation in which Ken gets a benefit of incinerating garbage, a complex postpositional phrase X-no-tame-ni 'for X' benefit' is used:(6) Anna-wa Anna-TOP Ken-no-tame-ni Ken-GEN-benefit-DAT gomi-o garbage-ACC yaite incinerate age-ta.give-PAST '(Lit.) Anna gave to Ken the benefit of incinerating garbage for him.' This explanation, however, does not hold for the following sentences in which the given contexts conspire to give to Ken the benefit of Anna's action described by the different transitive verbs, sagasu 'hunt' and yameru 'quit' with the same accusative-marked object shigoto 'job': (7) a. Context: Ken is not even trying to get a job and is just loafing around the house. Anna-wa Anna-TOPgive-PAST '(Lit.) Anna gave to Ken the benefit of hunting a job for him.' b. Context: Ken wants Anna to become a stay-at-home wife.*Anna-wa Anna-TOPgive-PAST 'Intended: Anna gave to Ken the benefit of quitting her job for him.' To show the intended meaning in (7b), the complex postpositional phrase as in (6)  give-PAST '(Lit.) Anna gave to Ken the benefit of quitting her job for him.' These observations immediately raises the question of what licenses the occurrence of the dative argument and what is the crucial factor determining the acceptability of these constructions.In this paper, we suggest that there is a semantic restriction on the basis of which we can predict the distribution of the dative argument. More specifically, we suggest that if transitive verbs fall into the class VERB OF CREATION or VERB OF OBTAINING where the meaning of transfer of possession is expressed, the benefactive sentence is judged to be acceptable.The paper is organized as follows. Section 2 shows some basic facts about Japanese benefactives. Section 3 presents the verb classification. Section 4 and 5 provides a formalization and a summary of the analysis presented in this paper. morat-ta. receive-PAST 'I had my younger sister bake a cake.' These constructions are formed using different verbs meaning 'to give' as in (9a) and (9b), or 'to receive' as in (9c). There are such seven verbs, as shown in Table 1. The benefactive verbs can be classified into three types cutting across different politeness levels. The polite verb ageru (type I) is used when talking politely to addressee. The honorific verbs indicate something about the speaker's relationship with one of the verb's arguments. For instance, kudasaru (type II) brings along the honorific contribution that the speaker regards a referent of the subject as socially superior to him. This meaning is independent of the constructional meaning of the benefactive sentence and thus we deal with only the basic verbs. Benefactive constructions conceptually include two human entities, the one who performs an act for someone's benefit (BENEFACTOR) and the one who receives the benefit (BENEFICIARY). In the sentences above, the BENEFACTOR is watashi 'I' in (9a) and imooto 'younger sister' in (9b) and (9c). The BENEFICIARY is imooto in (9a) and watashi in (9b) and (9c).There are two types of (basic) verbs of giving, yaru (type I) and kureru (type II). Yaru 'give' is used when the situation is described from the subject (giver)'s point of view, whereas kureru 'give' is used when the situation is described from the indirect object (receiver)'s point of view. 1  give-PAST 'Anna took the trouble to meet me.' In the following sentences, the BENEFICIARY can be the direct object, the indirect object as in (12a), or an adjunct as in (12b). 2 (12) a. Watashi-wa I-TOP This paper discusses the semantic and pragmatic properties of Japanese benefac-tives with the main focus on the yaru-construction. The benefactive sentence is judged to be acceptable if the transitive verb complement falls into a certain semantic class in which the meaning of transfer of possession is expressed. Hence, the distribution of the recipient role rather than the beneficiary role is crucial for determining the acceptability of the construction. To capture such a multi-dimensional linguistic information, HPSG account will be given.
Exploiting a Multilingual Web-based Encyclopedia for Bilingual Terminology Extraction In recent years two types of multilingual corpora have been an object of studies and research related to natural language processing and information retrieval: parallel corpora and comparable corpora. The parallel corpora are made up of original texts and their translations (Morin et al., 2004 ;Véronis, 2000). This allows texts to be aligned and used in applications such as computer-aided translator training and machine translation systems. This method could be expensive for any pair of languages or even not applicable for some languages, which are characterized by few amounts of Web pages on the Web. On the other hand, non-aligned comparable corpora, more abundant and accessible resources than parallel corpora, have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dejean et al., 2002;Fung, 2000;Goeuriot et al., 2009a;Goeuriot et al., 2009b;Morin et al., 2006; Nakagawa et al., 2000;Rapp, 1999;Sadat et al., 2003;Sadat et al., 2004). Comparable corpora are defined as collections of texts from pairs or multiples of languages, which can be contrasted because of their common features in the topic, the domain, the authors, the time period, etc. Comparable corpora could be collected from downloading electronic copies of newspapers and articles, on the WWW for any specified domain. Among the advantages of comparable corpora; their availability, consistency and utility for research on Natural Language Processing (NLP). In another hand, recent publications on bilingual terminology extraction from comparable corpora have shown promising results although most used comparable corpora are domain-specific, which causes limitations on the usage diversity, the domain and the quality of terminology. This paper intends to bring solutions to the problem of lexical coverage of existing linguistic resources such as multilingual ontologies and dictionaries, but also to the improvement of the performance of Cross-Language Information Retrieval. The main contribution of the current study is an automatic acquisition of bilingual terminology from Wikipedia 1 articles in order to construct a bilingual ontology or enhance the coverage of existing ontolgies. The remainder of the present paper is organized as follows: Section 2 presents an overview of Wikipedia. Section 3 presents the different steps for the acquisition of bilingual terminology using a two-stage corpus-based translation model. Experiments and evaluations are related in Section 4. Section 5 concludes the present paper. Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and to exploit the idea of using multilingual web-based encyclopedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and data. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statistics-based and linguistic-based approaches were applied on different pairs of languages including Japanese, French and English. These evaluations showed a real open improvement and a good quality of the extracted term candidates for building or enriching multilingual ontology, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.
Language Homogeneity in the Japanese Wikipedia Wikipedia 1 is a comprehensive Internet based encyclopedia, which is unusual in that the articles are largely written and maintained by volunteers, many of whom are anonymous. The permissive approach used by Wikipedia does however make it natural to question the correctness of the content, and the possibility of achieving a consistent writing style. An encyclopedia is expected to have a formal writing style consistently applied across all articles and in this paper, we examine the writing style of the entire Japanese Wikipedia using automated analysis. This approach is motivated by the presence in Japanese of very distinct writing styles that can serve to signal different degrees of formality and politeness. The difference in language usage is not limited only to word choice but includes different forms for sentence-final elements. By examining all sentences in Wikipedia, we attempt to identify any inconsistencies in writing style.Section 2 of this article lists related work, and Section 3 discusses Japanese writing styles. In Section 4, our analysis procedure is described, with the classification results presented in Section 5. Some of the Wikipedia articles that did not follow the style guidelines are analyzed in Section 6 and our conclusions are presented in Section 7. Wikipedia is a potentially very useful source of information, but intuitively it is difficult to have confidence in the quality of an encyclopedia that anyone can modify. One aspect of correctness is writing style, which we examine in a computer based study of the full Japanese Wikipedia. This is possible because Japanese is a language with clearly distinct writing styles using e.g., different verb forms. We find that the writing style of the Japanese Wikipedia is largely consistent with the style guidelines for the project. Exceptions appear to occur primarily in articles with a small number of changes and editors.
Incorporate Credibility into Context for the Best Social Media Answers The technology of web search has provided a powerful platform for seeking and accessing information on the Web. However, due to the limitations of keyword based retrieval, people often find it hard to locate their desired content through ad hoc searching. Therefore, it is no surprise that Web 2.0 system is trying to seek improvement in information foraging. Given that a generalpurpose, fully-automated question answering is still beyond the state-of-the-art, the humanpowered Collaborative Question Answering (CQA) services start to draw new attentions. Through CQA services, people could post their questions concerning any subject, even those the ad hoc searching may find difficult to answer, on a community portal, and get answers directly provided by community contributors. Due to the advantage of human interaction, CQA has become a popular question-answering platform and an active research area up till now. It turns out to be an effective supplement to the ad hoc searching and question answering ( Banerjee and Han, 2009;Weerkamp and Rijke, 2008).Although CQA tries to help users come up with the best answers with its human-oriented strategy, the quality of user-generated answers is sometimes difficult to control (Gyongyi et al, 2008). Among the answers received, there might be only few knowledgeable responses and helpful information. Some postings could be vague or even purposely misleading. Thus seeking the best answers to questions asked within the CQA community has been a challenge to the CQA system.The existing researches on best answer seeking in CQA can roughly be categorized by textual feature based and non-textual feature based approaches. Usually, best answers could be decoded as high-quality and authoritative. To date, there is no much attention on the content-based answer quality judgment in CQA research. Instead, non-textual features are utilized extensively in estimating the quality of answers ( Jeon et al., 2006). Much of the related work is based on user authority analysis by the user's interaction network. In some related area, researchers have also proposed textual feature based answer quality judgment. However, they usually equal it to the writing quality, and other features such as spelling errors, lacking of leading capitals and so on (Weerkamp &amp; Rijke, 2008).Since most of the existing researchers have focused on non-textual features, in this paper, we aim to explore the effectiveness of diverse textual features on the task of best answer detection. The problem of best answer detection is formulated as a text classification task. We will verify the effectiveness of different textual features, including n-gram, bag-of-word, and the relationship between questions and answers, also the combination of these features. Specially, we propose to model the textual features based on the linguistic theory of evidentiality. Evidentiality is concerned with linguistic expression reflecting users' degree of certainty, or commitment to the truth. It explicitly expresses information providers' specification for the information sources and their attitudes toward the information by the use of evidential. Thus evidentiality could be an explicit cue for the information quality in CQA answers.The following answer phrases are derived from the CQA service of Yahoo! Answers . -I doubt this is true but it's a neat thing to think about... -im not sure i was always told never to look directly at the sun cos its bad for ur eyes.The evidentials (such as doubt, not sure) in the context signals the uncertainty of the answer by the contributor himself and thus it is less possible to be considered as the best answer. In other words, evidentials could play the role as an important context clue that provides us with further insights in locating the best answers. In the current study, we model evidentiality within the framework of machine learning based text classification. The result from this study would contribute to both CQA and other applications in making judgments on textual information quality.This paper is organized as follows. We discuss related work in section 2. The proposed framework for best answer detection which utilizes several textual features is presented in section 3. In section 4, we focus on one of the textual features in the framework, namely cognitive evidentiality. Using these textual features, we report the results of a large scale evaluation over Yahoo! answers in section 5. Lastly, in section 6, we discuss further our findings from the experiments and conclude this paper. In this paper, we focus on the task of identifying the best answer for a user-generated question in Collaborative Question Answering (CQA) services. Given that most existing research on CQA has focused on non-textual features such as click-through counts which are relatively difficult to access, we examine the effectiveness of diverse content-based features for the task. Specially, we propose to explore how the information of evidentiality can contribute to the task. By the comparison of diverse textual features and their combinations, the current study provides useful insight into the issues of detecting the best answer to a given question in CQA without user features or system specific link structures.
A Query Focused Multi Document Automatic Summarization Text Summarization, as the process of identifying the most salient information in a document or set of documents (for multi document summarization) and conveying it in less space, became an active field of research in both Information Retrieval (IR) and Natural Language Processing (NLP) communities. Summarization shares some basic techniques with indexing as both are concerned with identification of the essence of a document. Also, high quality summarization requires sophisticated NLP techniques in order to deal with various Parts Of Speech (POS) taxonomy and inherent subjectivity. Typically, one may distinguish various types of summarizers.Multi document summarization requires creating a short summary from a set of documents which concentrate on the same topic. Sometimes an additional query is also given to specify the information need of the summary. Generally, an effective summary should be relevant, concise and fluent. It means that the summary should cover the most important concepts in the original document set, contain less redundant information and should be well-organized.In this paper, we propose a query focused multi document summarizer, based on clustering technique and sentence compression. Unlike traditional extraction based summarizers which do not take into consideration the inherent structure of the document, our system will add structure to documents in the form of graph. During initial preprocessing, text fragments are identified from the documents which constitute the nodes of the graph. Edges are defined as the correlation measure between nodes of the graph. We define our text fragments as sentence.Since the system produces multi document summary based on user's query, the response time of the system should be minimal for practical purpose. With this goal, our system takes following steps: First, during preprocessing stage (offline) it performs some query independent tasks like identifying seed summary nodes and constructing graph over them. Then at query time (online), given a set of query words and keywords, it performs query word and keyword search over the cluster to find a sentence identifying relevant phrases satisfying the query words and keywords. With the relevant phrases, the new compressed sentence has been constructed for summary. The performance of the system depends much on the identification of relevant phrases and compression of the sentences. Although, we have presented all the examples in the current discussion for English language only, we argue that our system can be adapted to work on other language (i.e. Hindi, Bengali etc.) with some minor addition in the system like incorporating language dependent stop word list, the stemmer and the parser for the language. The present paper describes the development of a query focused multi-document automatic summarization. A graph is constructed, where the nodes are sentences of the documents and edge scores reflect the correlation measure between the nodes. The system clusters similar texts having related topical features from the graph using edge scores. Next, query dependent weights for each sentence are added to the edge score of the sentence and accumulated with the corresponding cluster score. Top ranked sentence of each cluster is identified and compressed using a dependency parser. The compressed sentences are included in the output summary. The inter-document cluster is revisited in order until the length of the summary is less than the maximum limit. The summarizer has been tested on the standard TAC 2008 test data sets of the Update Summarization Track. Evaluation of the summarizer yielded accuracy scores of 0.10317 (ROUGE-2) and 0.13998 (ROUGE-SU-4).
A Look into the Acquisition of English Motion Event Conflation by Native Speakers of Chinese and Japanese Talmy (2000 a&amp;b) introduced the idea of complex events that he termed 'macro-events'. He said that such events can be conceptualized as being comprised of two simpler events and the relation between them. For example, (1a), shown below, can be conceptualized as one event, and thus expressed in one sentence or phrase, but is actually made up of the two events, shown in (1b) and (1c).(1) a. Jack rode his bike to school.b. Jack rode his bike. c. Jack went to school.Talmy (2000 a&amp;b) termed this conceptualization of two or more simpler events into one macroevent 'conflation'. After observing the conflation patterns of several different languages, Talmy (2000 a&amp;b) deemed that all languages could be broken into two types based on where the language encoded the 'main event'. When the macro-event is motion, the 'main event' is considered to be the path of the motion, while the 'sub-event' is considered to be the manner of motion. Talmy (2000 a&amp;b) dubbed languages that encoded the path of motion onto the main verb of the sentence 'verb-framed languages', and those that encoded the path of motion onto another particle 'satellite-framed languages'. Talmy (2000 a&amp;b) has determined Spanish to be a verb-framed language, and gives the example shown in (2) as evidence. As can be seen, Spanish encodes the path of motion onto the verb, and puts details about the manner of motion into other particles, such as adverbs.(2) a. La botella entró a la cueva flotando. The bottle moved.in to the cave floating. "The bottle floated into the cave." b. El globo subió por la chimenea flotando. The balloon moved.up through the chimney floating. "The balloon floated up the chimney."Meanwhile, English is considered to be a satellite-framed language by Talmy (2000 a&amp;b), as it generally encodes path onto prepositions, and conflates the manner of motion onto the main verb. This can be seen in the English translations of the Spanish examples in (2) above, as well as in example (3), shown below.(3) a. The boy jumped into the hole.b. The bird flew up the chimney.While there are some exceptions to Talmy's (2000 a&amp;b) typology, most languages have tendencies to generally fall into one category or the other. However, Slobin (2004) has since elicited the need to add a third category, which he calls equipollently-framed languages, to Talmy's original typology. According to Slobin (2004), some languages, such as Sino-Tibetan, Tai-Kadai, Austronesian, and Hokan, express path and manner of motion 'by equivalent grammatical forms' (Slobin 2004: 25), and thus would not fit into either of the original categories. Based on the typology introduced above, it is highly possible that speakers of the different language types conceptualize motion events differently. This could potentially have great impact on how they learn a second language of a different type. This study sets out to examine the effects of transfer from one's native language on second language motion event conflation acquisition in such a case. It specifically looks to compare the processes of acquisition of English (a satelliteframed language) by Chinese (equipollently-framed) and Japanese (verb-framed) native speakers. Furthermore, it puts forth the following hypotheses: (1) A significant difference between Japanese and Chinese English learners' acquisition of English motion event framing can be found, (2) learners will tend to simply replace path verbs in their native language with "go + {path satellite}" when thinking about motion event conflation in English, leading to a lower number of manner of motion verbs and deictic verb usage different from their native language, and (3) there will be a significant jump in the acquisition of English motion event framing from mid to high level learners. Since Talmy (2000 a&amp;b) introduced his linguistic typology based on event conflation, it has been the source of much debate and ongoing research. One area that can particularly benefit from such research is the field of Second Language Acquisition. Cadierno (2008), Inagaki (2002) and many others have attempted to unveil the differences and difficulties that occur when learning a second language that is of a different type than one&apos;s native language. However, the current research in this area has thus far only dealt with satellite-framed and verb-framed languages. According to Slobin (2004), Chen and Guo (2008) and others, languages such as Chinese can be considered to be of a third type, known as equipollently-framed languages. This paper presents research that has attempted to observe the differences and similarities in the acquisition of a satellite-framed language (English) by native speakers of a verb-framed language (Japanese) and an equipollently-framed language (Chinese).
Stage/Individual-level Predicates, Topics and Indefinite Subjects It has long been noted that the distribution of sentences with indefinite subject NPs is quite restricted, especially with respect to Mandarin Chinese which does not allow an indefinite NP to appear in the subject or topic position unless it is interpreted as specific (see Chao 1968, Li and Thompson 1981, Lee 1986, Tsai 2001, Xu 1996, among many others) or gets bound by an existential or a generic operator (Pan 2009). Some recent studies, however, claim that it is not correct to treat sentences with indefinite subjects uniformly without distinguishing between stagelevel and individual-level predicates (Erteschik-Shir1997, de Swart 1999), as shown by sentences from English (1)-(2) and Mandarin (3)-(4), which clearly shows that both these two languages allow sentences with the indefinite subject and a stage-level predicate, as in (1a) and (3a), but not sentences with the indefinite subject and an individual-level predicate, as in (1b) and (3b). However, when the subject is a bare NP, sentences with individual-level predicates are acceptable, as in (2b) and (4b), contrasting with the indefinite subjects in (2a) and (4a): English(1) a. A man arrived (stage-level predicate) b. *A man is smart.(individual-level predicate) (2) a.*A dog is intelligent.(indefinite NP) b. Dogs are intelligent (bare NP)1 I thank professor Haihua Pan for discussions and suggestions on the very draft, as well as the final version of this paper. Also, I thank Dr.Yurie Hara and the anonymous PACLIC 24 th reviewers for comments. Unlike Erteschik-Shir (1997) and de Swart (1999), we argue that the so-called Davidsonian arguments both in stage-level and individual-level predicates are able to function as topics, namely, existential stage topics and generic situation topics, respectively, which conforms with the well-known definiteness topic constraint. These two types of topics, however, are available only when the Davidsonian argument is bound via different means: the former gets bound in the existential domain, while the latter is constrained by the generic operator. Concerning the situation variable constrainer in sentences with individual-level predicates, we argue that besides general additional information, like adverbials &apos;usually&apos; in English, &apos;tongchang&apos;/&apos;yibande&apos;, &apos;yaoshi… jiu&apos; (if… then), etc, in Mandarin, bare NPs with kind-denoting and predicates denoting inherent property will help accommodate the generic operator, and thus a generic situation topic is available and sentences with indefinite subject NPs and individual-level predicates thus get licensed. As a consequence, we suggest that the necessary condition for the indefinite subject sentence should be that the indefinite subject NP is interpreted as specific or a stage topic/generic situation topic is available.
How Well Conditional Random Fields Can be Used in Novel Term Recognition  This paper explores the use of Conditional Random Fields (CRF) in novel term recognition. It investigates the recognition of medical terms using CRF model. A variety of methods have been used in term recognition, some are linguistics focused, some are statistically motivated, and a large part of approaches combine these two together. Recently, with the development of machine learning models, a lot of work has attempted to extract terms using machine learning methods.The usual practice of machine learning depends on training data and a set of discriminating features. Then, machine learning systems use training data to "learn" features useful for term recognition. The widely used standard features for machine learning include orthographic features, POS tags, prefix, and suffix information ( Krauthammer and Nenadic, 2004). However, few studies have tried to make use of dynamic linguistic features in respect of term usage in real text. As Zhang and Fang (2009) found out, syntactic functions can be used effectively in selecting and ranking term candidates, which means termhood can be captured by computing term ratios in syntactic paths.Furthermore, as studies concerning novel terms are not so common, this paper considers how syntactic information integrated under a machine learning framework can be helpful in discovering novel terms. As early as in 1995, Justeson and Katz defined novel terminology as terms that are newly introduced and not yet widely established, or terms that are current only in more advanced or specialized literature than that with which the intended audience can be presumed to be familiar. Utsuro et al. (2006) specifically define novel terms to be technical terms that are not included in any of existing lexicons of technical terms of the domain. In MeSH1, there will be annual changes to its descriptors (terms). As quoted from their website, "In biomedicine and related areas, new concepts are constantly emerging, old concepts are in a state of flux and terminology and usage are modified accordingly." And "in selecting the expressions to be used for a new MeSH descriptor, it is the usual practice to adopt the expression most commonly used by the authors writing in the English language." Therefore, novel terms may not only refer to those new words that are newly created and specifically for some meaning in a certain domain, but also could be some known word whose meaning is changed from the common understanding to be specialized in some domain. In this paper, a group of systematic experiments are performed to explore how well syntactic functions, including parent nodes, syntactic paths and term ratios, can be used as features to recognize terms under CRF model. In this paper, we describe the construction of a machine learning framework that exploit syntactic information in the recognition of biomedical terms and present the limits of machine learning in generating a novel term candidate list. Conditional random fields (CRF), is used as the basis of this framework. We make an effort to find the appropriate use of syntactic information, including parent nodes, syntactic paths and term ratios under this machine learning framework. The experiment results show that CRF model can achieve good precision in term recognition if trained with known term list. However, with regard to discovering potential novel terms for terminology lexicon editors, CRF model fails to show good performance, if trained with known term list only to predict novel terms in testing corpus. Therefore, this result suggests that more semantic information may be needed to determine a word to be a novel term during a specific period.
AUTOLEX: An Automatic Lexicon Builder for Minority Languages Using an Open Corpus Philippine natural language (NL) resources such as document corpora and lexicons are badly needed in natural language processing research and system development. Building these NL resources requires time and linguistic knowledge. These resources can be built automatically or manually. The manual process of building NL resources such as a lexicon is tedious and is prone to typographical errors and if copied one by one from an existing lexicon may create copyright issues. Thus, this study implemented an automated system that builds a language specific lexicon by using an open corpus or the World Wide Web.In our study, lexical enries are determined based on the frequency of the words in the corpus. Words with higher frequency were selected and words that are below a certain threshold were not selected. This kind of lexicon can be used in variety of human language technolog systems, such as word database, word processesors, software for read back by speech synthesis in Text-to-Speech systems and dictation by automatic speech recognition systems (Al- shalabi and Kanaan, 2004).Our approach in retrieving documents from the web was based on our previous study on a resource builder on a closed corpus (Dimalen, 2004). The resource builder is ideal since it is a system that automatically builds a minority language corpus. We extended the resource builder to retrieve documents in an open corpus by using google API which has access to the google search engine database.Both the corpus and the lexicon that were automatically built by our system are natural language resources that are significant to Lexicology. Lexicology is the branch of descriptive linguistics concerned with the linguistic theory and the methodology for describing lexical information, often focusing specifically on issues of meaning (Al-shalabi and Kanaan, 2004).Lexical information includes lexical semantics, and the study of the syntactic and morphological and phonological properties of words. The aim of this study is to build natural language resources for languages with limited resources or minority languages. Manually building these resources is tedious and costly. These natural language resources such as a language corpora and lexicon will be used for natural language processing research and system development. Tagalog, a minority language was considered in this study as a test bed. This study exploited the use of the WWW to retrieve documents that are written in a minority language. We employed a frequency-based algorithm to build the lexicon. For our evaluation, we considered 260 Tagalog documents extracted from the web as our corpus. From the corpus, the system automatically selected 1,386 candidate unique words based on the threshold (with value of 10) as the lexical entries. Each lexical entry is validated by a language expert. Our evaluation shows an accuracy of 97.84% and only 2.16% error rate. The error was based on incorrectly spelled words or words that are not Tagalog.
Terminological Ontology and Cognitive Processes in Translation * Imagine a situation where non-English speaking European and Asian are debating in English about the issue of the academic degree systems in their respective countries. While a German might be explaining about the Doctor of Science (Habilitation) as their highest academic degree, a Japanese might be having the highest academic degree in Japan -Ph.D. level -in his mind. This imagined conversation shows a typical situation revealing a deep inherent misconception between the two parties since each of them has their own conceptual -and correctunderstanding of the highest obtainable academic degree in their respective countries.Statistical Machine Translation (SMT) is one of the most widespread machine translation approaches. However, when considering rare language-combinations, the lack of "direct" parallel corpora is a critical concern for successfully applying the SMT approach. In case of the rare language-combination: Danish and Japanese, it is possible to collect a reasonably sufficient amount of corpora between Danish and English as well as English and Japanese. However, the availability of bilingual corpora that directly link Danish and Japanese are very limited. Hence, a transitive translation technique using English as a pivot language is often employed in translation of rare language-combinations. A question is how well the transitive machine translation approaches can convey original conceptual meanings of Source Language (SL) words into Target Language (TL) translations in the transitive translation. When considering human translations between rare combinations of languages, human translators typically look up SL-English dictionaries to identify meanings of SL words and then use English-TL dictionaries to determine appropriate TL translations. In such situations, the problem of word sense ambiguation becomes critical. Since a lexical representation often carries several meanings, the transitive translation approach using English as pivot simply amplifies the probability of selecting an inappropriate sense in a TL. Thus the problem of polysemy becomes especially serious in the process of transitive translations. This present work is primarily based on a terminological ontology method ( Madsen et al. 2004;2005), a domain-specific ontology which is used in standardized terminology works such as ISO 704 (2000). Since the traditional study of terminology is widely described as the standardization of terminology, a concept is ideally referred to by a single term. In other words, one single term unambigously designates one single concept. Thus, the use of a terminological method is highly suitable for domain specific translations. The terminological ontology method is potentially useful for providing a precise map of the terms covering the concepts and their interrelations, as well as to contribute to the identification of equivalences between selected languages. By mapping the language-dependent terminological ontologies between selected languages, it will potentially be possible to identify an appropriate translation in a TL based on semantic-rich information. The Feature Specification (FS) approach used in the terminological ontology method has similarities to the Distributed Conceptual Feature (DCF) model, one of the bilingual memory models proposed by De Groot (1997). The similarities explain why the terminological ontology method is potentially useful for conveying original conceptual meanings of SL words into TL translations.To outline the scope of this work, Chapter 2 addresses De Groot´s DCF model based on the theory of bilingual memory, followed by the terminological ontology principles in Chapter 3. In Chapter 4, an example of the terminological ontology mapping that has been manually carried out is demonstrated. Chapter 5 discusses issues concerning the mapping algorithms followed by a consideration of evaluation methods. Finally, the conclusions follow in Chapter 6. This work is based on a terminological ontology method proposed by Madsen et al. (2004; 2005). The main purpose is to bridge a semantic relation between domain specific terms in two languages by mapping two language-dependent terminological ontologies. To explain why this method is preferable from the view of the cognitive process in translation, the terminological ontology method is contrasted with a study made by De Groot (1997) in the area of bilingual memory research.
Detection of Users Suspected of Pretending to Be Other Users in a Community Site by Using Messages Submitted to Non-Target Categories In these days, many people use community sites, such as Q&amp;A sites and social network services, where users share their information and knowledge. One of the essential factors in community sites is anonymous submission. It is important to submit messages anonymously to a community site. This is because anonymity gives users chances to submit messages without regard to shame and reputation. However, some users abuse the anonymity and disrupt communications in a community site. For example, some users pretend to be other users by using multiple user accounts and attempt to manipulate communications in the community site. Manipulated communications discourage other submitters, keep users from retrieving good communication records, and decrease the credibility of the community site. As a result, it is important to detect users suspected of pretending to be other users to manipulate communications in a community site. In this case, identity tracing based on user accounts is not effective because these suspicious users often attempt to hide their true identity to avoid detection. A possible solution is authorship identification based on analyzing stylistic features of messages (Craig, 1999) ( de Vel et al., 2001) ( Koppel et al., 2002) (Argamon et al., 2003) (Zheng et al., 2006). However, most of previous studies selected their learning examples carefully and, consequently, limited the scope of target users. For example, if target users are set to submitters in a certain category of a community site, learning examples are generally selected from their messages submitted to the category, not other categories. Actually, in (Ishikawa et al., 2010), we developed learning examples by using target user's messages submitted to the target category. However, the scope of target users was limited because there is a limited number of users who submitted more messages to the target category than the minimum needed to develop their learning examples. In order to extend the scope of target users, it is necessary to relax the criteria for selecting learning examples. The point is extension's effects on the accuracy of user identification. In other words, when we use learning examples which were not used in cases of previous studies and extend the scope of target users,• how much the accuracy of identification may decrease, or• how many learning examples should be increased to maintain the accuracy. In this study, we propose a method of detecting users suspected of pretending to be other users in a certain category of a community site by using their answers submitted to other (non-target) categories and show the accuracy of user identification when we relax the criteria of selecting learning examples and extend the scope of target users. In this method, we use user identifiers which are developed by learning stylistic features of submitter's messages and determine by whom a series of input messages are submitted. Some users abuse the anonymity and disrupt communications in a community site. Authorship identification based on analyzing stylistic features of messages is effective in detecting these inadequate users. However, in this method, the scope of target users was often limited because the criteria for selecting learning examples were strict. To relax the criteria and extend the scope of target users, we propose a method of detecting users suspected of pretending to be other users in a certain category of a community site by using their messages submitted to other (non-target) categories. Also, we show the accuracy of user identification when we relax the criteria of selecting learning examples and extend the scope of target users.
Detecting Nasty Comments from BBS Posts We focus on Japanese nasty comments that are seen on the Web, particularly, on bulletin board systems (BBS). BBSs are used mainly for information-sharing, consultation, and discussion; however, unfortunately we also see some nasty comments posted on them. Recently, young people, such as primary and secondary students, have been posting such comments. Furthermore, there are many cases where these comments drain the victim emotionally for a long time, and can negatively affect the victim's social life. In a worst-case scenario, the victim commits suicide. These comments are increasing every year all over the world, and have become a social problem, which has been classified as cyber-violence or cyber bullying. This is an effect of the insufficient regulation of the Internet.The nasty comments must be managed automatically. There are companies in Japan that patrols Web pages manually to find nasty comments. However, manual patrolling is very expensive, so the process to detect nasty comments on BBSs should be automated as much as possible.Research on detecting nasty comments is similar to research on classifying harmful contents and detecting spam blogs. The POESIA project (Hidalgo et al., 2002) (Hidalgo et al., 2003), which was funded by the European Commission, created a filter for harmful content. This filter classifies whether Web text is pornographic, and is adapted for the English, Italian, and Spanish languages. In other cases, the NET PROTECT project (Grilheres et al., 2004) developed a text classifier for harmful information. Harmful information in Web texts, including pornography, bomb-making, drugs, and violence, is classified based on machine learning using an support vector machine (SVM) ( Lee et al., 2007). In addition, Kolari et al. discussed how SVM models based on local and link-based features can be used to detect spam blogs ( Kolari et al., 2006).These studies achieved a high level of accuracy, and several companies are now introducing filtering services against harmful sites; therefore, we can expect to find such harmful sites. However, this filtering is limited only to harmful words that deal with subjects such as pornography and drugs. In contrast, because there can be many patterns for nasty comments, they are difficult to detect. Since nasty comments can express harmful expressions not only in words but also in Copyright 2010 by Tatsuya Ishisaka and Kazuhide Yamamoto phrases, we also need to focus on nasty phrases. In addition, we must identify a meaning precisely, because the context and neighboring words can determine whether an expression should be classified as nasty. Following are two sample comments that include a nasty word:(a) (aono seijika wa shine.) (That politician must die. ) (b) (ano ryouri wa baka umai.) (That dish is very delicious. )The " (to die)" comment directs harm to another person, as shown in (a). A word such as "die" can be detected easily regardless of its neighboring words, because we need to only judge whether the comment includes "die". A word such as " (stupid)" make someone who is annoyed or impatient. However " (stupid)" in (b) means " (very)" which is used just for emphasis. Therefore, detection is difficult because we need to consider the neighboring words.Furthermore, a Japanese morpheme analyzer cannot segment the words of BBS posts correctly because they contain several coined words. Therefore, we cannot correctly detect them that are segmented, so nasty words cannot be registered sufficiently. Because of this, we use an n-gram to cope with context and with over-segmented words. The n-gram is used in some natural language processing tasks. Mori and Nagao(Mori and Nagao, 1996) presents a statistical method based on n-gram model for unknown word identification. The method estimates how likely the input string is to be a word. The method cannot cover low frequency unknown words.In the following sections, we present how we detect nasty comments on a BBS with many posts using an n-gram model. We propose a method to detect Japanese nasty comments from posts on bulletin board systems (BBS). Nasty comments can cause many social problem, because they express potentially harmful words and phrases. There are methods to recognize harmful words, but they are insufficient. Therefore, we present a method for detecting such comments on a BBS with many posts using an n-gram model. In addition, we compared our method with a support vector machine (SVM) that is based on nasty words. As a result, we detected nasty comments that are different to those by the SVM. We also observe higher detection accuracy by combining two methods.
Using Various Features in Machine Learning to Obtain High Levels of Performance for Recognition of Japanese Notational Variants In Japanese, there exist several different and similar notational variants of a word. For example, " " (su pa ge te i) and " " (su pa ge tsu te i) can be used for "spaghetti". A dictionary including such notational variants is extremely useful in many cases such as normalization of word expressions in information retrieval and normalization of word expressions in text mining or information extraction. Therefore, we studied the automatic extraction of the notational variants of a Japanese word.In terms of related studies, Brill et al. extracted pairs of Japanese Katakana expressions and their corresponding English expressions (Brill et al., 2001), McCallum et al. detected matching or mismatching in restaurant names or paper citation data (McCallum et al., 2005), and Tsuruoka et al., Aramaki et al., and Okazaki et al. extracted synonyms or notational variants in a restricted domain such as biology and medical science (Tsuruoka et al., 2007;Aramaki et al., 2008;Okazaki et al., 2008). In contrast to these studies, we handled the automatic extraction of Japanese notational variants in all the domains.We used supervised machine learning approaches, in which we can easily use various types of information in the process of extracting Japanese notational variants.By supervised machine learning approaches (Suzaki et al., 2002;Masuyama and Nakagawa, 2004;Shimada et al., 2005), we used the information that existing dictionaries, including notational variants, provide. We also used the information that similar characters such as "A/a" and " " (ga/ka) are likely to be used for notational variants. By using many other types of useful information, we realized extremely high levels of performance in recognition of Japanese notational variants.Furthermore, we not only used machine learning but also used pattern based approaches in order to extract a significant number of notational variant pairs (3 million).In this study, we handled only Japanese notational variants where the edit distance is only one character. This is because many Japanese notational variants satisfy this condition. We think that the extraction of Japanese notational variants where the edit distance is more than one character can be performed by an extended version of the method in the case when the edit distance is one character. We proposed a method of using machine learning with various features for the recognition of Japanese notational variants. We increased 0.06 at the F-measure by specific features using existing dictionaries and character pairs useful for recognizing notational variants and obtained 0.91 at the F-measure for the recognition of notational variants. By using the method, we could extract 160 thousand word pairs with a precision rate of 0.9. We also constructed a method using patterns in addition to machine learning and observed that we could extract 4.2 million notational variant pairs with a precision rate of 0.78. We confirmed that our method was much better than an existing method through experiments.
Korean Kes-Relative Clauses as Stages  The kes-relative clause (kes-RCs), the so-called internally headed relative clause (IHRC), is identified by a relative clause and the morpheme kes, which is followed by a predicate that requires an entity argument. Typical examples of the kes-RC are shown in (1), where the relative clause appears within the bracket.(1) a. [cholaha-n senpi-ka kama-lul syewu-ko ccelccelmya-nun] kes-ul [poor gentleman-Nom a palanquin-Acc stop in a fluster-Imprf ] kes-Acc manna-ass-ta meet-Pst-Decl 'A poor gentleman was in a fluster while holding on a palanquin and I met him.' b.[chenge-lul passak mali-n] kes-ul tocangkwuk-uy comisengpun-ulo [herring-Acc completely dry-Prf-Rel] kes-Acc soup seasoning-Instr sse-ss-ta use-Pst-Decl 'A/The herring had been completely dried, and I used it to give flavor to the soup. ' (Sejong Corpus)Kes-RCs have been claimed to be subject to the Relevancy Condition ( Kuroda 1976 for Japanese, Y.-B. Kim 2002, M.-J. Kim 2008 for Korean). This paper examines two recent renditions of the Relevancy Condition in Y.-B. Kim (2002) and M.-J. Kim (2002) and shows that they still suffers from empirical problems. As it will be argued later, the varying degree of acceptability of kes-RCs with respect to the relative clause's tense and aspect can be better explained under the notion of stages in the sense of Carlson (1997). 1 2 Recently, M.-J. Kim (2008) provided a reformulation of the Relevancy Condition on kes-relative clauses (kes-RCs) (Kim, Y.-B. 2002), the so-called internally headed relative clauses in Korean. In her analysis, the bipartite conditions of Kim (2002), one involving simultaneity between the main and the relative clause and the other involving a &apos;resultant theme&apos;, are collapsed into one involving a temporal overlap between the main and the relative clauses. In Kim (2008), the kes-RC is assumed to describe a temporary state which overlaps with the main event (i) if the relative clause contains an atelic predicate and the aspect is progressive, or (ii) if the relative clause contains a telic predicate and the aspect is perfect as well as progressive. This paper, however, claims that M.-J. Kim&apos;s analysis still suffers from empirical problems, based on new type of examples; they show that (i) the relative clause can contain an atelic, perfect predicate if it is a stative type and (ii) the relative clause denote the volition or prediction of the speaker. The present paper proposes that kes-RCs denote a stage, which instantiates at the time of the main clause&apos;s event.
The Acquisition of Imperfective Aspect Marking in Korean as a Second Language by Japanese Learners It has been observed that there is a strong association between the inherent aspect of verbs and the acquisition of tense-aspect morphology. The acquisition of tense-aspect morphology has shown an interesting universal pattern in both first and second language acquisition (Andersen &amp; Shirai, 1996). This universal tendency is referred to as the Aspect Hypothesis (Andersen &amp; Shirai, 1994), which claims that there is a universal developmental sequence of tense-aspect markers: past tense form starts with achievement verbs, and progressive starts with activity verbs.The Aspect Hypothesis has been verified through various cross-linguistic studies (see Bardovi-Harlig, 2000;Shirai, 2009 for a review). The studies of Asian languages have not only verified the Aspect Hypothesis (e.g. Shirai &amp; Kurono, 1998) but also extensively investigated how imperfective aspect is acquired (see Li &amp; Shirai, 2000 for the L1 acquisition of Chinese zai and zhe ;Ishida, 2004, Sugaya &amp; Shirai, 2007 for the L2 acquisition of Japanese -te i-). As for the L2 acquisition of Japanese imperfective aspect, it has been observed that -te i-appears first with activities for the progressive use and expands to achievements for result state use, with the assumption of progressive meaning as the prototype of -te i- (Shirai &amp; Kurono, 1998). An early study of L2 Korean (Lee &amp; Kim, 2007) found that the progressive -ko iss-developed earlier than the resultative -ko iss-and -a iss-in L1 English learners of Korean.This study tests whether this arguably universal developmental pattern (the progressive is acquired earlier than the resultative) holds true for the L2 acquisition of Korean by Japanese learners, and in addition, this study surveys the influence of instructional order and acquisition order on the L2 acquisition of Korean imperfective aspect. This paper investigates the developmental process through which L2 learners acquire two &quot;imperfective&quot; aspect markers in Korean,-ko iss-(progressive and resultative) and-a iss-(resultative) which attempts to identify language-general and language-specific patterns in the L2 acquisition of the Korean imperfective aspect by Japanese learners by comparing the results with previous research. Study 1 collected cross-sectional data from 55 Japanese learners of Korean as a foreign language and 18 Korean native speakers. The results show that the acquisition order was as follows: the progressive-ko iss-→ the resultative-ko iss-→ the resultative-a iss-. Study 2 examined the influence of instruction order by testing two groups of learners that were taught aspect markers in different orders. The results show that the order of instruction did not yield significant differences except in the rate of accuracy of the resultative marker-a iss-in the comprehension task.
Multi-aspects Review Summarization Based on Identification of Important Opinions and their Similarity As Web services like CGMs have become widely used, people can easily post reviews for products or services. Although handling these information (evaluative information) has become necessary, there exists too much information on the Web. Therefore, extracting information that users want and summarizing them have been expected recently. Intuitively, we can summarize a review with traditional document summarization methods. For instance, Brandow et al. (1995) have summarized a document by extracting sentences with some features such as the presence of signature words and the location in the document. For sentiment summarization, Pang and Lee (2004) have extracted all subjective sentences. They suggested that these extracted sentences could be used as summaries. However, a review basically consists of sentiments with various aspects (i.e., "image quality" and "usability" of a camera). Therefore, we need to extract information for each aspect in the case of review summarization. Aspect summarization can present information without biasing to a specific topic. We focus on multi-aspects review summarization in this research.We generate a summary by extracting important sentences and arranging them. Since we extract summary sentences, we need to discuss which sentences are important and how to extract important sentences. In the case of treating multi-reviews, we need to handle the redundant information. Pang and Lee (2008) have reported that while in traditional summarization redundant information is discarded, in sentiment summarization redundancy indicates the importance of opinions. Therefore, we treat redundancy as a feature for decision of important sentences. We assume that reviews we treat in this research have multiple aspects and a reviewer gives ratings of the aspects (i.e., 0 to 5 stars). Reviewers also write free comments about the target. We leverage three features: ratings of aspects of reviews, the tf -idf value, and the number of mentions with a similar topic. We apply a clustering algorithm to sentences to measure the number of mentions with a similar topic. Then, we aim to integrate those features and generate a more appropriate summary. Figure 1 shows the outline of our method. The development of the Web services lets many users easily provide their opinions recently. Automatic summarization of enormous sentiments has been expected. Intuitively, we can summarize a review with traditional document summarization methods. However, such methods have not well-discussed &quot;aspects&quot;. Basically, a review consists of sentiments with various aspects. We summarize reviews for each aspect so that the summary presents information without biasing to a specific topic. In this paper, we propose a method for multi-aspects review summarization based on evaluative sentence extraction. We handle three features ; ratings of aspects, the tf-idf value, and the number of mentions with a similar topic. For estimating the number of mentions, we apply a clustering algorithm. By integrating these features, we generate a more appropriate summary. The experiment results show the effectiveness of our method.
Generation of Summaries that Appropriately and Adequately Express the Contents of Original Documents Using Word-Association Knowledge Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web ( Hovy and Mareu, 1988;Mani and Maybury, 1999;Barzilay and Elhada, 1997;Goldstein et al., 1999;Marcu, 2000;Ker and Chen, 2000;Hongyan, 2000;Radev et al., 2001;Barzilay and Lee, 2004;Radev et al., 2004;Kato et al., 2005).In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary "terror" for sentences "A bomb went off. Some people were killed. This was triggered by rebel campaign." In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a suitable summary: (i) The contents of the original document are conveyed by the candidate. (ii) The content that is not described in the original document is not conveyed by the candidate.In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different from the existing methods that extract some sentences or words from the original document for generating its summary ( Knight and Marcu, 2002; Lin and Hovy, 2003;Kang et al., 2008).In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by using co-occurring words. We can also perform paraphrasing of various kinds of information by using co-occurring words. Banko et al. generated newspaper headlines by using statistical machine translation ( Banko et al., 2000).In this study, we handled only the case in which the output summary is one word for simplicity; however, we aim to generate a summary comprising sentences as the output in the future. The case where the output summary comprises two words or sentences can be handled by using an extended version of this method. In this paper, we have only described our idea for handling twoword summaries in Section 4. The summarization process that outputs one-word summaries can also be used to categorize documents, because that one word is representative of the main content and theme of the document. We handled the summarization of documents written in Japanese in this study. In this study, our purpose was to make a short summary for sentences. For example , we aimed to make a short summary &quot;terror&quot; for sentences &quot;A bomb went off. Some people were killed. This was triggered by rebel campaign.&quot; In this study, we proposed a new method that generates summaries that can appropriately and adequately express the contents of their respective original documents using word-association knowledge. In this method, we assumed that a good summary comprises words that can express the contents of the original document and does not contain words that are unable to express the contents of the original document. Using statistical tests, we confirmed that the use of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the &quot;lenient&quot; case of experiments.
Adjectival Modification to Nouns in Mandarin Chinese: Case Studies on &quot;cháng+noun&quot; and &quot;adjective+tú shū gun&quot; * In a sense enumerative lexicon, every fine-grained difference will be listed. However, the meaning of an adjective is dependent on the head it modifies. Considering this, (Pustejovsky, 1995) proposed selective binding, which takes place where a lexical item or phrase operates specifically on the substructure of a phrase, without changing the overall type in the composition. (Pustejovsky, 2000) further revealed that adjectives bind into the qualia structure of nouns to select a narrow facet of the noun's meaning. For example, a large (Formal) carved (Agentive) wooden (Constitutive) useful (Telic) arrow ([Formal; Constitutive; Agentive; Telic]) illustrates the richness of the qualia structure and each adjective selects individual qualia.Moreover, (Lapata, 2001) explored polysemous adjectives whose sense changes due to the nouns they modify. The probabilistic model he proposed can provide a ranking on the possible interpretation of the adjectives. (Saint-Dizier, 1998) proposed several extensions to the telic role through analyzing the French adjective "bon". (Lenci, et al., 2000) extended the possible qualia role values to express fine-grained distinctions between semantic types. These studies give us a general idea of the behavior of adjectival modification to nouns and the way of extending qualia structure. However, no previous research on adjectival modification based on selective binding has been conducted regarding Mandarin Chinese.The goal of our research is to find out the interaction between adjectives and their head nouns, including: $t finding out the argument types an adjective can combine with; $ushowing an adjective may modify individuals or events; $vlooking into what qualia role of the head noun that an adjective selects.The following sections are organized as follows. The theoretical basis is introduced in Section 2. Then we analyze "•w(cháng, long)+noun" and "adjective+WW fø™((tú shū gun, library)" respectively in Section 3. Finally, we summarize the paper and suggest future work in Section 4. (Pustejovsky, 1995) gave an interpretation of qualia structure as follows: This paper studies the adjectival modification to nouns in Mandarin Chinese based on selective binding. The main findings include: $tAn adjective can select different types of head nouns as arguments and an adjective may modify an individual or an event. $uThe qualia structure of a noun helps us better understand an adjective&apos;s selectional preference. Meanwhile, an adjective can modify multi-facet or one facet of the qualia role of a noun. $v The adjacent adjective of a noun is not necessarily modifying the noun.
Workshop on Advanced Corpus Solutions *  The initiative for this Workshop on Advanced Corpus Solutions has been taken in order to focus on the need for corpora that take into account that many users are linguists and philologists who do not have an interest in technical matters.
Degrees of Orality in Speech-like Corpora: Comparative Annotation of Chat and E-mail Corpora Traditional speech corpora with phonetic transcription are very labour-intensive to create, involving a huge effort in data collection, sound file management and manual transcription. Automatic transcription is today an alternative, at least for languages with a mature language technology base such as English, but the method is not error free and commercial tools will produce standard orthography, not phonetic transcription. Thus, Luz et al. (2008) report transcription speeds of 22-30 words per minute, for an ASR-assisted post editing method, with a final error rate of 3.3% -7.83%., translating into 20 man-years of work for the one-pass oneannotator transcription of a 25 million-word corpus. A third alternative -and the position taken in this paper -is to use data where people write in a speech-like fashion, without the constraints of ordinary written production, thus in fact providing their own transcriptions. This is the case for both chat-and sms-data, and to a certain degree e-mail text. The paper describes and evaluates the annotation of two such corpora, the Enron e-mail corpus and our own Fantasy chat corpus, comparing these to the written and oral sections of the BNC and the English section of the Europarl corpus which could be described as "listener-transcribed" rather than "speakertranscribed" and also differs from our other data in representing fairly formal, parliamentary speech. This paper describes and evaluates the automatic grammatical annotation of a chat and an e-mail corpus of together 117 million words, using a modular Constraint Grammar system. We discuss a number of genre-specific issues, such as emoticons and personal pronouns, and offer a linguistic comparison of the two corpora with corresponding annotations of the Europarl corpus and the spoken and written subsections of the BNC corpus, with a focus on orality markers such as linguistic complexity and word class distribution.
Parallel Suffix Arrays for Corpus Exploration Corpus search engines (CSE) are in high demand for a number of reasons: (a) They help corpus linguists to obtain statistics and usage information for words, and they do this with much higher accuracy and flexibility than web search engines (cf. Kilgarriff, 2007); (b) they enable the implementation of automated or interactive linguistic tests, for example to verify whether certain generalized syntactic constructions, e.g. represented by finite state grammars, do or do not occur in large corpus samples; (c) if sufficiently powerful, they transform an annotated text corpus into a tool for data mining (cf. Ananiadou, 2009).CSEs accomplish these things in two steps: First, an index is constructed from annotated text, then queries are either manually inputted by users or generated by a separate process; the corpus search engine returns all matching substrings it finds for each query, either in random order, or sorted or grouped by user-defined criteria. It may also identify duplicate results and provide a frequency list of distinct matches. Existing CSEs differ from each other in several ways: (a) The kind of annotations they can process, (b) the complexity of queries, e.g. keyword search as opposed to regular expressions, (c) the data structures used inside the index. The choice of the data structure is significant, as it has a strong influence on how large the index grows in memory and how fast it can process queries. Fast query processing is important, esp. when large sets of auto-created query variations are processed, or when a multi-user system is designed to process many queries in parallel.Section 2 gives an overview of existing CSEs. Their underlying indexes are all based on traditional relational databases or inverted file techniques. Section 3 describes parallel suffix arrays, a new data structure with very good theoretical performance for many types of complex queries, in many respects superior to existing data structures. Section 4 shows how the data structure can be expanded to a powerful CSE which will be called sufex. The final sections report on results of performance tests with a practical implementation of sufex and conclude with a brief discussion of its advantages and disadvantages. This paper describes how recently developed techniques for suffix array construction and compression can be expanded to bring a new data structure, called parallel suffix array, into existence, which is suitable as an in-memory representation of large annotated corpora, enabling complex queries and fast extractions of the context of matching substrings. It is also shown how parallel suffix arrays are superior to existing corpus search engines, in particular when sequential queries and corpora that are hard to tokenize are involved.
A multilingual speech resource: The Nordic Dialect Corpus * We present a corpus that is one of the major outcomes of the big collaborative dialect project Scandinavian Dialect Syntax (ScanDiaSyn) and the Nordic Centre of Excellence in Microcomparative Syntax (NORMS). In this paper we will focus specifically on features that the linguist users have asked for. The project has members from universities representing dialects from five languages in six countries in Northern Europe (Denmark, Faroe Islands, Finland, Iceland, Norway and Sweden). The majority of the project members (as well as present and future users) are linguists and dialectologists, and have little interest in and knowledge of technical matters; their main focus is dialectological research and data collection. The development of the Nordic Dialect Corpus ( Johannessen et al. 2009;Johannessen et al. 2010), which we focus on in this paper, and the Nordic Syntactic Judgments Database ( Lindstad et al. 2009) are the technical responsibility of the Text Laboratory, University of Oslo. When planning and constructing the corpus, we had to pay special attention to user-friendliness, but also to the requirements of the users w.r.t. search options and results handling.There are a number of factors that are challenging with respect to constructing search options and results handling in this corpus:• there are five different standard orthographies corresponding to the five standard languages (Danish, Faroese, Icelandic, Norwegian and Swedish) • the corpus contents consists of transcribed speech • some of the recordings have a double set of transcriptions -orthographic and phonetic • transcriptions should be linked to audio and video and presented nicely • the corpus should be tagged, needing five spoken language taggers, but the tagsets cannot be the same due to linguistic differences • where possible, the same tags should refer to the same types of entity in all the languages • metadata on informants should be usable as filters in search (age, sex, place)• different levels of geographical belonging should be specifiable (country, area, place) • search results should be possible to handle in a number of different ways, including exporting of different formats.• all text from all languages should be searchable at the same time • the users want the search results to come with a translation into English • the users want maps to see where the informants are from Because of the prospective non-technical users of the corpus (mainly linguists and dialectologists), all solutions should be user-friendly and searches with regular expressions should be avoided. We use the corpus system Glossa ( Johannessen et al. 2008), which has been developed at the Text Laboratory and which uses the corpus search system Corpus Workbench (Evert 2005). The latter is based on a query language of regular expressions. Glossa has a front end of clickable boxes and menus, whose information is translated to regular expressions inside the system. To our knowledge no other corpus system exists with this variety of options. This paper describes the Nordic Dialect Corpus, a corpus that consists of transcribed spoken dialects, with sound and video, from five North European languages (Danish, Faroese, Finnish, Icelandic, Norwegian and Swedish). The paper focuses on recent developments that have been added as a result of wishes expressed by the linguist users. These include map views of various selections of search results, English translations of every dialect concordance, and search possibiities and presentation of both orthographic and phonetic transcriptions.
Dialect Corpora Taken Further: The DynaSAND corpus and its application in newer tools The Syntactic Atlas of the Dutch Dialects (SAND) corpus is one of the results of a large-scale dialect syntax project conducted between 2000 and 2003 in the Netherlands and the Dutchspeaking parts of Belgium andFrance (cf. Barbiers, Cornips &amp; Kunst 2007, Barbiers &amp; Bennis 2007). The goal of the SAND project was to gain insight into the (morpho)syntactic variation within and between Dutch dialects, and to make this variation visible in a clear manner. The information uncovered is of great interest to linguists, in particular syntacticians and dialect researchers, because it provides them with a huge amount of dialect data, structured in a wellorganized manner. The output of the SAND project, i.e. the dialect data, has been made accessible in various ways.First, there are two printed atlases that bring into view the dialectal variation concerning various morphosyntactic features such as the use of reflexive pronouns and the construction of relative clauses. Second, there is a freely accessible web-based dynamic atlas (DynaSand, available at www.meertens.knaw.nl/sand/), which displays the dialect data in an orderly fashion. The DynaSAND web application is meant to make the data that was collected in the SAND project available to linguists who do not have advanced computational skills. The linguistic data is stored in a relational database, a search engine is included, and the data is linked to the audio files of the original fieldwork recordings. Also, the geographic coordinates of the locations of the fieldwork are added to the database, which are used for drawing dynamic maps on the basis of search results.A new development is the incorporation of individual language resources like DynaSAND in larger entities that are used to access and search different language resources simultaneously. This is accomplished by the addition of a web service interface to the DynaSAND corpus, i.e. a search interface that is not meant for humans but for other computer programs, so that the data from the corpus can be used in other applications besides DynaSAND itself. Two examples of projects that make use of this extra interface to the DynaSAND corpus are Edisyn (European Dialect Syntax) and MIMORE (Microcomparative Morphosyntax Research Tool).The SAND corpus contains data from 267 dialects collected in oral and telephone interviews and in a postal survey. The collection of the data took place in three phases. The first phase consisted of a written questionnaire consisting of 393 test sentences that served as a pilot study. This questionnaire was sent out to informants of the Meertens Institute at 321 locations in the Netherlands, Belgium and French Flanders, with mostly one informant per location. The informants had to judge whether the test sentence was attested in their dialect, or were asked to translate or complete it. The aim of this pilot study was to get a first impression of the syntactic variation in the dialects of Dutch and of its distribution across the language area. In the second phase of the project oral interviews (involving elicited, not spontaneous speech) were conducted at 267 locations spread across the Netherlands, Belgium and French Flanders. The interviews were based on the responses given in the written questionnaire, thus enabling to focus on a specific phenomenon that had come up in the written questionnaire. In total 456 sentences were asked in these interviews. The last phase of the data collection involved telephone interviews, which served to clear up any uncertainties in the data. In total, 331 sentences were tested in this round. These were either sentences that had been tested in the oral interviews but had not received a clear answer or new sentences that were required to get a more complete picture of some particular phenomenon. In this paper we will expand on the creation and structure of the DynaSAND database as a case study of a corpus tool. Furthermore we will focus on its implementation in other search engines, thereby illustrating how the underlying data is decoupled from its original interface and used in new ways.
The Use of a Cultural Protocol for Quantifying Cultural Variations in Verb Semantic between Chinese and French   In this methodological investigation, we examined the influence of cultural background on viewers&apos; interpretations of visual stimuli and verbs elicited by these materials. French and Mandarin native speakers&apos; interpretations of seventeen short movies, produced by French speakers, depicting various state-changing actions were collected by a 25-item cultural protocol. A slight difference in the familiarity rating of movies is found between French and Mandarin participants. We also found that Mandarin speakers used more general verbs when describing actions depicted by movies with low familiarity rating and children used more conventional forms with movies of higher familiarity. Hierarchical cluster analyses were conducted in selecting movies that were matched in action-interpretations by both language groups.
Towards the Global SentiWordNet  The discipline where sentiment/opinion/emotion has been identified and classified in human written text is well known as sentiment analysis. A typical computational approach to sentiment analysis starts with prior polarity lexicons where entries are tagged with their prior out of context polarity as human beings perceive using cognitive knowledge. Till date, all research efforts found in sentiment analysis literature deal mostly with English texts. In this article, we propose an interactive gaming (Dr Sentiment) technology to create and validate SentiWordNet in 56 languages by involving Internet population. Dr Sentiment is a fictitious character, interact with players using series of questions and finally reveal the behavioral or sentimental status of any player and store the lexicons as the players polarized during playing. The interactive gaming technology is then compared with other multiple automatic linguistics techniques like, WordNet based, dictionary based, corpus based or generative approaches for generating SentiWordNet(s) for Indian languages and other International languages as well. A number of automatic, semiautomatic and manual validations and evaluation methodologies have been adopted to measure the coverage and credibility of the developed SentiWordNet(s).
Towards an automatic measurement of verbal lexicon acquisition: the case for a young children-vs-adults categorization in French and Mandarin This paper focuses on the automatic measurement of verbal lexicon acquisition. Our research combine two sides: a psychological side and a computational side. In a recent work ( Gaume et al., 2008) we showed that the psychological results on verbal lexicon acquisition fit our computational model of the semantic organization of the verbal lexicon. Based on these results, we propose here to use our computational model to build a measurement of the lexical acquisition useful to categorize the healthy young children vs. healthy adults.The first part of this paper presents the Approx protocol, the data collected in French and Mandarin and the first psychological results about verbal lexicon acquisition. Then, the second part accounts for the computational model and describes three preliminary measurements useful in our categorization while the next section describes how to assign a "flexsemic" score to a participant. Then, the way we used this score for the categorization task and the results analysis are exposed. This paper ends with a short presentation of the web platform Flexsem usable to build online categorizations. In this paper we define a lexical metrology in graphs of verbal synonymy to compute the flexsemic score of speakers from their verbal productions in action denomination tasks. This flexsemic score is used to automatically categorize young children versus young adults. We show that this score is effective in French and in Mandarin.
Computational modeling of verb acquisition, from a monolingual to a bilingual study The study reported in this paper is part of a broader project (M3-Model and Measurement of Meaning) which investigates lexical organization of French and Mandarin. 1 The project combines psycho-linguistic and computational approaches for investigating the verb lexical organization of both languages. We therefore deal with two kinds of data: On the one hand the productions obtained through psycho-linguistic experiments performed with French and Mandarin Chinese speakers; On the other hand we systematically exploit existing electronic resources. A schematic organization of the project in proposed in Figure 1. Two parallel cross-linguistic studies were planned. First at the psycho-linguistic level, the same protocol, Approx ( Duvignau and Gaume, 2003;Duvignau et al., 2005;Duvignau et al., 2007)), for analyzing the productions at different ages (P i.f r/P i.tw = French/Taiwanese Population of Age Group i,...) would be applied to both languages for studying productions variability across languages. Then at a computational level, existing lexical resources, in our case paradigmatic graphs 2 (P G.f r/P G.tw = French paradigmatic graph, Mandarin paradigmatic graph), should be aligned thanks to translation resources in order to compare the structural properties and the organization of the graphs. For example, we would like to check whether some lexical clusters in a language are preserved when projected in the other language and if it is the case which ones. A validation across the methodologies is proposed as the following. Given a set of answers of a population, we can look at their properties in the paradigmatic graph. Then as explained in this volume , an hypothesis based on a measure, so-called flexsemy measure, ties the properties of the lexemes nodes in the graph with their psycholinguistic characteristics (which populations produced them) (see section 2). This methodological cross-validation is represented by the vertical arrows in the Figure 1.The paper is structured as follows. We first briefly introduced the whole model build on French and sum-up some previous validation studies and experiments on French language. Then in section 3 we recap the work done to setup and experiment the model on Mandarin language. In section 4 we present the main research issues emerging from the cross-linguistic move. Sections 5 and 6 address two specific issues: the update of an analysis protocol and the data alignment. Section 7 discusses the work on a more general level and gathers the implication of going cross-linguistic for the kind computational psycholinguistics study we started from. Going cross-linguistic is a an important but challenging track for validating a computational model of lexical organization. Our starting point is a computational model that has been established and validated on French language and we attempted to apply it on Mandarin language. The main ingredients of this model are computational lexical resources and a psycho-linguistic protocol involving extra-linguistic material (video-clips). At this stage, all the psycho-linguistic experiments have been ran, most of the resources have been built but some comparative analyses are not fully completed. Still the project is advanced enough to report on the issues we had to address while performing this cross-linguistic move concerning the resources, the analysis of the data and the data alignment across languages.
Cross-sortal Predication and Polysemy Cross-sortal predication is natural language phenomenon, which has not been paid much attention to, perhaps more in lexicographic circles where it serves as a sense disambiguating tool (Cruse, 2000b). This phenomenon nevertheless plays an important role in the development of formal account of lexical semantics and could be seen as one of fundamental problems whose solution could lead to better understanding of semantic well-formedness of natural language expressions. As argued in (Borschev and Partee, 2001), the biggest challenge for formal lexical semantics is the establishment of a relation between types and sorts as well as a formulation of a theory that would account for the contribution of sorts to the semantic well-formedness of expressions.We suggest that cross-sortal predication plays such an important role in the development of formal lexical semantics for three reasons. Firstly because it involves the problem of compositionality and in particular the conditions under which certain predicate can be applied to certain argument even in cases when the sort of the argument mismatches the domain of the predicate. Secondly, it introduces a problem of identity of an argument in composition of predicates. And thirdly, cross-sortal predication seems to be quite common even cross-linguistically and thus calls for a general and flexible solution. This paper develops new treatment of the problem of cross-sortal predication and co-predication in particular. We argue that the solution to these predicate-argument sort mismatches can be solved by a more flexible treatment of polysemy based on the notion of dependent type and dynamic construction of meaning.
Natural Language Production in Database Semantics ⋆  A theory of natural language production (speaker mode) has to answer the following questions: (i) Where does the content serving as input to language production come from? (ii) In which format is this relatively language-independent content stored, processed, and re-trieved? and (iii) How is activated content mapped into well-formed surfaces of a certain natural language? After brief answers to (i) and (ii), this paper concentrates on (iii), illustrating the time-linear method of Database Semantics (DBS) on some of the most notorious grammatical constructions of natural language.
Change of Location and Change of State: How Telicity is Attained  This paper basically discusses parallels between change of location (or space) and change of state. Change of state is analogous to change of location, involving an abstract state Source and an abstract state Goal with abstract Path, involving telicity and showing alternations crosslinguistically. Spatial change is least abstract, whereas temporal change and state change are more abstract and psychological change is most abstract, exhibiting various phenomena of degree modification and telicity differentiations. Abstraction causes argument reduction and change in syntactic behavior. State-oriented predicates are modified by the equivalents of the degree modifier very and process-oriented ones by the quality modifier well and its equivalents.
VARIOUS EVIDENTIALS in KOREAN  Evidentiality is a grammatical category which deals with the source a speaker has for his or her statement, whether he/she saw it, or heard it, or inferred it from indirect evidence. In some languages it is obligatory in every sentence, and there are also languages in which it is an optional category (Jacobsen, 1986;Aikhenvald, 2003a;Aikhenvald, 2004). According to Aikhenvald (2004: 1), about a quarter of world's langauges have obligatory evidentiality systems.Evidentiality has been typically considered as one of the subcategories of modality (Palmer, 1985(Palmer, /2002Willett, 1988;Frawley, 1992;Bybee et al. 1994). But recently some linguists are of the position that the two are separate categories (Bernd Heine, p.c., Aikhenvald, 2003a;de Haan, 2001de Haan, , 2005; see also Nuyts, 2006: 2, de Haan, 2006. This paper will take the second position and try to differentiate the two categories in Korean.Since Chafe &amp; Nichols (eds.) (1986) evidentiality has been one of the important issues in the recent studies of linguistic typology (cf. Johanson &amp; Utas (eds.), 2000; Aikhenvald &amp; Dixon (eds.), 2003;Aikhenvald, 2004). The Korean evidentials are thereby not so much talked about, although we observe some interesting phenomena in this language. Jae-mog Song (2002) might be the first attempt on the topic. It is concerned with the verbal ending -deo-, a visual evidential marker. In Korean we find some further meaningful evidential markers. This paper explores the evidential markers/expressions in Korean in general to contribute to the typological discussions in this area.Among Korean grammarians the ending -deo-is traditionally categorized as a past retrospective marker (cf. Hyun-Bae Choi, 1937;Ung Heo, 1987) or a mood or aspect marker (cf. Ho-min Sohn, 1975;Hyo-Sang Lee, 1991). Recently it is analyzed as an evidential marker by Jae-mog Song (2002). Primarily it is used as a visual/sensory evidential (cf. (1)-(2)).(1) Mary-ga bang-eseo ja-deo-ra. Mary-NOM room-LOC sleep-SEN-DCL. 'I saw that Mary was sleeping in the room. '(2) Mary-ga bang-eseo ja-deo-nde. Mary-NOM room-LOC sleep-SEN-DCL. 'I saw that Mary was sleeping in the room. 'If combined with a 'say' verb, the ending -deo-develops a report/hearsay evidential (cf. (3)-(4)).say-PST-DCL 'Peter said that Mary was sleeping. 'say-SEN-DCL 'I heard that Peter said that Mary was sleeping. 'The report/hearsay meaning came from the 'say' verb -malhada combined with the evidential marker -deo-. We may say that this ending functions yet as a sensory evidential here. But in the meanwhile the malha-deo-ra(say-SEN-DCL) developed to a separate ending and functions as report/hearsay evidential by itself. The developmental process may be explained as follows: In Korean we also use the verb hada, literally 'do', as 'say' verb instead of the full form -malhada, i.e. instead of (4) we can say this as in (5a). In (5a) we may delete the COMP to get the sentence (5b).(5) a. Peter-ga [Mary-ga ja-n-da-go] ha-deo-ra. Peter-NOM [Mary-NOM sleep-PRS-DCL-COMP] do/say-SEN-DCL. 'I heard that Peter said that Mary was sleeping. 'do/say-SEN-DCL. 'I heard that Peter said that Mary was sleeping. ' (COMP deletion from (5a))In (5b) we can delete the subject of the main clause to get the sentence (6). Now in (6) the status of ha-deo-ra as the main verb is doubtful. In this sentence we have namely two verbs, jan-da and ha-deo-ra. The first is related to the subject as main verb. But the latter is not directly related to the subject of the sentence. It rather relates to the position or attitude of the speaker. We may consider this an evidential auxiliary. 1 (6) Mary-ga ja-n-da ha-deo-ra. Mary-NOM sleep-PRS-DCL do/say-SEN-DCL. 'I heard from someone that Mary was sleeping. '(Main Cl. Subj. deletion from (5))What is more interesting is that we may contract the VP part of (6) like in (7), i.e. the evidential auxiliary ha-deo-ra is contracted to bound morpheme cluster -deo-ra, in which -deofunctions now as a report/hearsay evidential marker.(7) Mary-ga ja-n-da-deo-ra. Mary-NOM sleep-PRS-END-RPT-DCL. 'I heard/It is said that Mary is/was sleeping. ' (ha-deletion from (6)) If we compare the sentence (1) and (7), we find a slight difference between them. In (7) we have -n-da-, which is the trace of the erstwhile complex sentence such as (4). In present Korean, ja-n-da-deo-ra as shown in sentence (7) is written as a single word. But it is pointed out that the word status of the expression ja-n-da-deo-ra is dubious, because semi-final endings can be inserted between -da-and -deo-. In this case it is analyzed as a simple contracted form. The same point could be argued in the other contracted constructions below. 2 Now there are some interesting examples in (8) which are related to this discussion. The evidential function of deo in (8a) is obvious, which is supported by the ungrammatical sentence (8b). 3 But deo in (8c-d) seems to be something other than an evidential marker. A detailed discussion on this topic lies beyond the scope of this paper. Regardless, it is nontheless vital to distinguish at least two functional categories of deo in present Korean. 3 The 'say' verb (mal)hada and related phenomena Evidentiality is one of the important issues in the recent studies of linguistic typology whereby the Korean evidentials are not so much talked about. In Korean the evidentiality is not so systematically represented as other grammatical categories such as tense or honorifics. But it does have some means for evidential expression. The past retrospective ending-deo-has this function. And the &apos;say&apos; verb malhada underwent many kinds of formal reduction and contraction to develop various report/hearsay evidential markers which are very frequently used in colloquial speech. The &apos;see&apos; verb boda expresses also the evidential meaning in the biclausal structure or as an auxiliary. Besides we have some other auxiliaries for this purpose. We propose two simple tests to distinguish the modal and evidential auxiliaries.
  
Language Model Weight Adaptation Based on Cross-entropy for Statistical Machine Translation Language modeling is applied in many natural language processing (NLP) applications, including automatic speech recognition (ASR) and SMT. In reality, we often encounter the scenario in which the performance of language model learned from given dataset changes drastically among different datasets. Many adaptation techniques have been proposed to tackle this problem in the field of ASR. A similar situation arises with respect to SMT. In SMT we build language model from large amounts of monolingual data but incorporate it in the translation task of the dataset that is not well covered by the model. This inconsistency inevitably affects the SMT training procedure, making adaptation techniques a necessity.Different from other tasks, language model is incorporated under a log-linear framework in SMT. Specifically, for each source sentence f , we search for the final translation e * among all possible candidates under the following equation:P (e * |f ) = arg max e P r(e|f )Under log-linear model, the posterior probability P r(e|f ) can be decomposed as:where h m (e, f ) is a feature function and λ m is related weight for m = 1, . . . , M .Under the above framework, we tune the model weight on an independent development dataset, and then we use the obtained weight to translate diverse datasets whose domain or related information might be previously unknown. It is noticeable that the weight obtained from Minimum Error Rate Training (MERT) matches the development dataset well, whereas it would be bias-estimated for others. Although the value of each feature's weight represents its importance in the decoding procedure, such type of importance might vary for different datasets under a specific language model. In this article we concentrate on the bias-estimation of language model weight, i.e., the difference between the oracle and actual LM weight as shown in section 3. We measure the similarity between datasets based on cross-entropy of translation output according to a given language model, adapt the LM weight based on the ratio of the cross-entropy and obtain the final results through a second-pass translation. Our LM weight adaptation method is also related with density ratio estimation, as mentioned in ( Tsuboi et al., 2008), in which reweighting approach is proposed to overcome the bias due to the different distribution of test and training data.The remainder of this paper is organized as follows: Related work of LM adaptation is presented in Section 2. In Section 3 we discuss the problem of LM weight bias-estimation in machine translation. And in Section 4, cross-entropy is proposed as a metric for measuring the similarity between different datasets and we further present our adaptation method. Experimental results are shown in Section 5. We conclude and present several directions for future work in the last section. In this paper, we investigate the language model (LM) adaptation issue for Statistical Machine Translation (SMT). In order to overcome the weight bias on the LM obtained from the development data, a simple but effective method is proposed to adapt the LM for diverse test datasets by employing the cross entropy of translation hypotheses as a metric to measure the similarity between different datasets. Experimental results show that the cross entropy of a test dataset is closely correlated with the bias in estimating the language models and our adaptation strategy significantly outperforms a strong baseline.
A grammar design accommodating packed argument frame information on verbs In most "deep" grammar implementations, information about the argument frame of a verb is specified in the lexicon. This can be observed in grammatical frameworks such as Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994), Lexical Functional Grammar (LFG) (Bresnan, 2001), and Combinatory Categorial Grammar (CCG) (Steedman, 2000). Deep grammars need to be as precise as possible since they are not only expected to parse grammatical sentences, but also not to parse ungrammatical sentences. So, in order to avoid overgeneration, the grammar needs, among numerous other things, to contain information about what syntactic frames a verb is expected to appear in. The most natural place to put this information is in the lexicon.In this paper I will discuss one problem associated with the way argument frame information is specified in a lexicalist grammar, namely the use of multiple lexical entries for one verb form in cases where the verb may appear in more than one argument frame. Multiple lexical entries for one form leads to an increased processing effort for the parser. A possible solution to this problem is to pack the information from the different entries into one. I will present a constructionalist grammar design, where each verb form is assigned a single lexical entry with a packed representation of the possible argument frames of the verb, using a type hierarchy of argument frame types to account for argument frame alternations. I will show how the grammar design compares to HPSG, which is the framework mostly used for deep grammar implementations, and compare two versions of the implemented grammar, one with an expanded lexicon and one with a packed lexicon, with regard to competence and performance. This paper presents a comparison of two designs for implementing argument frame information on verbs. In the first design, alternating verbs will be represented with one lexical entry pr. possible argument frame. In the other design, each verb form will be associated with only one lexical entry containing a packed representation of the possible argument frames. The first design represents how valence alternations are treated in lexicalist grammars, while the second shows how valence alternations can be handled in a construction-alist grammar. The comparison is done with an implemented &quot;deep&quot; grammar of Norwegian.
Quantification and the Garden Path Effect Reduction: The Case of Universally Quantified Subject Japanese has several types of temporarily ambiguous sentences (TASs) (Inoue, 2006). This paper focuses on the pattern 'NP-NOM [  The first NP keekan-ga 'police.officer-NOM' tends to be construed as the subject of the following VP, yielding the reading "a police officer caught a criminal" but this interpretation crashes when another NP otoko-ni 'man-DAT' shows up after the V tsukamaeta 'caught'. To get the correct interpretation, the VP preceding otoko-ni 'man-DAT' must be construed as (part of) the relative clause modifying that NP, and the first NP must be construed as the subject of the sentence-final V itta 'said'. This reinterpretation process is known as the garden path (GP) effect.However, the GP effect of TAS slightly reduces when only a bare NP keekan 'police.officer' in (1) is replaced with a universally quantified NP subeteno keekan 'all police officers' as in (2). We are indebted to three anonymous PACLIC reviewers, Masakatsu Inoue and Michinao F. Matsui for their invaluable comments on an earlier version of this paper. All remaining inadequacies are our own. This research is partially supported by Osaka Gakuin University and Grant-in-Aid for Scientific Research (C), 18530578 and 21500152 of the Japan Society for the Promotion of Science (JSPS). This difference suggests that semantic representations play significant roles since the syntactic structure of (1) and (2) are all the same, 1 and that the quantificational structure introduced by the universal quantifier makes (2) easy to interpret. This paper investigates the effect of quantification in sentence processing. The experimental results show that temporarily ambiguous sentences that begin with the universally quantified NPs reduced the garden path effect in contrast to the ones that begin with bare NPs. This fact is accounted for by assuming that discourse representation structures are incrementally constructed, and a tripartite structure introduced by the universal quantifier gives a room for temporal ambiguity while a single box associated with a bare NP forces one interpretation and to get the correct interpretation, the single box must be rewritten, which results in the garden path effect.
A Simple Surface Realizer for Filipino * Text generation systems are computer applications that use theories of artificial intelligence and computational linguistics to automatically produce documents, reports, explanations, help messages, stories, and other kinds of texts. Such systems typically follow a basic three-stage pipeline presented by Dale and Reiter (2000) comprising of content determination, sentence planning, and surface or linguistic realization.Surface or linguistic realization is a process whereby abstract or symbolic representations of information are transformed into readable text in a target human language. A number of systems that generate text rely on an external surface realizer during their realization phase to form syntactically and morphologically correct sentence structures. SimpleNLG (Venour and Reiter, 2007) is a popular choice as its library of Java functions provides an interface for applications to build phrases and simple sentences, and to perform inflectional morphological operations and orthography to generate grammatically correct and formatted English sentences. It is a grammar-based realizer engine that accepts canned and non-canned input representations and produces an output string in a deterministic fashion (Gatt and Reiter, 2009).Various systems have utilized simpleNLG in the latter phase of their generation process. The automatic story generators Picture Books (Hong et al., 2009;Ang et al., 2011) used simpleNLG to transform text specifications in the abstract story tree into story text that can be read by young children. The learning tool for spelling and vocabulary, Pun World ( Aban et al., 2010), as well as the learning tool for Filipino heritage learners, SalinLahi (Cheng et al., 2009) utilized simpleNLG to perform formatting on the generated feedback before the surface text is displayed in the user interface. The text generator system, Vigan (Chen et al., 2008), used simpleNLG to transform internal descriptions of museum objects into surface text.Although the functions provided by simpleNLG to produce a sentence in the target language is flexible enough to accept phrases (i.e., noun phrase, verb phrase, prepositional phrase), the structure of a Filipino sentence necessitates the need for a Filipino text realizer that can support the nuances of Filipino grammar rules, specifically those involving word order, generation of markers for nouns, and morphological rules for generating verbs in various tenses.In this paper, we present the development of FilSuRe, a simple Filipino surface realizer that was patterned after simpleNLG. It provides a library of Java functions as an interface for applications to generate the correct markers, determiners and inflections of words to build phrases and simple sentences in the Filipino language.The rest of this paper is organized as follows. Section 2 presents an overview of grammar constructs from the balarila or the official grammar of the Filipino language and how this affected the design of FilSuRe. The discussion begins with the types of sentence structures, followed by verbs, nouns, adjectives, and adverbs. Testing of the API through interfacing with an NLG application, the Booklat story generator system, is presented in Section 3. The paper ends with a summary of research findings and further work to improve the library. Surface realizers are used at the final phase of natural language text generation to convert abstract or symbolic representations of information to linguistic forms in a particular human language. Rules of grammar for the target language are applied to produce text that is syntactically, morphologically and orthographically correct. In this paper, we present the development of FilSuRe, a simple surface realizer for Filipino. We also present how the Booklat system was produced by revising the realization phase of a prototype automatic story generator system, Picture Books, to use the API library of FilSuRe in order to generate children&apos;s stories in Filipino.
A Graph-based Bilingual Corpus Selection Approach for SMT * In statistical machine translation, large scale of bilingual corpus is very important. In order to improve the quality of translation, there are two viewpoints about the use of corpus.One way is to collect more and more bilingual corpus to improve the quality of translation model, such as extracting the sentence pairs from the comparable corpus ( Smith et al., 2010;Uszkoreit et al., 2010). However, some researchers found that, after the quantity of the sentence pairs ( Han et al., 2009) in the corpus reaches some extent, adding more sentence pairs will not improve the quality of translation significantly. On the other hand, larger and larger corpus will consume more and more resources, which hinders the research progress of machine translation in some degree.This type of approaches assumes the sentence pairs in the corpus are independent each other, not considering the relationship between sentence pairs and their effect on the translation.The other view is to mine the potential of training corpus through corpus selection and optimization to improve the quality of the translation model. And it also includes three ways: the first one is to select and optimize the training corpus to adapt to the test set ( Lu et al., 2007) or the domain ( Yasuda et al,. 2008); the second one is to select the sentence pairs with high quality as training corpus (Chen et al., 2006;Han et al., 2009), in which the quality is measured through the features of the sentence pair itself, such as the number of words that can be translated each other in the sentence pair; the third one is to measure and sort the sentence pairs based on the number of unknown n-grams in the sentences, and then select the sentence pair with the highest scores each time ( Eck et al. 2005).This type of approaches considers the quality difference between the sentence pairs in the corpus. However, it still views the sentence pairs as independent.In this paper, we assume the quality of the translation model is related to the coverage and quality of the selected corpus, and expect to select the sentence pairs with high quality as possible when maximizing the coverage of the selected corpus. And we propose a graph-based bilingual corpus selection approach, which makes use of the structural information of corpus to measure and update the importance of each sentence pair, and then selects a sentence pair with the highest importance each time. The underlying principle is that we should select a sentence pair each time to maximize the coverage and quality of the selected sentence pairs.In the rest of this paper, we first introduce how to measure the importance of each sentence pair based on the bilingual graph in Section 2, and then describe the framework of graph-based bilingual corpus selection approach in Section 3, emphasizing on corpus selection algorithm. Section 4 shows the results of the experiments, and we conclude in Section 5 and 6. In statistical machine translation, the number of sentence pairs in the bilingual corpus is very important to the quality of translation. However, when the quantity reaches some extent, enlarging the corpus has less effect on the translation quality; whereas increasing greatly the time and space complexity to train the translation model, which hinders the development of statistical machine translation. In this paper, we propose a graph-based bilingual corpus selection approach, which makes use of the structural information of corpus to measure and update the importance of each sentence pair, and then selects a sentence pair with the highest importance each time. Our experiments in a Chinese-English translation task show that, selecting only 50% of the whole corpus by the graph-based selection approach as training set, we can obtain the near translation result with the one using the whole corpus.
Context Resolution of Verb Particle Constructions for English to Hindi Translation Quality of a translation often depends upon how correctly the verb of the source language sentence is translated into the target language. In a sentence the verb acts as the binding agent, and is therefore considered the most important component of a sentence. Consequently, verbs need to be handled in a more systematic way for a Machine Translation (MT) system. This is more pertinent with respect to translation from English to Indian languages, Hindi in particular. This is because English verbs are often polysemic, whereas Hindi has different verbs for each of the senses. For illustration, according to WordNet 1 , the verb "break" has 59 senses, "make" has 49 senses, "give" has 44, "get" has 36 senses and so on. Almost for each of these senses a different and specific verb exists in Hindi. Consequently, selection of an appropriate Hindi verb is very important during translation from English to Hindi (or any other target language). This can be determined by identifying the sense in which the English verb is used. But it has been observed that this is not so straightforward, as often other features like semantics of other components of the sentence have to be looked into for correct translation.English verbs can be classified as: "Single verbs" and "Phrasal verbs" (Singh, 2003). Single verbs are formed using only single words, e.g. "go", "decide", "take", and "give". Phrasal verbs are made of two or more words, like "take off", "make up", "do away with", "put up with" etc. Our focus in this paper is on translation of this special category of verbs, the 'phrasal verbs' from English to Hindi. The motive here is to capture the correct sense and hence provide correct translation of phrasal verbs in Hindi, as their structures. In the rest of the paper we shall be using the term VPC instead of "phrasal verb".The difficulty in handling English VPCs is often due to their flexible syntactic structure in a sentence. Consider, for example, VPCs comprising a verb followed by a preposition. This can happen in two ways: a) Intransitive verbs are followed by some prepositions e.g. "refer to", "look at". b) Some VPCs have the structure of verb + preposition to convey some exact sense, e.g. "put out", "put off", and "run into". Hence the primary difficulty is to understand the sense of the verb + preposition combination. But the difficulty here is that in some cases the prepositions in phrasal verbs can appear before or after an object, whereas in other cases they can appear only before the object and never after it. Some examples to illustrate this point are:(a) She switched off the light vs. She switched the light off.(b) The customer threw away the plates vs. The customer threw the plates away.In examples (a) and (b) prepositions "off" and "away" can appear before as well as after the object (light in example (a) and plates in example (b)). However, in examples (c) and (d) below the preposition "out" can appear only before the object (change and solution, respectively) and not after it. It is worth noting that the preposition "out" in both the sentences is part of the VPC, and not because the verbs concerned are intransitive.(c) The new manager brought out a change. (d) The programmer found out a solution. VPCs are generally not handled properly in the existing English to Hindi Machine Translation (MT) systems. Table 1 shows translations of VPCs for two simple sentences as given by some of the most commonly used English to Hindi MT systems, namely Google 2 , MANTRA 3 , MaTra2 4 and Anuvadaksh 5 , which we shall refer to as MT1, MT2, MT3 and MT4, respectively in Table 1. By him light on change did -Sense of "switch" is incorrect in MT1, MT2 and MT4. -The preposition "on" is taken as an entity independent of the verb "switch" by all the systems.He put on the shoes. This shows the need and necessity to look into these issues and thus design methods to handle these problems. In this work we focus on this aspect of English to Hindi MT in detail. The paper is organized as follows. Section 2 discusses some of the previous works done for verb particle constructions (VPCs) and the problems faced during the English to Hindi translation of a VPC. A detailed analysis of the VPCs with their respective Hindi verbs based on the senses, and the VPC separability are also discussed in Section 2. Various rules for identification of a VPC and some methods for resolving the context for VPCs for English to Hindi machine translation are discussed in Section 3. Section 4 concludes the paper. Verb Particle Constructions (VPCs) are flexible in nature and hence quite complex and challenging to handle. As a consequence, VPCs generate a lot of interest for NLP community. Despite their prevalence in English they are not handled very well, and hence often result in poor quality of translation. In this paper we investigate VPCs for English to Hindi translation. An English VPC can have different meanings in Hindi based on what its neighboring entities are. The paper focuses on finding the correct Hindi verb for an English VPC. We also discuss some rules for VPC identification, and approaches for resolving the context of a VPC for English to Hindi machine translation.
Improving Sampling-based Alignment by Investigating the Distribution of N-grams in Phrase Translation Tables Phrase translation tables play an important role in the process of building machine translation systems. The quality of translation table, which identifies the relations between words or phrases in the source language and those in the target language, is crucial for the quality of the output of most machine translation systems. Currently, the most widely used state-of-the-art tool to generate phrase translation tables is GIZA++ (Och and Ney, 2003), which trains the ubiquitous IBM models (Brown et al., 1993) and the HMM introduced by (Vogel et al., 1996), in combination with the Moses toolkit ( Koehn et al., 2007). MGIZA++, a multi-threaded word aligner based on GIZA++, is proposed by ( Gao and Vogel, 2008).In this paper, we investigate a different approach to the production of phrase translation tables: the sampling-based approach (Lardilleux and Lepage, 2009b). This approach is implemented in a free open-source tool called Anymalign. 1 Being in line with the associative alignment trend illustrated by (Gale and Church, 1991;Melamed, 2000;Moore, 2005), it is much simpler than the models implemented in MGIZA++, which are in line with the estimating trend illustrated by (Brown et al., 1991;Och and Ney, 2003;Liang et al., 2006). In addition, it is capable of aligning multiple languages simultaneously; but we will not use this feature here as we will restrain ourselves to bilingual experiments in this paper.In sampling-based alignment, only those sequences of words sharing the exact same distribution (i.e., they appear exactly in the same sentences of the corpus) are considered for alignment.Part of the research presented in this paper has been done under a Japanese grant-in-aid (Kakenhi C, A11515600: Improvement of alignments and release of multilingual syntactic patterns for statistical and example-based machine translation).The key idea is to make more words share the same distribution by artificially reducing their frequency in multiple random subcorpora obtained by sampling. Indeed, the smaller a subcorpus, the less frequent its words, and the more likely they are to share the same distribution; hence the higher the proportion of words aligned in this subcorpus. In practice, the majority of these words turn out to be hapaxes, that is, words that occur only once in the input corpus. Hapaxes have been shown to safely align across languages (Lardilleux and Lepage, 2009a).The subcorpus selection process is guided by a probability distribution which ensures a proper coverage of the input parallel corpus:where k denotes the size (number of sentences) of a subcorpus and n the size of the complete input corpus. Note that this function is very close to 1/k 2 : it gives much more credit to small subcorpora, which happen to be the most productive (Lardilleux and Lepage, 2009b). Once the size of a subcorpus has been chosen according to this distribution, its sentences are randomly selected from the complete input corpus according to a uniform distribution. Then, from each subcorpus, sequences of words that share the same distribution are extracted to constitute alignments along with the number of times they were aligned. 2 Eventually, the list of alignments is turned into a full-fledged translation table, by calculating various features for each alignment. In the following, we use two translation probabilities and two lexical weights as proposed by (Koehn et al., 2003), as well as the commonly used phrase penalty, for a total of five features.One important feature of the sampling-based alignment method is that it is implemented with an anytime algorithm: the number of random subcorpora to be processed is not set in advance, so the alignment process can be interrupted at any moment. Contrary to many approaches, after a very short amount of time, quality is no more a matter of time, however quantity is: the longer the aligner runs (i.e. the more subcorpora processed), the more alignments produced, and the more reliable their associated translation probabilities, as they are calculated on the basis of the number of time each alignment was obtained. This is possible because high frequency alignments are quickly output with a fairly good estimation of their translation probabilities. As time goes, their estimation is refined, while less frequent alignments are output in addition.Intuitively, since the sampling-based alignment process can be interrupted without sacrificing the quality of alignments, it should be possible to allot more processing time for n-grams of similar lengths in both languages and less time to very different lengths. For instance, a source bigram is much less likely to be aligned with a target 9-gram than with a bigram or a trigram. The experiments reported in this paper make use of the anytime feature of Anymalign and of the possibility of allotting time freely. This paper is organized as follows: Section 2 describes a preliminary experiment on the sampling-based alignment approach implemented in Anymalign baseline and provides the experimental results from which the problem is defined. In Section 3, we propose a variant in order to improve its performance on statistical machine translation tasks. Section 4 introduces standard normal distribution of time to bias the distribution of n-grams in phrase translation tables. Section 5 describes the effects of pruning on the translation quality. Section 6 presents the merge of two aligners' phrase translation tables. Finally, in Section 7, conclusions and possible directions for future work are presented. This paper describes an approach to improve the performance of sampling-based multilingual alignment on translation tasks by investigating the distribution of n-grams in the translation tables. This approach consists in enforcing the alignment of n-grams. The quality of phrase translation tables output by this approach and that of MGIZA++ is compared in statistical machine translation tasks. Significant improvements for this approach are reported. In addition, merging translation tables is shown to outperform state-of-the-art techniques.
Plural Problems in the Nominal Morphology of Marathi* Marathi, an Indo Aryan language, makes a two-way number distinction, singular and plural. The number feature is synthetically marked on the noun itself. Two linguistic factors determine the plural formation of Marathi nouns: (a) grammatical gender, whether a noun is masculine, feminine or neuter and (b) the final segment of the noun. As a part of our ongoing research on the acquisition of Marathi morphology, we developed tests to see whether Marathi speaking children and adults apply tacitly known rules of inflectional morphology to new-coined words.One of the tests we developed was designed for plural formation with non-words. In this test, subjects were required to choose the correct plural suffix for a given non-word. Ample agreement cues for gender were also provided. This test was adapted from Jean Berko's classic study on the acquisition of the inflectional morphology of English (1958) and Bettina Spreng's work on the acquisition of plural morphology in German (2004). We also developed an intuition test for gender assignment in which subjects were asked to assign gender to non-words to measure the effect of the phonological shape of the token on the choice of gender. This test was inspired by A. Mill's (1986) work on acquisition of gender in German.The results of these two tests indicated that the performance of the subjects, both adults and children, is better for some noun classes compared to others. In other words, phonological cues were reliable for some of the noun classes and facilitated gender choice, but this was not a reliable or unique cue, and led us to study the distribution of nouns into gender classes in Marathi.In this paper, we describe the two tests and analyze their results in brief. We look at the distribution of nouns across the noun classes and genders and discuss the congruence between the problematic classes as observed in the tests and the actual distribution of the same in the language. In this paper, we describe the two tests developed and designed for Marathi using non-words, a) plural formation for non-words b) intuition test for gender assignment in which subjects were asked to assign gender to non-words. We look at the distribution of nouns across noun classes and genders and discuss the congruence between the problematic classes as observed in the tests and the actual class distribution and frequency in the language.
Semi-Automatic Identification of Bilingual Synonymous Technical Terms from Phrase Tables and Parallel Patent Sentences For both high quality machine and human translation, a large scale and high quality bilingual lexicon is the most important key resource. Since manual compilation of bilingual lexicon requires plenty of time and huge manual labor, in the research area of knowledge acquisition from natural language text, automatic bilingual lexicon compilation have been studied. Techniques invented so far include translation term pair acquisition based on statistical co-occurrence measure from parallel sentences (Matsumoto and Utsuro, 2000), translation term pair acquisition from comparable corpora (Fung and Yee, 1998), compositional translation generation based on an existing bilingual lexicon for human use (Tonoike et al., 2006), and translation term pair acquisition by collecting partially bilingual texts through the search engine (Huang et al., 2005).Among those efforts of acquiring bilingual lexicon from text, Morishita et al. (2008) studied to acquire technical term translation lexicon from phrase tables, which are trained by a phrasebased statistical machine translation model with parallel sentences automatically extracted from parallel patent documents. Recently, we further studied to require the acquired technical term translation equivalents to be consistent with word alignment in parallel sentences and achieved 91.9% precision with almost 70% recall. This technique has been actually adopted by a Japanese organization which is responsible for translating Japanese patent applications published by the Japanese Patent Office (JPO) into English, where it has been utilized in the process of semiautomatically compiling bilingual technical term lexicon from parallel patent sentences. In this process, persons who are working on compiling bilingual technical term lexicon judge whether to accept or not candidates of bilingual technical term pairs presented by the system. Based on the achievement so far, in this paper, we consider situations where a technical term is observed in many parallel patent sentences and is translated into many translation equivalents. More specifically, in the task of acquiring technical term translation equivalent pairs, this paper studies the issue of identifying synonymous translation equivalent pairs. First, we collect candidates of synonymous translation equivalent pairs from parallel patent sentences. Then, we analyze features for identifying synonymous translation equivalent pairs. Finally, we apply the Support Vector Machines (SVMs) (Vapnik, 1998) to the task of identifying bilingual synonymous technical terms, and achieve the performance of almost 98% precision and over 40% F-measure. Then, in order to improve recall, we introduce a semi-automatic framework, where we employ the strategy of selecting more than one seeds for each set of candidates bilingual synonymous term pairs. By manually judging whether each pair of two seeds is synonymous or not, we achieve over 95% precision and 50% recall. In the research field of machine translation of patent documents, the issue of acquiring technical term translation equivalent pairs automatically from parallel patent documents is one of those most important. We take an approach of utilizing the phrase table of a state-of-the-art phrase-based statistical machine translation model. In this task, we consider situations where a technical term is observed in many parallel patent sentences and is translated into many translation equivalents. We apply SVM to the task of identifying synonymous translation equivalent pairs and achieve almost 98% precision and over 40% F-measure. Then, in order to improve recall, we introduce a semi-automatic framework, where we employ the strategy of selecting more than one seeds for each set of candidates bilingual synonymous term pairs. By manually judging whether each pair of two seeds is synonymous or not, we achieve over 95% precision and 50% recall.
A Bare-bones Constraint Grammar Conventional wisdom has it that in the realm of natural language parsing (NLP), statistical methods are more cost-efficient and easier to build than rule-based systems. However, the latter are dependent on training data, and machine learning of morphosyntactic analysis relies on the existence of a fair-sized annotated corpus for the language in question. If no such corpus exists, manual annotation of a boot-strapping corpus will be necessary, eliminating part of the costeffectiveness advantage. Furthermore, in the face of Zipf's law, the limited availability of large linguist-revised corpora (especially treebanks) makes it difficult to achieve good lexical coverage. Rule-based NLP systems, on the other hand, while not dependent on training data, generally require extensive lexica and/or morphological analyzers as input modules. As it would seem, both statistical and rule-based approaches are resource-sensitive and can run into difficulties with minor languages, or in the face of licensing and financing limitations. What we are addressing in this paper, is the assumption that these problems are especially difficult to solve for rule-based systems, among them our own methodology of choice, Constraint Grammar (CG). At the disambiguational level, early work by Chanod &amp; Tapanainen (1995) showed that even a small set of CG rules can compete with an off-the-shelve statistical tagger. However, in their experiment, both systems had access to an extensive lexicon an a mature morphological analyzer (a finite-state transducer). All published CG parsers to date (Karlsson et al. 1995 andBick 2000) have included extensive lexico-morphological resources as input to their grammatical rules (as have other rule-based approaches like HPSG and LFG). To the best of our knowledge, no previous research has been done on how to build a CG in the absence of such resources. This paper presents a solution for overcoming the lexical resource gap when mounting rule-based Constraint Grammar systems for minor languages, or in the face of licensing and financing limitations. We investigate how the performance of a CG disambiguation grammar responds to shifting input parameters, among them lexicon limitations of various degrees, the lack a morphological analyzer or both. We propose solutions for a bare-bones system, introducing endings heuristics and so-called morphological APPEND rules. For English, even with an unadapted disambiguation grammar, our bare-bones tagger achieved F-scores of 90-96% for part of speech, and 94-97% for lemmatization, depending on the modules and mini-lexica used.
Spring Cleaning and Grammar Compression: Two Techniques for Detection of Redundancy in HPSG Grammars Grammars of natural language are highly complex objects. This complexity is reflected in implementations of linguistically motivated precision grammars. Understanding the role of a specific element in a broad coverage precision grammar is therefore not always straightforward, even for the engineer who implemented the grammar. Implementations for different phenomena interact, and revisions to the grammar may change the role of existing parts of the grammar. In this paper, we present two techniques for investigating the role that specific types play in implemented HPSG (Pollard and Sag, 1994) grammars. The first, dubbed 'spring cleaning', focuses on identifying portions of the grammar that do not play any role in the set of sentences it recognizes or the structures it assigns to them. Such artifacts can accrue in a grammar because abandoned analyses are not completely removed or because the grammar is built on a cross-linguistic resource but does not use all of the infrastructure that resource provides. Spring cleaning is intended to be used in the course of grammar development and as such must leave the grammar in a state that is still easy to maintain. In contrast, the second technique, 'grammar compression', computes the smallest subset of a type hierarchy that can assign the same structures to the same sentences as the original grammar. In grammar compression, we remove not only the types taken out in spring cleaning, but also those that exist only to express generalizations over their subtypes.We use these techniques to explore the degree of redundancy in a range of DELPH-IN 1 grammars, including the two grammars of Wambaya (Bender, 2010), the BURGER grammar of Bulgarian (Osenova, 2010), the ManGO grammar 2 of Mandarin Chinese, all built with the LinGO Grammar Matrix (Bender et al., 2002;Bender et al., 2010), and two much larger grammars, the English Resource Grammar (Flickinger, 2000) and German Grammar (Müller and Kasper, 2000; Crysmann, 2005). This paper is structured as follows: First, we describe the overall structure of the grammars under consideration. This section is followed by an overview of the first approach under examination: removing superfluous types from the grammar. Section 4 provides the details of our second investigation of relevant types: maximally reduced computationally equivalent grammars. The next section presents our quantitative results and their implications. Finally, we conclude by suggesting avenues for future research. This paper presents two approaches that identify which parts of an implemented grammar are used and which parts are computationally inactive. Our results lead to the following insights: even small grammars contain noise due to revised analyses, removing superfluous types from a grammar may help to detect errors in the original grammar and at least half of the types defined in the grammars we investigated do not play a role in the computational process of the grammar.
Creating the Open Wordnet Bahasa The dictionary is a very important lexical resource in any field of studies. However, WordNet, originally created by academics at Princeton University, is just as important if not greater (Fell- baum, 1998). In fact, it is a source of reference that takes the traditional dictionary to a whole new level. While a dictionary can provide information such as the meaning, synonyms and parts of speech, and can organise them in alphabetical order, a wordnet is able to organise the words into a set of cognitive synonyms (synsets) which express distinct concepts. This reason has been the motivation for the creation of the various wordnets for various languages.There is currently no wordnet available for Malay despite the great number of wordnets available for many languages. Hence, this paper will attempt to create a lexical database for the Malay language based on alignments with other lexical resources -the French-English-Malay (FEM) dictionary, the English wordnet, KAMI and wordnets for Chinese and French. Crossing lexicons over several languages contributes to the accuracy of the Wordnet Bahasa. This wordnet will be released under an open source license (Creative Commons Attribution) in order to make it fully accessible to all potential users.Bahasa Melayu "the Malay language" is one that had been standardized over time with the aim of formal usage of the language. It derived from the variety of Malay languages that exist in the different parts of the Malay Archipelago, and is now widely used in Malaysia, Singapore, parts of Thailand and Brunei. The language spoken in Indonesia (Bahasa Indonesia) is very similar, and largely mutually intelligible. In this paper we will use Malay for standard Malay (the official language of Malaysia, ISO 639-3 code zsm), Indonesian to refer to the official language of Indonesia (ind) and Bahasa to refer to the generic Malay language that includes both (msa Singapore. Some people from The Philippines, Thailand, Burma, Sri Lanka, Cocos Island and Christmas Island also use it. There are about 40 million native Bahasa speakers worldwide. 1 Spelling reforms in the 1970s harmonized the orthographic conventions of Malay and Indonesian, making the written forms very similar (Asmah Haji Omar, 1975). Because of the enormous overlap in vocabulary (close to 98% by our measure, see Section 4.3) we decided it was possible to create a single wordnet for both languages: the Wordnet Bahasa. The vast majority of words are usable for both Malay and Indonesian and we specially mark those words that are used exclusively in one language. We hope that by building a single, open wordnet for both Malay and Indonesian we can help to create a strong lexical resource for the region. This paper outlines the creation of the Wordnet Bahasa as a resource for the study of lexical semantics in the Malay language. It is created by combining information from several lexical resources: the French-English-Malay dictionary FEM, the KAmus Melayu-Inggeris KAMI, and wordnets for English, French and Chinese. Construction went through three steps: (i) automatic building of word candidates; (ii) evaluation and selection of acceptable candidates from merging of lexicons; (iii) final hand check of the 5,000 core synsets. Our Wordnet Bahasa is only in the first phase of building a full fledged wordNet and needs to be further expanded, however it is already large enough to be useful for sense tagging both Malay and Indonesian.
Automatic identification of words with novel but infrequent senses ⋆  We propose a statistical method for identifying words that have a novel sense in one corpus compared to another based on differences in their lexico-syntactic contexts in those corpora. In contrast to previous work on identifying semantic change, we focus specifically on infrequent word senses. Given the challenges of evaluation for this task, we further propose a novel evaluation method based on synthetic examples of semantic change that allows us to simulate differing degrees of sense change. Our proposed method is able to identify rather subtle simulated sense changes, and outperforms both a random baseline and a previously-proposed approach. 1 New word senses The meanings of words are not static, but can vary and change in a number of ways. In particular, words can undergo diachronic change-change over time-and come to be used in new senses. Contemporary examples of sense change can be seen in the following usages of rock, sick, and text, all of which correspond to relatively new senses of these words. 1. Marvez has rocked the mullet [hair-style] for years as a style statement. [rock = &apos;display with pride&apos;] 2. LeBron has one of the sickest vertical leaps in the game, yet how many alley-oops have you seen him convert in his career? [sickest = &apos;best&apos;] 3. After she ignored the first few texts and phone calls, I gave up. [text = &apos;text message&apos;] Furthermore, new word senses are not necessarily frequent. For example, the above usages are taken from the enTenTen corpus, 1 a very large corpus containing a wide variety of text types, but the corresponding senses of these words appear to be rare in this corpus. The identification of new word senses is an important task in lexicography, and is necessary to keep dictionaries up-to-date. But lexical semantic change has only recently been studied from a computational perspective, and only to a limited extent (e.g., Sagi et al., 2009; Cook and Steven-son, 2010; Gulordava and Baroni, 2011). Furthermore, despite the low frequency of many novel word senses, no work to date has specifically considered the identification of words with novel infrequent senses. In contrast, in this paper we focus specifically on this issue The manual identification of novel senses is becoming increasingly difficult nowadays due to the vast quantities of text being produced (e.g., through online social media) that must be searched. ⋆ We thank Afsaneh Fazly, Diana McCarthy, Suzanne Stevenson, and the members of the University of Melbourne
Annotating the Structure and Semantics of Fables  Stories are distinguished from other genres of discourse by their unique coherent structures and discourse relations. As a special kind of stories, fables share the typical structural and semantic properties of stories, and are often associated with a moral. It is thus even more cognitively demanding to understand fables than stories in general. Apart from the temporal and causal relations among the events happening in the story, one needs to figure out the lesson intended by the storyteller. Interestingly, the same fable could be retold in different ways, where storytellers deploy a wide range of lexico-grammatical constructions, rhetorical devices, and discourse strategies, within specific narrative structures, to convey the moral invariably. Hence, fables are often semantically deep despite their apparently simple structures.To provide a useful resource for research on story understanding, a corpus containing various published versions of the Aesop's Fables in English and Chinese is compiled. In this paper, we discuss the annotation scheme developed for marking up the discourse structure and semantics of the fables. In Section 2, we briefly review related work on discourse structure and story structure, and evaluate the applicability and adequacy of the various frameworks for annotating and analysing fables. The issue of basic analysis unit is addressed in Section 3. A concise set of structural and semantic tags is synergised, as described in Section 4 and Section 5. The paper is concluded with the work in progress and future directions in Section 6. Grosz and Sidner's (1986) computational model suggests that discourse structure comprises (1) a linguistic structure consisting of the discourse segments and some embedding relationship 25th Pacific Asia Conference on Language, Information and Computation, pages [275][276][277][278][279][280][281][282] that can hold between them, (2) an intentional structure accounting for the discourse purpose and individual discourse segment purposes, and (3) an attentional state dynamically recording the objects, properties and relations salient to the participants' focus of attention as their discourse unfolds. The intentional structure is essential for understanding, but it is the most difficult to identify as it might or might not be readily indicated by surface linguistic devices, and is closely related to discourse participants' beliefs and shared knowledge. Mann and Thompson's (1987) Rhetorical Structure Theory (RST) is considered closely related to the intentional structure in Grosz and Sidner's model, while it is more functionally oriented. RST aims at giving a descriptive account of discourse relations holding between adjacent text spans, indicating the coherence and structure exhibited among natural text. A text is thus divided into units, essentially clauses, hierarchically structured and functionally organised with respect to a set of discourse relations, e.g. EVIDENCE, ELABORATION, CONCESSION, etc. Each relation defines how the two involved text spans, the nucleus and the satellite, functionally relate to each other with respect to the effect on the reader. RST relations are annotated in many corpora, e.g. the Potsdam Commentary Corpus (Stede, 2004). This paper outlines an annotation scheme we developed for a corpus of fables. Reference is made to previous studies on discourse structure and story grammar, as well as discourse relations and text coherence. The applicability and adequacy of the various frameworks for annotating and analysing fables are considered. The current work addresses several issues including the basic units for discourse segments, the distinction between structure and semantics in stories, the characteristics of fables, and the practicality and annotator-friendliness of the annotation scheme. A concise set of structural and semantic tags is thus synergised and applied. Some interim results and future directions are discussed.
Word classes in Indonesian: A linguistic reality or a convenient fallacy in natural language processing? The notion of word classes, such as the nouns, verbs and adjectives, is fundamental in both linguistics and computational linguistics. Word classes are the basis for the labels in part-of-speech tagging, and also the building blocks for parsing. In grammar engineering, they are the primitives upon which context-free grammar rules are written. In linguistics, they are considered the categories that shape the organisation of the language, and the way the world is conceived through language. These categories may not align across languages: what is expressed as a verb in one language may be expressed as an adjective or noun in another. But one linguistic universality hypothesis that remains despite these variations is that the categories noun and verb exist in all languages (Croft, 2003).This paper examines the noun-verb distinction specifically for Indonesian (Bahasa Indonesia) as, at least in spoken Indonesian in particular regions such as Riau and Jakarta, it has been claimed that open class categories are indistinguishable (Gil, 2001;Gil, 2010). If correct, this would refute the universality of the noun-verb distinction in Linguistics, with significant implications for Linguistic Typology and Theoretical Syntax.The primary aim in this paper is to apply unsupervised data-driven analysis of Indonesian text to determine whether we can automatically learn the noun-verb distinction, and in doing so, shed light on whether Indonesian conforms to Croft's noun-verb universality hypothesis, or is indeed a counter-example to the hypothesis as claimed by Gil. In addition, these experiments are a litmus test to see if "morphological signatures", as used by Goldsmith (2001), are sufficient in Copyright 2011 by Meladel Mistica, Timothy Baldwin, and I Wayan Arka 25th Pacific Asia Conference on Language, Information and Computation, pages [293][294][295][296][297][298][299][300][301][302] determining parts-of-speech, and can be used to determine the part-of-speech of out-of-vocabulary items (OOV) in Indonesian text processing. One assumption we make throughout this work is that the same basic word class distinctions are invariant across different styles, genres and registers (such as spoken vs. written) of a given language. That is, a conclusion on word class distinctions drawn based on written data should apply equally to spoken data, for example.This paper is laid out as follows. In Section 2 we look at how word classes are determined, both qualitatively in Linguistics and statistically in the field of part-of-speech induction. Next, we look at the formal properties of Indonesian, and we give examples to show how the distinction between nouns and verbs can be illusive. In the next two sections (Sections 4 and 5) we describe the data, tools and method for our experiments, followed by the results in Section 6. Finally, we discuss our findings and the impact and contribution to both Linguistics and Computational Linguistics, particularly in natural language processing for Indonesian (Sections 7 and 8). This paper looks at Indonesian (Bahasa Indonesia), and the claim that there is no noun-verb distinction within the language as it is spoken in regions such as Riau and Jakarta. We test this claim for the language as it is written by a variety of Indonesian speakers using empirical methods traditionally used in part-of-speech induction. In this study we use only morphological patterns that we generate from a pre-existing morphological analyser. We find that once the distribution of the data points in our experiments match the distribution of the text from which we gather our data, we obtain significant results that show a distinction between the class of nouns and the class of verbs in Indonesian. Furthermore it shows promise that the labelling of word classes may be achieved only with morphological features, which could be applied to out-of-vocabulary items.
Automated Proof Reading of Clinical Notes Clinical notes contain valuable information about patients" status, however, retrieving information from them is challenging because they may comprise up to 30% non-word tokens, idiosyncratic spellings, abbreviations and acronyms, and poor grammatical structure. Besides resolving misspellings, knowing the correct expansions of abbreviations and acronyms is critical to the understanding of the document for both automatic natural language understanding and human comprehension and interpretation (Pakhomov et al., 2005).Proof reading is a process whereby a clinical text is validated to identify unknown tokens/words and their valid forms. Proof correcting is modifying the proofed text to make it notionally "correct" text and thereby more readily processible by automatic means. There are two principal tasks to be achieved, these are normalization and standardization. The normalization process changes the texts in a way so that a human reader would consider it as normal, such as correcting spelling, expanding abbreviations and acronyms. The standardization process converts the text into certain formats that an expert community has defined as standard; a good example is converting scores and measures into a standard layout.At first glance it would seem that standardization should be done initially before normalization, however it is more likely that both will need to be performed multiple times in a repeated cycle of processing as there is interaction between the two processes. Standardization converts various instances of the same scores and measures into standard forms so that the system does not need to be concerned about their details for later processing steps such as normalization. For example: "HR 70" and "HR 78" are standardized to the representation of heart rate. However, most measurements and scores contain acronyms or abbreviations which may need to be expanded during the normalization process. Normalization could improve the standardization process by correcting misspelling and other token error within standard forms. If normalization is executed first, a large amount of tokens within standard layouts will need to be processed while they could be excluded if they were ringfenced by a standardization process.Once both these tasks are completed the result is a corpus that is annotated by all of these processes. The final act is proof correcting or transformation which is to change the raw corpus into a proofed corpus, that is, the text can be read as a fully corrected corpus. The important process is to use the annotation properties of each token to change its representation in the source file to the correct form. This produces two versions of the source file, the uncorrected form and the corrected form. The former form has a set of annotations with properties defining the changes that needed to be made in the proof correcting process, and the latter form has all the text corrected and a set of annotations that define the original form of the token(s) and structure(s). Misspellings, abbreviations and acronyms are very popular in clinical notes and can be an obstacle to high quality information extraction and classification. In addition, another important part of narrative reports is clinical scores and measurements as doctors infer a patient&quot;s status by analyzing them. We introduce a knowledge discovery process to resolve unknown tokens and convert scores and measures into a standard layout so as to improve the quality of semantic processing of the corpus. System performance is evaluated before and after an automatic proof reading process by comparing the computed SNOMED-CT codes to the coding created originally by the clinical staff. The automatic coding of the texts increased the coded content by 15% after the automatic correction process and the number of unique codes increased by 4.7%. Accuracy of the automatic coding and annotations in the notes which have not been coded by the clinical staff is suggested by the system output.
Modelling Word Meaning using Efficient Tensor Representations Research in the area of natural language processing has demonstrated that psychologically relevant models of word meaning can be learnt from exposure to natural language (Lan- dauer and Dumais, 1997;Lund and Burgess, 1996;McRoy, 1992;Turney, 2008). Many of these models are based on vector representations built from word co-occurrence statistics that aim to model various semantic relationships. Even though these semantic space models appear to identify words with similar meanings, it has been argued that they do not incorporate syntax or achieve other basic cognitive language abilities (Perfetti, 1998).Recently, a number of semantic space models, that learn directly from unstructured text, have been developed that encode word order into the semantic space, hence capturing more structural information about word associations (Jones and Mewhort, 2007;Sahlgren et al., 2008). Jones and Mewhort (2007) concluded that a model that pays attention to both context and word order while learning, stands a greater chance of matching the trends found in human data. The strength of a geometric approach to encode word order is in the ability to work within a mathematically well defined framework, including the availability of many existing operators from linear algebra, such as Kronecker products. However, to our knowledge there has been very few efficient methods for implementing uncompressed Kronecker products when encoding word order information within a semantic space.The main contribution of this paper is to present a novel, efficient approach to using Kronecker products to encode word order information within a semantic space. The other significant contribution is to demonstrate how applications can use our single representation to access various task specific semantic information. Models of word meaning, built from a corpus of text, have demonstrated success in emulating human performance on a number of cognitive tasks. Many of these models use geometric representations of words to store semantic associations between words. Often word order information is not captured in these models. The lack of structural information used by these models has been raised as a weakness when performing cognitive tasks. This paper presents an efficient tensor based approach to modelling word meaning that builds on recent attempts to encode word order information, while providing flexible methods for extracting task specific semantic information.
A Study of Sense-disambiguated Networks Induced from Folksonomies Lexical-semantic resources are fundamental building blocks of natural language processing systems. For a long time, WordNet has been widely deployed in a great variety of tasks. A few years ago, the attention of researchers has turned to the so-called collaboratively created lexicalsemantic resources such as Wikipedia 1 and Wiktionary 2 (Gurevych and Wolf, 2010). They have been found to perform well as sources of background knowledge in multiple NLP tasks.While collaboratively created resources represent an excellent addition to conventional lexicalsemantic resources, their coverage is insufficient in terms of the represented domains. Also, there is often lack of contextual information about individual sense descriptions. This is why, in this paper, we turn to folksonomies as a promising source of lexical-semantic information. Folksonomy is a term used first by Vander Wal (2004) and defined as "tagging is ontology that works", in reference to social bookmarking systems like Delicious. Such applications allow users to assign tags to web sites or other resources. By doing so, they create a structure consisting of the three entities, "tag, the object being tagged and [the user's] identity" which is referred to as a folksonomy.Our goal is to derive a resource that can effectively complement conventional resources, e.g., WordNet, as folksonomies are known to contain special vocabulary reflecting users' interests, current trends, or neologisms typically lacking in conventional resources. To achieve this, we investigate two widely deployed folksonomies, Delicious and BibSonomy. We constructed three types of tag-based graphs utilizing the tag co-occurrence with resources, users, and other tags. Then, we apply graph clustering techniques to perform word sense induction and obtain a word sense-disambiguated lexical-semantic network. Based on this graph, we present a detailed analysis of the resulting networks in terms of their graph-theoretic properties and compare them with the properties of conventional resources. We also compare their coverage and find that a folksonomyderived resource includes web-specific vocabulary that is lacking in other resources. Furthermore, we investigate the nature of the resulting word sense distributions for individual lexemes and relate them to the word senses encoded in WordNet and Wiktionary.There has been work on inducing hierarchies ( Heymann and Garcia-Molina, 2006) and ontologies ( Schmitz, 2006) from folksonomies based on co-occurrence patterns. Recently, work has been published on clustering and disambiguating tag similarity graphs. Yeung et al. (2009) employ the Girvan-Newman algorithm ( Girvan and Newman, 2002) to optimize the betweenness of nodes by incrementally removing edges and recalculating betweenness. They only look at a small data set of ten exemplary tags. Our goal is to go beyond simple clustering and do actual disambiguation, meaning to instantiate the different readings of a tag as individual nodes within the network. Jurgens (2011) presents a way to use community detection to induce word senses from word collocations. He builds a graph from word co-occurrences and then applies a community detection algorithm. In contrast to the approach in this work, they do not split the nodes into different senses but use a clustering algorithm that deals with nodes belonging to multiple communities. The algorithm is a hierarchical agglomerative clustering operating on a similarity measure defined for edges, not vertices.The paper is structured as follows: First, we describe how to derive a tag similarity graph from folksonomies and introduce our approach for sense disambiguation of individual tags (Section 2) . We analyze the resulting graphs in terms of their properties (Section 3) and coverage (Section 4). With this analysis we are laying ground to combine traditional LSRs and folksonomies in applications.2 From Folksonomies to Lexical-Semantic Resources Lexical-semantic resources are fundamental building blocks in natural language processing (NLP). Frequently, they fail to cover the informal vocabulary of web users as represented in user-generated content. This paper aims at exploring folksonomies as a novel source of lexical-semantic information. It analyzes two prototypical examples of folk-sonomies, namely BibSonomy and Delicious, and utilizes NLP and word sense induction techniques to turn the folksonomies into word sense-disambiguated networks representing the vocabulary and the word senses found in folksonomies. The main contribution of the paper is an in-depth analysis of the resulting resources, which can be combined with conventional wordnets to achieve broad coverage of user-generated content.
Unsupervised Word Sense Disambiguation Using Neighborhood Knowledge 1 Word Sense Disambiguation (WSD), the task of indentifying the intended meaning (sense) of words in context is one of the most important problem in natural language processing. Though it is often characterized as an intermediate task rather than an end in itself, it has the potential to improve the performance of many applications including information retrieval, machine translation and so on.Existing methods conduct WSD usually using only the information contained in the ambiguous sentence to be disambiguated. They utilize context words within predefined window in sentence together with other syntactic information. It has been proved that expanding context window size around the target ambiguous word can help to enhance the WSD performance.However, expanding window size unboundedly will bring not only useful information but also some noise which may finally deteriorate the WSD performance. Can we find other way to expand context words without bringing too much noise?In this study, we proposed to conduct WSD using collaborative techniques. One common assumption of existing methods is that the sentences are independent of each other, and WSD is carried out separately without interactions among the sentences. However, some ambiguous words contained in article appear more than one time, the multiple sentences contain the same ambiguous word within the article actually have mutual influences and contain useful clues which will helpful to deduce word sense from each other. For example, sentence containing program design commonly shares similar topic with the sentence containing computer program, thus the meaning of the ambiguous words program in those two sentences can be deduced from each other. The idea is borrowed from the observation that ambiguous words appear in topic related sentences contained in the same article often share common meanings. Therefore, we can retrieve a small number of sentences containing the ambiguous word from the same article. These neighbor sentences can be used in the disambiguation process and help to disambiguate word sense for the specified sentence.This study proposes to construct an appropriate knowledge context for unsupervised WSD method by making use of a few neighbor sentences closed to the ambiguous sentence in the article. The framework for WSD consists of the step of neighborhood knowledge building and the step of word sense disambiguation. In particular, the neighborhood knowledge context is obtained by applying the similarity algorithm on the article. The graph-ranking based algorithm is employed to disambiguate word sense in a specified knowledge context. Instead of leveraging only the word relationships in a single sentence, the algorithm can incorporate the relationship in multiple sentences, thus making use of global information existing in the whole article.Experiments are carried out on dataset and the results confirm the effectiveness of our method. The neighborhood knowledge can significantly improve the performance of single sentence WSD. Furthermore, how the size of the neighborhood influences the WSD performance is also investigated. It has been reported that a small number of neighbor documents are sufficient to elevate the performance.The rest of paper is organized as follows: Section 2 briefly introduces the related work. The proposed method is described in detail in Section 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. Usually ambiguous words contained in article appear several times. Almost all existing methods for unsupervised word sense disambiguation make use of information contained only in ambiguous sentence. This paper presents a novel approach by considering neighborhood knowledge. The approach can naturally make full use of the within-sentence relationship from the ambiguous sentence and cross-sentence relationship from the neighborhood knowledge. Experimental results indicate the proposed method can significantly outperform the baseline method.
Dependency-based Analysis for Tagalog Sentences Figure 1: Three (3)of the possible translations of the sentence "The man gave the woman a book" in the Tagalog sentence "Nagbigay ng libro sa babae ang lalaki" (The man gave the woman a book). According to Schachter and Otanes (1972) as mentioned in Kroeger (1993), "the sentences include exactly the same components, are equally grammatical, and are identical in meaning". On the other hand, as briefly explained in Section 3, adverbial and nominal phrases may take pre-verbal position.For Tagalog sentences, dependency-based syntactic parsing is preferred over phrase-structure based analysis because dependency analysis does not rely on word positions( Tsarfaty et al., 2010), instead, sentence representations are based on links between words, called dependencies.Due to limited resource, this work deals only with unlabeled data. Our corpus are annotated with dependency heads but do not include dependency relations of the word (e.g.NMOD, ROOT). We look into the unlabeled attachment scores (UAS), or the number of correct heads, as well as complete scores. We also look into the number of sentence heads correctly predicted. The free word order nature of Tagalog may put a verb at the initial position or after a nominal or adverbial element in the sentence.We briefly introduce dependency parsing in section 2, section 3 briefly explains sentence structures in Tagalog and section 4 discusses the how heads are assigned to words in a sentence. Sections 5 describes the experiment set-up and results. Finally, we discuss some issues found in the parsing results in section 6. Interest in dependency parsing increased because of its efficiency to represent languages with flexible word order. Many research have applied dependency-based syntactic analysis to different languages and results vary depending on the nature of the language. Languages with more flexible word order structure tend to have lower performances compared to more fixed word order languages. This work presents, for the first time, a dependency-based parsing for Tagalog language, a free word order language. The parser is tested using manually assigned POS and auto-assigned POS data. Experiment results show an average of about 78% accuracy on unlabeled attachment scores, and about 24% on complete dependency scores. As the position of the sentence head is not fixed, we look into sentence head accuracy as assigned by the parser. Results show that around 83%-84% of the sentence head are assigned correctly.
Case study of BushBank concept  In this paper, we present a new type of annotated corpus, called BushBank, which improves handling of ambiguity in natural language. Unlike in traditional approaches where data are directly disam-biguated, in a BushBank, disambiguation is done later, based on application needs. This has major impact on the structures used in the corpus , since ordinary syntactic trees disallow ambiguity. Our approach was tested on 10.000 sentences and more than a hundred annotators when creating Czech BushBank. The paper contains information about creating such a resource and the methods used to obtain high inter-annotator agreement. Processing natural language is one of those areas where the quality and the quantity of lexical resources distinguish a great project from an inferior one. Lexical resources for major languages tend to address both these requirements, but the situation for smaller languages varies a lot. Thanks to various projects like WebBootCat [1], it is possible to build large corpora from documents available on the internet. Unfortunately this approach can&apos;t help us in the process of building high-quality annotated corpora. The most noticeable examples of annotated corpora are the PENN Treebank [11] and PDT [7]. Sentences in these corpora are parsed, i.e. annotated with (at least) syntactic structures. These structures are offered to users in the form of syntactic trees and they are un-ambiguous. In order to obtain high quality, unambiguous annotation of natural language (which is ambiguous on every level), skilled annotators and hundreds of pages of manuals [8] are needed. Building such complex lexical resources is out of reach for most of the less-used languages. This paper focuses on a new type of annotated corpus named BushBank and an example study performed on Czech language that belongs to the Slavic languages together with Russian or Polish. Slavic languages tend to very good for such experiments as they have rich morphology and fairly free word order. Czech language is one of the most described European languages and there are already several high quality resources like Prague Dependency Treebank [7] or a Czech version of EuroWordNet [13]. This gives us a reference to compare our results to.
In Situ Text Summarisation for Museum Visitors With the increasing saturation of mobile technology, museums and other cultural heritage institutions are increasingly looking to deliver content to visitors via their personal mobile device. This has led to a move away from a traditional mode of content delivery via static information on placards in the museum space, to interactive applications on mobile devices supporting path finding, social networking and personalised content delivery (Burnette et al., 2011;Filippini-Fantoni et al., 2011). This paper explores the feasibility and utility of in situ personalised content delivery in a museum context, focusing on document summarisation. Museums provide a compelling context for in situ summarisation, as visitors often wish to access key information relevant to their immediate surroundings, but want to avoid information overload. Personalised summarisation is an integral component of museum content delivery, as exhibits are typically associated with vast amounts of curated information, predominantly in textual form. This can range from simple tabular information such as the date of acquisition of an exhibit, to full-length research articles published by museum curators/researchers relating to the exhibit. Personalised summarisation offers the possibility to present the most salient facets of information to a visitor, according to their interests and preferences. The pragmatic choice of a mobile device such as a smart phone to deliver the content poses challenges in terms of the amount of content that can be effectively presented to the visitor ( Yang and Wang, 2003;Otterbacher et al., 2006).Personalised summarisation should ideally be coupled with tracking/geolocating technology to be situation aware (Bohnert et al., 2008;Bohnert and Zukerman, 2009;Bickersteth and Ainsley, 2011) and take place interactively (Callaway et al., 2005). In principle, any evaluation should take place in situ as part of an actual museum visit. However, in this preliminary research, we present the results of a web-based user study targeted at members of Melbourne Museum (Melbourne, Copyright 2011 by Timothy Baldwin, Patrick Ye, Fabian Bohnert, and Ingrid Zukerman. This research was supported in part by grant no. DP0770931 from the Australian Research Council. The authors thank Carolyn Meehan and her team from Museum Victoria for their assistance. Australia), based on a fixed path through the museum. To partly overcome this limitation, we elicit the participants' interest in an exhibit topic, and generate a summary which takes into account this interest level. In this way, we examine the impact of interest level on summary length and different summarisation strategies, as a guide for future research.Our contributions are: (1) we deploy a range of extractive summarisation methods over a fixed path through a museum, focusing on generating summaries for individual exhibit areas, personalised in length according to visitor preferences and interest levels, and diversification of summary content; (2) we carry out a medium-scale user study over the generated summaries, to determine the relative utility of the summaries and the effectiveness of the various strategies trialled; and (3) we present the results of a web-based museum visitor questionnaire on opportunities for in situ personalisation in a museum environment. This paper presents an experiment on in situ summarisation in a museum context. We implement a range of standard summarisation algorithms, and use them to generate summaries for individual exhibit areas in a museum, intended for in situ delivery to a museum visitor on a mobile device. Personalisation is relative to a visitor&apos;s preference for summary length, the visitor&apos;s relative interest in a given exhibit topic, as well as (optionally) the summary history. We find that the best-performing summarisation strategy is the Centroid algorithm , and that content diversification and customisation of summary length have a significant impact on user ratings of summary quality.
Iteratively Estimating Pattern Reliability and Seed Quality With Extraction Consistency * The rapid growth of the World Wide Web has attracted a lot of research effort on designing methods that automatically extract knowledge or useful information from large, unstructured text. Different from the conventional corpus, the magnitude and noisy natural of the Web has prohibited analytical approaches to be effective. Consequently, most of the systems that took this challenge proceed in a semi-supervised fashion with a human-provided starting point, such as a few instances of the desired extraction( Mann and Yarowsky, 2005;Muslea, 1999;Ravichandran and Hovy, 2002;Pantel and Pennacchiotti, 2006).In this paper, we focus on the extraction of relation instances. In this scenario, the system needs to be fed with prepared pairs, such as &lt;Barack Obama, Auguest 4th&gt;, or initial extraction patterns to bootstrap. Kozareva and Hovy (2010) mentioned that seed selection plays an important role in this kind of semi-supervised approaches. Therefore, how to select high quality seeds in the initial stage is a critical issue. Most researchers select seeds manually to avoid this problem, but the scalability of such manual selection is not promising. Then, these approaches utilize the given instances, called seeds, and generate extraction patterns that has the potential to locate more instances of the desired type in the text. For example, Ravichandran and Hovy (2002) use surface text patterns like &lt;Person&gt; was born on &lt;Date&gt; to answer questions about birth dates. Different from those approaches that heavily depend on the quality of the initial seeds, in this paper, we took an alternative direction that focuses more on the quantity of the seed instances. Such a pursuit is made possible by the advent of rich knowledge sources such as Wikipedia 1 and CIA Fact Book 2 . For example, Wikipedia Infoboxes 3 provide an opportunity to easily gather a vast amount of seed instances because the data is stored in a template form as illustrated in Figure 1. Using such sources, we can harvest a large number of pairs like &lt;Kobe Bryant, Pennsylvania&gt; from the below infobox as seed instances for birth place extraction. However, using arbitrary seeds to retrieve sentences from the Web will potentially result in a large number of irrelevant content, which will in turn hamper pattern production.To demonstrate such a situation, we conducted an experiment on seeds gathered from Wikipedia infobox. The results are presented in Table 1. For each relation type, we randomly select 200 seed instances for forming queries and for each query, evaluate first ten snippets returned from search engine. The relevance of the retrieved snippets are judged by two human annotators. We can see that the relevance ratio is not perfect even we have used both entities in the pair for forming the query. One factor behind such imperfection is that the open and voluntary nature of Wikipedia allows editors to fill in information of different specificity. For example, the birth place field of some people contains only less detailed information such as the country instead of more specific description like county or city. Moreover, as can be seen in Table 1, the relevance ratio is not consistent among different relation types and may be surprisingly low such as the type of death place. Such a situation will affect the performance of semi-supervised approaches greatly( Xu et al., 2007).Fortunately, having abundant seed instances offers us an opportunity to mitigate such a problem. In this paper, we propose a mechanism that iteratively assesses both the quality of seed instances and induced extraction patterns. Our strategy is to estimate the reliability of an extraction pattern by the consistency of its extractions, and alternately, reevaluate the usefulness of seed instances based on estimated pattern reliability. The resulting system works best when it is fed with a large number of seeds, so that the reliability of the induced pattern can be better estimated.In the next section, we review several semi-supervised approaches that are comparable to our system. We introduce the proposed CEPRA method in Section 3. In Section 4, we describe the experimental settings; and in Section 5, we discuss the experiments conducted to evaluate the performance of different selection approaches. We summarize the results in Section 6. Then, in Section 7, we provide some concluding remarks and consider avenues for future research. In this paper, we focus on the task of distilling relation instances from the Web. Most of the approaches for this task were based on provided seed instances or patterns to initiate the process. Thus, the result of the extraction depends largely on the quality of the instances and patterns. For this matter, we propose an iterative mechanism that estimates the reliability of a pattern by the consistency of its extractions , and reevaluate the usefulness of seed instance based on estimated pattern reliability. The resulting system is a semi-supervised method that can take a large quantity of seed instances with diverse quality. To evaluate the effectiveness of our approach, we experimented on 8 types of relationships. The empirical results show that our system performs quite consistency in different relationships while maintaining high precision and recall value.
A Listwise Approach to Coreference Resolution in Multiple Languages Reference resolution (Jurafsky and Martin, 2009) (chapter 21, section 21.4) is a task of determining to which entities are referred by which linguistic expressions. This task plays an important role in a large number of NLP applications such as Information Retrieval, Question Answering and Machine Translation. Therefore, it has attracted many attentions within the NLP community. Many works on various aspects (linguistic features (Ng, V., 2007), (Haghighi and Klein, 2009); machine learning models ( Soon et al., 2001); multiple languages ( Recasens et al., 2010a); and so on) of the coreference resolution task have been published.Until the release of the SemEval-2010 task 1 ( Recasens et al., 2010a), there has no competition or public corpus that allows evaluating different coreference resolution systems in multiple languages. Most published systems only focus on a specific language and use the same data sets for example ACE or MUC corpora to train and test the systems. This makes the systems easy to unintentionally adapt themselves to the corpus but not to the problem in general. Therefore, this SemEval-2010 task 1 ( Recasens et al., 2010a) made it possible to evaluate and compare various automatic coreference resolution systems in the aspects of: (i) the portability of systems across languages, (ii) the relevance of different levels of linguistic information, and (iii) the behavior of scoring metrics.This shared task attracted lots of researchers' attentions, but finally only six teams submitted their final results. The participating systems differed in terms of architecture, machine learning methods, etc. These systems mostly based on pairwise models, graph partitioning and entitymention models. Unfortunately, these models suffered from an important weakness (Ng, V., 2010). In these models, each antecedent candidate is resolved independently with the other candidates. So the models could not determine the best candidate in the relation with the other candidates. To address this drawback, ranking models were proved to be a useful solution (Denis and Baldridge, 2007), (Ng, V., 2005), ( Yang et al., 2003). Motivated from ranking models, in this paper, we present our proposal approach for learning-based reference resolution task in multiple languages.We exploit the listwise approach, which is originally proposed for learning to rank task in information retrieval (Cao et al., 2007), to solve the SemEval-2010 Task on Coreference Resolution in Multiple Languages. This method allows the system to choose the best candidate for a given mention in the relation with other candidates. This means that all candidates will be examined simultaneously and the candidate with the highest score will be selected as a correct antecedent. This listwise approach has been successfully applied to information retrieval task (Cao et al., 2007). Our experimental results on the corpora of SemEval-2010 shared task 1 showed that when applied to coreference resolution task, this new listwise approach usually gave the better results than previous approaches. When estimated on the latest metric BLANC, our proposed system got the state-of-the-art performance.The rest of the paper is organized as follows. Section 2 reviews related work proposed for this shared task. Section 3 describes our listwise approach to this shared task. Section 4 presents experimental results on the corpora of this SemEval-2010 shared task. Finally, section 5 gives some conclusion and future work. This paper presents a listwise approach as an alternative to commonly used pair-wise approaches to the task of coreference resolution in multiple languages. In this listwise approach, all antecedent candidates are examined simultaneously and assigned corresponding scores expressing the probability that each candidate is coreferent with a given mention. The experimental results on the corpora of SemEval-2010 shared task 1 showed that our proposed system gave the good results in English and Spanish, and comparative results in Catalan when compared to previous participating systems. These results prove that this approach is appropriate and quite efficient for Coreference Resolution in Multiple Languages.
Combining Dependency and Constituent-based Syntactic Information for Anaphoricity Determination in Coreference Resolution Coreference resolution plays a critical role in many NLP applications. It identifies which noun phrases (NPs) refer to the same entity in the real world and can be divided into two subtasks: anaphoricity determination and antecedent identification. The first subtask, anaphoricity determination, determines whether a given noun phrases (NP) is anaphoric or not. Here we say an NP is anaphoric if it has any antecedent in the context preceding it, and non-anaphoric otherwise. The second subtask, antecedent identification, identifies the antecedent of a given anaphoric NP. Although machine learning approaches have performed reasonably well in coreference resolution without explicit anaphoricity determination (e.g. Soon et al. 2001;Ng and Cardie 2002b;Yang et al. 2003Yang et al. , 2008), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a non-anaphoric NP does not have an antecedent and thus does not need to be resolved.Recently, anaphoricity determination has been the subject of increased attention in coreference resolution. A variety of techniques have been proposed to address anaphoricity determination as an independent task (e.g. Paice and Husk 1987;Lappin and Leass 1994;Kennedy and Boguraev 1996;Denber 1998;Bean and Riloff 1999;Vieira and Poesio 2000;Evans 2001;Cherry, Cherry and Bergsma 2005;Bergsma et al. 2008). Since then, more studies have been done to incorporate anaphoricity determination into coreference resolution in a pipeline way (e.g. Ng and Cardie 2002a;Yang et al. 2005;Kong et al. 2010) or in a joint way (e.g. Denis and Balbridge 2007;Luo 2007;Finkel and Manning 2008;Ng 2009), and achieved promising results.While it is well known that NP anaphoricity interacts with various kinds of structured factors, most of previous studies only consider constituent-based syntactic information and there are few studies on exploring dependency-based syntactic information for anaphoricity determination. In this paper, we first study the effectiveness of dependency-based syntactic information for anaphoricity determination. Then, we propose different ways to combine dependency and constituent-based syntactic information for anaphoricity determination in better exploring their complementary nature. Our study shows that dependency and constituent-based syntactic information are quite complementary and proper combination can much improve the performance of anaphoricity determination and that of coreference resolution as a whole even when simply integrating the anaphoricity determination module into a coreference resolution system as a filter.The rest of this paper is organized as follows. Section 2 describes some background knowledge about this task. Section 3 briefly overviews related work. Section 4 describes dependency and constituent-based syntactic information and different ways of combining them for anaphoricity determination. Section 5 reports the experimental results. Finally, we conclude our work in Section 6. This paper systematically explores the effectiveness of dependency and constituent-based syntactic information for anaphoricity determination. In particular, this paper proposes two ways to combine dependency and constituent-based syntactic information to explore their complementary advantage. One is a dependency-driven constituent-based structured representation, and the other uses a composite kernel. Evaluation on the Automatic Content Extraction (ACE) 2003 corpus shows that dependency and constituent-based syntactic information are quite complementary and proper combination can much improve the performance of anaphoricity determination, and further improve the performance of coreference resolution.
Sentiment Classification in Resource-Scarce Languages by using Label Propagation Over the last decade, document-level sentiment classification has attracted much attention from NLP researchers; its potential applications include opinion summarization and opinion mining (Pang and Lee, 2008). Most of the existing methods locate sentiment classification as a supervised classification problem and train a reliable classifier from a large amount of labeled data (Pang et al., 2002;Mullen and Collier, 2004;Matsumoto et al., 2005; Gamon, 2005). The main disadvantage of such supervised approaches is that it is quite expensive in both time and labor to annotate a large amount of training data.Unfortunately, in some languages such as Chinese and Hindi, a sufficient amount of training data is not always available. Sentiment classification becomes a quite challenging problem for such resource-scarce languages. While some studies have tackled this problem (Wan, 2009;Dasgupta and Ng, 2009), they still require substantial human efforts or specific linguistic resources that are only available in particular languages as we will see later in Section 2. We therefore want to develop a low-cost, general method that can be readily applicable to sentiment classification in any languages.In this paper, we explore the use of label propagation (LP) ( Zhu and Ghahramani, 2002) in building a document-level sentiment classifier under a minimally-supervised setting, where we have only a small number of labeled reviews other than the target reviews that we want to classify. Having a similarity graph whose vertices are labeled or unlabeled instances (reviews, here) and edges represent the similarity between the vertices, LP infers the label (sentiment polarity) of unlabeled (target) reviews based on labeled reviews that are similar to the target reviews. The key in applying LP to document-level sentiment classification is therefore in the way to represent reviews on the graph and to define the similarity between the them. We thus investigate the impact of the review representation and the similarity measure on the classification performance.The main contributions of our work are summarized as follows: • We evaluate LP on document-level sentiment classification in a resource-scarce language. Our method can be applied to any languages in which a small number of labeled reviews are available.• We run LP with different review representations 1 (content words, phrases, and adjectives), and various similarity measures (dice coefficient, overlap coefficient, Jaccard, cosine similarity) (Manning and Schütze, 1999). We thereby reveal their impact on the classification performance.• We compare our method with support vector machines (Vapnik, 1995) and transductive support vector machines (Joachims, 1999), and demonstrate the stability of the classification performance of our method in this task.The rest of this paper is organized as follows: Section 2 introduces related work, Section 3 explains LP algorithm in detail. Section 4 evaluates our method. Section 5 concludes this study and discusses future direction. With the advent of consumer generated media (e.g., Amazon reviews, Twitter, etc.), sentiment classification becomes a heated topic. Previous work heavily relies on a large amount of linguistic resources, which are difficult to obtain in resource-scarce languages. To overcome this problem, we investigate the usefulness of label propagation, which is a graph-based semi-supervised learning method. Extensive experimental evaluation on three real datasets demonstrated that label propagation performs more stable than support vector machines (SVMs) and transductive support vector machines (TSVMs) in a document-level sentiment classification task for resource-scarce languages (Chinese in our case).
Word-order and argument-marking: Japanese vs Chinese vs Naxi A common view about word-order is that if the arguments of a predicate are overtly marked, e.g., for Case, leading to the identification of their grammatical relations (subject, object and obliques), then the arguments may be ordered in more than one way. The semantic roles of the arguments can be recognized by their grammatical relations in conjunction with the voice of the predicate (active, passive). Conversely, if such marking is lacking, then linear order is the crucial means to identify the grammatical relations of the arguments and their interpretations with respect to their semantic roles.The processing point of view above works well for the comparison of a language like Japanese (or Korean) with one like Chinese (or English). The picture becomes a bit more complicated when Naxi comes into the picture. Naxi is a verb-final language like Japanese and Korean. It too marks the arguments to the effect that their grammatical relations may be identified. But the marking is optional; yet, the varying word-order is largely the same.In this paper I argue that for certain word-orders syntax must be appealed to since they do not fall under the processing account. If the underlying phrase structure is universally the same (Kayne 1994), then the superficial variation with respect to the relative positioning of the object and the verb must be due to movement. This account both derives the verb-final property, which evidently has no bearing on processing, and the varying position of the object.Processing nevertheless has a role to play, insofar as the parser needs to select more than one derivation for the same surface form. It is argued that varying word-order is related not to the marking on the arguments indicating their grammatical relations but to the syntax of object. The flexible word-order in Japanese and the lack thereof in Chinese may be attributed to the presence or absence of argument-marking, but evidence from Naxi shows that although argument-marking may help identify the semantic roles of the arguments from the processing point view, it is syntax that decides the word-order. Insofar as the relative positioning of the object and adverbs cannot be reduced to argument-marking, the mechanism allowing movement of the object past the verb from a universal order derives both the verb-final property and the varying word-order.
Classification of Filipino Speech Rhythm Using Computational and Perceptual Approach Speech prosody is an important aspect of natural speech. Pitch, duration, and intensity are some of the prosodic components that dictate the naturalness of speech and differentiate contextual meaning. The objective of this study is to present computational analyses and perceptual experiments to confirm the existence and applicability of rhythm classes to Filipino; to find their salient features for computational classification; and to find the rhythm typology of Filipino speech. Among the prosodic characteristics of speech, this study focuses on the temporal or the duration aspect.Speech prosody or suprasegmentals refer to the features of speech that can modify the meaning and information being carried by the words not found in the lexicon. Speech intensity, duration, pitch, and rhythm are the most common features that differentiate one prosodic unit from another. Embedded in the human perception is the distinction of speech rhythm or timing to discriminate between languages. Languages are traditionally classified as stress-timed or syllable-timed where syllable-timed language were said to exhibit near-equal duration for each syllable, and stress-timed languages were said to have near-equal duration between stresses or what is called isochrony (Pike, 1943). Years of research on the subject has shown development in which evidences have been piling up against isochrony theory, and there emerged other rhythm classes such as Mora-timing (Abercrombie, 1967;Benadon, 2009). New models have been developed to accommodate intermediate languages, and a continuous language distribution plane was proposed (Dauer, 1983).Instead of actually having equal duration between stresses and syllables, Dauer observed that the impression of syllable saliency for stress-timed and equal-saliency for syllable-timed is a result of the phonetic inventory, specifically syllable structures, of a language. Dauer"s continuously-distributed model of rhythm postulates that a characteristic typical to stress-timed languages is a wider syllable inventory (occurrence of C n V, CVC, etc.) resulting in heavier intervals. As a result, there are more frequent vowel reductions in the unstressed syllables, in a manner of speaking a compensation for the lengthening. These elements are consistent with stress-timed language"s underlying concept of syllable stress that makes some syllables longer than others. For languages that seem to be more syllable-timed, it"s just the opposite of the stress-timed. The simplicity of syllable (frequency of the simpler CV syllables) and fewer vowel reduction results in more controlled syllable durations (Fenk &amp; Fenk-Oczlon, 2006). From these characteristics, some prefer the term compensating and controlling instead of the traditional syllable and stress timing, respectively.Despite Japanese being classified as a distinct rhythm class called Mora (Bloch, 1950), in the course of this paper, we refer to Japanese as "syllable-timed," in adherence to the phonological model where we considered Japanese Mora as an extreme case of syllable-timing. As shown in ), Japanese is located at the extreme syllable-timed side of the syllable/stress-time spectrum. Similarly, English was chosen since it has a great degree of stress-timing characteristic (Tajima , 1998). This study incorporates computational and perceptual methods to classify Filipino speech rhythm. Speech rhythm may be described as a language&quot;s distinguishing durational sound pattern, resulting from the complexity of the language&quot;s syllable inventory. 1 Computational methods involve the correlation of rhythm-types to acoustic features such as the vocalic and consonantal intervals, one of which is the implementation of Multivariate Discriminant Analysis (MDA). Perceptual methods involve contrasting the rhythm of an unclassified language from prototype syllable-timed and stress-timed sentences. In order to isolate rhythm from speech, a data-stripping technique called flat sasasa resynthesis was implemented wherein the consonants are replaced with /s/ and vowels with /a/, producing a resynthesized alternating &quot;sasasa&quot; sounds at a constant pitch (F0). The rhythm discrimination and classification were closely examined for consistency between the data modeling and listening test results. The computational experiment was able to show that an MDA classifier trained to distinguish English and Japanese sentences tend to label Filipino sentences as Japanese 67% of the time, vis-à-vis the perceptual experiment showing that the listeners perceive Filipino to be more similar with Japanese, this study shows computational and perceptual validation that Filipino is syllable-timed, just like Japanese.
Disfluencies in Consecutive Interpreting among Undergraduates in the Language Lab Environment 1 Consecutive interpreting (CI) is "a process in which adequate information is orally presented and transferred into another linguistic and cultural system" (Hu, 2006, p. 3). Among the forms of interpreting, e.g. simultaneous interpreting (SI) featured by the dependence on high-tech booth facilities and remote interpreting (RI) featured by the reliance on high speed internet connection, CI is the most frequently adopted for its low cost and moderate requirement on technical support. It is applied in interpreter-on-site business negotiations, press conferences, parent teacher meetings, interviews, and individual consultations. Responding to the increasing social demand for CI interpreters and English users of certain CI skills, a number of universities and colleges in China have offered CI as a compulsory core course for English majors.In the assessment of CI output, fluency is an important criterion. Since CI output can be regarded as a natural speech delivered in the form of the target language and featured by spontaneous production, CI researchers share the findings on fluency in natural speech studies. Although categorized as disfluencies (Engelhardt et al., 2010;MacGregor et al., 2009;Corley &amp; Stewart, 2008;Shriberg, 1999;Clark &amp; Wasow, 1998), pause, fillers and repetition are by no means tantamount to fluency hindrance. Studies have shown that pause, fillers and repetition may help smooth speech, facilitate understanding and serve different communicative functions (Brennan &amp; Schober, 2001, in Xu, 2010Tissi, 2000;Hieke, 1981). Further, Clark &amp; Tree (2002) propose moving the status of the fillers um and uh toward that of an interjection. The positive role of disfluecies in spontaneous speech is undeniable. However, it is also beyond dispute that disfluencies impair speech fluency when they exceed acceptable limits. Goffman states that "[natural speech] segments must be patched together without exceeding acceptable limits of pauses, restarts, repetitions, redirections, and other linguistically detectable faults" (1981( in Mead, 2002. Evidence also shows that disfluencies can affect language comprehension (Arnold &amp; Tanenhaus, 2011;Xu, 2010;MacGregor, et al., 2009).The studies aforementioned are based on the corpora of natural conversation and output of professional interpreters. Few, if any, investigations have focused on CI beginning learners. It is clear that these would-be interpreters are not professionals ---hence the limited applicability of the research findings in CI teaching. Besides, the objectives of CI courses at the beginner's level vary in mainland China, as is the case in Taiwan. Kuo's (2001) research shows that some of the schools in Taiwan offering CI courses aim to enhance students' proficiency in listening comprehension and oral expression while some plan to prepare their students for advanced interpreting training at the graduate level. CI beginning learners in mainland China, mostly English majors in universities and colleges, demonstrate considerable difference in learning motivation and language proficiency. Further, had the previous studies investigated the frequencies of disfluencies, it would have been clearer whether pause, fillers and repetition are prevalent among the speakers, interpreters and learners.This investigation aims at filling this gap. Based on the examination of the frequencies of pause, fillers, and repetition in 28 CI outputs produced by Chinese undergraduates, the investigation attempts to answer the following research questions: 1) What is the frequency of pause occurring in CI learners' output? 2) What is the frequency of fillers occurring in CI learners' output? 3) What is the frequency of repetition occurring in CI learners' output? 4) What pedagogical suggestions can be made to improve CI learner's fluency? Consecutive interpreting (CI) has gained increasing popularity and application in today&apos;s world. Fluency is a criterion for CI output assessment. Although disfluencies among CI professionals were investigated in previous studies, little research has been done on the disfluency prevalence among CI undergraduate learners. Nor is the advantage of the language lab facilities studied in terms of helping beginning learners to improve speech fluency. Based on the analysis of 28 recorded CI outputs produced by students in a Chinese university, this paper attempts to identify clearly the frequencies of difluencies. The results show that the problems of overusing fillers and repeated words may be prevalent among CI beginning learners, while notable pause seems to be better controlled in the language lab environment. Then, the causes of the difluencies are explored and pedagogical suggestions are offered.
Learning-to-Translate Based on the S-SSTC Annotation Schema The S-SSTC-based framework for the construction of MT systems has been introduced in 2002 [Mosleh et. al. 2002] and developed since to an operational state (SiSTeC-ebmt for English-Malay and English-Chinese). In this article, we would like to stress a particular aspect, namely that this approach is better capable of modeling the translation knowledge of human translators than other example-based approaches. Because the translation knowledge is represented as alignments (synchronizations) between string-tree alignments (SSTCs, or structured string-tree correspondences), it is more natural to translators (and post-editors) than direct word-word, string-string or chunk-chunk correspondences used in classical SMT and EBMT models. It is also totally static, hence more understandable than procedural knowledge embedded in almost all RBMT approaches.The learning process which is an integral part of the development of SiSTeC-ebmt MT systems can in fact be viewed as a special case of the study of reasoning reported in [Khardon&amp;Roth 94], because it combines the interfaces to the 'world' used by known learning models with the reasoning task and a performance criterion suitable for it. In such a framework, the intelligent agent is given access to its learning interface, and is also given a grace period in which it can interact with this interface and construct its representation Knowledge Base (KB) of the 'world'. Its reasoning performance is measured only after this period, when it is presented with 'queries' from some query language, relevant to the 'world', and has to answer whether such 'queries' are implied by the learned 'world' model. In our case, the 'world' is the 'translation task' captured in terms of the parallel texts produced by human translators and enriched by their S-SSTCs, and the 'queries' are simply modeled by a predicate Translate(ST,TT) where ST is the source language text and TT is a variable to be instantiated by a target language text if the 'translation' model learned is capable of performing such translation.Our model directly deals with three main difficulties in the traditional treatment of MT which stem from its separation from the "translation task" (the 'world'). First, by allowing the system to learn from real translation examples directly, we avoid the need to indefinitely pursue the elusive goal of writing grammars to exactly describe intermediate syntactico-semantic monolingual representations and their correspondences. Second, we make explicit the dependence of the MT system performance on the input from the environment. This is possible only because the learning process uses feedback from the real translation knowledge when constructing its knowledge representation. Third, such MT systems using an inductively learned knowledge base yield a desirable non-regressive behavior by using translation mistakes to improve their knowledge base.Learning to translate is just like any other machine learning task; it is concerned with modeling and understanding learning phenomena with respect to the 'world' -a central aspect of cognition. Traditional theories of Machine Translation systems, however, have assumed that such cognition can be studied separately from learning. It is assumed that the knowledge is given to the system, stored in some representation language with a well-defined meaning, and that there is some mechanism which can be used to determine what source language text can be translated with respect to the given knowledge; the question of how this knowledge might be acquired and whether this should influence how the performance of the machine translation system is measured is not considered. We prove the usefulness of the 'learning-to-translate' approach by showing that through interaction with the world, the agent truly gains additional translating power, over what is possible in more traditional settings. We present the S-SSTC framework for machine translation (MT), introduced in 2002 and developed since as a set of working MT systems (SiSTeC-ebmt). Our approach is example-based, but differs from other EBMT approaches in that it uses alignments of string-tree alignments, and in that supervised learning is an integral part of the approach. Our model directly deals with three main difficulties in the traditional treatment of MT that stem from its separation from the &quot;translation task&quot; (the &apos;world&apos;). First, by allowing the system to learn from real translation examples directly, we avoid the need to indefinitely pursue the elusive goal of writing grammars to exactly describe intermediate syntactico-semantic monolingual representations and their correspondences. Second, we make explicit the dependence of the MT system performance on the input from the environment. That is possible only because the learning process uses feedback from the real translation knowledge when constructing its knowledge representation. Third, such MT systems using an inductively learned knowledge base yield a desirable non-regressive behavior by using translation mistakes to improve their knowledge base.
Translating English Names to Arabic Using Phonotactic Rules With the rapid increase of the published information, it is difficult to rely on human translation to translate such information from one language to another and translation of appropriate names is generally realized as a significant issue in many multi-lingual text and speech processing applications. In our work, a set of hand-crafted transformations for locally editing the phonemic spelling of an English word to conform to rules of Arabic syllabification are used to seed a transformationbased learning algorithm. The algorithm examines some data and learns the proper sequence of application of the transformations to convert an English phoneme sequence to a Arabic syllable sequence. Our paper describes a data driven counterpart to this technique, in which a cascade model is used to go from English names to Arabic transliteration. The system uses only plan for Translate English Names to Arabic, and that can be processed and printed easily. Moreover, the Translated names can be read and recognized by ordinary people. This paper also discusses the problem background and provides solutions to Arabize almost all English Names. In section 2, we briefly highlight some related works. In section 3, we introduce description of translation system. In section 4, some specific phonotactic rules are shown. In section 5, the evaluation experiment for auto transliteration is presented. Finally, in Section 6, we draw some concluding remarks. With the increasing numbers of the arrivals to the Arabian countries; it is necessary to use Arabic language in writing names on official documents. Because of the difference in writing English names in Arabic language, many methods have been spread for translating English names which led to first, duplication in every single English name written in Arabic and this will lead to negative effects security and property rights. The second difference is that even if the transliteration of Arabic names is standardized, it is difficult for a layperson to implement it. This paper is to provide algorithms based on some rules that can be used in programming a system to transliterate English names automatically. The system uses only plan for Translate English Names to Arabic, and that can be processed and printed easily. Moreover, the Translated names can be read and recognized by ordinary people.
System for Flexibly Judging the Misuse of Honorifics in Japanese * In Japan, linguistic honorifics play an important role in social activities, especially in conversations. There are few languages in which such linguistic honorifics are as highly developed as Japanese.One important function of linguistic honorifics is to awaken in each person a consciousness of the social-relationship among speakers, listeners, and individuals being addressed or referred to. Here, social-relationship is assumed to be represented by a combination of {relative social position among in the people in same group} and {in-group/out-group relationship} in our study.The correct usage of honorific expressions is indispensable to maintain appropriate social distance between individuals and assists smooth communication in Japanese society.Recently, however, increasing misusage of honorific expressions has been noted. One origin of such misusage may be poor education regarding linguistic honorifics. So, development of a computer-aided learning system for linguistic honorifics has been needed.Recently, Shirado (2007) proposed a system to judge the misusage of honorific expressions in Japanese. The system uses judgment rules constructed from textbooks of Japanese honorifics. However, these textbooks are not necessarily consistent with the awareness among the people regarding normative honorific expressions in modern Japanese society. Most of these textbooks are based on traditional studies even though linguistic norms change over time. So, more practical system to learn linguistic honorifics is needed.In this paper, we propose a system to flexibly judge the misusage of honorific expressions in Japanese sentences. This system uses the framework of the system proposed by Shirado, but our system is more practical than the previous one due to the following points:1. The judgment rules are generalized.2. The degrees of the validity of rules are quantified.3. Judgments are performed flexibly based on the learner's linguistic level. We propose a system for flexibly judging the misusage of honorifics in Japanese sentence. The system can point out misused words and phrases, and can also indicate how they are misused. The system uses judgment rules whose degrees of validity in modern Japanese society are quantified by psychological experiments. The system can judge sentences flexibly based on the learner&apos;s linguistic level by tuning thresholds regarding the degree of validity. The proposed system is expected to be applied in practical computer-aided education.
The Co-occurrence of Two Delimiters: An Investigation of Mandarin Chinese Resultatives Delimitednes 1 refers to "the property of an event's having a distinct, definite and inherent endpoint in time" (Tenny 1994: 4). For instance, He ate an orange describes a delimited event because eating an orange has a definite endpoint, that is, when the whole orange is eaten; in contrast, the event described in He ran is undelimited because a running event can take place forever if no endpoint is explicitly expressed (Tenny 1994: 4). Tenny (1987Tenny ( , 1994 proposes the Single Delimiting Constraint (SDC) that an event described by a verb can be delimited only once. A similar constraint the Unique Path Constraint (UPC) is proposed by Goldberg (1991Goldberg ( : 368-369, 1995) that only one "distinct path" can be predicated of an argument denoting a physical object. The path in the UPC can be understood in two senses, either as a path for physical motion, or as a metaphorical path where the object undergoes a change of state. And the notion of a distinct path entails that the physical object cannot be in two locations or in two states at a time, or in both a location and a state at a time (ibid.). Both the SDC and the UPC apply the effects of delimitedness as an aspectual property to the mapping of semantics and syntax, and explain why sentences such as (1) are not acceptable.(1) a. *Shirley sailed into the kitchen into the garden. (Goldberg 1991: 368 Before introducing counterexamples to the SDC and the UPC raised by linguists, we provide a few examples showing what elements can be delimiters, i.e. have a delimiting function. As illustrated in (1), prepositional directionals, e.g., into the kitchen and into the garden in (1a), and adjective resultatives, e.g., dry and clean in (1b), can be the delimiters contributing an endpoint or endstate for an event. In addition, telic verbs or verb phrases by themselves can be delimiting; these include achievements such as arrive and die, and accomplishments such as break and walk three miles (Tenny 1994, among others, cf. Goldberg 1991. According to the SDC and the UPC, no additional resultative is allowed if the verb in a clause is inherently delimiting. For instance, the box being open in (2) can only have a depictive reading, but cannot be understood as a result of the event of arrival (Rappaport Hovav and Levin 2010). For this reason, Zhou (2008) modifies the SDC that changes can be understood on different dimensions (e.g., time, space, property, degree) and delimiters describing changes on different dimensions can co-occur. As in (3a), Zhou points out that break and into pieces can occur together because the former specifies a change on the dimension of property (from being a whole into broken), whereas the latter describes a change on the dimension of degree (e.g., broken into several bigger pieces or into many smaller pieces). Matsumoto (2006:16) also proposes a revised UPC that spatial and non-spatial delimiters can co-occur if they describe aspects of "a single line of development" that an entity follows. For instance, the non-spatial phrase into thick pieces and the spatial phrase into the bowl in (3b) can co-occur because they specify a change of state and a change of location that an entity can undergo in a natural temporal order, that is, cheese is usually placed into a container such as a bowl after it is sliced into pieces. Tenny (1987, 1994: 79) proposes the Single Delimiting Constraint that the event described by a verb can only be delimited once (cf. Goldberg 1991, 1995). The constraint applies the effects of delimitedness as an aspectual property to the mapping of semantics and syntax, and explains why sentences with two delimiters (e.g., *Martha wiped the table dry 1 clean 2 Tenny 1994: 80) are unacceptable. The constraint is challenged with English counterexamples and modified by Matsumoto (2006) and Zhou (2008). However, this study proposes that the constraint holds for Mandarin Chinese resultatives, whereas the revised constraints do not. Furthermore, we point out that while two independent delimiters usually do not co-occur in Chinese, a second delimiter that further specifies or reinforces the endpoint/ endstate denoted by the first delimiter is allowed (cf. Tenny 1994, Goldberg 1991). The results of this study may shed light on event structure of Chinese.
The Order of Mandarin Chinese Motion Morphemes and the &quot;Scalar Specificity Constraint&quot; ＊ Mandarin Chinese (hereafter "Chinese") often expresses directed motion events through a concatenation of verbal morphemes, e.g., gun "roll" and jin "enter" in gun-jin shui-li "roll into the water". 1 However, the order of these co-occurring motion morphemes is not flexible, e.g., *jin-gun shui-li enter-roll water-inside. This paper first reviews previous studies on the order of Chinese motion morphemes, and then provides proposals that can better explain the order. This study investigates semantic constraints affecting the order of motion morphemes in Mandarin multi-morpheme motion constructions (e.g., tui-hui recede-return). It classifies Chinese motion morphemes into three major types and proposes a &quot;Scalar Specificity Constraint&quot; to account for the order in multi-morpheme motion constructions. The constraint not only provides a better coverage of the data of Chinese motion constructions from the perspective of the syntax-semantics interface, but also illuminates the distribution of motion verbs in other serial verb languages.
Verbs and (sub)Event Structure: A Case Study from Italian * The relationship between events 1 and verbs is notoriously strong, since in most cases verbs lexicalize event predicates, and event predicates are lexicalized by verbs. However, the class of verbs is not uniform (although at least in Italian and in other European languages it is easily defined on the basis of morphological criteria). And, at the same time, it is not exclusively verbs that can be used to code events. A brief overview of some cases of mismatch between verbs and (lexicalizations of) events will be useful for defining the class of verbs with predicative complement in Italian. In this paper, I try to show the advantages of analyzing events in terms of subevent structure, by taking into consideration the case of a specific class of verbs: transitive and intransitive verbs which obligatorily require the presence of a predicative complement (e.g., English seem, consider). The proposed analysis is exemplified with data from Italian. Two verbs are described in detail: rimanere, &apos;remain&apos; and rendere, &apos;make&apos;. It is shown that subevent structure representation is useful for the description of the different uses and meanings of these verbs. This type of description can also be the basis for an accurate treatment in computational semantic lexica.
The Effects of EFL Learners&apos; Awareness and Retention in Learning Metaphoric and Metonymic Expressions Researchers in the field of SLA contend that learners' awareness of motivations is the key in second language acquisition (Ammar, Lightbown, &amp; Spada, 2010;O'Mally &amp; Chamot, 1990;R. Ellis, 2002). Cognitive linguists, applying the idea to research on figurative language learning, demonstrate the beneficial effects of enhanced awareness (Boers, 2000ab, 2001Boers &amp; Lindstromberg, 2006;N. Ellis, 2006ab ;Chung &amp; Ahrens, 2004;Deignan et al., 1997;Dong, 2004;Kövecses, 2001;Low, 1988, among others). These studies mainly implement metaphoric themes-called conceptual metaphor-during the learning processes in order to raise L2 learners' awareness of semantic motivation behind figurative expressions. The results have proved that L2 learners' enhanced awareness of conceptual metaphor is indeed beneficial in comprehension and retention.However, the method of providing conceptual metaphor is not unproblematic. First of all, their method mainly focuses on metaphor; metonymy is seldom taken into consideration. The underestimation of the importance of metonymy may lead to an overlook of its effects on learning figurative expressions. Metonymy and metaphor, in extant literature, are believed to interact with each other in intricate ways and their boundary is fuzzy (Barnden, 2010;Goossens, 1990;Radden, 2003). Chen &amp; Lai (in press), manifesting their interactions as a continuum, has found that L2 learners respond differently to figurative expressions locating on different spots of the continuum. However, with the significance of metonymy taken into consideration, whether L2 learners would respond to figurative expressions differently hasn't been determined.Moreover, focusing mainly on awareness raising may lead to an underestimation of effects of one important element: the gaps caused by different cultures between native and target languages. In the early stage of learning, both similarities and differences between L1 and L2 may facilitate L2 learning (Kellerman,1977;Odlin, 1989;Ringbom, 1987); with the advance in L2 learning process, such as metaphor and metonymy learning which involve not only languages but also cultures and conventions, conflicts between L1 and L2 knowledge may cause greater difficulties (Kövecses, 2001).In order to help numerous EFL learners around the world to be more aware of figurative language use, Kövecses' (2001) proposal of integrating metaphoric mappings seems very promising. Presumably, ontological mappings that characterize the correspondences between basic constituent elements in the source and in the target domain may help learners to create links between distinct linguistic expressions of the two languages; epistemic mappings that carry over knowledge about elements in the source domain onto elements in the target domain may help learners to relate their knowledge of the used and abstract half to the unused and concrete half. The idea of using metaphoric mappings as explicit instructions to facilitate domain linking processes between L1 and L2 figurative concepts not only follows the trend of cognitive linguistics but also deals with cultural gaps by utilizing learners' already-existent world knowledge and universal concepts. However, up till now the idea hasn't been empirically tested yet, and hence hasn't been able to claim its effects on L2 learning.The present study, therefore, intends to compare the two methods in teaching EFL learners metaphoric and metonymic expressions, determine their effects on L2 figurative language learning, and find a compromising way for EFL learners with different native languages. The two methods are the method of conceptual metaphor (CM), which focuses on giving conceptual metaphors and has learners compare two domains to find associative characteristics, and the method of metaphoric mappings (MM), which emphasizes mapping processes and has learners map between domains and between cultures. The present study targets at Chinese native speakers who are learning English as a foreign language. Their ability of finding figurative expressions and their retention of what have learned are under investigation. Cognitive linguists contend that learners&apos; awareness of motivations is the key in not only second language acquisition but also figurative language learning. Two cognitive-oriented methods are proposed to raise L2 learners&apos; awareness on metaphoric/metonymic expressions and to enhance retention: instruction involving conceptual metaphors (CM) and instruction involving metaphoric mappings (MM). The present study aims to examine their effectiveness in an EFL context. The results show favorable influences on learners&apos; awareness and retention, which confirm that cognitive-oriented instructions indeed can assist learners to make better sense of figurative language. Moreover, the instruction on metaphoric mappings seems to result in better awareness of expressions which involve more complicated and abstract mapping relationships. The findings of the study can shed light on the application of metaphor and metonymy to EFL teaching and learning of figurative language
NERSIL: the Named-Entity Recognition System for Iban Language Named entities (NEs) are recognized as an important source of information for many applications. In information retrieval, they improve the detection of relevant documents. In machine translation, post-editing is very expensive when the errors of a machine translation system are mainly due to NEs. Named entity recognition (NER) is the process of detecting NEs and classifying them into semantic categories. NERSIL is the first Iban NER. One of the policies of the Malaysian government is to develop more Malaysian digital contents by including indigenous languages like Iban. Presently, NER can accommodate only major languages such as English and Chinese. Indigenous languages are largely neglected. Hence, NER which can accommodate indigenous languages such as Iban is required. Besides the development of the tool itself, our project will provide access to indigenous knowledge. These may in turn encourage more research, which in long term will preserve the culture. In addition, the proposed method may help other under-resourced languages.The paper is structured as follows. Section 2 presents the background and related works. Section 3 explains the proposed method for building Iban rules and gazetteers. Section 4 describes the experiment performed to evaluate NERSIL, and section 5 depicts the analysis of the errors. Section 6 concludes the presentation and highlights our plans for future work. This paper presents NERSIL, the first Iban named entity recognition using ANNIE, an information extraction tool available within GATE. We proposed a method for building rules and gazetteers for Iban language. Rules were determined based on named entities that were not recognized by ANNIE. Then, the investigation of the contexts of these non-recognized Iban named entities allowed us to write the rules. NERSIL achieves 76.4% F-measure.
Fully-Automatic Marker-based Chunking in 11 European Languages and Counts of the Number of Analogies between Chunks The example-based approach (Nagao, 1984) contrasts with the statistical approach ( Brown et al., 1990;Brown et al., 1993) to machine translation in that it uses a bilingual corpus of aligned sentences as its main knowledge at run time. We aim at building an EBMT system based on proportional analogies.A translation method based on proportional analogies has been proposed by Lepage and De- noual (2005b). The following procedure gives the basic outline of the method to perform the translation of an input chunk. Let us suppose that we have a corpus of aligned chunks in two languages, German and French. Let x = "ein großes programm und" be a source chunk to be translated into one or more target chunks Its solution is a candidate translation of the source chunk: x = "un gros programme et" For such an EBMT system to work well, the more numerous the proportional analogies, the better the translation outputs are expected to be. The method can work on small sentences like the ones in the BTEC corpus (Lepage and Denoual, 2005a), but cannot handle long sentences like the ones in the Europarl corpus (Koehn, 2005). For long sentences, translating chunk by chunk could be a solution. We have inspected the quality of translation of chunks obtained by marker-based chunking in English and French in both directions in (Takeya and Lepage, 2011a;Takeya and Lepage, 2011b). Our results have shown that more than three quarters of the chunks can be translated by the one-step analogy-based translation method, and that a little bit less than half of the chunks has at least one translation that matches exactly with one of the references. As the number of analogies is the crucial point, this paper inspects ways of counting sentences into chunks using different markers and examines the number of proportional analogies between them in 11 European languages.The rest of the paper is organized as follows. Section 2 describes the basic notion of markerbased chunking used in the reported experiments. Section 3 explains the notion of proportional analogy. Section 4 presents the data for the experiments which are sample sentences from the Europarl corpus in 11 European languages and the experimental protocol. Section 5 describes the results of the experiments and analyzes them. A conclusion is given in Section 6. Analogy has been proposed as a possible principle for example-based machine translation. For such a framework to work properly, the training data should contain a large number of analogies between sentences. Consequently, such a framework can only work properly with short and repetitive sentences. To handle longer and more varied sentences, cutting the sentences into chunks could be a solution if the number of analogies between chunks is confirmed to be large. This paper thus reports counts of number of analogies using different numbers of chunk markers in 11 European languages. These experiments confirm that the number of analogies between chunks is very large: several tens of thousands of analogies between chunks extracted from sentences among which only very few analogies, if not none, were found.
Extraction of Broad-Scale, High-Precision Japanese-English Parallel Translation Expressions Using Lexical Information and Rules Non-native speakers often have problems explaining ideas or presenting achievements in written English, in part because of the large amount of time needed to determine which possible translation of an expression most suits the context. Trying to develop English-writing support tools that will enable non-native speakers to produce nearly perfect English sentences for mixed English-Japanese sentences-in which expressions without know translations are simply written in Japanese- Ma et al. (2008;2009) have developed systems that can provide support at the word and phrase levels. That is, the given Japanese parts in the mixed English-Japanese sentences can be words or phrases. For phrase-level support in those systems, the Japanese parts are extracted from the mixed sentences and segmented into words, the candidate English equivalents of the segmented Japanese words are identified by searching through a Japanese-English dictionary, and the best equivalents of the Japanese phrases are selected from the combinations of the candidate translations of the single words. This kind of support, however, has two problems. One is the variety of the phrase patterns that the system can support is limited because the English phrases are generated only from the combinations of the candidate translations of single words. The other is the processing is time-consuming because a large number of the translations of single words results in an enormous number of combinations.We think that an effective way to solve these problems would be to use an approach based on translation patterns: to first construct a dictionary of the Japanese-English translation patterns and use that dictionary to provide English-writing support. We therefore tried to extract broadscale, high-precision Japanese-English parallel translation expressions from large aligned parallel corpora first. 1 To acquire broad-scale parallel translation expressions, we used a new method to extract single Japanese and English word n-grams, by which we could then extract as many parallel translation expressions as possible. To achieve high extraction precision, we first used hand-crafted rules to prune the unnecessary words often found in expressions extracted on the basis of word ngrams, and we further used lexical information from a Japanese-English dictionary to refine these parallel translation expressions. Computer experiments with aligned parallel corpora consisting of about 280,000 pairs of Japanese-English parallel sentences found that more than 125,000 pairs of parallel translation expressions could be extracted with a precision of 0.96. These figures show that the proposed methods for extracting a broad range of parallel translation expressions have reached a level high enough for practical use. Extraction was attempted of broad-scale, high-precision Japanese-English parallel translation expressions from large aligned parallel corpora. To acquire broad-scale parallel translation expressions, a new method was used to extract single Japanese and English word n-grams, by which as many parallel translation expressions as possible could then be extracted. To achieve high extraction precision, first, hand-crafted rules were used to prune the unnecessary words often found in expressions extracted on the basis of word n-grams, and lexical information was used to refine the parallel translation expressions. Computer experiments with aligned parallel corpora consisting of about 280,000 pairs of Japanese-English parallel sentences found that more than 125,000 pairs of parallel translation expressions could be extracted with a precision of 0.96. These figures show that the proposed methods for extracting a broad range of parallel translation expressions have reached a level high enough for practical use.
A Construction Grammar Approach to Prepositional Phrase Attachment: Semantic Feature Analysis of V NP1 into NP2 Construction * The preposition into describes the path of motion event which typical involves an object, or figure, moves along the path to enter a reference object, or ground (Talmy, 2000). An example of motion event is the caused-motion construction involving a verb (V) and two noun phrases (NP1 and NP2) as a direct and an indirect object, respectively. Sentence (1), extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus 1 (Charniak, et al., 2000), illustrates such a V NP1 into NP2 construction (shown in bold with lexical categories glossed underneath). The basic semantics of the construction involves a motion event that requires the direct object (NP1) to be moved and directed to the confinement of indirect object (NP2). In this case, an unspecified number of airplanes undergo movement towards a deictic space.(1) To shove even more airplanes into this space is asking for trouble, V NP1 Prep NP2 experts say. (WSJ-V1141) However, this type of prepositional phrases poses an ambiguity problem in parsing. Sentence (1) serves as an example for one means of parsing in which the preposition closely associates with the verb but not NP1. The second possibility of parsing is where the PP is required to be interpreted with NP1, as illustrated in bold in sentence (2). * The authors would like to thank the anonymous reviewers for their comments on this work. This study is supported in part by National Science Council Research Grants (NSC 99-2410-H-004-206-, NSC 100-2628-H-004-137-, and NSC-100-2221-E-004-014). 1 All the examples discussed in this paper are from the WSJ corpus unless specified.(2) And he soon became aware that the government was able to show a flow of millions of dollars in illicit funds into his account. V NP1 Prep NP2 Here the head of NP1 (a flow) is to be interpreted along with into and NP2 (his account), rather than with the preceding verb (to show). Computational linguists have found these two structures causing parsing problems in natural language processing (NLP) and referred to this problem of determining the site of PP to be attached as the PP attachment problem (e.g., Hindle and Rooth, 1993;Volk, 2006). As illustrated in (1) and (2), this problem is conventionally formalized as a binary choice (Merlo and Ferrer, 2005), either verb-attached for (1) or noun-attached for (2). In the minimalist syntax, ternary structures like (3a) are to be transformed by deriving an explicit causative construction (3b) (Radford, 2004 Although the plausibility of equating the two constructions has long been questioned (e.g., Fodor, 1970), the causative structure (3b) cannot provide a direct solution to the PP attachment problem for (3a). Moreover, the binary solution to the problem has been challenged by computational linguists. For example, Merlo and Ferrer (2005) contend that such a dichotomous treatment may be a simplification. They propose to take into account of the nature of the attachment by distinguishing PP arguments from PP adjuncts. Sentence (4) is an example of two verb-attached PPs that maintain different relationships with the verb shown in the gloss. (4) Put the block on the table in the morning. V NP1 PP argument PP adjunct ( Merlo and Ferrer, 2005, p. 342, with gloss added) Since PP arguments carry the core message and PP adjuncts provide additional information to the core meaning, their distinction further refines NLP tasks. Although studies like Merlo and Ferrer (2005) provide novel approaches to tackle the PP attachment problem, the notion of binary sites for PP attachment has not been scrutinized. The presupposition of binary attachment sites, however, may result in a forced selection from one of the two choices and may overlook other possibilities for correct parsing. Consider the construction in bold in sentence (5)   Goldberg, 1995, p. 152, with gloss added) According to our first choice, verb-attached parsing, the verb sneezed is to be analyzed with NP1 the tissue. The grouping is semantically invalid since the verb is normally intransitive without a direct object. Yet, it is not any less awkward as the noun-attached option is considered (the tissue off the table). In Goldberg's (1995) seminal work on construction grammar, she discusses the basic semantics of caused-motion construction or that "the causer argument directly causes the theme argument to move along a path designated by the directional phrase: that is, 'X CAUSES Y to MOVE Z'" (p. 152). In brief, the caused-motion construction includes a directional phrase like into PP and entails a movement feature. However, (5) illustrates an atypical example of caused-motion construction where the construction fails to be interpreted through its components or what the PP attachment problem is based on. According to Goldberg, the semantic meaning of (5) can only be derived by taking into account of the entire construction. In other words, to address the PP attachment issue in sentences like (5), we need to take into account of a third possible structure in addition to a binary choice from verb-or nounattachment. 2 In this study, we take the construction grammar approach to reformalize the PP attachment problem. In addition to the conventional binary approach to determining the PP attachment sites, we suggest a third possible structure where the PP co-attaches to both verb and noun based on the construction grammar framework. We also develop a semantic analysis of the feature movement (denoted as [+movement] or [-movement]) for the verb and direct object in the V NP1 into NP2 construction to determine the PP attachment site. Our proposal examines the WSJ corpus data by means of manual annotation. This paper provides a construction grammar perspective to identifying the ambiguity of prepositional phrase (PP) attachments (i.e., whether a PP is attached to the closest VP or NP1). Despite the wide discussion of these two structures (VP-attached and NP1-attached), we raise the possibility for a third parsing structure (about 11.3% from all 1845 instances analyzed), a co-attachment to both verb and noun. A co-attachment structure denotes the lack of [movement] feature in both the verb and noun surrounding a PP. This proposal is arrived when we annotate the semantic feature [-movement] to both VP and NP1, respectively, in a caused-motion construction of V NP1 into NP2 (e.g., vote an individual into the presidency; shamed us into pity; define ourselves into a box).
Supervised and Semi-supervised Methods based Organization Name Disambiguity Twitter is an online social networking and microblogging service, which rapidly gained worldwide popularity, with 200 million users as of 2011 1 , generating over 200 million tweets and handling over 1.6 billion search queries per day 2 . How to manage this information to grasp the response of people to governmental policies, the feedback and comment of people on commercial products have received considerable attention in research community. There are some researches such as opinion mining, online reputation management, which focus on monitoring user generated media. One of the essential things of these researches is first to get the information which is related to the studied entity, such as product, company, or certain event.This paper focuses on finding related tweets to a given organization. This is a challenging task due to the potential organization name ambiguity. For example, the name of company "Apple" which has a separate meaning fruit apple. The word "Amazon" could be used to refer river or company. Filtering spurious name matches is important to effectively detect and analyze relevant contents that people say about the organization.To overcome the problem that the tweets and organizations contain little information, we induce external resources to enrich the information of organization. The organizations in training data are different with those in test data, which leads that we could not train a classifier to a certain organization. Therefore, supervised and semi-supervised methods are adopted in two stages to classify the tweets. This is a try to utilize both training and test data for this specific task.The remainder of the paper is organized as follows: Section 2 describes the related work on name disambiguity. Section 3 gives overview of the problem and our methods. Section 4 presents supervised method to classify tweets. Section 5 introduces semi-supervised method to classify the tweets which is a step of modifying initial classification results gotten by supervised method. Section 6 gives the experiments and results. Finally section 7 summarizes this paper. Twitter is a widespread social media, which rapidly gained worldwide popularity. Pursuing on the problem of finding related tweets to a given organization, we propose supervised and semi-supervised based methods. This is a challenging task due to the potential organization name ambiguity. The tweets and organization contain little information. The organizations in training data are different with those in test data, which leads that we could not train a classifier to a certain organization. Therefore, we induce external resources to enrich the information of organization. Supervised and semi-supervised methods are adopted in two stages to classify the tweets. This is a try to utilize both training and test data for this specific task. Our experimental results on WePS-3 are primary and encouraging, they prove the proposed techniques are effective in performing the task.
Study and Implementation of Monolingual Approach on Indonesian Question Answering for Factoid and Non-Factoid Question * * * * Question Answering (QA) is a task in Natural Language Processing (NLP) that will automatically provide answers to questions posed in natural language. QA system can use a database or document collection (local or web) as the sources of the answer.A QA system usually consists of three main components ( Harabagiu et al., 2000): question analyzer, passage retriever, and answer finder. Question analyzer component aims to classify the question according to the Expected Answer Type (EAT) as well as extract the keywords in question. These keywords will be used as input query in passage retriever component to get candidate documents/paragraphs that contain the answer. Answer finder component searches for candidate answers from documents/paragraphs that have been found previously. Each candidate answer will be given a score based on its compliance with the question. Some candidate answers will be selected as the best answers to the question. Each component can use various methods based on the language, question domain, question type, and available tools.QA system for Indonesian language that has been built: handle factoid questions only (Purwarianti et al., 2007;Wisudawan, 2010), handle non-factoid questions only (Yusliani, 2010), the domain of questions is limited (Mahendra et al., 2008), cross lingual and use NLP tools for English (Wijono et al., 2006;Wisudawan, 2010). There are also researches in NLP for Indonesian language that have been done, including parser, stemmer (Adriani et al., 2007), Part-of-Speech (POS) tagger (Wicaksono and Purwarianti, 2010), and Named-Entity (NE) Tagger (Budi et al., 2005).From these studies, we obtained some things that can be explored further and improved to produce a QA system for Indonesian language using monolingual approaches with wider scope 25th Pacific Asia Conference on Language, Information and Computation, pages [622][623][624][625][626][627][628][629][630][631] of questions that can be handled. This QA system is expected to handle factoid (Person, Organization, Location, Datetime, Quantity) and non-factoid questions (Definition, Reason, Method). Moreover, the approach to be used in this QA system is monolingual approach, with expectation to produce a QA system with better accuracy.The rest of this paper is organized into discussion of methods that are used in each component (question analyzer, passage retriever, answer finder), experiments, and conclusions. We developed an open domain QA system that can handle factoid and non-factoid questions in Indonesian language by using monolingual approaches. EAT classification is done by identifying question word and clue words. Keyword extraction from question is done by looking at POS information of each word in question, eliminating stop words, and stemming. We use articles from Indonesian Wikipedia as corpus and Lucene framework as the base for passage retriever component, with three additional processing: query expansion, boost EAT, and boost term. For factoid questions, answer finding is done by using Named Entity Recognition. Answer scoring is done by calculating keyword occurrences and answer-keywords distance (MRR = 0.6191). For non-factoid questions, answer finding is done by identifying sentence pattern and clue words. Answer scoring is done by considering pattern priority and keyword occurrences (MRR = 0.8079).
Welcome Message from Honorary Chairs PACLIC 26 Organizers Steering Committee: Honorary Chairs: Local Organizing Committee: The First Workshop on Generative Lexicon for Asian Languages (GLAL) Organizers  On behalf of the Organizing Committee of the 26th Pacific Asia Conference on Language, Information and Computation (PACLIC 26), we would like to extend our warm welcome to all of the participants and speakers, and in particular, we would like to express our sincere gratitude to our invited speakers. This international conference is organized by the Faculty of Computer Science, Universitas Indonesia and is supported by the I-MHERE DIKTI project. We are very keen to host a conference about language processing fields which involves many researchers in this Asia Pacific region. We believe that this international conference will open up the opportunities for sharing and exchanging original research ideas and opinions, getting inspiration for future research, and broadening knowledge about various new topics and approaches in language study. We hope that in this conference, the attendees would have the opportunity to meet with new people and discuss the opportunity to collaborate together. We chose to organize PACLIC 26 in Bali so that aside from attending this interesting conference, you can also enjoy the scenery and the culture of Bali. We realize that there might not be enough time to see all the nice places in Bali, but we hope that you can bring home some good memories. We would like to express our sincere appreciation to the members of the Program Commitee for a fruitful reviews of the submitted papers, as well as the Organizing Commitee for the time and energy they have devoted to editing the proceedings and arranging the logistics of holding this conference. We would like to give an appreciation to the authors who have submitted their excellent works to this conference. Last but not least, we would like to extend our gratitude to the Ministry of Education and Culture of the Republic of Indonesia and the Dean of the Faculty of Computer Science at Universitas Indonesia for their continued support towards the PACLIC 26 conference. Have a nice time in Bali and enjoy the conference. Honorary Chairs: Mirna Adriani (Universitas Indonesia) I Wayan Arka (ANU / Universitas Udayana) iii Welcome Message from Program Co-Chairs Welcome to Bali! This is the first time that the PACLIC conference is being held in Indonesia, and we are very excited about this fact. By all accounts, Indonesia is a linguistic treasure trove, with over 700 living languages today according to the Ethnologue report. Moreover, with an increasing number of its 240 million population active on the Internet via the Web and social networks, clearly these are exciting times to be engaging in computational approaches towards the languages of Indonesia. However, this PACLIC conference in 2012 is special for other reasons, most notably the commemoration of 25 years of the conference series. Over the years, the conference has developed into one of the leading conferences in the fields of theoretical and computational linguistics, extending beyond the Asia-Pacific region. This year, the specific research topics that the papers focus on can be classified into the following: discourse &amp; pragmatics, grammar &amp; syntax, information extraction, information retrieval, lexical semantics, machine translation, parsing, sentiment analysis, text summarization &amp; paraphrasing, and word sense disambiguation &amp; distributional semantics. Moreover, there is an interesting mix of both theoretical and computational approaches to almost all of the aforementioned topics. We received paper submissions representing immense diversity, with authors from 29 countries or regions, and Vietnam. To ensure that all accepted papers met the high quality standard of the PACLIC conference, all papers were sent to three reviewers. Of the 117 submissions that we received, 39 papers (33%) were accepted for oral presentation, and another 18 papers (15%) were accepted for poster presentation. We believe this has yielded an interesting, diverse, and high-quality collection of papers, and are confident that the conference will be successful as a result. A successful conference is the result of many peoples efforts and contributions. Aside from the efforts of the authors who will be presenting their current work, thanks must be given to the tremendous efforts made by the program committee members in their paper reviews. Besides the oral and poster paper presentations, the conference is enriched by several invited speakers. Firstly there is a Special Session commemorating 25 years of PACLIC, which brings together Prof Kiyong Lee from Korea University, Prof Yuji Matsumoto from the Nara Institute of Science and Technology, and Prof Benjamin T&apos;sou from the Hong Kong Institute of Education, three figures who have been instrumental in the formation of the PACLIC tradition. We have also scheduled invited talks from Prof I Wayan Arka from ANU &amp; Universitas Udayana and Prof Tim Baldwin from the University of Melbourne. The expertise in the respective fields of all five speakers will undoubtedly provide us with new insights for research. On behalf of the program committee, we express our heartfelt thanks to them all. We would also like to thank the steering committee for their guidance, and the local organizing committee at Universitas Indonesia for their dedicated efforts and their excellent coordination with all parties, which has ensured that this conference will be a successful event. Finally, we wish that you will all enjoy the conference presentations and resulting discussions between old and new friends, and also have some time to enjoy the wondrous setting that is the island of Bali.
From All Possible Worlds to Small Worlds: A Story of How We Started and Where We Will Go Doing Semantics The story goes back to the early 1970s with generative semantics and the dawning of Montague semantics. The beginning was concerned with big open worlds, all possible worlds, for truth meant, in the eyes of philosophers, being true in all possible worlds. And linguists inherited their notion of truth in constructing a formal theory of natural language semantics. In the 1980s, however, the focus of linguistic semantics changed from necessary or possible truth to something more contingent or informative, namely various sorts of information obtainable from small worlds, called situations. A new trend developed in the 1990s towards the computational modeling of semantic theories, based on socalled real language data or large corpora such as BNC (the British National Corpus). This required various situations of language use to be constrained with an idealized set of conditions. Then around the turn of the second millennium, semanticists have followed a data-driven approach to the construction of their model-theoretic semantics, which requires a large amount of language resources or raw corpora tagged with a variety of information, both morphosyntactic and semantic. As a result, some semanticists including myself have proposed doing semantics using annotated language resources, which was known as annotation-based semantics.My story will narrate how our colleagues have reacted to all these changes. Not being an historian, however, the speaker dares not guarantee his view to be fair and objective. Instead, it will be very subjective and introspective. Hence, it will simply be head-driven without being data-driven. I, as an old member of the PACLIC community, justify this narrowly defined role of an invited speaker because I trust that other PACLIC founding members, Benjamin T'sou and Akira Ikeya, will balance whatever might be one-sided in my talk. This is a short story of how we have evolved over the last 40 years, doing semantics. It could partially overlap with a history of PACLIC which is commemorating the 25th year or a quarter of a century of its founding. The story tells how we semanticists of natural language moved from all possible worlds to small worlds, now living in and with a tiny mobile world.
Developing a Deep Grammar of Indonesian within the ParGram Framework: Theoretical and Implementational Challenges  This paper discusses theoretical and implementational challenges in developing a deep grammar of Indonesian (IndoGram) within the lexical-functional grammar (LFG)-based Parallel Grammar (ParGram) framework, using the Xerox Linguistic Environment (XLE) parser. The ParGram project involves developing and processing computational grammars in parallel to test the LFG&apos;s theoretical claims of language universality, while at the same time testing its robustness to handle typologically quite different languages. Two relevant cases are discussed: voice-related morphosyntactic derivation and crossed-control dependency in Indonesian. It will be demonstrated that parallelism should be taken as a matter of degree, that it cannot always be maintained for good language-specific reasons and that the participation of IndoGram has also contributed to the rethinking and improvement of certain parallelism standards.
  
Social Media: Friend or Foe of Natural Language Processing?  In this talk, I will outline some of the myr-iad of challenges and opportunities that social media offer for natural language processing. I will present analysis of how pre-processing can be used to make social media data more amenable to natural language processing, and review a selection of tasks which attempt to harness the considerable potential of different social media services.
Towards a Semantic Annotation of English Television News - Building and Evaluating a Constraint Grammar FrameNet  This paper presents work on the semantic annotation of a multimodal corpus of English television news. The annotation is performed on the second-by-second-aligned transcript layer, adding verb frame categories and semantic roles on top of a morphosyntactic analysis with full dependency information. We use a rule-based method, where Constraint Grammar mapping rules are automatically generated from a syntactically anchored Framenet with about 500 frame types and 50 semantic role types. We discuss design decisions concerning the Framenet, and evaluate the coverage and performance of the pilot system on authentic news data.
Compositionality of NN Compounds: A Case Study on [N 1 +Artifactual-Type Event Nouns] Event nouns in Mandarin Chinese have generated extensive interest ( Han 2007Han , 2011Liu 2004;Ma 1995;Wang &amp; Huang 2011a, 2011b, 2012a, 2012c, 2012d). However, little research has concerned about the compositional mechanisms at work in [N 1 +event noun] type [N 1 N 2 ] N compounds.Generative Lexicon theory (GL) provides a rich compositional representation through generative devices (Pustejovsky 1993(Pustejovsky , 2001(Pustejovsky , 2006(Pustejovsky , 2011Pustejovsky &amp; Jezek 2008). Under a tripartite system of the domain of individuals, including natural types, artifactual types and complex types (Pustejovsky 2001(Pustejovsky , 2006; Pustejovsky &amp; Jezek 2008), GL establishes three mechanisms at work when a predicate selects an argument.1) Pure Selection (Type Matching): the type a function requires is directly satis¿ed by the argument;2) Accommodation: the type a function requires is inherited by the argument;3) Type Coercion: the type a function requires is imposed on the argument type. This is accomplished by either:(i) Exploitation: taking a part of the argument's type to satisfy the function; (ii) Introduction: wrapping the argument with the type required by the function. Following Pustejovsky (2001,2006) and Pustejovsky &amp; Jezek (2008), Wang &amp; Huang (2012e) establish a type system for event nouns, including natural types, artifactual types, natural complex types and artifactual complex types. The current paper only focuses on artifactual-type event nouns and explores the compositional mechanisms of nominal modification to these nouns in NN compounds. Furthermore, the domain information contribution to the reading of a NN compound is surveyed. Generative Lexicon theory (GL) establishes three mechanisms at work when a predicate selects an argument, i.e. pure selection, accommodation and type coercion. They are widely used in verbal selection of nouns in the entity domain. However, little attention has been devoted to the compositionality of [N 1 +event noun] type NN compounds. This paper extends the usage of these mechanisms in two ways: 1) the eventive nominal head selection of a nominal modifier, and 2) their use in the eventive domain, through the case study on [N 1 +bsài &apos;competition&apos;]. Moreover, it reveals a new compositional mechanism sub-composition. It also discovers the domain contribution in type coercion. This work enriches the study on compositionality and GL.
Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers Classifiers in standard supervised machine learning have been trained for data in domain A using manually annotated data in domain A, e.g., to train classifiers for newswires using newswires. However, classifiers for data in domain B have sometimes been necessary when there have been no or few manually annotated data, and there have only been manually annotated data in domain A, which has been related to domain B. Domain adaptation (DA) involves adapting the classifier that has been trained from data in domain A (source domain) to data in domain B (target domain). This has been studied intensively in recent years.However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.There are many methods of DA for WSD but we assume that the optimal method varies according to each instance. This paper proposes automatic DA based on comparison of the degrees of confidence of multiple classifiers for each instance when Japanese WSD is performed. Our experiments show that the average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.This paper is organized as follows. Section 2 reviews related work on DA and Section 3 explains how a DA method is automatically determined. Sections 4 and 5 describe the methods and the data we used, respectievly. We present the results in Section 6 and discuss them in Section 7. Finally, we conclude the paper in Section 8. Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.
Calculating Selectional Preferences of Transitive Verbs in Korean Selectional Preference Strength (henceforth, SPS) refers to the degree of correlation between two cooccurring linguistic items. This study, exploiting some Korean language resources and employing the Kullback-Leibler Divergence model formulated by Resnik (1996), aims to calculate SPS between transitive verbs and the classes of co-occurring nouns that function as objects.As far as we know, there has been no previous study to calculate SPS in Korean. Now that several Korean resources constructed on a comprehensive scale are currently available, it would be very interesting to conduct a systematic analysis of SPS in Korean and to see what kind of significant patterns and results can be found through such analysis. This research is an endeavour in that direction, and reports some results of our analysis of SPS between predicates and their object argument, which is based on language resources like treebanks, wordnets, and electronic dictionaries. We also expect that our analysis would make a meaningful contribution to our understanding of the semantic interaction between verbal items and argument structure in Korean.This paper is structured as follows. Section 2 discusses why it is necessary to look into SPS in NLP, and offers a brief explanation of the background knowledge. Section 3 covers the computational model that this study employs, and Section 4 measures SPS using a Korean wordnet (i.e. KorLex) and a development corpus (i.e. the Sejong Korean Treebank). The results are evaluated quantitatively as well as qualitatively in Section 5. This paper closes in Section 6 with a brief look at our further work to help NLP systems perform better. This study calculates the selectional preference strength between transitive verbs and their co-occurring objects, and thereby investigates how much they are co-related to each other in Korean. The selectional preference strength is automatically measured in a bottom-up way, and the outcomes are evaluated in comparison with a manually constructed resource that indicates which verb takes which class(es) of nouns as its dependents. The measurement offered by this study not only can be used to improve NLP applications , but also has a theoretic significance in that it can play a role as distributional evidence in the study of argument structure.
Entity Set Expansion using Interactive Topic Information The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities ( ). For example, the user inputs the words "Apple", "Google" and "IBM", and the system outputs "Microsoft", "Facebook" and "Intel". Many set expansion and relation extraction algorithms are based on bootstrapping algorithms (Thelen and Riloff, 2002;Pantel and Pen- nacchiotti, 2006), which iteratively acquire new entities from corpora. These algorithms suffer from the general problem of "semantic drift". Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction.Recently, topic information is being used to alleviate semantic drift. Topic information means the genre of each document as estimated by statistical topic models. Sadamitsu et al. (2011) proposed a bootstrapping method that uses unsupervised topic information estimated by Latent Dirichlet Allocation (LDA) ( Blei et al., 2003) to alleviate semantic drift. They use a discriminative method (Bellare et al., 2006) in order to incorporates topic information. They showed that the use of topic information improves the accuracy of the extracted entities.Although unsupervised topic information has been confirmed to be effective, the topic models and target entity sometimes demonstrate grain mismatch. To avoid this mismatch, we refine the topic models to match the target entity grain. Deciding the entity grain from only positive seeds is difficult ( . For example, the positive seed words are "Prius" and "Civic". In this situation, whether "Cadillac" is positive or negative depends on the user's definition. If the user thinks that "Japanese car" is positive grain, "Cadillac" should be placed into the negative class but if "car" is the positive grain it should be placed into the positive class. Note that we use the term "class" to refer to a set of entities denoted as C P .We control the topic models using not only positive seed entities but also a very small number of negative entities as distinguished from the output of the preliminary set expansion system. To implement this approach, we need topic models that offer con-trollability through the addition of negative words and high response speed for re-training. We utilize a variation of interactive topic models: interactive Unigram Mixtures ( Sadamitsu et al., 2012). In a later section, we show that proposed method improves the accuracy of a set expansion system.2 Set expansion using Topic information We propose a new method for entity set expansion that achieves highly accurate extraction by suppressing the effect of semantic drift; it requires a small amount of interactive information. We supplement interactive information to retrain the topic models (based on interactive Unigram Mixtures) not only the con-textual information. Although the topic information extracted from an unsupervised corpus is effective for reducing the effect of semantic drift, the topic models and target entities sometimes suffer grain mismatch. Interactive Unigram Mixtures can, with very few interactive words, ease the mismatch between topic and target entities. We incorporate the interactive topic information into a two-stage dis-criminative system for stable set expansion. Experiments confirm that the proposal raises the accuracy of the set expansion system from the baselines examined.
Combining Social Cognitive Theories with Linguistic Features for Multi-genre Sentiment Analysis Sentiment analysis is an important step for both Natural Language Processing (NLP) tasks such as opinion question answering ( Yu and Hatzivassiloglou, 2003) and practical applications such as commercial product reputation mining ( Morinaga et al., 2002), movie review mining ( Pang et al., 2002) and political election prediction ( Tumasjan et al., 2010).With the prevalence of social media, spontaneously user generated content such as tweets or forum posts have become an invaluable source of people's sentiments and opinions. However, as with other NLP tasks, sentiment analysis on such informal genres presents several challenges: (1) informal text expressions; (2) lexical diversity (e.g., for example, in our training data only 10% of words in the discussion forums and tweets appear more than ten times, while in movie reviews over 20% of words appear more than ten times); (3) unpredictable shift in topics/issues. The prevalence of debate in both forum posts and tweets leads to the use of more complicated discourse structures involving multiple targets and sentiments, as well as the second-person voice. These difficulties are magnified in tweets due to necessarily compressed contexts (tweets are limited to 140 characters).In this paper, we tackle these challenges from two perspectives. First, we approach the sentiment analysis task by identifying not only a specific "target" (e.g., presidential candidate) but also its associated "issues" (e.g., foreign policy) before detecting sentiment. This approach is similar to the idea of modeling "aspect" in product reviews (Titov and M- cDonald, 2008;Wang et al., 2011).Second, a detailed error analysis has shown that currently available sentiment lexicons and various shallow linguistic features are not sufficient to advance simple bag-of-words baseline approaches due to the diverse ways in which sentiment can be expressed as well as the prevalence of debate in social media. Fortunately, documents in informal genres are often embedded in very rich social structures. Therefore, augmenting the context available for a target and an issue based on social structures is likely to provide a much richer context. We propose three hypotheses based on social cognitive theories and incorporate these hypotheses into a new framework of propagating consistent sentiments across documents. Without using any additional labeled data 127 this new approach obtained significant improvement (up to 12% higher accuracy). With the rapid development of social media and social networks, spontaneously user generated content like tweets and forum posts have become important materials for tracking people&apos;s opinions and sentiments online. In this paper we investigate the limitations of traditional linguistic-based approaches to sentiment analysis when applied to these informal genres. Inspired by various social cognitive theories, we combine local linguistic features and global social evidence in a propagation scheme to improve sentiment analysis results. Without using any additional labeled data , this new approach obtains significant improvement (up to 12% higher accuracy) for various genres in the domain of presidential election.
Two Types of Nominalization in Japanese as an Outcome of Semantic Tree Growth The particle no in Japanese displays two types of nominalization: "participant" nominalization (1) and "situation" nominalization (2).(1) [Akai no In participant nominalization, the particle no turns a preceding clause into a nominal that denotes an object or a person. In situation nominalization, the particle no turns a preceding clause into a nominal that denotes an event or a proposition. A case of ambiguity is presented in (3). Participant nominalization is exemplified by (3a), and situation nominalization by (3b). 1 One issue that immediately arises is whether no in (1, 2, 3) should be treated uniformly. In other words, does no in (1, 2, 3) form a single item or are there two nos one of which appears in (1, 3a) and the other of which appears in (2, 3b)? Seraku (in press) defends a uniform analysis based on several motivations (e.g. methodological, cross-linguistic, functional, diachronic). Despite these motivations, a unified analysis of no has been largely untouched (e.g. Kitagawa, 2005;Kitagawa and Ross, 1982;Murasugi, 1991;Shibatani, 2009;Tonoike, 1990).Against this background, the aim of the present paper is twofold as follows. First, I shall articulate a unified analysis of no-nominalization within the grammar formalism Dynamic Syntax ( Cann et al., 2005;Kempson et al., 2001). Second, I shall show 1 Seraku (in press) summarizes diachronic data that give credence to the exclusion of such data as (i) from the analysis to be developed in this paper. The particle no in Japanese exhibits two types of nominalization: &quot;participant&quot; and &quot;situation&quot; nominalization. Despite several motivations for a uniform account, only a few attempts have been made to address no-nominalization uniformly. In this paper, I shall develop a unified account within the formalism Dynamic Syntax, and show that a number of properties of the phenomenon follow from the analysis.
Language Independent Sentence-Level Subjectivity Analysis with Feature Selection Subjective text expresses opinions, emotions, sentiment and beliefs, while objective text generally report facts. So the task of distinguishing subjective from objective text is useful for many natural language processing applications like mining opinions from product reviews (M. Hu and B. Liu, 2004), summarizing different opinions(K. Ganesan.et.al, 2010), question answering (A. Balahur.et.al, 2009) etc.But research work performed earlier on subjectivity analysis has been applied only on English and mostly at document-level and word-level. Some methods (Wiebe and Riloff, 2005) which concentrated at sentence-level to learn subjective and objective expressions are boot-strapping algorithms which lacks scalability. But, recently focus shifted to multilingual space (R. Mihalcea.et.al, 2007). Banea (C. Banea.et.al, 2008) worked on sentencelevel subjectivity analysis using machine translation approaches by leveraging resources and tools available for English . Another approach (C. Banea.et.al, 2010) used multilingual space and meta classifiers to build high precision classifiers for subjectivity classification.However, aforementioned work (C. Banea.et.al, 2008) concentrated more on language specific attributes due to variation in expression of subjectivity in different languages. This create a problem of portability of methods to different languages. Other approach (C. Banea.et.al, 2010) which tried achieving language independence created large feature vectors for subjectivity classification. Different languages parallel sentences are taken into consideration to build high-precision classifier for each language. This approach not only increases the complexity and time for classification but also completely dependent on parallel corpus to get good accuracies. A weakly supervised method (C. Lin.et.al, 2011) for sentence-level subjectivity detection using subjLDA tried to reduce training data is available only for English. There are some experiments conducted for Japanese (H. Kanayama.et.al, 2006), Chinese (T. Zagibalov.et.al, 2008), Romanian (C. Banea.et.al, 2008; R. Mihalcea.et.al, 2007) languages data. But these approaches are performed at document level and not language independent.In this paper, we try to address three major problems highlighted from earlier approaches. First, canlanguage portability problem be eliminated by selecting language independent features. Second, can language specific tools like POS taggers, Named Entity recognizers dependency can be minimized as they vary with language. Third, can accuracy of subjective classification is maintained after feature reduction using feature selection methods which are consistent across languages.Remainder of this paper is organized into following sections. Related work is mentioned in the Section 2. Next Section 3 discuss about our approach for feature weighing and selection methods. While the experimental setup Section 4 describes collection and evaluation metrics used to analyze the accuracy of approach. Experimental Section 5 explains experiments performed on different languages, while results and performance between SVM and NBM is analyzed in Section 6 and Section 7 respectively. Conclusion and future work is discussed in Section 8. Identifying and extracting subjective information from News, Blogs and other user generated content has lot of applications. Most of the earlier work concentrated on English data. But, recently subjectivity related research at sentence-level in other languages has increased. In this paper, we achieve sentence-level subjectivity classification using language independent feature weighing and selection methods which are consistent across languages. Experiments performed on 5 different languages including English and South Asian language Hindi show that En-tropy based category coverage difference criterion (ECCD) feature selection method with language independent feature weighing methods outperforms other approaches for subjective classification.
Annotation Scheme for Constructing Sentiment Corpus in Korean There has been much research on the automatic identification and extraction of sentiments and opinions in text. Researchers have been working on these issues by focusing mainly on subjectivity and sentiment classification either at the document or sentence level. Classifying editorials or movie reviews as positive or negative are examples of a document classification tasks while classifying individual sentences as subjective or objective would be an example of a sentence-level task ( Wiebe et al., 2005).Along with these lines of research, a need for corpora annotated with rich information about opinions and emotions has also emerged. This would allow for the development of statistical and machine learning approaches for various practical NLP applications. As such a resource, the Multiperspective Question Answering (MPQA) Opinion Corpus, developed by , Wiebe et al. (2005), and Wilson et al. (2008), plays an important role in sentiment and opinion analysis. It contains the manual annotation of a 10,000 sentence-corpus of articles from the world press. Since this corpus provides a fine-grained annotation scheme, it is widely used as a source for training data in machine learning approaches and serves as the gold standard in sentiment classification tests.We started constructing a cross-language sentiment corpus, called the Korean Sentiment Corpus. We received two years of support in this project by the Korean Research Foundation (KRF) for two years. We aim to provide both a solid theoretical background for the Corpus, reflecting the characteristics of the Korean language, as well as fine-grained annotations for the 8,050 sentencecorpus of news articles. The total number of annotated sentences is less than that of the MPQA, but since our annotation is morpheme-based due to the agglutinative nature of Korean, the number of annotation units is much greater. We have also adopted the basic annotation scheme of the MPQA for comparative research purposes. This paper describes the first year of work constructing the Korean Sentiment Corpus, focusing on the theoretical background such as the annotation scheme. Inter-annotator agreement tests were performed to improve annotation quality. The remainder of this paper is organized as follows. Section 2 gives a brief overview of the MPQA corpus as a starting point. Section 3 elaborates on the annotation scheme for the Korean sentiment corpus, providing examples of annotations with attributes. Section 4 shows observations on the inter-annotator agreements. Section 5 presents future work and conclusions. This paper describes the first year of work constructing the Korean Sentiment Corpus, focusing on the theoretical background such as the annotation scheme. Our aim is to provide a solid theoretical background for the corpus which reflects the characteristics of the Korean language and includes approximately 8,050 sentences taken from news articles. The corpus annotation scheme, based on the MPQA, is described along with the results of inter-annotator agreement tests with a view to improving the annotation scheme.
Lexical Gaps and Lexicalization: Implications for Word Segmentation Systems for Chinese NLP The issue of antonym canonicity has been empirically investigated in English and other languages ( Paradis et al., 2009;Willners and Paradis, 2010). This paper is motivated by the observation that not all adjectives in Chinese have a generally accepted antonym. For example, although the antonym of chengshi 'honest', according to antonym dictionaries (e.g., Han and Song, 2001;Xu, 2000), can be xuwei 'hypocritical', xujia 'unreal', and jiaohua 'cunning', none of them are canonical for most native speakers. Intriguingly, in translating dishonest into Chinese, most Chinese speakers also choose to translate the English word into a word string bu chengshi 'not honest' instead of any antonym candidates of chengshi. The aim of this paper is thus to address the question: Will the lexical gap for a canonical antonym of chengshi enable the string bu chengshi to evolve into a word?To answer the question, we adopt a corpus-based approach and see how bu chengshi behaves in discourse. The results will have implications for word segmentation systems for Chinese NLP in that if bu chengshi functions like a word both linguistically and conceptually, then the string may not need to be further segmented into 'bu + chengshi' in a segmentation system. This line of study can shed light on the segmentation task from a discourse perspective.This paper is organized as follows. Section 2 reviews different views of what a word is. Section 3 introduces the data examined in the present study, and Section 4 presents the results. Section 5 discusses the implications of the results for Chinese wordhood and the segmentation task in Chinese. Section 6 offers the conclusion and some suggestions for future research. This paper is motivated by the observation that not all adjectives in Chinese have a canonical antonym. For example, most Chinese speakers choose to translate the English word dishonest into a word string bu chengshi &apos;not honest&apos; instead of any antonym candidates of chengshi suggested in antonym dictionaries. Our discourse evidence from corpus data suggests that bu chengshi is evolving into a word in discourse at a faster pace than some other &apos;bu + adjective&apos; strings, and this may result from the lexical gap for a canonical antonym of chengshi and the communicative need for such a word. As a consequence, it is proposed that if the lexicalization process of bu chengshi continues in the future, the string may need to be considered a single word in a segmentation system (i.e., buchengshi &apos;dishonest&apos;). For a segmentation system to distinguish between words and phrases, discourse factors should be taken into consideration.
Extracting Keywords from Multi-party Live Chats Keywords or keyphrases 1 are an effective way of representing the core topic of a document, and can effectively summarize and/or help index documents. They are usually found in the form of either simplex nouns (e.g. library) or noun phrases (e.g. social issue). They have been studied in the past to provide topic-related information for many applications such as text summarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summarizers. D ´ Avanzo and Magnini (2005) used them to organize documents for search engines. Dredze et al. (2008) used keywords as summaries of email in order to better manage and prioritize emails. Ham- mouda et al. (2005) used keywords extracted from multiple documents in order to discover the topics of documents for clustering. Gutwin et al. (1999) used automatically extracted keywords to refine the queries to improve precision of search in an online library browser.There has been much research on automatic keyword extraction (Frank et al., 1999;Turney, 1999;Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction ( Kim et al., 2010b). A small minority of researchers have used different sources of data such as email ( Dredze et al., 2008) and HTML documents ( Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure).In this paper, our aim is to automatically extract keywords for multi-party live chats. Live chats are essentially text-based dialogues, with less disfluencies than spoken dialogues but greater scope for overlapping utterances and out-of-sequence subthreading (Ivanovic, 2008). Researchers have variously proposed to use dialogue acts (or DAs) to analyze the structure of discourses. In this paper, we are primarily interested in extracting keywords, but hypothesise that keywords not only serve as summaries of live chats, but they can also track the topics of the conversation. Furthermore, keywords provided at different points of a chat can benefit participants who are absent from the chat for a period of time. This would be especially beneficial to multiparty conversations which pose great challenges due to tangled and asynchronous nature of the interaction. One may easily imagine that keywords could provide contextualizing information for a conversation, helping a new participant to join a conversation mid-stream. Hence, the ability to extract keywords at any given time during the conversation has the potential to enhance the user-friendliness of live chat systems.In this research, we target multi-party written dialogues for keyword extraction due to their popularity on the web, the ability for participants to readily join and leave chats, and the novel semi-asynchronous nature of interactions. However, we believe that the proposed methodology could be adapted to spoken dialogues, noting the challenges of automatic speech recognition, and the import of acoustic and prosodic features in keyword extraction.In analyzing chat data, we observed that keywords vary over time due to topic changes as the conversation progresses. Also, we found that keywords are highly associated with specific dialogue acts. As such, we explored the structural information and dialogue acts predicted by our dialogue act classification system to accommodate the characteristics of live chats. During evaluation, we compared our proposed methods with the well-known KEA keyword extraction system. For our work, we collected data from live chat forums from the US Library of Congress (see Section 3 for details). Unlike casual chats (e.g. NPS live chats), the conversations are based on specific issues, and are thus similar to taskoriented settings such as meetings. Live chats have become a popular form of communication, connecting people all over the globe. We believe that one of the simplest approaches for providing topic information to users joining a chat is keywords. In this paper, we present a method to automatically extract contextually relevant keywords for multi-party live chats. In our work, we identify keywords that are associated with specific dialogue acts as well as the occurrences of keywords across the entire conversation. In this way, we are able to identify distinguishing features of the chat based on structural information derived from live chats and predicted dialogue acts. In evaluation, we find that using structural information and predicted dialogue acts performs well, and that conventional methods do not work well over live chats.
Extracting Networks of People and Places from Literary Texts To fully understand a matter, one must be able to answer, as it were, the "Five W" questions: who, what, where, when, and why. In Humanities research, scholars comb texts to answer similar questions ---who the principal figures were, with whom they interacted, what they did, where and when they lived, and why they made an impact. The vast amount of texts available in digital libraries has, on the one hand, enlarged the breadth on which scholars can perform textual research (Crane, 2006); on the other hand, the sheer volume overwhelms an individual's ability to read the texts in depth to answer these questions.Overviews -information abstracted from a collection of texts -can help a reader rapidly grasp the scope and nature of the collection in question ( Greene et al., 2000), thereby supporting "distant reading" of large text corpora (Moretti, 1999). Ideally, they should also serve as gateways to the primary source by helping the reader locate points of interest for closer reading.Manually written overviews tend to be centered on one of the W's. For example, biographies summarize the "who" in a text; a plot précis explains the "what" of a novel; and a gazetteer gives a list of locations. Most approaches in computational linguistics also focused on each of the W's in isolation. Named entity recognition systems retrieve lists of personal entities, organizations, geographical names, and the like ( Chinchor et al., 1999); temporal resolution systems detect temporal expressions (Mani and Wilson, 2000); discourse parsers can help answer why questions (Marcu, 1998).In more recent work, there has been much effort to synthesize two or more of the W's, for example, detecting co-occurrences of dates and place names (Smith, 2002); linking time to events (Pustejovsky et al., 2005); connecting people to the events in which they interact with others ( Doddington et al., 2004;; as well as "nexus points" of groups of people at particular locations ( Bingenheimer et al., 2009). This paper contributes another step in this direction, reporting the first attempt to automatically construct social networks from literary texts integrating who, what, and where.The rest of the paper is organized as follows. The next section reviews previous work in the automatic generation of social networks. Section 3 defines the research question. Section 4 describes the baseline and our generation algorithm. Sections 5 and 6 outline our data and evaluation results. The paper concludes with future work in the last section. We describe a method to automatically extract social networks from literary texts. Similar to those in prior research, nodes represent characters found in the texts; edges connect them to other characters with whom they interact, and also display sentences describing their interactions. Furthermore, other nodes encode places and are connected to characters who were active there. Thus, these networks present an overview of the &quot;who&quot;, &quot;what&quot;, and &quot;where&quot; in large text corpora, visualizing associations between people and places.
Pre-vs. Post-verbal Asymmetries and the Syntax of Korean RDC Predicates in Korean are generally fixed at the clause final position, although the dependents are freely ordered, as in (1). It is observed in Nam and Ko (1986: 250-251) and Huh (1988: 263) among others, however, that Korean allows the so-called right dislocated construction (RDC), in which some apparent part of the sentence may show up at the post-predicate position, as in (2).(1) a. Cheli-ka Yuni-lul manna-ess-ta (SOV)Ch.-Nom Y.-Acc meet-Pst-DE 'Cheli saw Yuni.' b. Yuni-lul Cheli-ka manna-ess-ta (OSV) (2) a. Cheli-ka manna-ess-ta Yuni-lul (SVO) b. Yuni-lul manna-ess-ta Cheli-ka (OVS) c. manna-ess-ta Cheli-ka Yuni-lul (VSO) d. manna-ess-e Yuni-lul Cheli-ka (VOS)The RDC in Korean has recently received a great deal of attention as to the architecture of the structure. (See J.-S. Lee 2007a,b, 2008a,b, 2010, Chung 2008a, Lee and Yoon 2009, C.-H. Lee 2009, among others.) Among various issues around the RDC are the basic word order in Koran and the grammatical relation the RDed element in the post-verbal position assumes with the rest of the construction. Lee (2007aLee ( ,b, 2008aLee ( , 2009aLee ( ,b, 2010Lee ( , 2012 proposes a mono-clausal structure based on Kayne's (1994) universal SVO hypothesis and treats the RDed element as a direct dependent of the preceding predicate. According to this analysis, (2a) is taken as the base word order and all other structures in (1) and (2) are derived from (2a), In contrast, Chung (2008aChung ( , 2009bChung ( . 2010, basically following Tanaka's (2001) analysis of Japanese RDC, advocates a non-mono-clausal analysis, according to which the RDC is derived as follows: 1 1 See also Kuno (1978), Whitman (2000), and Kato (2007), among others, for non-mono-clausal approaches. Chung (2008aChung ( , 2009bChung ( , 2010 . This paper does not opt for any particular version of nonmono-clausal analysis since the discussions may go through as far as the RDed element is taken as a fragmental expression. Among various important issues pertaining to the so-called right dislocated construction (RDC) in Korean are the basic word order and the grammatical relation the right dislocated (RDed) element assumes to the rest of the structure. In his series of papers, J.-S. Lee proposes a mono-clausal analysis of Korean RDC, according to which the RDed element is a direct dependent of the preceding predicate and Korean conforms to Kayne&apos;s (1994) universal SVO word order hypothesis due to the very existence of the RDC. In contrast,) advocates a non-mono-clausal approach, as in Tanaka (2001) and Kato (2007) for Japanese RDC, according to which the RDed element is taken as a fragment of a continuing sentence to which massive ellipsis has applied, while the head-finality is preserved. The current work tries to show that RDed elements cannot be viewed as direct dependents of the preceding predicate due to various asymmetries observed between pre-vs. post-verbal positions, favoring a non-mono-clausal analysis of Korean RDC.
Pattern Matching Refinements to Dictionary-Based Code-Switching Point Detection Code-switching (CS) is "the use of two or more linguistic varieties in the same interaction or conversation" (Myers-Scotton and Ury, 1977). It is often prevalent in communities where there is language contact. According to linguistic studies (Bautista, 1991;Borlongan, 2009), code-switching reasons are mainly driven by proficiency or deficiency in the languages involved. Proficiency-driven code-switching takes place when a person is competent with the two languages and can easily switch from one to the other "for maximum efficiency or effect". On the other hand, deficiency-driven code-switching takes place when people are forced to code-switch to one language because they are "not competent in the use of the other language". Oral communication in both languages can be enhanced by the detection of code-switching points (CSPs). To detect CSPs, we developed a dictionary-based approach using a rule-based engine (Naber, 2003), and we also developed pattern matching refinements (PMRs) to improve accuracy.As testbed, this study focuses on TagalogEnglish code-switching, which can be classified into (1) intra-sentential and (2) intra-word codeswitching. Intra-sentential CS is the switching between Tagalog and English words and clauses, while intra-word CS is the use of English root words with Tagalog affixes and morphological rules. An example of intra-sentential CS is "Unless let us say may mga bisita siya" (translated as: Unless let us say he/she has visitors) and an example of intra-word CS is "nagdadrive" (incompeleted aspect of the English verb "drive").The system developed can effectively be used to detect intra-sentential (Tagalog to English and English to Tagalog) and intra-word CSPs. This paper is organized as follows: related works in section 2, CSP detection in section 3, pattern matching refinements in section 4, testing and discussion in section 5, and conclusion in section 6. This study presents the development and evaluation of pattern matching refinements (PMRs) to automatic code switching point (CSP) detection. With all PMRs, evaluation showed an accuracy of 94.51%. This is an improvement to reported accuracy rates of dictionary-based approaches, which are in the range of 75.22%-76.26% (Yeong and Tan, 2010). In our experiments, a 100-sentence Tagalog-English corpus was used as test bed. Analyses showed that the dictionary-based approach using part-of-speech checking yielded an accuracy of 79.76% only, and two notable linguistic phenomena, (1) intra-word code-switching and (2) common words, were shown to have caused the low accuracy. The devised PMRs, namely: (1) common word exclusion, (2) common word identification, and (3) common n-gram pruning address this and showed improved accuracy. The work can be extended using audio files and machine learning with larger language resources.
An Adaptive Method for Organization Name Disambiguation with Feature Reinforcing Twitter is an online social networking and microblogging service, which rapidly gained worldwide popularity, with 140 million active users as of 2012 1 , generating over 340 million tweets and handling over 1.6 billion search queries per day 2 . People share their opinions on almost anything on Twitter, such as news, governmental policies, products and companies. Therefore, Twitter becomes an important information resource for the purpose of marketing strategies and online reputation management. How to retrieval, analyze and monitor Twitter information has been receiving a lot of attention in natural language processing and information retrieval research community (Kwak, et al., 2010;Boyd, et al.,2010;Tsagkias, et al., 2011). One of the essential things of these researches is first to get the information which is related to the studied entity, such as product, company, or certain event. This work is caused by the ambiguity of entities. For example, the name of company "Apple" has a separate meaning referring to one kind of fruit. The word "Amazon" could be used to refer river or company. Therefore, when the entity name is ambiguous, filtering spurious name matches is important to accurate detection and analysis of contents that people say about the given entity. This paper focuses on finding related tweets to a given organization. Assuming that tweets are retrieved by the query of organization name, such as "apple", the task is to identify whether a tweet is relevant to the target organization ("Apple Inc.") or not. Yerva et al. (2010) adopt support vector machines (SVM) classifier to classify tweets with external resources. Yoshida et al. (2010) classify organization names into "organization-like names" or "general-word-like names" categories, classify tweets by rules. Kalmar (2010) adopts bootstrapping method to classify the tweets.This task is challenging owing to the fact of lacking sufficient information. A tweet contains less than 140 characters and is often freely written. Therefore the tweet is short and informal. It does not provide sufficient word occurrence or context shared information for effective similarity measure (Phan et al., 2008). Furthermore, the representation of each organization is also an obstacle. Different from conventional word disambiguation, there is no authoritative source which lists all possible interpretations of an organization name. The information gotten from the homepage of organization is limited. It is difficult to cover the word occurring in tweets which are related to the given organization.Aim to process any organization names but not one or some given organization names, the organization names in training data are different from those in test data. This leads that we could not train a classifier to a certain organization. It also makes the task more difficult than conventional classifying task.In this paper, we propose an adaptive method for organization name disambiguation. We build a general classifier with the training data. Then we use the general classifier to label unlabeled twitter messages of a given organization. With more features derived from these twitter messages, we train an adaptive classifier to a given organization. The major contributions of our approach are as follows: Try to mine organization information from web sources, such as Wikipedia, linked pages and related pages. This is a way to solve the problem of insufficient information.  Train an adaptive classifier for a given organization name with more features derived from twitter messages labeled by general classifier. This is a way to let the classifier more suitable for a given organization. The remainder of the paper is organized as follows: Section 2 describes the related work on name disambiguation. Section 3 gives problem description and an overview of our approach. Section 4 presents supervised methods to classify tweets based on information from web sources. Section 5 introduces adaptive method to classify the tweets based on derived features. Section 6 gives the experiments and results. Finally section 7 summarizes this paper. Twitter is an online social networking, which has become an important source of information for marketing strategies and online reputation management. In this paper, we probe the problem of organization name disambiguation on twitter messages. This task is challenging due to the fact of lacking sufficient information both from organization and the tweets. We mine organization information from web sources to train a general classifier. Further, we mine tweets information. We train an adaptive classifier for a given organization name with more features derived from twitter messages labeled by the general classifier. The experiments on WePS-3 show mining web sources to enrich organization are effective. The adaptive classifier trained for a given organization is promising.
Predicting Answer Location Using Shallow Semantic Analogical Reasoning in a Factoid Question Answering System The task of a question answering system (QAS) is to provide a single answer for a given natural language question. In a factoid QAS, the system tries to give the best answer of an open-domain fact-based question. For example, the question "Where was an Oviraptor fossil sitting on a nest discovered?". A QAS should return 'Mongolia's Gobi Desert' as the final answer.A typical pipeline architecture in a fact-based QAS consists of four main processes, i.e.: question analysis, query formulation, information retrieval and answer selection. The main source of complexity in a QAS lies in the question analysis and answer selection process rather than in the information retrieval (IR) phase, which is usually achieved by utilizing third-party modules such as Lucene, Indri, or a web search engine.The question analysis process seeks to determine the type of a given question, which in turn provides the expected answer type (EAT) of that question as a specific fact type, such as person, organization or location. The EAT will be used to select the best answer during the answer selection process, usually by utilizing a named-entity recognizer (NER) tool in a factoid QAS ( Schlaefer et al., 2006). Different approaches have been used in order to improve the performance of the answer selection component. Ko et al. (2010) employed probabilistic models for answer ranking of NER-based answer selection by utilizing external semantic resources such as WordNet. More advanced techniques utilizing linguistic tools have been proposed in Sun et al. (2005), which uses syntactic relation analysis to extract the final answer, and Moreda et al. (2010), which employs semantic roles to improve NER-based answer selection. Recent work by Moschitti and Quarteroni (2011) proposed classification of paired texts that learn to select answers by applying syntactic tree kernels to pairs of questions and answers.In our current work, we try to reduce the dependency of the answer selection process on linguistic tools such as NER systems. Our main concern is that in reality we do not always have a complete N-ER tool for every fact type. In our example mentioned above, the answer has a fact type which is neither an exact location, person nor an organization, i.e.: 'Mongolia's Gobi Desert'. In such case, a NER-based system might fail to extract the answer. Further, if we have a complete NER-tool, it is still a complex problem to predict the location of the exact answer in a retrieval result.We propose an approach which we call semantic analogical reasoning (SAR). Our approach tries to predict the location of the final answer in a textual passage by employing the analogical reasoning 246  Silva et al. (2010). We hypothesize that similar questions give similar answers. Based on the retrieved similar questions, our approach tries to provide the best example of question-answer pairs and use the influence level (weights) of the semantic features to predict the location of the final answer.In the remainder of this paper, our basic idea and related works of semantic analogical reasoning will be presented in Section 2. The system architecture, procedures, experiments, and performance evaluation will be presented in Sections 3 and 4. Finally, our conclusions and future work will be drawn in Section 5. In this paper we report our work on a factoid question answering task that avoids named-entity recognition tool in the answer selection process. We use semantic analogical reasoning to find the location of the final answer from a textual passage.We demonstrate that without employing any linguistic tools during the answer selection process, our approach achieves a better accuracy than a typical factoid question answering architecture.
On the Alleged Condition on the Base Verb of the Indirect Passive in Japanese Japanese passives have attracted much attention and have been a topic of intense debate due to their peculiar characteristics which present challenges to contemporary linguistic theories. One such characteristic is the existence of two types of passive: direct and indirect passives. Direct passives are passives with the active counterparts, where the passive subject corresponds to an object in the active, as in (1), whereas indirect passives have no such counterparts, as shown in (2). 1 (1) Boku-wa sensei-ni home-rare-ta 1SG-TOP teacher-DAT praise-PASS-PST 'I was praised by the teacher.' cf. Sensei-ga boku-o home-ta teacher-NOM 1SG-ACC praise-PST 'The teacher praised me.' (2) Boku-wa kodomo-ni nak-are-ta 1SG-TOP child-DAT cry-PASS-PST 'The child cried on me.' cf. Kodomo-ga (*boku-o/-ni) nai-ta child-NOM 1SG-ACC/-DAT cry-PST 'The child cried (*me).'Of the many issues brought up by these two types of passive, it is sometimes proposed that indirect passives are restricted with respect to the base verb they take. Specifically, researchers such as Dubinsky (1985Dubinsky ( , 1997, Kageyama (1993Kageyama ( , 1996 and Washio (1989-90) argue for what we refer to as the unaccusative restriction, which bans unaccusatives from appearing as the base verb of the indirect passive. For instance, Kageyama (1996) presents the following examples in (3) and (4), whose base verbs are unergatives and unaccusatives, respectively. This paper attempts to shed some light on Japanese indirect passives by placing special focus on the unaccusative restriction. Specifically, I will make the following two claims: first, the restriction is empirically too strong and it is at most a tendency, not a solid descriptive generalization; second, pragmatic inferences derive the purported cases for the unaccusative restriction, which in turn proves to be illusionary and superfluous.The paper is organized as follows: Section 2 discusses the nature of the unaccusative restriction and issues concerning split intransitivity. Section 2.1 examines the counterevidence pointed out in the literature, and then Section 2.2 presents novel empirical evidence from two-place unaccusatives. Section 3 considers the purported cases for the unaccusative restriction and argues that pragmatic inferences play a crucial role in accounting for them, thereby showing that the unaccusative restriction can be dispensed with entirely. Section 4 concludes the paper.Before going into discussion, I would like to mention three things that should be kept in mind. First, I assume without argument that, aside from the customary distinction between direct and indirect passives, the distinction between nipassives and niyotte-passives is real (Kuroda, 1979 inter alia) and that ni-passives involve the introduction of an affected argument by one type of -rare, an unaccusative applicative predicate, as in (5)a (cf. Dubinsky, 1985Dubinsky, , 1997Pylkkänen, 2002), while niyotte-passives involve the suppression of the external argument of the base verb by another type of -rare, the passive voice head (Kratzer, 1996), as in (5) Thus, I assume two homophonous morphemes which function completely differently. Though I consider that the homophony is not accidental and should receive a principled explanation along with other uses of -rare, I keep to the naïve assumption for the purposes of this paper. Second, my aim in this paper is rather modest: it is to show that no stipulations, syntactic or otherwise, need to be introduced in accounting for the strong aversion to unaccusative-based indirect passives because it can be derived by what we already know about pragmatics, and it is not to choose between inferential pragmatic theories like Grice (1975), Horn (1984), Levinson (1987Levinson ( , 2000, and Sperber and Wilson (1995), although I couch my analysis in neo-Gricean terms. To this end, I simply follow the common view on the divide between grammar and pragmatics, with the former defined as a set of codes and the latter as inference.Finally, there is great variability in acceptability judgments, especially when unaccusative-based indirect passives are involved. Thus, when I cite examples from the previous literature, I cite their reported judgments as well, with minor changes made to the examples when necessary. While the divide appears to be wide between 'conservative' and 'liberal' speakers, it is also true that an example once judged as unacceptable can become acceptable if a proper context of utterance is carefully constructed and provided, suggesting that the divide results partly from inadequate control of the context of utterance. With this in mind, I will spell out contextual and conceptual settings as much as possible when I present my analysis. This paper argues against the view that Japanese indirect passives are restricted with respect to the base verb that they take. Specifically, it argues that, despite its initial plausibility, the oft-proposed generalization that unaccusatives cannot appear in indirect passives is too strong and the alleged distribution is a tendency at most, albeit a strong one. After closely examining the restriction along with the counterevidence discussed in the literature, this paper presents novel empirical evidence against the restriction. Moreover, it also argues that no stipulations specific to indirect passives need to be introduced in accounting for the purported evidence for the unaccusative restriction: pragmatic inferences play a crucial role in deriving the observed aversion to unaccusative-based indirect passives.
Product Name Classification for Product Instance Distinction Traditional work on product entity recognition has been conducted on competing products for comparative opinion mining from forum data ( Jindal and Liu, 2006;Ding et al., 2009;Li et al., 2010), but not on the same type of products purchased at different times, thus failing to distinguish products at the instance level. The use of temporal information would help to make such distinction, but previous studies of temporal information have been made only for the detection and determination of temporal relations between time expressions and events, through the relevant shared tasks, or TempEval-1 and TempEval-2 tasks ( Pustejovsky et al., 2003;Verhagen et al., 2009;Verhagen et al., 2010), but not for the distinction of products.There is evidence that temporal relations between product instances of the same type are found quite often in product reviews, to give rise to important differences in the respective opinion of the reviewer. Consider Examples (1) and (2) 1 below, with two product names other Levis 501s and these new ones that refer to the product instances that the customer bought. While the former refers to the past purchase, the latter refers to the recent purchase. c. I'm done with buying jeans online. Resolving such different product instances properly is found crucial to identifying long-term customers, among others, whose opinions count at least as important as those of human annotators for influential reviews . Moreover, it is also crucial to identifying such long-term customer's sentiment change over several purchases of the same product (cf. .First, we note that sortal anaphoric expressions such as these jeans in (1) indicate the presence of a temporal cue but may also refer to the whole product as shown in (2a). We also note that the product name without a demonstrative or definite article as shown in (2b) may refer to the purchased product instance, unlike the one without temporal cue in (2c) that refers to a generic object.We thus argue that, for the proper resolution of such product names, it is important to see if the given product name bears temporal information and to identify the temporal order among the product instances. We propose to formulate the resolution of such product names as a classification problem, by utilizing time expressions, event features and other temporal cues as relevant features for a classifier. We construct the classifier in two stages, first detecting the existence of such temporal cues and then detecting the recency of the purchase time. The proposed features are utilized in conjunction with the event-based temporal features in the TempEval task and the experience mining task.We employ a support vector machine (SVM) classifier with cost-sensitive learning by taking minor classes into consideration. The empirical results show that the term-based features and existing event-based features can be made to work in different combinations to enhance the overall performance for product instance distinction.We also apply our results of product name classification to two applications. One is to classify product reviews with respect to a customers' sentiment change (cf. ). The other is to automatically rate product reviews based on the detected sentiment in the given review. Our results show that the results of the product name classification are important for distinguishing the sentiment towards the recent purchase from the sentiments towards other purchases in the past.The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 examines product names with temporal information. Section 4 compares event and time expression features with term-based temporal features. Section 5 shows the classification results and Section 6 discusses classification errors. Section 7 shows the applications and Section 8 concludes the paper with further work. Product names with a temporal cue in a product review often refer to several product instances purchased at different times. Previous approaches to product entity recognition and temporal information analysis do not take into account such temporal cues and thus fail to distinguish different product instances. We propose to formulate the resolution of such product names as a classification problem by utilizing time expressions, event features and other temporal cues for a classifier in two stages, detecting the existence of such temporal cues and identifying the purchase time. The empirical results show that term-based features and existing event-based features together enhance the performance of product instance distinction.
Automatic Detection of Gender and Number Agreement Errors in Spanish Texts Written by Japanese Learners In this paper we describe the creation of a grammar to automatically detect agreement errors -in gender and number-in Spanish texts written by Japanese learners.Automatic detection of grammatical learner errors can be used for the automatic annotation of learner corpora and for the creation of intelligent computer-assisted language learning systems (Heift and Schulze, 2007). Such tools can benefit both teachers -who will be able to study learner errors and the language acquisition process more systematically-and learners -who can foster their language learning with the help of automatic tools and improved traditional language materials-.There are two reasons why we focus on agreement errors. First, for Japanese students, agreement is a problematic aspect for learning Spanish and indeed agreement errors are significantly more frequent among Japanese learners than among speakers of other languages (Fernández, 1997). Second, agreement errors in texts can be identified and corrected straightforwardly by a native speaker, unlike other type of errors like article and preposition usage, for example, where annotator agreement may be problematic.While there is a substantial research on detecting grammatical errors in Learners' English, Spanish has received little attention, probably because of the lack of freely available large learner corpora (Lozano, 2009). For the construction of the grammar, we have manually annotated with agreement error tags a fragment of 25,000 words from the CORANE learner corpus ( Mancera et al., 2001) and to control false positives of the grammar, we have also used native corpora: 22,000 words for development amd 12,000 words for test.The paper is organized as follows. Section 2 deals with the characteristics of gender and number agreement in Spanish and the coverage of the grammar, section 3 deals with the development phase (the corpus, grammar formalism and design principles), section 4 gives the results and analysis of the evaluation, section 5 studies the data in the learner corpus and section 6 presents the conclusions. This paper describes the creation of a grammar to automatically detect agreement errors (gender and number) in Spanish texts written by Japanese learners. The grammar has been written using the Constraint Grammar formalism (Karlsson et al., 1995), and uses as input the morphosyntactic analysis provided by the Spanish parser HISPAL (Bick, 2006). For developing and testing the grammar, a learner corpus of 25,000 words has been manually annotated with agreement error tags. Both the grammar and the data from the corpus serve us to draw some conclusions about the characteristics of agreement errors in Japanese learners&apos; Spanish.
A Reranking Approach for Dependency Parsing with Variable-sized Subtree Features In dependency parsing, graph-based models are prevalent for their state-of-the-art accuracy and efficiency, which are gained from their ability to combine exact inference and discriminative learning methods. The ability to perform efficient exact inference lies on the so-called factorization technique which breaks down a parse tree into smaller substructures to perform an efficient dynamic programming search. This treatment however restricts the representation of features to in a local context which can be, for example, single edges or adjacent edges. Such restriction prohibits the model from exploring large or complex structures for linguistic evidence, which can be considered as the major drawback of the graphbased approach.Attempts have been made in developing more complex factorization techniques and corresponding decoding methods. Higher-order models that use grand-child, grand-sibling or trisibling factorization were proposed in ( Koo and Collins, 2010) to explore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing.Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000;Charniak and Johnson, 2005;Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007;Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary orders and structures, as long as the extraction process is tractable. It is an open question how to design this subtree extraction process that is able to selects a set of subtrees which provides reliable and concrete linguistic evidence. Another related challenge is to design a proper back-off strategy for any structures extracted, since large subtree instances are always sparse in the training data.In this paper, we explore a feature set that makes fully use of dependency grammar, can capture global information with less restriction in the structure and the size of the subtrees, and can be encoded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing.We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005;McDonald and Pereira, 2006) as our base parsers.In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. Employing higher-order subtree structures in graph-based dependency parsing has shown substantial improvement over the accuracy, however suffers from the inefficiency increasing with the order of subtrees. We present a new reranking approach for dependency parsing that can utilize complex subtree representation by applying efficient subtree selection heuristics. We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank. Our system improves the baseline accuracy from 91.88% to 93.37% for English, and in the case of Chinese from 87.39% to 89.16%.
Applying Statistical Post-Editing to English-to-Korean Rule-based Machine Translation System There have been many improvements in machine translation from rule-based machine translation (RBMT) to the latest statistical machine translation (SMT). Approaches for machine translation can be typically classified into conventional rule-based approach and statistical approach ( Jin et al., 2008). RBMT translates a source sentence to a target sentence through analysis process, transfer process and generation process using analysis rules, dictionaries and transfer rules as its main translation knowledge. On the other hand, SMT system accomplishes translation using translation model and language model obtained from training large parallel corpus composed of source sentences and the corresponding target sentences ( Koehn et al. 2003). Comparing two approaches, they have opposite features. That is, rule-based approach is better than statistical approach in the aspect of translation accuracy. However, fluency is contrary to each other. The language pairs that linguistic differences are huge such as English-Korean show these kinds of features apparently. We aim to improve the translation fluency of RBMT system by introducing SPE. The proposed method is similar to SMT. Difference is the composition of parallel corpus used to build statistical models. To build model for post-editing, parallel corpus should be ready, which is composed of the pairs of the sentence translated by RBMT and the corresponding correct sentence translated by human translators. Using this parallel corpus, we can build statistical model to post-edit RBMT results. Also, we explain some points to consider when applying SPE to English-to-Korean translation by various experiments. Our method consists of the following steps: Constructing parallel corpus composed of translation results by English-to-Korean RBMT system and translation results by human translator of English source sentences. Building translation model and language model for applying SPE (at this phase, SMT toolkits are used). Applying decoder for SPE to the output of RBMT system.The section 2 of this paper presents weakness of conventional rule-based machine translation system. And the overview of our method will be described in the section 3. The section 4 describes experimental parameter, experimental results. In the section 5 and the section 6 we sum up the discussion and show the future research direction.2 Weakness of RBMT system Figure 1 shows the configuration of our rule-based machine translation system, FromTo-EK. The flow of machine translation as follows. First, roots of words in an input sentence is restored and part-ofspeech (POS) tagging is carried out by morphological analysis and tagging module. Second, syntactic structure is found out by syntactic analysis module (parser). FromTo-EK engine employs full parsing strategy to analyze English source sentences. Third, input sentence structure (parse tree) is transferred to adequate target language structure using transfer patterns. At this step, lexical transfer based on context is conducted using dictionaries and word sense disambiguation knowledge ( Yang et al., 2010). Fourth, Korean generator generates final Korean translation sentence. The advantage of RBMT engine is that it can catch the exact dependency relation between the words in input sentence. It is very helpful to achieve high translation accuracy. In particular, in the case of language pairs which are very similar in the sense of linguistics (for example, Korean-Japanese),rule-based approach has showed good translation performance.However, the problem of RBMT system is its poor fluency compared to SMT system. In particular, in translating spoken language sentences, such features are outstanding. Actually, when translating English spoken language sentences, it is found that unnatural, rigid and dried expressions are frequently used. It results from stereotyped language transfer phase, translation knowledge and the limit of translation methodology. We cannot achieve the fluency like human translation by assembling translation knowledge pieces. This is why we propose SPE for RBMT system. 3 Statistical Post-Editing Conventional rule-based machine translation system suffers from its weakness of fluency in the view of target language generation. In particular, when translating English spoken language to Korean, the fluency of translation result is as important as adequacy in the aspect of readability and understanding. This problem is more severe in language pairs such as English-Korean. It&apos;s because English and Korean belong to different language family. So they have distinct characteristics. And this issue is very important factor which effects translation quality. This paper describes a statistical post-editing for improving the fluency of rule-based machine translation system. Through various experiments, we examined the effect of statistical post-editing for FromTo-EK 1 system which is a kind of rule-based machine translation system, for spoken language. The experiments showed promising results for translating diverse English spoken language sentences.
Emotion Estimation from Sentence Using Relation between Japanese Slangs and Emotion Expressions The words that are not registered in the dictionaries are called unknown words. In the field of Natural Language Processing unknown words have been traditionally studied. However, many of these studies focused on proper noun, onomatopoeia or emoticon, while a few research targeted slang such as Wakamono Kotoba. One of the reasons might be that Wakamono Kotoba has been usually treated as "improper expression" or "bad word" (Noguchi , 2004).However, considering that these words are getting more and more frequently used on WWW, it is inevitable to treat Wakamono Kotoba even though they are improper expressions.In Japan, many of the Internet users are people from teens to people in their fourties 1 . One of the characteristics of Wakamono Kotoba is that they are specialized in expressing how people especially in their younger age feel. By dealing with such Wakamono Kotoba, we will be able to use effectively the huge amount of documents on WWW as precious resources for language processing.This paper aims to estimate emotion from utterances including Wakamono Kotoba. Most of the existing emotion estimation studies from text did not treat the problem of slang such as Wakamono Kotoba. One of the reasons was that there were few text corpora including Wakamono Kotoba. Currently Weblog became very popular and many documents on WWW are written in spoken language. These texts are available as huge corpus. As the result, recently there are active research on new words or unknown words (Murawaki , 2010), (Jiean et al., 2011) and there are also research on emotion estimation based on their findings ),( . For example, in , they used the conventional statistic method to estimate emotion of the sentence including Wakamono Kotoba, then compared the estimation accuracy when Wakamono Kotoba was included in the sentence and it was not included in the sentence. In ( , they tried to estimate emotion of Wakamono Kotoba by using features of character.In this paper, we focused on Wakamono Kotoba, which was traditionally not intended for research on Natural Language Processing, and proposed an emotion estimation method which was robust for utterance including Wakamono Kotoba. Because the notation of Wakamono Kotoba is various, many Wakamono Kotoba are generally low-frequency words in the corpus. Therefore, we attempted to improve the estimation accuracy by using the emotion expressions with strong relation with Wakamono Kotoba as feature instead of using Wakamono Kotoba as feature. Most of Japanese slang words such as Waka-mono Kotoba are analyzed as &quot;unknown word&quot; or segmented wrongly by the morphological analysis system. These problems are causing negative effect on sentiment analysis in text. These words generally have many varieties of notations and conjugations, and they lack versatility. As a result, many of them are not registered in the dictionaries, making morphological analysis more difficult. In this paper , we aimed to decrease such negative effects of Wakamono Kotoba for the accuracy of emotion estimation from sentence and proposed a method to increase the accuracy by using a classification method based on machine learning. In this method we used emotional expressions which had high relevance with Wakamono Kotoba as feature. As a result, the proposed method obtained 20% higher accuracy than the method only using morpheme N-gram as feature. Emotion Corpus, Japanese Slang, Out of Vocabulary
Answering Questions Requiring Cross-passage Evidence  This paper presents methods for answering, what we call, Cross-passage Evidence Questions. These questions require multiply scattered passages all bearing different and partial evidence for the answers. This poses special challenges to the textual QA systems that employ information retrieval in the &quot;con-ventional&quot; way because the ensuing Answer Extraction operation assumes that one of the passages retrieved would, by itself, contain sufficient evidence to recognize and extract the answer. One method that may overcome this problem is factoring a Cross-passage Evidence Question into constituent sub-questions and joining the respective answers. The first goal of this paper is to develop and put this method into test to see how indeed effective this method could be. Then, we introduce another method, Direct Answer Retrieval, which rely on extensive pre-processing to collect different evidence for a possible answer off-line. We conclude that the latter method is superior both in the correctness of the answers and the overall efficiency in dealing with Cross-passage Evidence Questions.
Anaphora Annotation in Hindi Dependency TreeBank In this paper we present a scheme for annotating anaphoric relations in the Hindi Dependency TreeBank. Anaphora Resolution is one of the important problems in Natural Language Processing, and is used by various applications such as Text Summarization, Question answering etc. An anaphora annotated corpus along with other features (like POS, morph, Parse structure etc.) is required in both statistical as well as rule based anaphora resolution systems. Various corpus based studies of anaphoric variation also make use of such a corpus. While a significant number of corpora with anaphora annotation for English and other languages like Spanish, Czech etc. are available, for Indian languages, such corpora are scarce. With a view of developing an Anaphora Resolution system in Hindi, our project aims at extending the dependency annotated (Hindi Dependency TreeBank) corpus with anaphoric relations. Hence we propose an anaphora annotation scheme in accordance with the representation format (SSF)( Bharati et al., 2007) of the Treebank, that uses attributevalue pairs to represent linguistic information. In this scheme, we attempt to address some of the issues that are commonly faced while annotating anaphora and require efficient handling. Although the scheme is developed while keeping in view the structure of the Dependency Tree-Bank, it is convertible to other formats of annotation as well. In recent years, due to increasing interest in development of statistical systems for anaphora resolution, there have been significant attempts for creation of anaphora annotated corpora and annotation schemes. The most well known among these are MUC-7 annotation scheme (Hirschman and Chin- chor, 1997) and other MUC based schemes, which are used for co-reference annotation via markup tags. The MATE/GNOME project has another important scheme suitable for different types of dialogue annotations (Poesio and Artstein, 2008). Ku- cova and Hajicova (2005) is also a notable work to-wards annotating co-reference relations in a dependency TreeBank (Czech, PragueDT). Some other proposed schemes are, in Spanish and Catalan (Re- casens et al., 2007;Navarro et al., 2004) and in Basque ( Aduriz et al., 2004) for 3LB corpus. A known attempt for Hindi is, for demonstrative pronouns in EMILLE corpus (Sinha, 2002).The above mentioned schemes are used for anaphora annotation in English and various other languages. The motivation behind proposing a new scheme is that some of the challenges like annotation of distributed referent span, annotation of multiple constituents, and identification of head and modifiers are difficult to handle in above mentioned schemes. Such challenges, though faced in various languages, are more frequent in Hindi. In this paper these issues are discussed in detail and an annotation scheme is proposed in order to handle them consistently. In this paper, we propose a scheme for anaphora annotation in Hindi Dependency Treebank. The goal is to identify and handle the challenges that arise in the annotation of reference relations in Hindi. We identify some of the issues related to anaphora annotation specific to Hindi such as distribution of markable span, sequential annotation, representation format, annotation of multiple refer-ents etc. The scheme hence incorporates some characteristics specific to these issues in order to achieve a consistent annotation. Most significant among these characteristics is the head-modifier separation in referent selection. The modifier-modified dependency relations inside a markable is utilized for this head-modifier distinction. A part of the Hindi Dependency Treebank, of around 2500 sentences has been annotated with anaphoric relations and an inter-annotator study was carried out which shows a significant agreement over selection of the head referent using the proposed scheme as compared to MUC annotation format. The current annotation is done for a limited set of pronominal categories.
Improving Statistical Machine Translation with Processing Shallow Parsing In SMT, the reordering problem (global reordering) is one of the major problems, since different languages have different word order requirements. The SMT task can be viewed as two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words (reordering problem). Currently, phrase-based statistical machine translation ( Koehn et al., 2003;Och and Ney, 2004) is the state-of-the-art of SMT because of its power in modelling short reordering and local context.However, with phrase based SMT, long distance reordering is still problematic. In order to tackle the long distance reordering problem, in recent years, huge research efforts have been conducted using syntactic information. There are some studies on integrating syntactic resources within SMT. Chiang (Chiang, 2005) shows significant improvement by keeping the strengths of phrases, while incorporating syntax into SMT. Some approaches have been applied at the word-level ( Collins et al., 2005). They are particularly useful for language with rich morphology, for reducing data sparseness. Other kinds of syntax reordering methods require parser trees , such as the work in (Quirk et al., 2005;Collins et al., 2005;Huang and Mi, 2010). The parsed tree is more powerful in capturing the sentence structure. However, it is expensive to create tree structure, and building a good quality parser is also a hard task. All the above approaches require much decoding time, which is expensive.The approach we are interested in here is to balance the quality of translation with decoding time. Reordering approaches as a preprocessing step ( Xia and McCord, 2004;Xu et al., 2009;) is very effective (improvement significant over state of-the-art phrasebased and hierarchical machine translation systems and separately quality evaluation of reordering models).Inspiring this preprocessing approach, we have proposed a combine approach which preserves the strength of phrase-based SMT in local reordering and decoding time as well as the strength of integrating syntax in reordering. Consequently, we use an intermediate syntax between POS tag and parse tree: shallow parsing. Firstly, we use shallow parsing for preprocessing with training and testing. Second, we apply a series of transformation rules which are learnt automatically from parallel corpus to the shallow tree. The experiment results from EnglishVietnamese pair showed that our approach achieves 401 significant improvements over MOSES which is the state-of-the art phrase based system.The rest of this paper is structured as follows. Section 2 reviews the related works. Section 3 briefly introduces phrase-based SMT. Section 4 introduces how to apply transformation rules to the shallow tree. Section 5 describes and discusses the experimental results. And, conclusions are given in Section 6. Reordering is of essential importance for phrase based statistical machine translation (SMT). In this paper, we would like to present a new method of reordering in phrase based SMT. We inspired from (Xia and Mc-Cord, 2004) using preprocessing reordering approaches. We used shallow parsing and transformation rules to reorder the source sentence. The experiment results from English-Vietnamese pair showed that our approach achieves significant improvements over MOSES which is the state-of-the art phrase based system.
Psycholinguistics, Lexicography, and Word Sense Disambiguation Word sense ambiguities tend to escape people's awareness in everyday communication, except in deliberately biased artificial examples or when context is severely limited, since otherwise we almost effortlessly resolve them using a variety of linguistic and extra-linguistic knowledge. This wide range of information is often rendered as various knowledge sources in automatic word sense disambiguation (WSD) systems, partially modelled with different feature sets.As exemplified in recent SENSEVAL and SEMEVAL evaluation exercises (e.g. Kilgarriff and Rosenzweig, 1999;Edmonds and Cotton, 2001;Mihalcea et al., 2004), state-of-the-art WSD systems are mostly based on supervised approaches. Machine learning algorithms are trained on sense-tagged examples, using a wide range of features extracted from the text approximating a variety of knowledge sources deemed useful for the purpose. Ensembles of different types of classifiers based on different feature sets with some voting scheme often report better performance than individual classifiers alone, though the advantage may just be marginal. While complex interactions between learning algorithms and knowledge sources have been observed (e.g. Mihalcea, 2002;Yarowsky and Florian, 2002), and although factors like sense granularity, availability of training data, part-of-speech (POS), etc. are found to relate to such interactions in one way or another, the nature underlying such interactions, which points to the lexical sensitivity issue of WSD, is still somehow under-explored.In particular, more qualitative analysis is needed for disambiguation results, possibly from an interdisciplinary perspective, for a better understanding of the issue.In the current study, we make a preliminary effort in this regard, and attempt to analyse disambiguation results with respect to the relation between sense concreteness and the means for sense distinction in the first place. To this end, we refer to the context availability model proposed in psycholinguistics and common practice in modern corpus-based lexicography.In Section 2, we will first briefly review related work with particular focus on the complex interaction between learning algorithms and knowledge sources in WSD revealed in recent evaluation exercises and various comparative studies, and present the Context Availability Model and discuss how it accounts for the concreteness effect in psycholinguistics. Section 3 reports on our qualitative analysis of the results from a simple WSD experiment on the noun samples in the SENSEVAL-3 English lexical sample task, for which we also made use of the Sketch Engine, a corpus query system popularly used in lexicography, as a tool for comparing the linguistic context availability among word senses. The paper will be concluded with future directions in Section 4. Mainstream word sense disambiguation systems have relied mostly on supervised approaches. Complex interactions have been observed between learning algorithms and knowledge sources, but the factors underlying such phenomena are under-explored. This calls for more qualitative analysis of disambiguation results, possibly from an inter-disciplinary perspective. The current study thus preliminarily explores the relation between sense concreteness and the linguistic means for sense distinction with reference to the context availability model proposed in psycholinguistics and common practice in corpus-based lexicography. It will be shown that to a certain extent the varied usefulness of individual knowledge sources for target words, nouns in particular, may be related to the concreteness of the meanings concerned, which predicts how the sense is distinguished from other senses of the word in the first place. A better understanding of this relation is expected to inform the design of disambiguation systems which could then combine algorithms and knowledge sources in a genuine lexically sensitive way.
  Thought Thought Thought Thought De De De De se se se se, , , , first first first first person person person person indexicals indexicals indexicals indexicals and and and and Chinese Chinese Chinese Chinese reflexive reflexive reflexive reflexive ziji ziji ziji ziji Abstract Abstract Abstract Abstract In this paper, we make a distinction between the de se and non-de se interpretations of first person indexicals and Chinese reflexive ziji. Based on the distinction, we discuss the relationship between these expressions in Chinese, and point out the problems with Wechsler&apos;s (2010) de se theory of person indexicals as well as the inappropriateness of characterizing Chinese long-distance ziji as a logophor.
The Headedness of Mandarin Chinese Serial Verb Constructions: A Corpus-Based Study This section first introduces the kind of serial verb constructions (SVCs) under discussion; then, it shows the difficulties in identifying the head verb of an SVC, both in terms of theoretical linguistics and automatic parsing. Existing treebanks of Mandarin Chinese such as the Sinica Treebank, the Harbin Institute of Technology Treebank, and the Penn Chinese Treebank, parse Chinese serial verb constructions incorrectly or inconsistently in terms of headedness, i.e. which verb to be assigned with the label of syntactic and/or semantic &quot;head&quot;. Aspectual markers in serial verb constructions can help determine the head of these constructions (Li, 1991; among others). However, the majority of Chinese serial verb constructions do not have overt aspectual markers. Based on large-scale corpus studies, this work investigates the distribution of aspectual markers in Chinese serial verb constructions in order to explore which verb in the serial verbs is more likely to function as the head, and thus provides a reference for parsing serial verb constructions without overt aspectual markers. We find that contrary to previous studies such as Collins (1997), Law and Veenstra (1992) and Sebba (1987) that treat the first verb in a serial verb construction as the head, Chinese serial verb constructions more often have the second verb as the head. The results of this work can not only serve as a reference for automatic parsing of Chinese data, but also shed light on theoretical studies of the structure of serial verb constructions in Chinese and other serial verb languages.
  Japanese dare-mo has been widely acknowledged to be an NPI, furthermore, a &quot;strict&quot; NPI in the sense of Giannakidou (2011) as it seems to be licensed only in an &quot;antiveridical&quot; environment, specifically, with a clausemate negation. However, there is a type of positive sentences in which dare-mo can appear, i.e. non-episodic sentences, which indicates that dare-mo is in fact not an NPI and its NPI-like distribution is an epiphenomenon due to dare-mo&apos;s lexical meaning and the resulting interpretational properties of dare-mo sentences. In the current work, based on novel data we will propose that dare-mo is an &quot;unrestricted&quot; universal quantifier and demonstrate that the proposed meaning of dare-mo and a reasonable assumption about episodic predicates predict that positive episodic dare-mo sentences will be contradictory while negative episodic ones and non-episodic ones, positive or negative will be contingent, nicely characterizing the grammaticality facts of dare-mo sentences.
Automatic Tripartite Classification of Intransitive Verbs An automatic classification of verbs that are distinct in terms of their syntactic behavior is a challenging NLP task. Some works have been done for automatic determination of argument structure of verbs (Merlo and Stevenson, 2001) as well as automatic classification of verbs ( Lapata and Brew, 1999;Schulte, 2000;Schulte, 2006) following (Levin, 1993) proposal. However, automatic sub-classification of intransitive verbs has not been attempted majorly till now. Sub-classification of intransitive verbs has bearing on various NLP tasks such as machine translation, natural language generation, parsing etc. For example, we take here a case from English-Hindi MT system. English uses nominative subject for all kinds of intransitive verbs whereas Hindi uses ergative case marker 'ne' on subject when the verb is unergative and in perfect tense whereas unaccusative doesn't as exemplified in (1a) and (1b) respectively.(1) a. English: Ram ran a lot. Hindi: glaas TuT-aa.Glass break-3 pftClassifying intransitive verbs of (1a) and (1b) into subclasses can result in producing right case marking on the subject in the target language Hindi. In parsing, identifying the subclass of the intransitive verb helps in predicting the position of the subject in the Phrase structure tree. One effort of subclassification of intransitive verbs is described in Sorace (2000) where intransitive verbs are further automatically classified into unergative and unaccusative following Perlmutter's (1978) proposal of Unaccusativity Hypothesis. This paper follows the proposal of Surtani et al. (2011) where it has been argued that a a tripartite classification better classify Hindi intransitive verbs. This paper develops a multi-class SVM classifier based model for the automatic classification of intransitive verbs in the tripartite classification scheme. We propose in this paper two approaches for developing multi-class classifier: (a) a Language dependent Classifier and (b) a Language Independent Classifier. 446The paper is organized into the following subsections. In Section 2, we present the related works. Section 3 discusses the issues involved in a bipartite classification of intransitive verbs. Section 4 talks about the Data preparation. In Section 5, we introduce the tripartite classification scheme and gives a mathematical formulation of how it captures the distribution better than the bipartite distribution. Section 6 discusses the ranking and scoring of the syntactic diagnostics proposed by Bhatt (2003). Section 7 presents the SVM-based classification model. Section 8 presents the results of the two classification models which are compared in Section 9. Section 10 concludes the paper and discusses the future directions. In this paper, we introduce a tripartite scheme for the classification of intransitive verbs for Hindi and claim it to be a more suitable model of classification than the classical binary unac-cusative/unergative classification. We develop a multi-class SVM classifier based model for automatic classification of intransitive verbs into proposed tripartite classes. We rank the unaccusative diagnostic tests for Hindi based on their authenticity in attesting an intransitive verb under unaccussative class. We show that the use of the ranking score in the feature of the classifier improves the efficiency of the classification model even with a small amount of data. The empirical result illustrates the fact that judicious use of linguistic knowledge builds a better classification model than the one that is purely statistical.
Classifying Dialogue Acts in Multi-party Live Chats Dialogue Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations -spoken/written dialogue contributions ( Stolcke et al., 2000;Wu et al., 2002;Kim et al., 2010a), sentence-level ( Lampert et al., 2008), paragraph-level ( Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition ( Stolcke et al., 2000), or human social intention detection ( Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums ( Kim et al., 2010b;Wang et al., 2011).Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difficult to reuse existing methods for live chats. For example, spoken dialogue introduces difficulties due to errors inherent in speech recognition output, but allows acoustic and prosodic features to be leveraged (e.g. Stolcke et al. (2000)). Conversely, live chats introduce other types of complications, including ill-formed data and entanglement (especially for multi-party conversations) due to the semi-asynchronous nature of the interaction (e.g. (Werry, 1996)). As a result, studying live chats is a necessary step toward building accurate live chat systems.To date, relatively little work has targeted dialogue act classification over live chat data. Wu et al. (2002) and Forsyth (2007) investigated multi-party casual chats, while Ivanovic (2008) and Kim et al. (2010a) focused on 1-on-1 chats in customer service centre settings. However, these previous approaches are not directly applicable to other types of live chats, such as forum-style chats that allow multiple participants. Additionally, many live chat applications, such as online forums and online meetings, presume an environment that allows multiple 463 participants to discuss specific topics. While Forsyth (2007) investigates chat involving multiple participants, the conversations are casual and not topicfocused. The semantics and structure of dialogues depend on the nature and structure of the conversations, thus requiring different dialogue act categories and classification approaches.In this paper, we target the classification of dialogue acts in multi-user forums carried out through live chats. 1-on-1 live chats are popular for consumer service support or individual meetings. However, this does not allow multiple users to participate in the chats. On the other hand, as more meetings are taking place via live chat, we believe that studying live chats in multi-user environments is a necessary step towards building such systems. In addition, we have developed a live chat dataset from library forum chats, involving multiple simultaneous users. The dataset contains live chats extracted from online forums conducted at the US Library of Congress.To develop automatic methods for dialogue act classification in live chats, we explored four types of features: context, structure, keyword, and dialogue interaction. In addition, we compare the systems in terms of the number of participants as well as the types of chats (i.e., casual vs. forum chats). In evaluation, we investigate the utility of each feature category over different types of live chat over two multi-user datasets: (i) online forums from the US Library of Congress, and (ii) Forsyth's NPS (Naval Postgraduate School) casual chats, and. We consider the task of classifying chat contributions by dialogue act in a multi-party setting. This extends the problem significantly over the 1-1 chat scenario due to the semi-asynchronous and &quot;entangled&quot; nature of the contributions by chat participants. We experiment with a number of machine learning approaches, using different categories of features: lexical, contextual, structural, keyword and dialogue interaction information. For evaluation, we developed gold-standard data using online forums from the USA Library of Congress. We found that, for multi-party dialogues , features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats.
Chinese Sentiments on the Clouds: A Preliminary Experiment on Corpus Processing and Exploration on Cloud Service With the emergence of huge amount of web data available in recent years, corpus linguistics as well as other related empirical fields such as the collecting and processing of language resources, and their evaluation are facing with the greatest challenges ever. The spread of corpus and lexical resources in linguistics has been led to a great level of theoretical survey and enhanced the empirical foundation, not only with respect to sampling and annotation, but also with exploratory data analysis. However, more recently there have been long discussions about what the current state of art in corpus linguistics fails to do, which can be pinpointed at least in two respects: (1) the lack of socio-cultural (meta-) information reflected in the data, is incompetent for pragmatic usages and discourse analysis; (2) rather skewed with data in the public domain, heterogeneity of (individualized) language usages and development is not able to be traced.With the advanced technological progress in data availability with storage and computing ability, the issues mentioned can be tackled to a great extent. We take it as the turning point for corpus-based linguistics to transform into a data-intensive and cloudbased linguistics. In light of that, we want to explore the transformation viability in this paper. As a first step, we present a novel pipeline architecture to build Chinese Polarity Lexicon on the cloud environment by taking the data from the web as resource. Polarity lexicon contains sentiment-bearing words and phrases, encoded with polarities to each word or phrase, usually either assigned as positive or negative. The study of polarity lexicon has attracted much attention in recent years for classifiers to train on the lexical dataset, and is becoming important for applications such as Sentiment Analysis and Opinion Mining.For the purpose of constructing automatic identifying and classifying polarity lexicon systems, a lot of (semi-) unsupervised machine learning methods for recognizing polarities of words and phrases have been proposed. In terms of language resources, these approaches either consider the information provided from the synonyms or glosses of a thesaurus or WordNet ( Hu and Liu, 2004;Kamps et al., 2004;Kim and Hovy, 2004;Esuli and Sebastiani, 2005), or based on the co-occurrence relationship 491 messages derived from the corpus ( Hatzivassiloglou and McKeown, 1997;Turney, 2002;Kanayama and Nasukawa, 2006) to assign and determine the word polarity.Notwithstanding their significant success in achieving accuracy rate, in this paper, we will argue that current approaches to the problem might face with the methodological drawbacks due to the lack of scalability on the one hand, and indifference to the individual sentimental varieties on the other hand. First, referring to the lack of scalability, it is rather difficult to handle out-of-vocabulary (OOV) issue on the lexical and corpus resources, in particular, those OOV words and phrases (or called as neologisms) often carried with popular usage meanings generated from the social network, and given with explicit polarities; and secondly, regarding the individual sentimental varieties, which may correspond to the linguistic varieties, subjectivities and sentiments, are largely ad-hoc, that is with whom s/he chats and temporal, geographical, and communication situations, etc. will have influence on her/his sentiment. Those heterogeneous properties are not properly embodied in lexical and corpus resources. This study aims to propose a novel pipeline architecture in building and analyzing large-scaled linguistic data on the cloud-based environment , an experimental survey on Chinese Polarity Lexicon will be taken as an example. In this experiment, data are evaluated and tagged by applying crowd sourcing approach using online Google Form. All the data processing and analyzing procedures are completed on-the-fly with free cloud services automatically and dynamically.The paper shows the advantages of using cloud-based environment in collecting and processing linguistic data which can be easily scaled up and efficiently computed. In addition, the proposed pipeline architecture also brings out the potentials of merging with mashups from the web for representing and exploring corpus data of various types.
Cross-Lingual Topic Alignment in Time Series Japanese / Chinese News Among various types of recent information explosion, that in news stream is also a kind of serious problems. This paper studies issues regarding topic modeling of information flow in multilingual news streams. If someone wants to find differences in the topics of Japanese news and Chinese news, it is usually necessary for him/her to carefully watch every article in Japanese and Chinese news streams at every moment.In such a situation, topic models such as LDA (Latent Dirichlet Allocation) ( Blei et al., 2003) and DTM (dynamic topic model) ( Blei and Lafferty, 2006) are quite effective in estimating distribution of topics over a document collection such as articles in a news stream. Especially, as a topic model, this paper employs DTM, but not LDA, since it can consider correspondence between topics of consecutive dates. In DTM, we suppose that the data is divided by time slice, for example by date. DTM models the documents (such as articles of news stream) of each slice with a K-component topic model, where the k-th topic at slice t smoothly evolves from the k-th topic at slice t − 1.Based on the results of estimating distribution of topics in Japanese / Chinese news streams, this paper proposes how to analyze cross-lingual alignment of topics in time series Japanese / Chinese news streams. The overall flow of the proposed framework is illustrated in Figure 1. In order to bridge the gaps between the two languages, namely, Japanese and Chinese, we use Japanese and Chinese term translation pairs extracted from Wikipedia utilizing interlanguage links. With those translation knowledge, we first cross-lingually align Japanese and Chinese news articles. Then, after collecting those cross-lingually aligned news article pairs, we then apply DTM to those collected news articles and estimate time series monolingual topic models for both Japanese and Chinese. Finally, those monolingual  Figure 2 shows an example of estimating time series topics monolingually for both Japanese and Chinese. The proposed method of cross-lingual topic alignment is successfully applied to those Japanese and Chinese time series news articles, where several topics such as "Toyota vehicle recalls" and "Chile earthquake" are cross-lingually aligned between Japanese and Chinese. Once we have such a cross-lingual topic alignment, it becomes quite easier for us to find certain differences in concerns. For example, in the case of the topic "Chile earthquake", in Japan, "warn of tsunami" is apparently one of the major concerns, while in Chinese, "emergency assistance was dispatched to Chile" is one of the major concerns. Among various types of recent information explosion, that in news stream is also a kind of serious problems. This paper studies issues regarding topic modeling of information flow in multilingual news streams. If someone wants to find differences in the topics of Japanese news and Chinese news, it is usually necessary for him/her to carefully watch every article in Japanese and Chinese news streams at every moment. In such a situation, topic models such as LDA (Latent Dirichlet Allocation) and DTM (dynamic topic model) are quite effective in estimating distribution of topics over a document collection such as articles in a news stream. Especially, as a topic model, this paper employs DTM, but not LDA, since it can consider correspondence between topics of consecutive dates. Based on the results of estimating distribution of topics in Japanese / Chinese news streams, this paper proposes how to analyze cross-lingual alignment of topics in time series Japanese / Chi-nese news streams.
A CRF Sequence Labeling Approach to Chinese Punctuation Prediction Punctuation prediction, also referred to as punctuation restoration, aims at inserting proper punctuation marks at right position of an unpunctuated text ( Gravano et al., 2009;Guo et al., 2010). Punctuation is obviously an essential indicator for sentence construction. For Chinese, adding proper punctuation marks can not only enhance the readability of text, but also can provide additional information for further language analysis, such as word segmentation, phrasing and syntactic analysis ( Guo et al., 2010;Chen and Huang, 2011;Xue and Yang, 2011). As such, punctuation prediction plays a critical role in many natural language processing applications such as automatic speech recognition (ASR), machine translation, automatic summarization, and information extraction ( Matusov et al., 2006; Lu and Ng, 2010).Over the past years, numerous studies have been performed on the insertion of punctuations in speech transcripts using supervised techniques. However, it is actually very difficult or even impossible to develop a large high-quality corpus to achieve reliable models for predicting punctuations in speech transcripts or ASR outputs ( Takeuchi et al., 2007). Furthermore, most previous research on punctuation prediction exploited very shallow linguistic features such as lexical features or prosodic cues (viz. pitch and pause duration) ( Lu and Ng, 2010), few studies have been done on the exploration of deeper linguistic features like syntactic structural information for punctuation prediction, particularly in Chinese ( Guo et al., 2010).In this paper we draw our motivation from speech transcripts to written texts. On the one hand, a number of large annotated corpora of written texts are available to date. On the other hand, we intend to examine the role of different linguistic features on Chinese punctuation prediction. To this end, we reformulate Chinese punctuation prediction as a multiple-pass labeling task on word sequences, and then explore multiple features at three linguistic levels, namely words, phrases and functional chunks, for punctuation labeling under the framework of conditional random fields (CRFs). Furthermore, we have also performed evaluation on the Tsinghua Chinese Treebank (Zhou, 2004).The rest of the paper is organized as follows: In Section 2, we will provide a brief review of the related work on punctuation prediction. In Section 3, we will describe in detail a labeling method to Chinese punctuation prediction. Section 4 will summarize the experimental results. Finally in Section 6, we will give our conclusion and some possible directions for future work. This paper presents a conditional random fields based labeling approach to Chinese punctuation prediction. To this end, we first reformulate Chinese punctuation prediction as a multiple-pass labeling task on a sequence of words, and then explore various features from three linguistic levels, namely words, phrase and functional chunks for punctuation prediction under the framework of conditional random fields. Our experimental results on the Tsinghua Chinese Treebank show that using multiple deeper linguistic features and multiple-pass labeling consistently improves performance.
Text Readability Classification of Textbooks of a Low-Resource Language The readability of a text relates to how easily human readers can process and understand a text as the writer of the text intended. There are many text related factors that influence the readability of a text. These factors include very simple features such as type face, font size, text vocabulary as well as complex features like grammatical conciseness, clarity, underlying semantics and lack of ambiguity.Nowadays, teachers, journalists, editors and other professionals who create text for a specific audience routinely check the readability of their text. Readability classification, then, is the task of mapping text onto a scale of readability levels. We explore the task of automatically classifying documents based on their different readability levels. As input, this function operates on various statistics relating to lexical and other text features.Automatic readability classification can be useful for many Natural Language Processing (NLP) applications. Automatic essay grading can benefit from readability classification as a guide to how good an essay actually is. Similarly, search engines can use a readability classifier to rank its generated search results. Automatically generated documents, for example documents generated by text summarization systems or machine translation systems, tend to be error-prone and less readable. In this case, a readability classification system can be used to filter out documents that are less readable. The system can also be used to evaluate machine translation output. A document of higher readability tends to be better than a document that belongs to a lower readability class.Research in the field of readability classification started in 1920. English is the dominating language in this field although much research has been done for other languages like German, French, Chinese and so on. These languages are considered as highdensity languages as many language resources and tools are available for them. However, many languages are considered to be low-density languages, either because the population speaking the language is not very large or because insufficient digitized text material is available for the language even though it is spoken by millions of people. Bangla is such a language. Bangla, an Indo-Aryan language, is spoken in Southeast Asia, specifically in present day Bangladesh and the Indian state of West Bengal.With nearly 230 million speakers, Bangla is one of the largest spoken languages in the world, but only a very small number of linguistic tools and resources are available for it. For instance, there is no morphological analyzer, POS tagger or syntax parser available for Bangla.To create a supervised readability classification, it is important to use a corpus that is already classified for the different levels of readers. In this work, the corpus is collected from textbooks that are used in primary and middle school in Bangladesh. The collected documents are classified according to their readability. So the extracted corpus is ideal for a readability classification task.In this paper, we present a readability classification based on information-theoretic and lexical features. We evaluate this classifier in comparison with traditional readability formulas that, even though they were proposed in the early stages of readability classification research, are still widely used.The paper is organized as follows: Section 2 discusses related work followed by an introduction of the corpus in Section 3. The features used for classification are described in Section 4, and our experiments in Section 5 are followed by a discussion in Section 6. Finally, we present our conclusions in Section 7. There are many languages considered to be low-density languages, either because the population speaking the language is not very large, or because insufficient digitized text material is available in the language even though millions of people speak the language. Bangla is one of the latter ones. Readabil-ity classification is an important Natural Language Processing (NLP) application that can be used to judge the quality of documents and assist writers to locate possible problems. This paper presents a readability clas-sifier of Bangla textbook documents based on information-theoretic and lexical features. The features proposed in this paper result in an F-score that is 50% higher than that for traditional readability formulas.
A Hybrid Approach for the Interpretation of Nominal Compounds using Ontology Understanding and interpretation of nominal compounds has been a long-standing area of interest in NLP research. The main reasons that make understanding compound nouns an interesting and challenging task are: (1) Compound nouns are a frequent phenomenon in many languages, occurring in different languages with varying frequencies. English and Sanskrit are two languages that display great flexibility in compounding ( ´ O Séaghdha, 2008). About 3.9 % of the words in Reuters are bigram nominal compounds ( Baldwin and Tanaka, 2004). (2) Compounding is a recursive process that can lead to formation of large and complex compounds, that are difficult for comprehension. (3) Compounds usually carry an implicit meaning that may sometimes differ significantly from that of the combining concepts. Consider the example of a garden knife. A garden knife is interpreted as a knife used in the garden. Here the modifier garden modifies the locative information of the head noun knife. Alternatively, consider the example of a gamma knife. A gamma knife is a device used to treat brain tumors by administering gamma radiations in a particular manner. Here, the modifier does not necessarily modify the head, instead they both combine together to denote a different concept. This understanding of the difference in the structure and purpose between gamma knife and other kinds of knife cannot be achieved by any means of statistical predictions or morphological and syntactic analyses of the compound.The most common representations adopted for the interpretation of nominal compounds involve an inventory of verbs, prepositions or abstract semantic relations. Verb and preposition paraphrases bring in lexical ambiguity in the interpretation of the compound, essentially owing to the polysemous nature of verbs and prepositions. For example, morning tea and bar lights would both be paraphrased using the preposition in. However, the paraphrase 'tea in the morning' conveys the temporal aspect of the compound, while the paraphrase 'lights in the bar' describes the location information in the compound. Due to this polysemous behaviour of prepositions and verbs, a restricted inventory of abstract semantic 554 relations is more favorable for the compound interpretation problem.Most of the approaches proposed for interpreting nominal compounds fall into one of the two classes (a) supervised machine learning approaches, and (b) unsupervised data driven approaches. These approaches fail to handle the sparseness of data, which is a major issue in case of noun compounds. They collect statistics that use occurrence frequencies of the compounds. Therefore, rarely occurring compounds lead to wrong estimations of probabilities and thereby unreliable interpretations. A third and less frequently adopted alternative involves the use of large-scale, domain-independent, lexical and conceptual hierarchies that provide detailed natural language semantics. Such ontologies promise reliability and accuracy of data but fail to cover equally, lexical items and semantic relations. Moreover, construction of such ontologies is extremely timeconsuming, due to which manually built ontologies are never up to date with changes in the language. This motivates us to argue that the most optimal approach to compound interpretation would be the combination of a lexical hierarchy for the frequent and idiosyncratic compounds (Johnston and Busa, 1996) and WordNet-based similarity for those that are not listed in the hierarchy. We show in this paper, that adopting our hybrid approach helps us achieve significant results (70% accuracy) in ontology-based compound interpretation, irrespective of the size, coverage and domain of the ontology. We perform all our experiments using PurposeNet ( KiranMayee et al., 2008), which is a purpose-centric ontology of artifacts and semantic relations.The rest of the paper is divided into the following sections. In section 2, we discuss some related works that use ontologies and also motivate our choice of a hybrid approach using ontology. In section 3, we discuss the architectural design of PurposeNet in brief. We then proceed to explain our hybrid approach in section 4, and discuss the preparation and analysis of data in section 5. We finally produce in section 6, the results for the compound interpretation experiments performed, and then discuss the scope of improvement and future work in section 7. Understanding and interpretation of nominal compounds has been a long-standing area of interest in NLP research for various reasons. (1) Nominal compounds occur frequently in most languages. (2) Compounding is an extremely productive word formation phenomenon. (3) Compounds contain implicit semantic relations between their constituent nouns. Most approaches that have been proposed so far concentrate on building statistical models using machine learning techniques and rely on large-scale, domain-specific or open-domain knowledge bases. In this paper we present a novel approach that combines the use of lexical hierarchies such as PurposeNet and WordNet, with WordNet-based similarity measures for the interpretation of domain-specific nominal compounds. We aim at building a robust system that can handle most of the commonly occurring English bigram nominal compounds within the domain.
Improved Constituent Context Model with Features Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition ( Jones et al., 2010), improve machine translation ( DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars ( Klein and Manning, 2002;Bod, 2006;Seginer, 2007;Cohn et al., 2009;Ponvert et al., 2011), or the dependency grammars ( Klein and Manning, 2004;Headden III et al., 2009;Cohen and Smith, 2009;Spitkovsky et al., 2010;).Among these approaches, the Constituent Context Model (CCM) ( Klein and Manning, 2002;Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and following improved unsupervised models ( Smith and Eisner, 2004;Mirroshandel and Ghassem-Sani, 2008;Golland et al., 2012) is the problematic evaluation framework. The previous approaches train and evaluate models on the same dataset, so there is no reasonable way to choose model parameters unless setting them empirically.In this paper, we focus on CCM and present a general feature-based framework in which various overlapping features could be easily added. Previous dependency induction approach (Cohen and 564 Smith, 2009) demonstrates enabling factored covariance between the probabilities of different derivation events could improve the induction results. The proposed feature-based model provides a simpler and more flexible way to share information between constituents, e.g. different sequences may share the same boundary words. Various features could capture rich information about span contents, which alleviates the data sparsity problem and estimation problem of CCM mentioned above. In addition, features are combined in the log-linear form with local normalization, so the EM algorithm can be adopted to estimate model parameters with minor change, without increasing the computing complexity. To avoid overfitting, we use ℓ 1 -norm regularization to control the model complexity. Finally, we advocate to estimate model probabilities on training set, use a separated development set (a.k.a. the validation set) to perform model selection, and measure the generative ability of trained model on an additional test set. Under this framework, we could automatically select suitable model and parameters rather than choosing them manually. We carry out experiments on the English treebank. Compared to original CCM, the proposed feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. After examining the effect of grammar sparsity, we conclude that with good regularization parameter (tunned on the development set), the learned grammar could be both compact and accurate.The main contributions of this paper can be summarized as follows:(1) We present a general feature-based CCM, where knowledge can be easily incorporated.(2) We use ℓ 1 -norm to control the model complexity, leading to compact grammars.(3) We propose to use separated development set to tune parameters instead of heuristically choosing parameters.This paper is structured as follows. Section 2 gives an overview of the original CCM. Section 3 proposes the feature-based CCM and corresponding parameter estimation method. Section 4 lists the feature templates used in experiments. Section 5 shows the experimental results. We compare our work to related approaches in Section 6 and conclude in Section 7. The Constituent-Context Model (CCM) achieves promising results for unsupervised grammar induction. However, its performance drops for longer sentences. In this paper, we describe a general feature-based model for CCM, in which linguistic knowledge can be easily integrated as features. Features take the log-linear form with local normalization, so the Expectation-Maximization (EM) algorithm is still applicable to estimate model parameters. The ℓ 1-norm is used to control the model complexity, leading to sparse and compact grammar. We also propose to use a separated development to perform model selection and an additional test set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences.
Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU ( Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER ( Tillmann et al., 1997), CDER ( Leusch et al., 2006), WER ( Nießen et al., 2000), and TER ( Snover et al., 2006).MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores.We explore the potential of alternate similarity metrics on word vectors such as the Jensen Shannon divergence, the Dice's coefficient and Jaccard coefficient apart from cosine similarity and the Min/Max metric with mutual information employed by Lo et al. (2012) in their work. We show that Jaccard coefficient not only outperforms the Min/Max metric with mutual information, in achieving higher Kendall correlation against human adequacy judgments, but all the other similarity measures in comparison.In order to test the robustness of the method across various data sets, we conduct experiments across GALE-A, GALE-B and GALE-C data sets examining the Kendall correlation against human adequacy judgments following NIST MetricsMaTr protocols (Callison-Burch et al., 2010). We train the weights used for computing the weighted f-score over matching role labels using a grid search and then test them on a combination of these data sets and since each data set has different average sentence length and number of sentences we identify robust metrics that perform across all the variations after thorough analysis on the quality of the weights assigned to the role labels.The strategy used in evaluating the phrasal similarity score from the component token similarity scores is critical in deciding the overall performance of the MEANT metric, as role fillers are often phrases. In contrast to the arithmetic mean and competitive linking strategies we show that that using the geometric mean for this purpose is more reliable.In order to examine the optimum amount of contextual information to be captured while training the word vectors, we vary the window size while training the word vectors from 3 to 13. Surprisingly, we achieve both high performance and robustness at the window size of 5 not only for Jaccard coefficient but across almost all the metrics in comparison.Our results indicate that Jaccard coefficient on word vectors trained with a window size of 5, and using geometric mean style of aggregation as the criterion for aligning semantic frames and significantly enhances the performance in comparison to other metrics and robustness across varying data sets of MEANT. We present larger-scale evidence overturning previous results, showing that among the many alternative phrasal lexical similarity measures based on word vectors, the Jaccard coefficient most increases the robustness of MEANT, the recently introduced , fully-automatic, state-of-the-art semantic MT evaluation metric. MEANT critically depends on phrasal lexical similarity scores in order to automatically determine which semantic role fillers should be aligned between reference and machine translations. The robustness experiments were conducted across various data sets following NIST Met-ricsMaTr protocols, showing higher Kendall correlation with human adequacy judgments against BLEU, METEOR (with and without synsets), WER, PER, TER and CDER. The Jaccard coefficient is shown to be more discriminative and robust than cosine similarity, the Min/Max metric with mutual information , Jensen Shannon divergence, or the Dice&apos;s coefficient. We also show that with Jaccard coefficient as the phrasal lexical similarity metric, individual word token scores are best aggregated into phrasal segment similarity scores using the geometric mean, rather than either the arithmetic mean or competitive linking style word alignments. Furthermore , we show empirically that a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets.
On Interpretation of Resultative Phrases in Japanese  The present paper attempts to formalize the semantic interpretation of resultative phrases in Japanese in the framework of Generative Lexicon, with a focus on the semantic subject of resultative phrases, i.e. the entity which resultative phrases are predicated of. The semantic subject cannot always be identified with the direct object of transitive verbs or the subject of unaccusative verbs, as generally believed, but also is expressed as an oblique NP or not syntactically expressed at all. It poses a challenge to the interpretation of resultative phrases since it cannot be tied to a specific syntactic constituent. The interpretation of resultative phrases is encoded in terms of the FORMAL quale and its argument built through the co-composition operation.
Psych-Predicates: How They Are Different We will characterize psych-predicates = experiencer-predicates and predicates of personal taste in Korean, focusing on the status of the Experiencer in relation to arguments and examining the first-person subjectivity data (constraint). The relevant cause and effect relation and consequent coerced event function is postulated for coherent interpretation. 2 will show data and raise issues; 3 will discuss issues in the GL spirit; and 4 will conclude the discussion. This paper is concerned with characterizing psych-predicates in Korean and possibly in Japanese in the GL spirit. We focus on the the status of the Experiencer (or &apos;judge&apos;) in relation to other arguments and examine the first-person subjectivity data (constraint). The relevant cause and effect relation and consequent coerced event function is postulated for coherent interpretation.
Gap in &quot;Gapless&quot; Relative Clauses in Korean and Other Asian Languages In Korean (Chinese and Japanese as well) the so-called gapless relative clauses (GRC) have been discussed in Cha (1997,2005), J. Lee 2012, and others, representatively illustrated in (1, 2, and 3) (Adn = adnominal).(1) cause-effect relation with sensory head noun[sayngsen-i tha-nun] naymsay fish-Nom burn-Adn smell 'the smell that comes from fish burning' (2) cause-effect relation with non-sensory head noun[thayphwung-i cinaka-n] huncek typhoon-Nom pass-Adn trace 'the trace left after a typhoon hit' It is observed that there exists a semantic cause-effect relation holding between the GRC and its modifying head noun: the content of the adnominal GRC constitutes cause and the denotation of its head noun effect. Without the cause-effect relation, the GRC is not allowed (e.g., [sayngsen-I tha-nun] ?*hyangki ('fragrance)/?*moyang ('appearance') /*huncek ('trace)). GRC is different from a typical relative clause (RC) like (4) containing a gap which is externally realized as a head noun. Thus, GRCs in Korean are different from regular RCs, and they are not noun complements; therefore, as most researchers claim, GRCs are like gapless clausal modifiers for the following head nouns (Yoon, JH 1993, Cha 1997, 1998, 2005 in Korean and papers for Japanese and Chinese). In this paper, we for the first time claim that for the correct, coherent interpretation in GRCs like (3), for example, the required cause-effect relation should be 640 fully realized by the addition or coercion of a verb like pel-'earn,' which comes from the agentive role in the qualia structure of ton 'money,' in conjunction with the main event predicate phal-'sell,' as shown in (6).father-Nom ox-Acc sell earn-Adn ton money 'the money that father earned by selling an ox'We then argue that the meaning of the hidden verb pel-'earn' in (3) can be successfully recovered from the reservoir containing the lexical-semantic (-pragmatic) information of the given lexical items specified under the GL framework. In section 2, we observe more related phenomena to claim that recovering the hidden verb has actual empirical bearing as seen in examples like (6). In section 3, we elaborate the current proposal in detail within the GLT, offering the lexical-semantic information of the elements of the GRC construction. In section 4, we briefly discuss cross-linguistic implications of the proposed GL analysis. Finally, section 5 concludes the paper. This paper attempts to argue that the so-called gapless relative clause (GRC) in Korean (Chinese and Japanese as well) can best be dealt with by the Generative Lexicon Theory (GLT) put forward in Pustejovsky (1995). There arises a superficial conflict in the construction: the GRC, with no apparent gap, contains a relative verb that does not directly relate to the head noun in terms of cause-effect relation required between the GRC and the following head noun. The paper shows that this incomplete realization of the cause-effect relation can be fully recovered from the lexical-semantic(-pragmatic) information specified under the GL framework. Thus, the qualia structure of GLT can successfully fill the meaning of the best hidden relative verb in the GRC for the correct interpretation.
Global Approach to Scalar Implicatures in Dynamic Semantics Scalar implicatures (SIs) arise on the basis of the maxim of quantity by Grice (1975):(1) The maxim of quantity:a. Make your contribution as informative as is required (for the current purposes of the exchange).b. Do not make your contribution more informative than is required.It is the first maxim of quantity that is relevant to SIs. When a stronger statement is relevant to the context and a speaker utters a weaker statement, it is implicated that the stronger statement is not true in the speaker's information state. Assuming that the speaker is well-informed and that he knows the stronger alternative is false, the hearer accepts the implicature as true.Grice did not explicate the precise procedure of getting a SI. Horn (1972,1989) suggested that a SI arises by comparing a set of alternative statements that arises by replacing a scalar term in the original statement with a stronger scalar alternative expression in the language system. Behind this idea lies the assumption that a set of scalar terms, which is called a scalar set, is given in the language system. Sauerland (2004) gives a more precise procedure, within the Neo-Gricean tradition that a SI arises based on a set of scalar alternatives. He assumes that SIs have an epistemic status, following Gazdar (1979), but deviates from his idea by assuming that the maxim of quantity gives rise to only uncertainty inferences, which he calls primary implicatures. Primary implicatures have the form of 'K', in which K means 'know' and is a stronger alternative sentence of the original utterance. The hearer tentatively strengthens each of the primary implicature of the form 'K'. If the stronger implicature is compatible with the meaning of the statement and all the primary implicatures, it gets the status of a SI, which he calls a secondary implicature.His idea is illustrated in the following:(2) John broke some glasses. The use of some yields a stronger statement φ (= 'John broke {all, many} glasses') and the primary implicature is ¬Kφ, which can be strengthened into ¬Kφ since it is compatible with the statement itself plus all the primary implicatures.Neo-Griceans naturally accepted that SIs are calculated from a whole statement. In this respect, they can be called globalists. SIs are inferences based on the maxim of quantity by Grice (1975). Implicatures are supposed to be calculated from utterances, which are always dealt with as a whole. This implies that SIs only arise from stronger statements than the original statement.However, linguists like Chierchia (2002) claim that implicatures are included in the meaning of a statement, as part of the strengthened meaning (= the literal meaning plus its implicatures) of a CHUNK of a statement as a scope-site of a scalar expression, in the process of compositional semantic interpretation, following Krifka (1995), and the strengthened meaning of the sentence chunk is combined with the meaning of the rest of the sentence. The plain meaning of an expression α is represented as [[α]] and the implicature is ¬S(α ALT ), where S(α ALT ) is the weakest alternative of α that entails α. Thus the strengthened meaning of α is the conjunction of the two meanings: [[α]] ∧ ¬S(α ALT ), which entails other strengthened meanings from the stronger alternatives of α. Chierchia introduces the negation operator to get a stronger meaning. For the same purpose, Fox (2006) instead introduces the exhaustivity (exh, hereafter) operator. They can be called localists.Their analysis is illustrated in the following:(4) Mary believes that John broke some glasses.(5) a. LF:It is assumed that the quantifier some glasses is Quantifier-raised witin the complement clause of the propositional attitude verb believes. And a SI from the use of some is calculated when the strengthened meaning of the complement clause is obtained in (5c). The strengthened meaning of the complement clause is the conjunction of the plain meaning and a SI of the clause, the latter of which is expressed as ¬S(φ ALT ), where φ is the complement clause [some glasses [John broke t i ]]. We are assuming that the weakest stronger alternative of some is many. The alternative meanings are derived in a similar way to the alternative semantics by Rooth (1985) for focus. The strengthened meaning of the complement clause is combined with the meaning of the rest of the sentence, as in (5d).Localists' approaches may look more systematic, manageable and more constrained than globalists', because they are based on syntactic structures and calculation of SIs is precisely defined. However, one theoretically serious problem with localists is that, as Horn (1989) pointed out, SIs do not arise within downward entailing contexts and that SIs are based on strengths of statements as a whole. Even if they calculate SIs locally, they have to check whether an alternative involved in the calculation makes the whole sentence a stronger statement to see if it really leads to a valid SI. In this respect, SIs are inherently global.Empirically, actual data do not take part with either of the two positions. Consider the following examples:(6) Some students who drank beer or wine were allowed to drive.a. Some students who drank beer or wine, but NOT both, were allowed to drive.b. NOT[some students who drank both were allowed to drive] (= No students who drank both were allowed to drive.)  In (6), it is plausible that no students who drank both beer and wine were allowed to drive, which is calculated by negating the whole stronger alternative. In (7), a linguist at MIT is likely to have read one of the two books, and the global SI is more likely. On the other hand, the following two examples show the opposite:(8) Some students who watched TV or played games failed math. a. Some students who watched TV or played games, but not did both, failed maths. (conveyed) b. NOT[Some students who watched TV and played games failed maths] (global SI) (9) Every student wrote a paper or made a classroom presentation. a. Every student wrote a paper or made a classroom presentation but did not do both. b. NOT(every student wrote a paper and made a classroom presentation)In (8), it is more likely that a student who watched TV and played games failed math. For this reason the global SI that no students who watched TV and played games failed math is not acceptable. Similarly, in (9), if either of the two requirements is sufficient to get a grade, it is more plausible to assume that no students satisfied both requirements. This corresponds to the local SI. Thus we do not get the global SI that not every student did both. Then we could take a position in which we exploit both ways of calculation of SIs. But if we cannot provide clear criteria for when we get global SIs and when we get local ones, it is not an explanation at all. Moreover, if syntactic structures are not what we directly deal with in calculating SIs, we cannot choose a localistic approach anyway. In this paper I will show that calculation of SIs needs more fine-grained structures than syntactic structures. And I will also show that local SIs are contextual effects on global SIs. It has been disputed whether scalar implica-tures (= SIs) arise globally or locally. Basically SIs should be global because they arise by comparing strengths of whole alternative statements. On the other hand, there are a lot of examples in which local SIs are preferable. Linguists like Chierchia (2002) and Fox (2006) even claim that SIs arise by applying an operator to syntactic constituents to get their stronger meanings. In this paper, I claim that SIs are global and seemingly local implicatures are effects of contexts on global implicatures. Moreover, I will show that no syntactic analyses work.
Head-internal Relatives in Japanese as Rich Context-Setters Japanese displays so-called HIRs (Head-Internal Relatives), where the relative clause lacks a gap, the head is found inside the relative clause, and the relative clause ends with the particle no.(1) [Ringo- This paper addresses Japanese HIRs in Dynamic Syntax (DS; Cann et al., 2005;Kempson et al., 2001). Sect. 2 surveys previous studies. Sect. 3 introduces DS. Sect. 4 argues that the past DS account of no ( Cann et al., 2005) fails to capture the non-nominality of HIRs. Sect. 5 presents an alternative DS account. Sect. 6 argues that the past DS account of no models change relatives (but not HIRs). Sect. 7 concludes the paper. Head-Internal Relatives (HIRs) in Japanese are regarded as rich context-setters within Dynamic Syntax (DS): the propositional tree of the HIR clause is mapped onto a &apos;partial&apos; tree, which establishes a rich context for the embedding clause to be parsed. This partial tree contains a situation node decorated with the Relevancy restriction and a node for an internal head. This account handles some new data and makes a novel prediction. Further, it is shown that the past DS analysis of HIRs in fact models change relatives (but not HIRs).
Prosodic Convergence, Divergence, and Feedback: Coherence and Meaning In Conversation Human language provides an especially cogent platform for studying the phenomenon of imitative and convergent behaviors in human communication, as speech communication integrates a complex mix of cognitive, emotional, and interactive social processes that are expressed in a number of different forms: the language specific choice of lexical items to communicate meaning, visuallybased information exchange of gestures and facial expressions, and the shaping of the oral and aural environment through variations in prosodic flow. Scientific studies have shown convergent behavior in body movements and gesturing in conversation ( Condon and Sander, 1974;Nagaoka, et al., 2007, Campbell andStefan, 2010), and in speech Jonsdottir, et al., 2007, Buschmeier, et al. 2011, Lelong and Bailly, 2011Heylen, et al., 2011;Ward, 2006), and focused on their role in creating harmony and rapport between conversational participants through the use of feedback markers, and through timing and frequency of non-verbal facial and movement gesturing ( Lelong and Bailly, 2011;Heylen, et al., 2011).Spontaneous conversation is multi-functional in both its goals and processes: the most evident goal of transmitting information simultaneously carries a social goal of building rapport and the sharing of attitudes and emotions towards the information transmitted. In the conversational process, speakers provide propositional and emotional and information through prosody, gesturing, and feedback, and engage in interactional probing to build a shared knowledge state and guide topic in a mutually desired direction. Prosody plays a key role in this process, as it provides a powerful and informative resource to communicate multiple levels of coherence and meaning by providing a direct and immediate link to fundamental expressive states. 1 Introduction
A Quantitative Comparative Study of Prosodic and Discourse Units, the Case of French and Taiwan Mandarin Interest for the studies of discourse prosody interface has arisen in the last decade as illustrated by the vitality of the events and projects in this domain. However, while theoretical proposals and descriptive works are numerous, quantitative systematic studies are less widespread due to the cost of creating resources usable for such studies. Indeed, prosodic and discourse analysis are delicate matters requiring lower-level processing such as the alignment with speech signal at syllable level (for prosody) or at least basic syntactic annotation (for discourse). Moreover, many of these studies are dealing with read or monologue speech. The extremely spontaneous nature of conversational speech renders the first levels of processing complicated. Previous works ( Liu and Tseng, 2009;Chen, 2011;Bertrand et al., 2008;Blache et al., 2009;Afan- tenos et al., 2012) give us the opportunity to produce conversational resources of this kind. We then took advantage of a bilateral project for working on conversational speech in a quantitative fashion, and this for two typologically diverse languages: French and Taiwan Mandarin. We believe this combination of linguistic resources and skills for these two languages is a rather unique situation and allows for comparative quantitative experiments on high-level linguistic analysis such as discourse and prosody.Our objective is to understand the commonalities and the differences between discourse prosody interface in these two languages. More precisely, we look at how prosodic units and discourse units are distributed onto each other.In spirit, our work is closely related to the one of (Simon and Degand, 2009;Lacheret et al., 2010;Gerdes et al., 2012), however our focus here are the insights we can get from a comparative study. Moreover our dataset has a more conversational nature than the datasets studied in their work. About the data, (Gerdes et al., 2012) wanted to have an interesting spectrum of discourse genres and speak-ing styles while we focused on conversations both for making possible the comparative studies and to make sure to have enough coherent instances in the perspective of statistical studies. Also, while (Lacheret et al., 2010) requires a purely intuitive approach, we used a more balanced approach combining explicit criteria from different language domains. Finally, our annotation experiments are largely produced either by automatic tools (trained on experts data) or by naive coders. This is a major difference with the studies listed above that are based on experts annotations since it allows us scale up in data size more easily.The paper is structured as follows. We will start in section 2 by presenting how we built a comparable dataset from existing corpora. Then we will address in section 3 and 4 respectively the creation of prosodic and discourse units. Based on these new datasets, we will investigate the discourse prosody interface in a comparative and quantitative way (Section 5). Finally, in section 6 we will pay some attention at what is happening syntactically at various types of boundaries as defined in the preceding section. First of all, corpora from both languages were recorded in very similar conditions. There are both face-to-face interaction in an anechoic room and speech was recorded via headsets on separate channels. The original recordings are also very comparable in size. The raw figures of both datasets are presented in Table 1. 1 We had to decide which linguistic information and which part from the full corpora to include in our joint dataset. About the later point, we extracted narrative sequences from the French data that included also more interactive topic negotiation sequences. About the linguistic levels, our study concerned prosodic and discourse levels but we wanted to be able to perform fine-grained study involving syntactic and phonetic aspects. We therefore agreed to include syllables, tokens and part-ofspeech information in our data as can be seen in Ta- ble 2. As the POS tagsets are different in both lan- Studies of spontaneous conversational speech grounded on large and richly annotated corpora are still rare due to the scarcity of such resources. Comparative studies based on such resources are even more rarely found because of the extra-need of comparability in terms of content, genre and speaking style. The present paper presents our efforts for establishing such a dataset for two typologically diverse languages: French and Taiwan Mandarin. To the primary data, we added morpho-syntactic, chunking, prosodic and discourse annotation in order to be able to carry out quantitative comparative studies of the syntax-discourse-prosody interfaces. We introduced our work on the data creation itself as well as some preliminary results of the boundary alignment between prosodic and discourse units and how POS and chunks are distributed on these boundaries.
Corpus-based Research on Tense Analysis and Rhetorical Structure in Journal Article Abstracts Previous studies have highlighted the indispensable importance of JA abstract in the contemporary flow. The pivotal role of JA abstract has received considerable attention in academic written genre among the international community. Swales (1990) appeals to the academia, claiming that the research in JA abstracts ought not to be ignored inasmuch of its influential significance upon the genre investigation and disciplinary discourse communities.As the knowledge of proficient preferences for language choice as well as rhetorical structure has a great influence on academic written genre, many investigators have recently turned to the relevant research in relation to genre analysis, thematic organization, formulaic language, rhetorical structure, etc. (Cortes, 2004;Hyland, 2008a;Lorés, 2004;Martín, 2002;Swales, 1990;Wang &amp; Chan, 2011;Wang &amp; Kao, 2012). Furthermore, research in terms of corpora decoding for rhetorical structures such as moves and steps is also regarded as one of the recommendations for further research expansion by Flowerdew (2010).Taking the contribution of the previous studies, this current reserach sets out to explore the variation of tense within the reporting verbs among the transitions of moves via the structural analysis in JA abstracts. There has long been a growing interest in journal articles (JA) abstract writing, and this pervading interest has boosted the exigency for further instructive research. This current study aims to investigate both the variant application of the verb tense as well as the rhetorical structure within JA abstracts. A 9.9 million word corpus of 1000 JAs was collected based on four prestigious journals, i.e., Journal of Pragmatics, Journal of Research in Reading, Journal of Second Language Writing, and Reading and Writing, respectively. The quantitative analysis indicates the tendency of tense shown in the commonly applied reporting verbs. On the other hand, the qualitative analysis shows the prevailing adoption of three-, four-, and five-move theories in terms of the CARS model, the IMRD structure, and the IPMPrC structure. The results not only reveal the explicit tendency of the variance within reporting verbs but also suggest a distinct pervasiveness of the IMRD structure over the other models. These findings not only present a more systematic pattern within JA abstracts, but also show potentials for enlightening further pedagogy-oriented composition instruction for JA abstract.
Towards a Revised Motor Theory of L2 Speech Perception  This study aims to review, through experiment proof of a salient effect of articulatory gestures on L2 perception, the time-honored but still put-to-sideways
Towards Automatic Error Type Classification of Japanese Language Learners&apos; Writing Automatic error detection is one area that has been widely studied. One of the challenges in this work is generalizing the great number of error patterns. Given that the different types of learners' errors are too numerous to detect, some researchers have broken down the error detection task according to the types of errors, such as spelling errors, mass count noun errors and preposition errors. If the error type classification is made in advance, it will help the automatic error detection system more accurate.Classifying error types has other advantages. First, it will help resulting learner corpora useful in linguistic research. It can offer teachers with effective feedback on patterns of errors repeatedly made by students. Secondly, through classification of errors, learners are able to correct their own errors by comparing acceptable and unacceptable sentences.Learner corpora are useful for statistical analysis of learner output and provide positive and negative examples that contribute to improving writing skills. According to Ellis's input theory (El- lis, 2003), both positive and negative input are required in learning a second language. Positive input provides grammatically correct and acceptable models of the language. Negative input is comprised of incorrect sentences that are made by non-native speakers. It teaches learners the sentences they should not produce. Learners' writing skills are improved by exposure to both. A system to organize both correct sentences (for positive evidence) and incorrect sentences that language learners are likely to produce (for negative evidence) would benefit language learners considerably. To master a foreign language, it is very effective to see where a problem lies and what caused it, rather than merely learning the correct expression.We propose a machine learning-based approach on automatic error type classification in Japanese learners' writing by looking at the local contextual cues around a target error.In Section 2, we give a brief overview of previous related work. Section 3 then outlines our annotation schema for the Japanese learners' errors. Then, we propose a machine learning-based approach to automatic error type classification in the writing of learners of Japanese learners by looking at the local contextual cues around a target error in Section 4. We discuss the experimental results with both in-domain and out-of-domain settings and also compare the characteristics of the classification between the machine and the human Learner corpora are receiving special attention as an invaluable source of educational feedback and are expected to improve teaching materials and methodology. However, they include various types of incorrect sentences. Error type classification is an important task in learner corpora which enables clarifying for learners why a certain sentence is classified as incorrect in order to help learners not to repeat errors. To address this issue, we defined a set of error type criteria and conducted automatic classification of errors into error types in the sentences from the NAIST Goyo Corpus and achieved an accuracy of 77.6%. We also tried inter-corpus evaluation of our system on the Lang-8 corpus of learner Japanese and achieved an accuracy of 42.3%. To know the accuracy, we also investigated the classification method by human judgement and compared the difference in classification between the machine and the human.
The Development of Coherence in Narratives: Causal Relations Previous research relied on a variety of schemes to explore how narrators relate categories of information in a narrative (e.g., Berman and Slobin, 1994;. Some researchers examine narrative structure (Peterson and McCabe, 1983); some concern more about the conceptual basis for relating narrative information ( Trabasso and Nickels, 1992;). Regarding cognitive processing, capacities for working memory and theory of mind were considered relevant to a narrator's ability to organize and integrate narrative information ( . Given the significant role of narratives in children's development (Chang, 2004), the present work aimed to explore Mandarinspeaking children's progress in relating events and hence in maintaining coherence in narratives. One intriguing assumption of the research by  is that narrators tend to encode a protagonist's actions as relevant to a goal plan. They suggested that knowledge of goal/plans serves as the conceptual basis underpinning narrative representations. Children, with increasing age, were found to be more advanced in applying knowledge of goal/plans to integrate narrative events coherently.Acknowledging the significance of goal/ plan knowledge aside, Trabasso et al. (Trabasso and Sperry, 1985;Trabasso and van den Broek, 1985) indicated that it is causal inferences that unite elements (such as goals, actions, and outcomes) in a goal-plan. Similarly, Stein and Albro (1997) suggested that causal reasoning is required to organize content and structure coherently. In other words, causal relation is regarded as a basic mechanism for integrating episodic and thematic information. As Karmiloff- Smith (1985) indicated, coherence refers to global representation of story meaning and connectedness, which is embodied in the temporal and causal structure of a story.Given the significance of causal relations for narrative construction, Trabasso and Sperry (1985) outlined procedure to identify causal networks so as to assess causal connectivity between linguistic units in a narrative. Research has shown that causal networks provide explanations for variance in story recall ( Trabasso et al., 1984). In particular, compared with measures of story grammar, causal networks were found to be a more reliable predictor of story recall (Trabasso and van den Broek, 1985). Research has also shown that the derived causal connections correlated with the importance ratings for narrative events (Trabasso and Sperry, 1985). Additional credence of the predictive power of causal networks is given by Diehl et al.'s (2006) research, which revealed that the system of causal networks is a potential tool to assess narrative coherence.In recent decades, most developmental research of Mandarin-speaking children's narrative ability has focused on typicallydeveloping preschool children (e.g., Chang, 2004;Chen et al., 2011;Li, 2012). Many of these studies used high-point analysis or story grammar to analyze preschoolers' narrative structure. However, very little is known about older Mandarin children's ability to relate narrative events. Even less is known about Mandarin children's progress in maintaining narrative coherence. Much prominent research on other languages adopted a cross-sectional research paradigm to investigate narrative development by examining data based on the frog story (e.g., Bamberg and Marchman, 1990;Berman and Slobin, 1994); nevertheless, only a few studies (Chang, 1995;Li, 2012;Sah, 2013) on Mandarin-speaking children followed this paradigm. Among them, Chang's (1995) and Sah's (2013) research included both preschool and school-age children, while the other studies focused on only preschoolers (Li, 2012). Nevertheless, we still lack of knowledge about Mandarin children's development in maintaining narrative coherence. It is, however, important for us to understand more about this, for such ability is integral to narrative construction. To extend the line of frog-story-based research and to replicate previous findings based on causal networks, the present study explored how Mandarin-speaking children maintain narrative coherence by posing the following research questions.(1) Is there any difference in Mandarinspeaking five-and nine-year-olds' ability to encode events in the causal chain? (2) Is there any difference in five-and nineyear-olds' ability to establish causal connections between narrative events? (3) Is there any difference between the two groups of children in encoding events with different levels of causal connectedness? This study explored Mandarin-speaking children&apos;s ability in maintaining narrative coherence. Thirty Mandarin-speaking five-year-olds, 30 nine-year-olds and 30 adults participated. The narrative data were elicited using Frog, where are you? Narrative coherence was assessed in terms of causal networks. The results displayed children&apos;s development in achieving narrative coherence by establishing causal relations between narrative events. Results were considered in relation to capacities for working memory and theory of mind. Narrators&apos; differences in communicative competence and cognitive preferences were also discussed.
Age Related Differences in Language Usage and Reading between English Monolinguals andBilinguals  This study investigates age related differences in standardized tests scores of language usage and reading from elementary to high school for students who are either monolinguals whose L1 is English or bilinguals whose L1 is not English. An interactioneffect between grade level andreading and language usage standardized test scores was hypothesized because as bilinguals become proficient in Cognitive Academic Language Proficiency (CALP) in English,they are able tonarrow the &apos;achievement gap&apos;in comparison to their monolingual classmates and even experience cognitive advantages (Cummins, 1999).Participants were 1081 students from an international school. Language usage and readingwere measured using MAP standardized achievement tests.The2x2 ANOVA showed an interaction between grade level and languages spoken on language usage (p&lt;0.05).There was a main effect for languages spoken and grade level on language usage (p&lt;0.05). No interaction was found forgrade level and languages spoken on reading (p&gt;0.05). A main effect was found for languages spoken andgrade level on reading (p&lt;0.05).Significant differences exist between bilingual and monolingualsand these differences change over time. As bilingual students are immersed in English education, their performance on standardized tests catches up with their monolingual counterparts by grade 5 for language usage but not for reading, but no cognitive advantages are shown.
BCCWJ-TimeBank: Temporal and Event Information Annotation on Japanese Text Temporal information processing in natural language texts has received increasing scholarly attention in recent years. Since temporal order of events often has implications for causal relations (cause and effect), identifying them is an essential task for deep understanding of language. Several types of resource for English temporal information processing have been developed, such as an annotation specification TimeML ( Pustejovsky et al., 2003) and annotated corpora TimeBank (Pustejovsky et al., 2010) and Aquaint TimeML Corpus. The English annotation specification has been extended as an ISO standard of a temporal information mark-up language -ISO TimeML (ISO, 2008), which covers Italian, Spanish, Chinese and other languages. Temporal information-annotated corpora in various languages have been developed and shared by natural language processing researchers. TempEval-2 ( Verhagen et al., 2010), a task for the SemEval-2010, andTempEval-3 (Uz- Zaman et al., 2013), a task for the SemEval-2013, have been proposed as shared temporal-relation reasoning tasks. In these shared tasks, datasets for English, Italian, Spanish, Chinese, and Korean are provided.However, there is no such resource for the Japanese language. In this paper, we present a means of porting ISO-TimeML into the Japanese language and also describe the basic specifications of 'BCCWJ-TimeBank' which is a realisation of the temporal information annotation of the Balanced Corpus of Contemporary Written Japanese, or BCCWJ (Maekawa, 2008). Temporal information extraction can be split into the following three tasks: temporal expression extraction, time normalisa-tion, and temporal ordering relation resolution. This paper describes a time expression and temporal ordering annotation schema for Japanese, employing the Balanced Corpus of Contemporary Written Japanese, or BCCWJ. The annotation is aimed at allowing the development of better Japanese temporal ordering relation resolution tools. The annotation schema is based on an ISO annotation standard-TimeML. We extract verbal and adjective event expressions as ⟨EVENT⟩ in a subset of BCCWJ. Then, we annotate temporal ordering relation ⟨TLINK⟩ on the above pairs of event and time expressions by previous work. We identify several issues in the annotation.
A Corpus-Based Approach to Linguistic Function Recent years have seen data-driven approaches to natural language processing successfully applied to a wide range of problems including syntactic (Collins, 2003;Klein and Manning, 2003) semantic ( Gildea and Jurafsky, 2002;Pradhan et al., 2004) and discourse (Hernault et al., 2010) analysis. Computational processing of functional aspects of linguistic data, on the other hand, is a relatively underexplored research area. In linguistics, functional analysis refers to the study of language use in context. Among the theories for analyzing the functions of language, Systemic Functional Linguistics (SFL, Halliday and Matthiessen, 2004) is a linguistic framework that is becoming increasing influential in recent years. SFL provides an ideal handle to exploring language as intentional acts of meaning, complementing more syntactically oriented approaches to linguistic study. Despite its power, traditional analysis with SFL is done manually, a time-and effort-consuming process.We are motivated in our study to extend the power of the framework to computational analysis. The difficulty in automating analysis of linguistic functions lies in both the fuzziness in the functional domain and a lack of relevant computational resources. The most significant lack of resource is a high-quality reference corpus crucial to statistical analysis and modeling. In the following sections, we discuss our initial efforts in constructing such a resource on a collaborative annotation platform and present the initial results from the corpus. The corpus is our first step in bridging the gap between the linguistic theory and application of such theory including automated analysis of language functions. In this paper, we present our recent experience in constructing a first-of-its-kind functional corpus based on the theoretical framework of Systemic Functional Linguistics. Annotated on selected texts from the Penn Treebank, the corpus was built by a collaborative team on web-based annotation platform with several advanced features. After a discussion on the background and motivation of the project, we present our solutions to some of the challenges encountered in the collaborative annotation process. With fine-grained annotations of an initial corpus now available, the corpus can serve as a valuable linguistic resource that complements existing semantically annotated corpora and aid in the development of a larger-scale resource crucial for automated systems for analysis of linguistic function.
ChinGram: A TRALE Implementation of an HPSG Fragment of Mandarin Chinese This paper presents a grammar fragment of Chinese which is built in the framework of HPSG ( Pollard and Sag 1994) and implemented in the grammar development system Trale ( Meurers et al. 2002;Penn 2004). The grammar is one of the grammars that are developed in the CoreGram project (Müller 2013a). Apart from the Chinese grammar, which will be documented in Müller and Lipenkova (In Preparation), there are smaller fragments of several languages and larger fragments of German, Persian, Danish, and Maltese (see Müller (2013b) for details on size). These grammars share a common core und hence crosslinguistic generalizations are captured. We see the advantages of the HPSG framework for a formal analysis of Chinese as follows:* The work reported in this paper was supported by the grant ChinGram MU 2822/5-1 by the Deutsche Forschungsgemeinschaft.** The following abbreviations are used:• HPSG sign features: HD: head; SS: synsem; IND: index• Tree arc symbols: Arg: argument daughter; Spr: specifier daughter; H: head daughter; NH: non-head daughter; Adj: adjunct daughter• Glosses: CL: classifier; ATTR: attributive particle de; LOC: localizer particle• HPSG provides a range of powerful formal tools for the description of linguistic expressions which are embedded into the logical framework of Typed Feature Structure Logic (Carpenter 1992) and allow a seamless implementation in logical programming paradigms.• HPSG makes restricted use of a-priori theory-internal statements about the empiricial properties of linguistic signs. Since Chinese phenomena often cannot be explained using the terminology and assumptions of the Western linguistic tradition, HPSG provides us with a 'neutral' framework for the formalization of language-specific phenomena based on which more general principles can be derived.• In contrast to most formal theories, HPSG is not a syntax-driven framework. That is, there is no central syntactic component from which a Phonological Form and a Logical Form is derived. Instead, the different levels of linguistic representation -phonology, syntax, semantics, pragmatics -have equal weight. This is especially beneficial for Chinese, which has a poor morphological system and exhibits a high degree of surface ambiguity. The use of a powerful semanticpragmatic component with fine-grained definitions of semantic types and selectional restrictions and preferences thus helps disambiguation.In the following, we first introduce the basic feature architecture and formal tools of the grammar formalism. Then, we review existing work in HPSG and grammar development for Chinese. Finally, we describe the theoretical and empirical basis of our research and provide a synopsis of the covered phenomena; the main analytical choices are illustrated using a subset of example phenomena. In this paper, we present our effort in the development of a HPSG grammar for Chinese. We present the basic notions of the HPSG framework, review existing theoretical analyses and implementations of Chinese grammar fragments in HPSG and present a range of deep linguistic analyses that are part of our own implementation.
Effects of Parsing Errors on Pre-reordering Performance for Chinese-to-Japanese SMT Statistical machine translation is a challenging and well established task in the community of computational linguistics. One of the key components of statistical machine translation systems are word alignment techniques, where the words from sentences in a source language are mapped to words from sentences in a target language. When estimating the most appropriate word alignments, it is unfeasible to explore every possible word correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures.An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted ( Xia and McCord, 2004;Genzel, 2010), or linguistically motivated ( Xu et al., 2009;Isozaki et al., 2010;Han et al., 2012;Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure.However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of this work is two fold. First, we provide an empirical analysis where we quantify the aggregated impact of parsing errors on pre-reordering performance. Second, we define seven patterns of the most common and influential parsing errors and we carry out a descriptive analysis to examine their relationship with reordering errors. We combine an empirical and descriptive approach to present a three-stage incremental comparative analysis to observe the effect of different parsing errors on reordering performance.In Section 2, after a brief description on the prereordering method that we use for experiments, we will introduce some related works on parsing error analysis and analysis on the relation between parsing and machine translation. From a general perspective, we describe our analysis methods for this work in Section 3. Then, we carry out the analysis and exhibit the results in Section 4 and Section 5. The last two sections are dedicated to discussion, future directions and summarize our findings. 2 Background Linguistically motivated reordering methods have been developed to improve word alignment especially for Statistical Machine Translation (SMT) on long distance language pairs. However, since they highly rely on the parsing accuracy, it is useful to explore the relationship between parsing and reordering. For Chinese-to-Japanese SMT, we carry out a three-stage incremental comparative analysis to observe the effects of different parsing errors on reordering performance by combining empirical and descriptive approaches. For the empirical approach, we quantify the distribution of general parsing errors along with reordering qualities whereas for the descriptive approach, we extract seven influential error patterns and examine their correlation with reordering errors.
Reduplication across Categories in Cantonese Reduplication is found across syntactic categories noun, verb and adjective in Cantonese. They all share a similar surface order, but the interpretation can be quite differently. Nominal reduplication denotes an exhaustive list such as 'everybody, every apple'. Verbal reduplication displays either durative or iterative reading, depending on the telicity of the verbal predicate. Adjectival reduplication shows a hedging and diminutive reading, as in 'a little fat' or 'reddish ' The goal of this paper is to establish a unified account for the cross-categorial reduplication that can interpret the various meanings. We argue that the common thread behind these interpretations is summation. Building on the notions of cumulativity and quantization, the interpretations of reduplication are predictable.In what follows, section 2 lists out the distribution and characteristics of reduplication in Cantonese. Section 3 reviews previous studies and points out that they cannot account for the behaviour of reduplication across categories. Section 4 discusses the formal property of cumulativity (Krifka, 1998;Rothstein, 2004), which provides a basis to account for the surface differences across categories. To test the hypothesis, section 5 provides the details of the proposal and shows how various interpretations can be handled by the present cumulativity analysis. Section 6 discusses the advantage of this approach and also the theoretical implications. This paper investigates the formal semantics of reduplication in Cantonese, i.e. how the meaning of reduplicated forms are encoded and computed with the given meaning from the base forms. In particular, this paper argues that reduplication denotes a summation function that adds up arguments (be they object-, event-or degree-arguments) and return a collection of the elements. The surface difference across categories is accounted for in terms of cu-mulativity and quantization (Krifka, 1998; Krifka, 2001; Rothstein, 2004). The present approach makes use of scalar structure and summation as formal tools to model the cross-categorial behaviour of reduplication. It provides the advantage of a unified theory for lexical composition across categories nouns, verbs and adjectives.
Yet Another Piece of Evidence for the Common Base Approach to Japanese Causative/Inchoative Alternations It has been standard since the early days of generative grammar that causative verbs are assumed to be syntactically more complex than their inchoative counterparts, reflecting their semantic complexity. While specific analyses like Dowty (1979) are long gone, their spirit remains alive and well and has been instantiated in later theoretical constructs such as Lexical Conceptual Structure (e.g., Rappaport and Levin, 1988) and double VP structure (e.g., Hale and Keyser, 1993). Moreover, while structural complexity per se is independent of the issue of the derivational relationship between the causative and inchoative alternants, it has been also standard to assume that causatives are virtually derived from their inchoative counterparts via operations like Predicate Raising in Generative Semantics or Head Movement in the GB/MP framework. Thus, the causativization approach, which views causatives as based on inchoatives, has been influential across a variety of theoretical perspectives.Two more approaches can be discerned as alternatives to handle the derivational relationship between the two alternants. One involves the process of anticausativization of causative verbs (e.g., Levin and Rappaport Hovav, 1995), and the other approach, proposed by researchers like Piñón (2001) and Alexiadou et al. (2006), is the common base approach, where both the causative and inchoative alternants are derived independently from an identical root. The three possible approaches to the causative/inchoative alternation are given in (1). 1 This paper presents yet another argument for the view that both the causative and in-choative verbs in the Japanese causative alternation are syntactically equally complex, derived independently from common bases. While morphological considerations alone suggest that such an approach must be taken to account for cases involving equipollent alternations, which have overt morphemes for both the causative and inchoative affixes, the question remains unsettled as to whether there are cases where such processes as causativization of inchoative verbs or anti-causativization of causative verbs are involved. In an attempt to answer this question, this paper examines the three approaches to the causative alternation by utilizing the possibility of having idiomatic interpretations as a probe into the syntactic structure. Evidence from idioms reveals that the common base approach still fares better. As a further consequence, postulation of a phonologically null morpheme is forced for some causative and inchoative affixes.
Are Mandarin Sandhi Tone 3 and Tone 2 the Same or Different? The Results of Functional Data Analysis This paper aims to take a closer look at tone sandhi, and in particular Tone 3 sandhi, a traditional research question in Chinese linguistics. Tone sandhi occurs whenever a prosodic context is met for disyllabic units of certain tonal combinations; the tone of the first syllable is changed from its underlying tone in a systematic manner. Tone 3 sandhi and yi sandhi are the two major tone sandhi phenomena in Mandarin 1 . Tone 3 sandhi means that a low tone is realised as a rising one when followed by another low tone. For example, zong3 tong3 (' 總統', means 'president') is realised as zong2 tong3. Tone 3 sandhi is also seen to be cyclically applied according to a hierarchical prosodic structure (Shih, 1997). The scope of yi sandhi, on the other hand, is somewhat limited. yi here means the number 'one' in Mandarin and its underlying high-level tone (Tone 1) undergoes respective changes according to its following morpheme being a classifier of a specific tone. 2 That is to say, yi sandhi requires matching phonological and morphological contexts.Myers and Tsay (2003) supplemented their study on tone sandhi in general with yi sandhi and concluded that more empirical data is needed to elucidate the most ubiquitous but still unclear nature of Tone 3 sandhi in Chinese phonology. Xu (2004, p. 796) similarly speculated the following on the mechanism of Tone 3 sandhi, "It is unclear whether the largely rising contour in the first L of L L is due to a complete change of the target to [rise] as in R or due to implementation of the same complex target as in isolated L with a time constraint. Further studies are needed to sort this out." This paper follows their advice and experiments with a method called Functional Data Analysis (FDA) to see whether further details can be exposed via this advanced statistic method. Functional Data Analysis (FDA) is used to investigate Tone 3 sandhi in Taiwan Mandarin. Tone 3 sandhi is a tone change phenomenon that arises when two low tones occur in succession resulting in the first tone being realised as a rising tone. Tone dyads T2T3 and T3T3 were compared in terms of their F 0 contours and velocity profiles. No difference was found between the F 0 contours of the two tone dyads. In contrast, velocity profiles showed an increased difference in the later part of the seemingly similar rising movements of T2 and sandhi T3, with a steeper rising-falling movement in the former than the latter. This research demonstrates that FDA can elucidate more detail in the time dimension than that of conventional techniques commonly employed in the current phonetic literature.
Of-constructions in the Predicate of demonstrate and show in Academic Discourse In Sinclair's (1991) book chapter "The meeting of lexis and grammar", he provides his insightful analysis on the word of to demonstrate the fusion of lexis and grammar. The word of, being one of the commonest English words, is conventionally conceived as a preposition with a postmodifying function. However, Sinclair underlines the encompassing roles of of. In particular, nominalization structures (e.g., the effectiveness of the telescope; the importance of symoblisation) have drawn much research attention (e.g., Halliday &amp; Martin, 1993;Kreyer, 2003;Quirk et al., 1985). For example, Quirk et al. (1985) investigate the substitutability of genitive constructions (e.g., China's economy) with ofnominalization (e.g., the economy of China) and the results suggest that several restrictions comply. In a similar vein, Kreyer (2003) investigates corpus data which also allow for a possible alternation between genitive and ofconstruction (e.g., the chairman of the committee and the committee's chairman) and shows that processability and degree of human involvement are two crucial factors influencing speakers' selection of the constructions. Specifically, ofconstruction is more likely to be selected when the second noun phrase is pre-modified (e.g., the son of the Royal Bucks secretary) and when the semantic relationship between the two noun phrases is more objective, attributive and partitive. In other words, in comparison with genitive constructions, of-constructions are hardly used when it comes to describing possessive, and kinship relations. The word of, along with other prepositions, also plays a role in nominalization structure. Prepositional phrases are conventionally regarded as postmodifiers (e.g., the overall ehthalpy charge for the conversion of graphite to cardon dioxide) to provide additional semantic content in scientific texts (Halliday &amp; Martin, 1993). Halliday and Martin examine scientific texts and show a high degree of nominalization in such texts. They also found that objectification (e.g., diamond is energetically unstable can be objectified into the energetic instability of diamond), or object-like status as a result of nominalization, allows the nominal group to be less negotiable. They also point out that an important function of nominalization is to structure scientific knowledge in a static, synoptic representation of reality. According to these two functions, nominalization plays a crucial role in constructing scientific discourse to represent objectivity.While previous studies have established the functions of of-constructions like demonstrating objectivity or expressing attributive and partitive relations between the two noun phrases (i.e., N1 and N2), few studies actually investigate if these functions would vary under different linguistic environment. To fill this research gap, we follow the co-occurrence approach (Gries &amp; Otani, 2010) to examining the distributive characteristics of two verbs, namely, demonstrate and show, in academic discourse. According to Gries and Otani (2010), the co-occurrence approach takes the position that "the distributional characteristics of the use of an item reveals many of its semantic and functional properties and purposes (p. 122)". This approach follows researchers such as Firth (1957) andBolinger (1968) to emphasize on the dependence of linguistic context for any lexical items. Gries and Otani (2010) also indicate the application of the underlying principles of this approach to a number of synonymy studies. In this study, we focus on demonstrate and show, two reporting verbs in academic discourse. A large number of studies on reporting verbs has been carried out, but they mainly focus on citational functions (e.g., Hyland, 1999), evaluation (e.g., Thompson &amp; Ye, 1991), and disciplinary variation (e.g., Hyland, 2000;Charles, 2006). Both demonstrate and show can be considered to be in the same sub-class of reporting verbs that report research activities which have been accepted by the reporting writer (Thomas &amp; Hawes, 1994). To the best of our knowledge, the co-occurrence approach has been rarely applied to the research of reporting verbs in academic writing.In sum, we would like to identify if the semantic relationships of N1 and N2 in ofconstructions (i.e., N1 of N2) would vary when associated with different neighboring words and if such semantic relationships can help us distinguish near-synonyms like demonstrate and show. In other words, we want to compare the types of-constructions predicated in demonstrate N1 of N2 and show N1 of N2. We ask the following two research questions:(1) How do the N1 of N2 predicates of demonstrate and show differ in terms of their distribution of N1-N2 semantic relationships? (2) What major functions can be found from the of-predicates that are associated with each verb? The rest of the paper is organized as follows. Section two presents a brief review of semantic analyses of of-phrases. Section three presents the current study and criteria used and Section four introduces our methodology. Sections five and six present our results. Finally, we discuss and conclude our study in sections seven and eight. This study investigates of-constructions in the predicates of two verbs, demonstrate and show, in academic discourse. A construction perspective is taken to examine how the two predicate constructions (&apos;demonstrate N1 of N2&apos; and &apos;show N1 of N2&apos;) would differ when the information-weighting of N1 and N2 are considered. The noun phrases were compared following Sinclair&apos;s (1991) conception of semantic headedness. He notes the peculiarity of of through the expression of double-headed constructions (i.e., considering both N1 and N2 as the semantic heads). This study adopts this framework and applies it to analyze the of-constructions of the two synonymous verbs. The results show that headedness of the of-constructions can be used to identify the subtle differences between the two synonyms. Demonstrate displays greater information weight predominated by double-headed constructions and tends to be associated with abstract conception. Show follows closely after demonstrate, but further analysis reveals that show tends to provide more &apos;relational&apos; evidence described in terms of partitive uses through nouns like variety, degree, incidence, level, rate and range.
Spatial Particles in English: A Quantitative Corpus-based Approach to the Conceptualization of Symmetry in Bodily Orientation Languages differ in their granularity in dividing up various aspects of the spatial domain.Linguists seem to have agreed that languages tend to be more resistant to adding a new lexical item to the existing set of closed-class words (Tyler &amp; Evans, 2003). Therefore, English Preposition Constructions often serve as a good candidate for the study of the conceptualization of spatial orientation.Among all the controversial topics related to English prepositions, we would like to focus on the notion of geometrical symmetry. Spatial orientation is a projection with respect to the axes of the visual field from a personal to an impersonal perspective (Langacker, 1987). Even though spatial particles such as up/down, in/out, before/after, contrast with one another in a geometrically symmetric way in the absolute Cartesian world, they are not necessarily defined by such oppositional features. Their meanings may be subject to the influence of the culturalspecific communities, thus lending themselves "semi-autonomous from and semi-dependent upon the conceptual space labeled by other spatial particles in the language" (Tyler &amp; Evans, 2003, p. 108). In other words, the contrast partners of the spatial particles along the same dimensions may not be straightforwardly oppositional. Therefore, the present study would like to investigate whether bipolar spatial particles (e.g., up/down) on the same spatial dimension (e.g., vertical axis) exhibits a symmetrical extension to similar sets of target domains in the real language use. This study investigates the conceptualization of our bodily orientation in a quantitative corpus-based approach of collostructional analysis. Based on the symbolic nature of constructions, we examine the correlation patterns of the covarying collexeme NPs and 13 major spatial particles in English Preposition Construction through exploratory statistical methods. The distributional patterns of the spatial particles have far-reaching implications for the embodiment of conceptual metaphors. It is concluded that the (a)symmetry of metaphorical patterns along each spatial dimension may be attributed to the recurring (a)symmetrical daily interaction and bodily experiences with the surrounding physical environment. While cultural specificity is of great concern for future study, a hypothesis for the implicational scale of conceptual symmetry in bodily orientation is proposed.
#Irony or #Sarcasm- A Quantitative and Qualitative Study Based on Twitter Philosophers and rhetoricians have been interested in irony and sarcasm for over 2500 years (Katz, 2000). In recent years, irony and sarcasm have been popular issues discussed qualitatively and quantitatively. Being a special way of language creativity, irony and sarcasm provide the opportunity to explore the interaction between cognition and language. Many frameworks have been proposed to illustrate the mechanisms underlying, such as Echoic Mention ( Wilson, 1981, 1986) Current study is with the aim to identify similarities and distinctions between irony and sarcasm by adopting quantitative sentiment analysis as well as qualitative content analysis. The result of quantitative sentiment analysis shows that sarcastic tweets are used with more positive tweets than ironic tweets. The result of content analysis corresponds to the result of quantitative sentiment analysis in identifying the aggressiveness of sarcasm. On the other hand, from content analysis it shows that irony owns two senses. The first sense of irony is equal to aggressive sarcasm with speaker awareness. Thus, tweets of first sense of irony may attack a specific target, and the speaker may tag his/her tweet irony because the tweet itself is ironic. These tweets though tagged as irony are in fact sarcastic tweets. Different from this, the tweets of second sense of irony is tagged to classify an event to be ironic. However, from the distribution in sentiment analysis and examples in content analysis, irony seems to be more broadly used in its second sense.
Collective Sentiment Classification based on User Leniency and Product Popularity In document-level sentiment classification, early studies have exploited language-based clues (e.g., n-grams) extracted from the textual content (Tur- ney, 2002;Pang et al., 2002), followed by recent studies which adapt the classifier to the reviews written by a specific user or written on a specific product (Tan et al., 2011;Seroussi et al., 2010;Speriosu et al., 2011;). Although the user-and product-aware methods exhibited better performance over the methods based on purely textual clues, most of them use only the user information (Tan et al., 2011;Seroussi et al., 2010;Speriosu et al., 2011), or they assume that the user and the product of a test review is known in advance ( ). These assumptions heavily limit their applicability in a real-world scenario where new users and new products are ceaselessly emerging.This paper proposes a method of collective sentiment classification that is aware of the user and the product of the target review, which benefits from the skewed distributions of polarity labels: intolerant users tend to report complaints while popular products are likely to receive praise. We introduce global features to encode these characteristics of a user and a product (referred to as user leniency and product popularity), and then compute the values of global features along with testing. Our method is therefore applicable to reviews written by users and on products that are not observed in the training data.Because global features depend on labels of test reviews while the labels reversely depend on the global features, we need to globally optimize a label configuration for a given set of reviews. In this study, we resort to approximate algorithms, easiest-first ( Tsuruoka and Tsujii, 2005) and twostage strategies (Krishnan and Manning, 2006), in decoding labels, and empirically compare their speed and accuracy.We evaluated our method on two real-world datasets with product (Maas et al., 2011) and user/product information (Blitzer et al., 2007). Experimental results demonstrated that the collective sentiment classification significantly improved the classification accuracy against the state-of-the-art methods, regardless of the choice of decoding strategy.The remainder of this paper is organized as follows. Section 2 discusses related work that exploits user and product information in a sentiment classification task. Then, Section 3 proposes a method that collectively classifies polarity of given set of reviews. Section 4 reports exper-imental results. Finally, Section 5 concludes this study and addresses future work. We propose a method of collective sentiment classification that assumes dependencies among labels of an input set of reviews. The key observation behind our method is that the distribution of polarity labels over reviews written by each user or written on each product is often skewed in the real world; intolerant users tend to report complaints while popular products are likely to receive praise. We encode these characteristics of users and products (referred to as user leniency and product popularity) by introducing global features in supervised learning. To resolve dependencies among labels of a given set of reviews, we explore two approximated decoding algorithms , &quot;easiest-first decoding&quot; and &quot;two-stage decoding&quot;. Experimental results on two real-world datasets with product and user/product information confirmed that our method contributed greatly to the classification accuracy.
Locative Postpositions and Conceptual Structure in Japanese Japanese postpositions ni and de indicate locations, which are exemplified below. As shown in (1a) de cannot be used with the stative verb aru 'be' to indicate a location where an object exists, and as shown in (1b) ni cannot occur with non-stative verb suberu 'slide' which expresses motion of an object. It can be argued that ni indicates "location of a state", while de indicates "location of an event or action". However, the locational verb aru 'be' shows the following alternation between ni and de. Since the postposition de can be used with the stative verb aru 'be' as shown in (2b), we cannot simply refer to the stative/non-stative distinction of the predicate involved in order to predict the distribution of ni and de.Although many descriptive and theoretical studies have discussed the syntactic and semantic properties of these postpositions (e.g. Kageyama, 1974;Kamio, 1980;Martin, 1987;Moriyama, 1988;Nakau, 1994a;1994b;1995;1998;Tera- mura, 1982, among others), none of them have fully accounted for the distribution of the postposition ni and de and the semantic difference between them.In this paper we consider the semantic difference between the two locative postpositions and give an account of the semantic structures for sentences involving locative ni-or de-phrases within the framework of Jackendoff's (1983;1990;1991) Conceptual Semantics. This paper proposes two syntax-semantics correspondence rules which consistently account for the distribution of Japanese loca-tive postpositions ni and de. We demonstrate how to adapt the machinery of the occurrence of the postpositions based on the assumption of Conceptual Semantics (Jack-endoff, 1983; 1990; 1991) to fit the organization of Japanese grammar. The correspondence rules correlate with semantic distinction of verb classes: the semantic field distinction between Spatial and Temporal with respect to the BE-function encoded in the lexical conceptual structure of several verbs. As a result, this paper elucidates the mechanism of locative alternation of the verb aru &apos;be&apos;, which has not been fully explicated.
Transliteration Systems Across Indian Languages Using Parallel Corpora India is home to languages from four language families namely Indo-Aryan, Dravidian, Austroasiatic and Tibeto-Burman. There are 22 official languages and more than 1000 dialects, which are written in more than 14 different scripts 1 in this country. Hindi, an Indo-Aryan language, written in Devanagari, is the lingua-franca of India (Ma- sica, 1993, p. 6). Most Indians are orally proficient in Hindi while they lack a good proficiency in reading and writing it. In this work, we come up with transliteration systems, so that non-native speakers of Hindi don't face a problem in reading Hindi script. We considered 7 Indian languages, including 4 Indo-Aryan (Punjabi, Gujarati, Urdu and Bengali) and 3 Dravidian (Telugu, Tamil and Malayalam) languages, for this task. The quantity of Hindi literature (especially online) is more than twice as in any other Indian language. There are approximately 107 newspapers 2 , 15 online newspapers 3 and 94067 Wikipedia articles 4 (reported in March 2013), which are published in Hindi. The transliteration systems will be helpful for nonHindi readers to understand these as well as various other existing Hindi resources.As the transliteration task has to be done for 7 languages, a rule-based system would become very expensive. The cost associated with crafting exhaustive rule-sets for transliteration has already been demostrated in works on Hindi-Punjabi ( Goyal and Lehal, 2009), Hindi-Gujarati ( Patel and Pareek, 2009) and Hindi-Urdu ( Malik et al., 2009;Lehal and Saini, 2010). In this work, we have modelled the task of transliteration as a noisy channel model with minimum error rate training (Och, 2003). However, such a statistical modelling needs an ample amount of data for training and testing. The data is extracted from an Indian language sentence aligned parallel corpora available for 10 Indian languages. These sentences are automatically word aligned across the languages. Since these languages are written in different scripts, we have used an Indian modification of the soundex algorithm (Russell and Odell, 1918) (henceforth Indic-Soundex) for a normalized language representation. Extraction of the transliteration pairs (two words having the similar pronunciation) is then followed by Longest Common Subsequence (henceforth LCS) algorithm, a string similarity algorithm. The extracted pairs are evaluated manually by annotators and the accuracies are calculated. We found promising results as far as the accuracies of these extracted pairs are concerned. These transliteration pairs are then used to train the transliteration systems. Various evaluation tests are performed on these transliteration systems which confirm the high accuracy of these transliteration systems. Though the best system was nearly 70% accurate on word-level, the character-level accuracies (greater than 70% for all systems) along with the encouraging results from the human evaluations, clearly show that these transliterations are good enough for a typical Indian reader to easily interpret the text. Hindi is the lingua-franca of India. Although all non-native speakers can communicate well in Hindi, there are only a few who can read and write in it. In this work, we aim to bridge this gap by building transliteration systems that could transliter-ate Hindi into at-least 7 other Indian languages. The transliteration systems are developed as a reading aid for non-Hindi readers. The systems are trained on the translit-eration pairs extracted automatically from a parallel corpora. All the transliteration systems perform satisfactorily for a non-Hindi reader to understand a Hindi text.
Classifying Questions in Question Answering System Using Finite State Machines with a Simple Learning Approach Classifying a question to its appropriate class in an important subtask and plays a substantial role in the Question Answering (QA) systems. It can provide some useful clues for identifying potential answers in large collections of texts. The goal of this current work is to develop a classifier using Finite State Machines (FSM) to classify a set of questions into their relevant classes. Various techniques have already been tried by the community either to classify a question to its relevant class or to a finer subclass of a specific class.Results of the error analysis acquired from an open domain QA system demonstrates that more or less 36.4% of the errors were generated due to the wrong classification of questions ( Moldovan et al., 2003). So, this issue can be highlighted as a subject of interest and has arisen the aim of developing more accurate question classifiers (Zhang and W. Sun . Usually the answers generated from the classified questions have to be exact in nature and the size of the answer has to be within a restricted size ( Peters et al., 2002;Voorhees, 2001) which greatly emphasizes the need of an accurate question classifier.  Mit- chell, 2002) method was also used in the question classification task with limited accuracy rate of around 79.2%. In another work (Fan Bu et al., 2010), where a function-based question classifi-cation technique is proposed, the authors of that paper claimed to have achieved as high as 86% precision levels for some classes of questions. Some attempts have been made to develop a language independent question classifier (Thamar Solorio et al., 2004) with not a mentionable success rate. This work 1 focuses on the questions posed only in English language and uses questions from the Question Answering (QA) track of the Conference and Labs of the Evaluation Forum (CLEF) (QA4MRE, 2013). It classifies the questions into 5 major classes namely Factoid (FA), Definition (DE), Reason/Purpose (RP), Procedure (PR) and Opinion (OP) Class. CLEF QA track have some diverse types of questions and we are required to fit each of the questions into any of the above mentioned classes. Factoid class of questions are mainly fact oriented questions, asking for the name of a person, a location, some numerical quantity, the day on which something happened such as 'What percentage of people in Bangladesh relies on medical insurance for health care?', 'What is the price of an airconditioning system?' etc. Definition questions such as 'What/Who is XYZ?' asks for the meaning of something or important information about someone or an organization. 'What is avian influenza?', 'Define SME', 'What is the meaning of Bluetooth signal?' are some examples of the definition class questions. Reason/Purpose questions ask for the reasons/goals for something happening. 'Why was Ziaur Karim sentenced to death?' and 'What were the objectives of the National meeting?' are the example questions of this class. Procedural questions ask for a set of actions which is an accepted way of doing something. Such as: 'How do you calculate the monthly gross salary in your office?' Opinion questions ask for the opinions, feelings, ideas about people, topics or events. An example question of this type may be like 'What did the Academic Council think about the syllabus of informatics department?' A question is either mapped to only one class or may be classified as 'other'.The next section of the paper describes the procedure used to create the states and transitions in the FSMs involving a simple learning mechanism and the section 3 presents the data set for the experimental verification of the procedures and outcome of the experiments followed by a section covering a discussion about the future works. Question Classification plays a significant part in Question Answering system. In order to obtain a classifier, we present in this paper 1 a pragmatic approach that utilizes simple sentence structures observed and learned from the question sentence patterns, trains a set of Finite State Machines (FSM) based on keywords appearing in the sentences and uses the trained FSMs to classify various questions to their relevant classes. Although, questions can be placed using various syntactic structures and keywords, we have carefully observed that this variation is within a small finite limit and can be traced down using a limited number of FSMs and a simple semantic understanding instead of using complex semantic analysis. WordNet semantic meaning of various keywords to extend the FSMs capability to accept a wide variety of wording used in the questions. Various kinds of questions written in English language and belonging to diverse classes from the Conference and Labs of the Evaluation Forum&apos;s Question Answering track are used for the training purpose and a separate set of questions from the same track is used for analyzing the FSMs competence to map the questions to one of the recognizable classes. With the use of learning strategies and application of simple voting functions along with training the weights for the keywords appearing in the questions, we have managed to achieve a classification accuracy as high as 94%. The system was trained by placing questions in various orders to see if the system built up from those orders have any subtle impact on the accuracy rate. The usability of this approach lies in its simplicity and yet it performs well to cope up with various sentence patterns.
Use of Combined Topic Models in Unsupervised Domain Adaptation for Word Sense Disambiguation In this paper, we propose an unsupervised method of domain adaptation for Word Sense Disambiguation (WSD) using topic models.An inductive learning method is used in many tasks of natural language processing. In inductive learning, training data is created from corpus A, and a classifier learns from the training data. A original task is solved by using the classifier. During this analysis, the data for the task is in corpus B that differs from the domain of corpus A. In cases, the classifier learned from corpus A (i.e., the source domain) cannot analyze the data of corpus B (i.e., the target domain). This problem is called the domain adaptation problem, which is also regarded as a component of transfer learning in the field of machine learning. The domain adaptation problem has been extensively researched in recent years.The methods of domain adaptation can be divided into two groups from the viewpoint of whether labeled data is to be used in the target domain. When using labeled data, it is called supervised learning, while unsupervised learning does not use labeled data. There is substantial research on supervised learning techniques. Conversely, not much attention has been paid to unsupervised learning because of low precision; however, we adopt the unsupervised learning approach because it is does not require labeling.Shinnou and Sasaki examined the unsupervised domain adaptation for WSD ( Shinnou and Sasaki, 2013). In their study, the topic model is built from the target domain corpus, and topic features constructed from the topic model are added to training data in both source and target domains. As a result, the accuracy of the classifier made by training data in the source domain is improved; however, in their study, the topic model is made by only the target domain. As indicated by Shinnou, it is unclear how topic models can be used for WSD. Further, in the domain adaptation task for WSD, the following three types of topic models are available: (1) a topic model constructed from the source domain corpus; (2) a topic model constructed from the target domain corpus, and (3) a topic model constructed from both domains. It is also unclear whether there is an effective combination of these topic models. The aim of this paper is to illuminate the latter problem.The use of topic models in this paper adopts a similar approach to Shinnou ( Shinnou and Sasaki, 2013). Basically, three topic features made from each topic model are added to the normal features used for WSD, and a classifier learns using the ex-tended features; however, the topic features constructed from the source domain have weights describing the similarity between the source corpus and the entire corpus because the topic features made from the source domain do not necessarily improve the accuracy of WSD, and sometimes actually reduce the accuracy. When it can be determined that the topic features made from the source domain are effective for WSD, the value of weight r is approximately 1. In contrast, when it can be determined that the topic features made from the source domain are not effective for WSD, the value of the weight r is approximately 0.The weight r is set by following equation:where S is the source domain corpus, T is the target domain corpus, and S+T is the combined domain corpus; further, KL(A,B) is the Kullback Leibler (KL) divergence of A on criterion B.In our experiments, we chose three domains, PB (books), OC (Yahoo! Chie Bukuro), and PN (news) in the BCCWJ corpus, and selected 17 ambiguous words that had a comparatively high frequency of appearance in each domain.Domain adaptation has the following six transitions: (1) from PB to OC, (2), from OC to PB, (3), from PB to PN, (4), from PN to PB, (5), from OC to PN, and (6) from PN to OC. In every domain adaptation, we conducted experiments by varying the combination of the topic features. Through our experiments, we show the effectiveness of our proposed method. Topic models can be used in an unsuper-vised domain adaptation for Word Sense Disambiguation (WSD). In the domain adaptation task, three types of topic models are available: (1) a topic model constructed from the source domain corpus: (2) a topic model constructed from the target domain corpus, and (3) a topic model constructed from both domains. Basically, three topic features made from each topic model are added to the normal feature used for WSD. By using the extended features, SVM learns and solves WSD. However, the topic features constructed from source domain have weights describing the similarity between the source corpus and the entire corpus because the topic features made from the source domain can reduce the accuracy of WSD. In six transitions of domain adaptation using three domains, we conducted experiments by varying the combination of topic features, and show the effectiveness of the proposed method.
Vietnamese Text Accent Restoration With Statistical Machine Translation Accent predicting problem refers to the situation where accents are removed (e.g. by some email preprocessing systems), cannot be entered (e.g. by standard English keyboards), or not explicitly represented in the text (e.g. in Arabic). We resolve the languages using Roman characters in writing together with additional accent and diacritical marks. These languages include European languages such as Spanish and French and Asian languages such as Chinese Pinyin and Vietnamese.Vietnamese accentless texts coexist with official Vietnamese texts and it is relatively common texts on the internet. Official Vietnamese language is a complex language with many accent (including acute, grave, hook, tilder, and dot-below) and Latinh alphabets. These are two inseparable components in Vietnamese. However, many Vietnamese choose to use accentless Vietnamese because it is easier and quickly to type. For example, a official Vietnamese sentence: chúng tôi ss bay ti Hà Ni vào chh nhht ('We will fly to Hanoi on Sunday') will be written as an Vietnamese accentless sentence as chung toi se bay toi Ha Noi vao chu nhat. Decoding such a sentence could be quite hard for both human and machine because of lexical ambiguity. For instance, the accentless term "toi" can easily lead to confusion between the original Vietnamese "tôi" ('we') and the plausible alternative "ti" ('to').Nowadays, the application of information technology to exchanging information is more and more popular. We daily receive many of emails, SMS but the majority of them are without accents which may cause troubles for interpreting the meaning. Therefore, automatic accent restoration of accentless Vietnamese texts have many of applications such as automatically inserting accent to emails, instant message, SMS are written without diacritics Vietnamese, or assistant for website administration in which accent Vietnamese is required. Therefore, it is essential to develop supporting tools which can automatically insert accent to Vietnamese texts.Accent predicting problem is the particular problem of lexical disambiguation. The recent approach to lexical disambiguation is corpus-based such as n-gram, dictionary of phrases, ...In this paper, we propose the method for automatic accent restoration using Phrase-based SMT. Vietnamese accentless sentence and Vietnamese accent sentence (office Vietnamese sentence) will be source and target sentence in Phrase based SMT, respectively. We also improve quality of accent predicting by applying some techniques such as adding dictionary, changing size of n-gram of language model. The experiment results with Vietnamese corpus showed that our approach achieves promising results.The rest of this paper is organised as follows. Related works are mentioned in Section 2. The methods for accent restoration using SMT are proposed in Section 3. In Section 4, we describe the experiments and results for evaluating the proposed methods. Finally, Section 5 concludes the paper. Vietnamese accentless texts exist on parallel with official vietnamese documents and play an important role in instant message, mobile SMS and online searching. Understanding correctly these texts is not simple because of the lexical ambiguity caused by the diversity in adding diacritics to a given accentless sequence. There have been some methods for solving the vietnamese accentless texts problem known as accent prediction and they have obtained promising results. Those methods are usually based on distance matching, n-gram, dictionary of words and phrases and heuristic techniques. In this paper, we propose a new method solving the accent prediction. Our method combine the strength of previous methods (combining n-gram method and phrase dictionary in general). This method considers the accent predicting as statistical machine translation (SMT) problem with source language as accentless texts and target language as accent texts, respectively. We also improve quality of accent predicting by applying some techniques such as adding dictionary , changing order of language model and tuning. The achieved result and the ability to enhance proposed system are obviously promising.
A Compact FP-tree for Fast Frequent Pattern Retrieval Frequent pattern mining is an important task because its results can be used in a wide range of mining tasks, such as association rule, correlation, causality, sequential pattern, etc. as reviewed by Han (2000). In some mining tasks (e.g. association rule, correlation, or causality), frequent patterns are used as intermediate data for computing final results, so there is no need to access these patterns again. However, in some other tasks, such as query suggestion (or query recommendation) (Li, 2008), when a user enter a keyword, the search engine will recommend the potential phrases (or patterns) the user may want to use, in order to: (a) save time for users, (b) make the convenience of use, and even (c) guide the user in case he/she is not sure about what to search for. In such tasks, we need to frequently search for frequent patterns containing a certain keyword (or phrase), hence, we want to have a method that supports quick retrieval of patterns. In information retrieval, one of the contemporary methods for fast retrieval of documents containing a certain word (or phrase) is inverted indexing (Manning, et al, 2008), which manages a mapping from a keyword to a set of documents containing it. Thus, given a keyword, we will quickly have the list of related documents.We found that FP-tree can be used as an inverted indexing which can provide us a list of patterns containing a certain item. Thus, we propose to modify FP-tree to store the frequent patterns for later fast retrieval. The difference between our FP-tree and the original one is:• The original FP-tree stores the compact version of a transaction database, and an algorithm (called FP-growth) is used to find out the frequent patterns; • Our FP-tree stores the frequent patterns for quick access, so each path in the tree is already a pattern. Since the number of frequent patterns generated from a transaction database can be very large, we propose an algorithm to compress them into a much smaller (compact) set and store in FP-tree data structure. We also propose to modify related algorithms to make FP-tree compatible with frequent patterns instead of transaction data. We call the tree of compact pattern set compact FP-tree. With the compact FP-tree, it is easily to restore the original frequent pattern set. The results of the experiments on benchmark transaction database show that our compact FP-tree has very good compression ratio.Our paper is organized as follows: Section 2 introduces about FP-tree, and summarizes some typical literature; Section 3 introduces our compact FP-tree and the algorithms for compressing frequent patterns as well as restoring the original pattern set; Experiment and evaluation is discussed in Section 4 while conclusion and future work are provided in Section 5. Frequent patterns are useful in many data mining problems including query suggestion. Frequent patterns can be mined through frequent pattern tree (FP-tree) data structure which is used to store the compact (or compressed) representation of a transaction database (Han, et al, 2000). In this paper, we propose an algorithm to compress frequent pattern set into a smaller one, and store the set in a modified version of FP-tree (called compact FP-tree) as an inverted indexing of patterns for later quick retrieval (for query suggestion). With the compact FP-tree, we can also restore the original frequent pattern set. Our experiment results show that our compact FP-tree has a very good compression ratio, especially on sparse dataset which is the nature of query log.
ML-Tuned Constraint Grammars Constraint Grammar (CG) is a rule-based paradigm for Natural Language Parsing (NLP), first introduced by Karlsson et al. (1995). Partof-speech tagging and syntactic parses are achieved by adding, removing, selecting or substituting form and function tags on tokens in running text. Rules express linguistic contextual constraints and are written by hand and applied sequentially and iteratively, ordered in batches of increasing heuristicity and incrementally reducing ambiguity from morphologically analyzed input by removing (or changing) readings from so-called readings cohorts (consisting of all possible readings for a given token), -optimally until only one (correct) reading remains for each token. The method draws robustness from the fact that it is reductionist rather than generative -even unforeseen or erroneous input can be parsed by letting the last reading survive even if there are rules that would have removed it in a different context. Typical CG rules consist of an operator (e.g. REMOVE, SELECT), a target and one or more contextual constraints that may be linked to each other: Rule (a), for instance, removes a target finite verb reading (VFIN) if there is an unambiguous (C) article or determiner 1 position to the left (-), while rule (b) selects a finite verb reading, if there is a personal pronoun in the nominative immediately to the left, and no (NOT) other finite verb is found anywhere to the right (*1).Mature Constraint Grammars can achieve very high accuracy, but contain thousands of rules and are expensive to build from scratch, traditionally requiring extensive lexica and years of expert labor. Since grammars are not datadriven in the statistical sense of the word, domain adaptation, for instance for speech (Bick 2011) or historical texts (Bick 2005), is traditionally achieved by extending an existing general grammar for the language in question, and by using specialized lexica or two-level text normalization. However, due to its innate complexity, the general underlying grammar as a whole has properties that do not easily lend themselves to manual modification. Changes and extensions will usually be made at the level of individual rules, not rule interactions or rule regrouping. Thus, with thousands of interacting rules, it is difficult for a human grammarian to exactly predict the effect of rule placement, i.e. if a rule is run earlier or later in the sequence. In particular, rules with so-called C-conditions (asking for unambiguous context), may profit from another, earlier rule acting on the context tokens involved in the C-condition. Feed-back from corpus runs will pinpoint rules that make errors, and even allow to trace the effect on other rules applied later on the same sentence, but such debugging is cumbersome and will not provide information on missed-out positive, rather than negative, rule interaction. The question is therefore, whether a handcorrected gold corpus and machine-learning techniques could be used to improve performance by data-driven rule ordering or rule adaptation, applied to existing, manual grammars. The method would not only allow to optimize general-purpose grammars, but also to adapt a grammar in the face of domain variation without actually changing or adding any rules manually. Of course the technique will only work if a compatible gold-annotation corpus exists for the target domain, but even creating manually-revised training data from scratch for the task at hand, may be warranted if it then allows using an existing unmaintained or "black box" grammar. Other areas where ML rule tuning of existing grammars may be of use, is cross-language porting of grammars between closely related languages, and so-called barebones Constraint Grammars (Bick 2012), where grammars have to cope with heuristically analyzed input and correspondingly skewed ambiguity patterns. In such grammars, linguistic intuition may not adequately reflect inputspecific disambiguation needs, and profit from data-driven tuning. In this paper we present a new method for machine learning-based optimization of linguist-written Constraint Grammars. The effect of rule ordering/sorting, grammar-sectioning and systematic rule changes is discussed and quantitatively evaluated. The F-score improvement was 0.41 percentage points for a mature (Danish) tagging grammar, and 1.36 percentage points for a half-size grammar, translating into a 7-15% error reduction relative to the performance of the untuned grammars.
Comparative Analyses of Textual Contents and Styles of Five Major Japanese Newspapers Newspapers are an important media from which people obtain a wide variety of information, ranging from contemporary political and economic issues to ordinary incidents. Particularly in Japan, where newspapers delivery remains popular, many people read them in their own spaces and have continued to use them as popular information resources, even after the advent and spread of the Web. According to Nihon Shinbun Kyokai (2012), 87.3 percent people in Japan read newspapers, which is second only to television (98.7 percent) among the five surveyed media including newspaper, television, radio, magazines, and Internet.There are five major newspapers in Japan: Asahi, Mainichi, Nikkei, Sankei, and Yomiuri, which have publication offices in Tokyo, Osaka, and other areas, and are distributed to almost all regions of Japan. Though all of these newspapers regard correctness, neutrality and unbiased reporting as important, they have their own opinions and ideologies. According to the Hosyu (conservative)-Kakushin (liberal) image survey by Shinbun Tsushin Chosakai (2009), the five major newspapers scored as follows: Yomiuri 5.6, Sankei 5.3, Nikkei 5.2, Mainichi 5.0, and Asahi 4.4, where larger numbers indicate a newspaper perceived as more conservative and smaller numbers indicate a newspaper perceived as more liberal. Excluding Nikkei, which is a specialized newspaper that focuses on economic issues, the survey results show that people see Yomiuri and Sankei as more conservative and Mainichi and Asahi as more liberal. These differences might affect the textual characteristics of newspapers; however, they have not been investigated in a comprehensive and systematic manner.With the development of natural language processing techniques and the creation of many online text corpora, quantitative text analysis has been expanding in scope. Such methods have begun to be recognized as important tools for solving many theoretical and practical social science research questions. Particular to newspapers, some studies have applied these quantitative text analysis methods. For example, Newman and Block (2006) determined topics using a probabilistic mixture decomposition method with the Pennsylvania Gazette, a major colonial U.S. newspaper that was in publication from 1728 to 1800. Higuchi (2011) investigated whether there is significant association between the content of newspaper articles and social consciousness trends by using three Japanese newspapers, Asahi, Yomiuri, and Mainichi. However, these previous newspaper text analyses focused only on the content. Examination of textual characteristics, such as styles of texts, which can reveal attitudes, personalities, psychologies, emotions, text genres, and authors (Argamon et al., 2007;Suzuki, 2009), have rarely been focused despite being intriguing aspects for analysis.Therefore, this study analyzes the differences among editorials in the five major Japanese newspapers. Among the many types of articles, editorials are one of the most intriguing and colorful, wherein respective viewpoints are expressed (Goto, 1999), and thus are good materials for investigation. We first apply principal component analysis (PCA) to observe the overall distribution of these texts in scatter plots and investigate the factors affecting the textual characteristics. Next, we apply random forests classification experiments using newspapers, editorial dates, and ideology types as classes in order to examine the classification performance and important features of these experiments. Throughout these analyses, we use function words as well as content words as features, which is useful for investigating the similarities and differences of these classes. In addition, these features enable us to clarify which of the two characteristics-styles or content-more powerfully affects these classification types, which is also an interesting text analysis topic. This study contributes to text classification studies by deliberately comparing the classification performance provided by different feature sets, function words and content words. In addition, this study provides empirical findings useful for understanding the characteristics of the five newspapers. Newspapers remain an important media from which people obtain a wide variety of information. In Japan, there are five major newspapers, having their own opinions and ideologies. Although these are readily recognized, they are infrequently investigated from the viewpoint of their textual characteristics. This study analyzes these differences among the five newspaper editorials. We apply morphological analysis and count the frequency of morphemes within the text data. We then apply principal component analysis and random forests classification experiments to examine their similarities and differences. Throughout these statistical analyses, we use function words and content words as features, which enables us to determine which of the two characteristics-styles or content-more powerfully affects the classification types. This study contributes to text classification studies by deliberately comparing the classification performances provided by different feature sets, function words and content words. In addition, this study will provide an empirical basis for understanding the similarities and differences among the five newspapers.
The Island Effect in Postverbal Constructions in Japanese Japanese is descriptively a verb-final language. In some cases, however, non-verbal elements come at the end of sentences, as shown in (1). 1,2 (1) a. Taro-ga ano mise de tabe-ta yo, Taro-NOM that shop at eat-PAST FP susi-o. sushi-ACC 'Taro ate sushi at that shop.' b. Taro-ga susi-o tabe-ta yo, Taro-NOM sushi-ACC eat-PAST FP, ano mise de. that shop at In (1a), the object susi-o 'sushi-ACC' appears in postverbal position, and in (1b), the adverbial phrase ano mise de 'at that shop' does so. I refer t o t h e s e p h e n o m e n a a s t h e p o s t v e r b a l construction in Japanese (JPVC), and refer to 1 The relevant elements are in boldface. 2 The abbreviations used in glossing the data are as follows: ACC = accusative, DAT =dative, FP = sentence-final particle, NEG = negative, NOM = nominative, TOP = topic. elements in sentence-final position as postverbal elements (PVE). 3 Some researchers (e.g., Endo, 1989;Kaiser, 1999;Whitman, 2000;Tanaka, 2001;and Abe, 2004) claim that the PVE is derived by movement because of the obedience of the PVE to island constraints such as the so-called Complex NP Constraint (CNPC), as shown in (2). In (2), e is used to mark the position associated with the moved element, namely the PVE, and the identical subscript indicates that the PVE corresponds to e. It has been generally assumed that a violation of island constraints indicates that the relevant syntactic phenomena involves movement. That is, if what look like displacements violate island constraints but remain acceptable, this means that they should not be derived by movement. A careful examination of postverbal constructions in Japanese reveals that no movement is involved in the derivation of the construction despite the fact that in some cases island effects are observed. The effects, which have up to now been dealt with purely in syntax, can receive a better account in terms of language processing. This suggests that the human parser should undertake explanations of part of the output of the competence system.
Evaluation of Corpus-Assisted Spanish Learning The trend of using corpus has expanded into all sub-areas of linguistics, including applied fields such as foreign language teaching and learning. According to Lee (2010), almost 360 corpora have been constructed for various purposes in 57 languages. Sixty-three percent of these corpora have been analyzed in previous research on language analysis and English teaching. In the past decade, the majority of corpus users have been researchers and teachers. Therefore, we are interested in extending the usage of corpus to foreign language learners, and studying how the perspective of corpus application can benefit these learners. Moreover, instead of English, we have selected Spanish as the target language of this research because the popularity of second foreign language acquisition is increasing in Taiwan, and multilingualism has become a novel research topic in applied linguistics.Among the related literature, the application of existing corpora in teaching or learning has focused primarily on native corpus. Moreover, although there have been several studies on parallel corpus, very few have examined learners' corpus. The reason that less attention has been drawn to the evaluation of effectiveness might be attributable to the lack of access to parallel and learners' corpora. Moreover, to our knowledge, no study has compared the various types of corpora. The discussed reasons have motivated us to conduct this research. This study examines the advantages and disadvantages of the three types of corpora from the learners' perspective, and applies them complementarily to maximize the learning outcomes.By applying extant sources, language learners can learn how to apply created corpora for the self-learning of foreign languages. As the final goal, we hope that learners can capitalize on the complementary merits of various types of corpora to achieve the best results, and maximize the efficiency of their learning through the application of information technology. In the development of corpus linguistics, the creation of corpora has had a critical role in corpus-based studies. The majority of created corpora have been associated with English and native languages, while other languages and types of corpora have received relatively less attention. Because an increasing number of corpora have been constructed, and each corpus is constructed for a definite purpose, this study identifies the functions of corpora and combines the values of various types of corpora for auto-learning based on the existing corpora. Specifically, the following three corpora are adopted: (a) the Corpus of Spanish; (b) the Corpus of Taiwanese Learners of Spanish; and (c) the Parallel Corpus of Spanish, English, and Chinese. These corpora represent a type of native, learner, and parallel language, respectively. We apply these corpora as auxiliary resources to identify the advantages of applying various types of corpora in language learning from a learner&apos;s perspective. In the environment of auto-learning, 28 participants completed frequency questions related to semantic and lexical aspects. After analyzing the questionnaire data, we obtained the following findings: (a) the native corpus requires a more advanced level of Spanish proficiency to manage ampler and deeper context; (b) the learners&apos; corpus facilitates the distinction between error and correction during the learning process; (c) the parallel corpus assists learners in connecting form and meaning; (d) learning is more efficient if the learner can capitalizes on specific functions provided by various corpora in the application order of parallel, learner and native corpora.
Automatic Clause Boundary Annotation in the Hindi Treebank Clause boundary is important for various NLP systems like machine translation, parallel corpora alignment, parsing etc. (Leffa, 1998;Gadde et al., 2010;Ejerhed, 1988). This information is furnished by an automatic tool often called clause boundary identifier. Both data driven (Puscasu, 2004) and rule based (Leffa, 1998) approaches have been explored in past for building such a system, however recent inclination has been towards the data-driven approaches due to their robustness. In order to built a clause boundary identifier, using data driven approach, one needs to have a good clause boundary annotated corpus for training. At present, such a resource is not available in Hindi. However, the syntactic treebank with dependency relations annotated has been developed. We wish to expand this manually annotated treebank with the clause boundary annotation in this work.Several insightful approaches, in past, have enriched existing resources by first utilizing the explicit information available to derive new implicit information ( Klein and Manning, 2003;Kosaraju et al., 2012) and then explicitly annotating it back into the original resource. Conversion of a treebank from one grammatical formalism to the other serves as a good example of how an implicit information can be mapped and extracted (Xia and Palmer, 2001). Instead of starting from scratch, an already existing treebank is transformed into a new grammatical formalism.  is one such effort for Hindi. They have automatically transformed dependency structures to phrase structure utilizing Hindi Dependency Treebank and Hindi PropBank (Palmer et al., 2009). Following such insights, we attempt to automatically generate the clause marked data from existing resources for Hindi. We propose that the clause information is implicitly encoded in the Hindi Dependency Treebank and thus, can be extracted and explicitly specified as an additional layer of annotation in the treebank. This paper presents a systematic approach towards incorporating clausal information in the Hindi Dependency Treebank utilizing the information (mopho-syntactic, dependency etc.) already available in the treebank. This paper is structured as follows: In Section 2, we discuss the related works that have been done earlier on clause identification and classification. In Section 3, we talk about clause and its types. In Section 4, we discuss about Hindi-Urdu treebank. Section 5 describes our methodology and in Section 6 we discuss the results achieved and outline the issues faced. In Section 7, we conclude with some future directions. In this paper, we propose a method for automatic clause boundary annotation in the Hindi Dependency Treebank. We show that the clausal information implicitly encoded in a dependency structure can be made explicit with no or less human intervention. We exercised the proposed approach on 16,000 sentences of Hindi Dependency Treebank. Our approach gives an accuracy of 94.44% for clause boundary identification evaluated over 238 clauses. The resultant corpus has varied usages and can be utilized for developing a statistical clause boundary identifier.
Myths in Korean Morphology and Their Computational Implications This paper aims at examining some popular misanalyses in Korean morphology. Focusing on the verbal ha-and what is called the present tense marker -(nu)n-, we will see that, contrary to popular myth, they cannot be analyzed as a derivational affix and as a present tense marker, respectively. In providing reasonable analyses of them, we will consider some implications of the misanalyses, especially from a computational point of view.Most Korean linguists assume that the ha-in kongpwu-ha-('to study'), for example, is a derivational affix and, hence, kongpwu-ha-as a whole is a verb. 1 However, as we can see shortly, ha-itself is an independent word and [kongpwu ha-] is a phrase. More Korean linguists assume that the element -(nu)n-is a present tense marker. However, the Korean tense system becomes far simpler, if we assume that the present tense marker is null (-ø-) rather than -(nu)n-. This paper examines some popular misanalyses in Korean morphology. For example, contrary to popular myth, the verbal ha-and the element-(nu)n-cannot be analyzed as a derivational affix and as a present tense marker, respectively. We will see that ha-is an independent word and that-(nu)n-is part of a portmanteau morph. In providing reasonable analyses of them, we will consider some computational implications of the misanalyses. It is really mysterious that such wrong analyses can become so popular in a scientific field of linguistics.
iPad Reading: An Innovative Approach to New Literacies New technologies provide new ways to read, and how readers interact with electronic texts-through such new devices as iPads, Tablet PCs, Amazon Kindle, and Sony's Reader Digital Book--is worthy of further exploration (Larson, 2010). The emergence of these mobile devices intensifies the need to integrate digital technologies and equip learners with new literacy skills that are different from traditional printed reading and writing skills (Coiro, Knobel, Lankshere, and Leu, 2008). Among these new devices, iPads--equipped with LCDs, Internet surfing functions, e-books, and application (app) downloading capabilities--are the most popular medium of screen-based reading (Connell, Bayliss, and Famer, 2012) and can facilitate new literacy practices (Coiro, et al., 2008). Godwin-Jones (2010) pointed out that iPads have contributed to the rise of e-books because they are portable, have off-line reading functions, and provide a huge storage capacity within a small device. Apple applications, such as games and incorporating audio functions into e-books, have also created learning opportunities (Godwin-Jones, 2011). The personal profiles archived in iPads, including the history of browsing pages and look-up behaviors in online dictionaries, can create a personal learning history. Moreover, the iPad's built-in camera enables users to capture pictures or videos to use in creating responses to readings (Hutchison, Beschorner, and Schmide-Crawford, 2012). The convenience of such mobile devices as iPads has the potential to encourage learner autonomy and combine formal and informal learning (Godwin-Jones, 2011).Studies of iPad reading mostly focus on how iPads support L1 readers in primary literacy programs (Ellis, 2011); relatively few examine how iPads can assist L2 learners' language learning in tertiary education (Sekiguchi, 2011). One exception is Sekiguchi (2011), who experimented with the use of iPads as a learning resource outside the classroom with 20 EFL undergraduate students in Japan. The results showed that students improved their self-regulated learning by using iPads to tweet about their learning experiences.The general trend has been to use iPads to replace textbooks or to provide supplementary reading material outside classrooms. Few studies have examined the use of iPads as a major learning instrument or as part of the curriculum to facilitate learning (Hutchison, et al., 2012). Hutchison et al. (2012) reported on the exploratory use of iPads among fourth graders for literacy instruction. Students read a story and used graphic organizer apps to create a visual diagram to describe it, which led to the creative presentation of ideas. Students also drew illustrations using the Doodle Buddy app to show their understanding, which provided opportunities to reread materials and thereby promoted better reading comprehension. These activities, which successfully met multi-literacy goals and helped learners convey meaning digitally, inspired this researcher to design iPad reading activities to promote literacy instruction. Because the integration of iPads into language education is relatively new and still underexplored, this iPad project aimed to investigate the use of iPads as a learning tool in a college EFL reading class and to explore these language learners' perceptions of iPad reading. The researcher posed two questions: 1. How can iPads be integrated into EFL reading instruction? 2. What characterized students' iPad learning experiences? This study aimed to investigate the use of iPads as a learning tool for college-level EFL students and to explore these language learners&apos; perceptions of iPad reading. Drawn from an intermediate EFL reading class, three students with limited experiences of iPad reading participated in this study. Data from weekly journals and interviews showed that the iPads&apos; palm size, light weight, and accessibility to the Internet through wireless connections not only promoted mobile learning outside the classroom but also achieved learning goals inside the classroom. The various iPad applications enabled students to learn English through games and easy access to helpful resources and thereby increased their motivation to learn. Students also improved their communication skills by using iPads to create videos. iPads have provided useful opportunities for new literacy instruction.
Evaluation on Second Language Collocational Congruency with Computational Semantic Similarity Collocation learning is an important research area because it involves structural, semantic and cognitive variations in lexical components which underpin the foundation of language competence. The notion of collocational congruency distinguishes whether an L2 collocation is congruent or incongruent with L1 counterpart. Evaluation of collocational congruency is currently performed by human judgment. This subjective evaluation mostly depends on individual lexical knowledge and word meaning interpretation. Human judgment on meaning accordance lacks a clear criterion as to clear-cut L2 collocation in congruency. How much similar in word meaning can be considered as congruent collocation? How much different should be regarded as incongruent? This vagueness is not resolvable by human judgment and can only expect inconsistent evaluation.The current study identified a research gap in the literature of L2 collocation where theoretical concepts of collocation congruency remain vague and lack explicit criteria for subjectively dichotomous congruency classification ( Koya, 2005;Webb &amp; Kagimoto, 2009;Yamashita &amp; Jiang, 2010;Wolter &amp; Gyllstad, 2011). This research proposed an objective and systematic method for congruency evaluation by exploiting computational measures of lexical semantic similarity. Based on literature review, it was found that WordNet (Miller, 1995) incorporated eight computational algorithms of semantic similarity measures and provided convenient online use. Of the eight algorithms, two (Adapted Lesk and Gloss Vectors) were selected in terms of their computational features and measuring stability. Three sets of word pairs with different semantic relations were composed and tested for lexical similarity values by the two measures.The current study further applied the two similarity measures to the experimental set of collocation so as to objectively evaluate the properties of congruency. Semantic similarity between a collocate and an L2 transferred word with L1 word sense was quantified by the two computational similarity measures. Statistical and analytical comparisons were made, which led to further understanding of the potential advantage of exploiting semantic similarity for congruency evaluation. Collocation learning is one of the important building blocks for the development of language competence. Remarkably, it is influenced by L1 and L2 congruency. The present study thus focused on the distinguishability of the computational similarity values between L2 collocates and L1 counterparts to establish the use of semantic similarity measure as a research instrument. The results showed that the inconsistency between human (subjective) and computational (objective) congruency classification of verb-noun collocations.
A Corpus-Based Tool for Exploring Domain-Specific Collocations in English There has long been a shared belief among English for academic or specific purposes (EAP and ESP) instructors that it is necessary to provide students with a list of academic vocabulary 1 irrespective of their specialized domain(s). There are two main reasons why academic vocabulary receives so much attention in EAP instruction. First, academic vocabulary accounts for a substantial proportion of words in academic texts (Nation, 2001). Sutarsyah et al. (1994), for example, found that about 8.4% of the tokens in the Learned and Scientific sections of the Lancaster-Oslo/Bergen (Johansson, 1978) and Wellington corpora (Bauer, 1993). Second, academic words very often are non-salient in written texts and less likely to be emphasized by content teachers in class (Flowerdew, 1993). Consequently, EAP researchers have been convinced that students need a complete list of academic vocabulary, and several lists were thus compiled. Among the attempts to collect academic lexical items, Coxhead's (2000) Academic Word List (AWL) has been considered the most successful work to date. In the AWL, Coxhead offered 570 word families which were relatively frequent in a 3.5-milliontoken corpus of academic texts. The corpus was composed of writings from four disciplines: arts, commerce, law, and science. By considering certain selection principles such as frequency and range, Coxhead gathered a group of word families which were specialized in academic discourse and generalized across different fields of specialization. On average, the AWL accounted for 10% of Coxhead's academic corpus and showed distributions of 9.1-12% of the four disciplines. Since its publishment, the AWL has been frequently used in EAP classes, 1 Academic words are also variously termed sub-technical vocabulary (Yang, 1986), semi-technical vocabulary (Farrell, 1990), or specialized non-technical lexis (Cohen et al., 1979) in the literature. They generally refer to words which are common in academic discourse but not so common in other types of texts. covered by numerous teaching materials, and reexamined by various domain-specific corpora (e.g. Vongpumivitch et al., 2009;Ward, 2009). The AWL, as Coxhead (2011) herself claims, indeed exerts much greater effects that the author ever imagined.Although well-received, the AWL is not without criticisms. For instance, Chen and Ge (2007), while confirming the significant proportion of the AWL in medical texts (10.07%), found that only half of the AWL words were frequent in the field of medicine. In Hancioğlu et al. (2008), the authors criticized that the distinction that Coxhead (2000) made into academic and general service words was questionable. In actuality, there were several general service words contained in the AWL (e.g. drama and injure). Arguably the strongest criticism came from Hyland and Tse (2007), who questioned whether there was a single core academic word list. Hyland and Tse called Coxhead's corpus compilation "opportunistic" (p. 239) and built a new database better controlled for its selection of texts to examine Coxhead's findings. Utilizing a more rigorous standard, Hyland and Tse found that only 192 families in the AWL were frequent in their corpus. Furthermore, numerous most frequent AWL families did not show such high-frequency distributions in Hyland and Tse's dataset. In addition to these methodological problems, as Hyland and Tse emphasized, the AWL as well as those previous academic word lists ignored an important fact that words tend to behave semantically and phraseologically differently across disciplines. Many academic words, such as analyze, tend to co-occur with different words and contain different meanings across research areas. What EAP learners actually need and have to study, accordingly, should be "a more restricted, discipline-based lexical repertoire" (p. 235).Inspired by Hyland and Tse's (2007) insights and analyses, we devise and create a learning tool which is able to generate domain-specific lexico-grammatical knowledge for EAP students. The knowledge that we focus on here concerns collocations. Specifically, we develop an online corpus-based tool, TechCollo, which can be used by EAP students to search for and explore frequent word combinations in their specialized area(s). The tool, by processing written texts in several medium-sized domain-specific corpora, enables students to study collocational patterns in their own domain, compare collocations in different disciplines, and check whether certain combinations or word usages are restricted to a specific field. To decide whether a pair of cooccurring words constitutes a candidate collocation, TechCollo uses measures such as frequency, traditional mutual information (MI) (Church and Hanks, 1990), and normalized MI ( Wible et al., 2004). We will discuss these measures in more detail in Section 3.This paper is structured as follows. In Section 2 we briefly discuss some related work. Section 3 describes the online learning tool and the corpora from which TechCollo extracts collocations. In Section 4, we present results of a pilot study to exemplify how to exploit TechCollo to discover differences in collocations across two domains. Finally, we propose our future plans for improving TechCollo in Section 5. Coxhead&apos;s (2000) Academic Word List (AWL) has been frequently used in EAP classrooms and reexamined in light of various domain-specific corpora. Although well-received, the AWL has been criticized for ignoring the fact that words tend to show irregular distributions and be used in different ways across disciplines (Hyland and Tse, 2007). One such difference concerns collocations. Academic words (e.g. analyze) often co-occur with different words across domains and contain different meanings. What EAP students need is a &quot;discipline-based lexical repertoire&quot; (p.235). Inspired by Hyland and Tse, we develop an online corpus-based tool, TechCollo, which is meant for EAP students to explore collocations in one domain or compare collocations across disciplines. It runs on textual data from six specialized corpora and utilizes frequency, traditional mutual information, and normalized MI (Wible et al., 2004) as measures to decide whether co-occurring word pairs constitute collocations. In this article we describe the current released version of TechCollo and how to use it in EAP studies. Additionally, we discuss a pilot study in which we used TechCollo to investigate whether the AWL words take different collocates in different domain-specific corpora. This pilot basically confirmed Hyland and Tse and demonstrates that many AWL words show uneven distributions and collocational differences across domains.
Automatic Identification of English Collocation Errors based on Dependency Relations Collocations play a very important role in second language learning (cf. Lewis, 1993). They reflect users'depth of vocabulary knowledge as well as their language proficiency levels (cf. Schimitt, 2000Schimitt, , 2010Nation, 2001;Nation and Webb, 2011).Researchhas shown that collocations are one of the most significant feature which distinguishes native from non-native writings. Furthermore, non-native writers tend to makecollocation errors unconsciously, many of which arise from first language interference. All these suggest the necessity of developing a miscollocation identification system to help learners detect their collocation errors as well as raise their language awareness. .Such a system might also havegreat impact for second language acquisition (SLA) research, as collections and analyses of collocation errors are vital to our understanding of the difficulties and problems learners encounter (cf. Nesselhauf, 2005). Just like other errors in learner corpora, error-tagged miscollocations are not widely and readily accessible to researchers.Traditionally, miscollocations can only be identified viavery time-consuming process of manual error tagging. Thanks to recent advance in natural language processing (NLP), automatic identification of miscollocations has been made possible. This paper presents an English miscollocation identification system by drawing on NLP tools and resources such as the Stanford parser, Google 1T ngrams, and WordNet. We will show that such a system not only has pedagogical valuebut also can facilitate the study of English miscollocations by non-native speakers. We present an English miscollocation identification system based on dependency relations drawn from the Stanford parser. We test our system against a subset of error-tagged Chinese Learner English Corpus (CLEC)and obtain an overall precision of 0.75. We describe some applications and limitations of our system and suggest directions for future research.
PADS Restoration and Its Importance in Reading Comprehension and Meaning Representation According to Lyons (1977), two different conceptions of kernel-sentences have been formalized in transformational grammar: one by Harris and the other by Chomsky. Zellig Harris defined a kernel as one that is not derived from any other sentence by means of a transformation rule; while Chomsky (1957) regarded a kernel as one generated in the grammar without the operation of "optional" transformations. Without looking into how kernels are conceptualized differently, kernels refer to "simple, complete, active, affirmative declaratives (or statements)", from which surface structures are derived. When Chomsky postulated theory of transformational grammar, he has PADS (permutation, addition, deletion and substitution) in mind as the stumbling rules that alienate Deep Structure from Surface Structure. For example, active sentences are transformed into passive either because the Agent is unknown or so that the Agent is moved to the end of a clause to serve as a link to the following clause. This need to link in texture organization might cause a careless reader to misread since the Agent and the Patient are swapped. Misreading is even more likely if the passive is in a participial, in which the verb-to-be is deleted. Ambiguity or misreading caused by PADS is sometimes referred to by reading researchers as the garden-path phenomenon.Whether they are called PADS gaps or garden-path, the derivations are causes of misinterpretation for computer reading and for underachieved readers. Nevertheless, for Halliday and Hasan (1976), they are great devices for cohesion, which refers to "the relations of meaning that exist within the text". Halliday and Hasan classify cohesive devices into five categories: reference, ellipsis, substitution, lexical cohesion, and conjunction. The mechanism "reference" relates one element of the text to another for its interpretation because they express the same referent. "Ellipsis" is used to omit an item to avoid repetition. "Substitution" refers to the use of pronouns or pro-forms to avoid using the same phrase for the same referent mentioned earlier. "Lexical cohesion" refers to two elements that share a lexical field or collocation. "Conjunction" refers to particular expressions used to create parallel connections.It's interesting to note that two linguistic schools established two decades away from each other should use similar mechanisms to refer to two very different concepts, one for generating surface sentences and the other in achieving text meaning. Chomskyan Generative Grammar and Hallidayan cohesion concept are mentioned here to draw attention to two things: 1) they point out that transformation rules and cohesive devices both involve missing, displaced or surrogate words or phrases that are extremely difficult for sequential or distance-based computation or for L2 learners; 2) the answer to their restoration should be in the study of language knowledge.In the following sections, the author will first point out that the occurrences of PADS gaps are almost entirely predictable. In other words, we know where they are from and how they are used. With this, how the English parser achieves different goals of PADS restoration, namely, clause boundary detection, PP attachment, zero anaphor resolution, anaphor resolution, pronoun co-reference resolution, event relation finding, will be reported. I will then show that the problems addressed are also causes of gardenpath phenomena. The next section illustrates an explicit kernel-like meaning representation that is used to integrate PADS restoration and highlight explicit referents as well as intra-event and interevent relations. Then, some preliminary results and evaluation methods will be reported. At the end, the paper will show how the parsing outputs in XML form can be used to help with CALL (computer-aided language learning), information extraction and knowledge discovery. Unlike competent human readers capable of inferring, tracing, and filling out gaps or hurdles left behind by authors&apos; use of transformations in their writing such as permutation, addition, deletion, and substitution (PADS), these operations are challenging to computer readers and new foreign language learners. This paper reports a parser&apos;s use of a suite of NLP technologies-clause boundary detection, resolution of different anaphors, inter-event relation finding, and case frame building-to fill out PADS gaps and output a much more explicit kernel-like meaning representation that includes case relation tuples of &quot;Who Did What to Whom&quot; and the inter-event relations based on conjoining, embedding, branching, insertion and apposition. According to Halliday and Hasan (1976), those gaps serve as cohesive devices to achieve better texture of the text organization. The transformations are ruled-based and they are important part of native speakers&apos; competence. Though the rule-based parser is still short of perfection, the necessary design is in place and it has quite a few encouraging results. This report will also show the usefulness of PADS restoration technology in CALL and information extraction.
Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation (PACLIC 28) Edited by  
Robust Semantics for Semantic Parsing  The paper presents a robust semantics for NLP applications including QA, text entailment and SMT that combines a (fairly) standard treatment of logical operators such as negation and quantification (Steedman 2012) with a highly nonstandard paraphrase-and entailment-based semantics of relational terms derived from text data by machine reading (Lewis and Steedman 2013a; 2013b). I&apos;ll consider the extension of the latter component to temporal and causal entailment using text-based methods, building on Lewis and Steedman 2014.
Social Media Understanding by Word Cloud Timeline  Text from social media is significant key information to understand social movement. However, the length of the social media text is typically short and concise with a lot of absent words. Our task is to identify the proper keyword representing the message content that we are accounting for. Instead of training the model for keyword extraction directly from the Twitter messages, we propose a new method to fine-tune the model trained from some known documents containing richer context information. We conducted the experiment on Twitter messages and expressed in word cloud timeline. It shows a promising result.
Registerial cartography: context-based mapping of text types and their rhetorical-relational organization  This paper is concerned with one of the three types of variation inherent in language-viz. register variation, or variation in meaning according to context of use. It reports on a long-term research programme designed to map the registers that collectively make up a language using one parameter within the context of use as the starting point-the field of activity characteristic the context in which a text of a given register unfolds. I present a typology/ topology of fields of activity, and go on to show how different types of activity favour different logico-semantic relations in the global organization of texts instantiating different registers. I then also illustrate registerial variation in the lexicogrammatical realization of logico-semantic relations. The part of the long-term research I focus on here is thus concerned with registerial variation relating to the chain of realizations from context (field of activity) to semantics (logico-semantic relations), and from semantics (logico-semantic relations) to lexicogrammatical realizations (with particular attention to congruence, i.e. congruent vs. incongruent realizations). At the end of the paper, I suggest that registerial cartography is an integral part of the development of appliable linguistics, a synthesis approach to language transcending the thesis and antithesis pair of theoretical linguistics and applied linguistics.
Discourse for Machine Translation  Statistical Machine Translation is a modern success: Given a source language sentence, SMT finds the most probable target language sentence, based on (1) properties of the source; (2) probabilistic source-target mappings at the level of words, phrases and/or sub-structures; and (3) properties of the target language. SMT translates individual sentences because the search space even for a single sentence can be vast. But sentences are parts of texts, and texts have properties beyond those of their individual sentences, including: • document-wide properties, such as style, register, reading level and genre, that are visible in the frequency and distribution of words, word senses, referential forms and syntactic structures; • patterns of topical or functional sub-structures that mean that frequencies and distributions of words, word senses, referential forms and syntactic structures will vary across a text; • relations between clauses or between referring expressions that can be signaled explicitly or implicitly, that reflect a text&apos;s coherence; • frequent appeal to reduced expressions that rely on context to • efficiently convey their message. Recognizing and deploying these properties promises to improve both fluency and accuracy in SMT-i.e., whether the sequence of sentences in the target text conveys the same information as those in its source, in as readable a manner. This presentation describes how researchers are attempting to do this, without bringing translation to a halt.
Invited Talk: Word Sense Induction for Machine Translation  We have witnessed the research progress of machine translation from phrase/syntax-based to semantics-based and from single sentence-based to discourse and document-based. This talk presents our work of word sense-based translation model for statistical machine translation, which is one of semantics-based SMT research at word sense level. The sense in which a word is used determines the translation of the word. The talk begins with how to build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language, and then focuses on the proposed word sense-based translation model that enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. The talk ends with experiential results and some conclusions. To the best of our knowledge, this is the first attempt to empirically verify the positive impact of lexical semantics (word sense) on translation quality. This is a joint work with Deyi Xiong, Soochow University. Biography
Setting Syntactic Parameters with Implicit Negative Evidence: The Case of Zero-derived Causatives in English [name] [address1] [address2] [address3] [email] An important question in language learnability is how to converge on a target grammar when all relevant grammars are compatible with the input. Indeed, this is a general challenge for various prominent models of parameter setting in generative syntax (e.g. Gibson and Wexler, 1994;Sakas and Fodor, 2001;and Yang, 2002). Consider a binary Parameter P concerning the complement of a head X 0 : the complement could simply be YP (1a) or the more complex ZP containing YP (1b).(1) a. [ XP X [ YP ]  Further, suppose that the target setting for a learner is the simpler structure in (1a), but that all the input the learner receives is ambiguous as to the parametric choice in (1). In such a case, we can ask how the learner can be sure to arrive at the adult grammar of (1a). In this paper, I present a simple case study that illustrates the learning challenge in (1) with zero-derived causatives (ZDCs) in English under Pylkkänen's (2008) theory of causatives. I propose a Bayesian model for parameter setting that learns the target setting from implicit negative evidence: given repeated instances of ambiguous input, the structure in (1a) has a greater likelihood of being the correct analysis. This result is a consequence of the learning process itself, and there is thus no need to invoke some principle such as the Subset Principle (Berwick, 1986), or to resort to default values for parameter setting. In this paper, I introduce a learning challenge for various models of parameter setting in generative syntax, namely a scenario where all input to the learner underdetermines the target parameter setting. This scenario is exemplified by the case of zero-derived causatives in English, as discussed in Pylkkänen (2008). I then propose a model for parameter setting that uses a simple Bayesian learning procedure to learn from implicit negative evidence and arrive at the target parameter setting.
Pseudo-Passives as Adjectival Passives It is well-known that once an argument is assigned Case, it cannot undergo further A-movement. However, pseudo-passives are quite peculiar in that the DP that appears to be the complement of a preposition moves to a Case position.(1) a. The hat was sat upon. b. These carpets have never been walked on.A plausible approach to this peculiarity is to argue that in (1a) sit upon is a constituent, and the hat is the complement of sit upon, not upon ( Radford 1988, Drummond &amp; Kush 2011). The pseudo-passive is peculiar in that (i) the DP that appears to be the complement of a preposition undergoes passivization, and (ii) it is semantically characterized by the fact that it describes a resultant state or a characteristic of the Theme. The first peculiarity can be explained if the DP is not the complement of P but the complement of the V-P complex. However, the problem with this approach is that V and P cannot form a constituent in the corresponding active. In this paper, however, I propose that we can maintain the V-P complex approach if it is an adjectival passive. The adjectival passive describes a characteristic of the Theme, and it does not necessarily correspond to its active counterpart with regard to the internal argument structure. This suggests that the peculiarities of the pseudo-passive follow if it is an adjectival passive. This paper claims that it is indeed the case. In short, I claim that the passive morpheme in the pseudo-passive is the adjectival passive-en, which is empirically supported by the fact that they display the properties of adjectival passives.
Phonological Suppression of Anaphoric Wh-expressions in English and Korean As Chung (2013) notes, the interrogative expression in Korean corresponding to the whexpression in English cannot be phonologically suppressed 1 , as follows:(1) A: na-nun chelswu-ka ecey mwues-ul I-Top Chelswu-Nom yesterday what-Acc sass-nunci molu-keyss-ta. 1 We occasionally use the theory-neutral notion 'phonological(ly) suppress(ion)' to refer to such terms as (phonological) dropping, copy trace deletion, ellipsis/deletion, etc.bought-Interr don't know 'I don't know what Chelswu bought yesterday.' B: na-to yenghuy-ka ecey *(mwues-ul) I-also Yenghuy-Nom yesterday what-Acc sass-nunci molukeyssta. bought-Interr don't know 'I don't know what Yenghuy bought yesterday.'In the conversation between speakers A and B, speaker B's sentence is required to bear the interrogative expression mwues 'what', despite the fact that another token of the same expression is mentioned in the previous sentence spoken by speaker A. Apparently, the same distribution of the whexpression is found in English, as follows:(2)A: I don't know what John bought yesterday. B: *I don't know Bill did (buy what yesterday), either. B': I don't know *(what) Bill did (buy t yesterday), either.As in (2B), the wh-expression what cannot be included in the portion deleted by VP ellipsis. Nor is it phonologically suppressed after it is moved to the embedded [Spec,CP] position, as in (2B'). Chung (2013) attempts to account for the impossibility of phonologically suppressing the interrogative expression in Korean by adopting the pro hypothesis for the null argument. More specifically, Chung follows the line of analysis advanced by Ahn and Cho (2012), who propose that the null argument as pro always substitutes for NP, but not for the next higher QP projected by the functional element Q such as a quantity word or a wh-feature, as schematized below: This paper follows the lead of Chung (2013), examining the phonological suppression of the wh-expression in English and Korean. We argue that the wh-expression itself cannot undergo ellipsis/deletion/dropping, as it carries information focus. However, it can do so, when in anaphoricity with the preceding token of wh-expression, it changes into an E-type or sloppy-identity pronoun. This vehicle change from the wh-expression to a pronoun accompanies the loss of the wh-feature inherent in the wh-expression. In a certain structural context such as a quiz question, the interrogative [+wh] complementizer does not require the presence of a wh-expression, thus the expression being optionally dropped.
Zero-Shot Learning of Language Models for Describing Human Actions Based on Semantic Compositionality of Actions Constructing topic-dependent language models is useful for many applications such as text mining, speech recognition, statistical machine translation, natural language interfaces, and textual description of images or video contents. In most methods of topic-dependent language model construction, one general model is first constructed from a large amount of language data, and then the general model is modified with a small amount of language data regarding the target topic. The technique of taking the weighted sum of language models is often used for the modification ( Bacchiani and Roark, 2003;Jelinek and Mercer, 1980). However, correcting language data for all target topics is demanding and difficult. In particular, when each target topic becomes narrower and the number of target topics increases, it becomes impractical to correct language data for all topics.In this paper, we propose a novel framework for zero-shot learning of topic-dependent language models, which enables the learning of language models corresponding to specific topics without observing language data regarding the topics on the basis of the semantic compositionality of the target topics.In the following, we consider rather fine-grained topics such as human activities. Such detailed topics are normally composed of several elementary semantic components. For example, a human action "raising left leg in the forward direction" is considered as a topic. The action includes components such as "up (raise)", "left", "leg", and "in the forward direction". Another action "raising left hand in the side direction" shares the common elements "up" and "left" with the previous action. In this way, actions are related to each other through common components. Hence, the language models generated from natural language sentences describing those actions are also expected to be related to each other. We will show that using this kind of compositionality, we can generate language models corresponding to actions for which we do not have natural language data.To demonstrate the effectiveness of the proposed methods, we apply the methods to the problem of generating natural language descriptions of short  In summary, the original contributions of this work are as follows: 1) the problem of zeroshot learning of topic-dependent language models is newly formulated, 2) novel simple methods for zeroshot learning are proposed, and 3) the effectiveness of the methods is confirmed with real data.The remainder of the paper is organized as follows: The problem is formalized and solutions are proposed in Section 2, Section 3 discusses related works, Section 4 presents application to the video description problem including experimental setup and results of the experiments, and Section 5 presents the conclusion and discusses future work. We propose a novel framework for zero-shot learning of topic-dependent language models, which enables the learning of language models corresponding to specific topics for which no language data is available. To realize zero-shot learning, we exploit the semantic compo-sitionality of the target topics. Complex topics are normally composed of several elementary semantic components. We found that the language model that corresponds to a particular topic can be approximated with a linear combination of language models corresponding to elementary components of the target topics. On the basis of the findings, we propose simple methods of zero-shot learning. To confirm the effectiveness of the proposed framework, we apply the methods to the problem of generating natural language descriptions of short Kinect videos of simple human actions.
Partial Case-Marking in Japanese Stripping/Sluicing: A Dynamic Syntax Account There is a growing body of research on ellipsis in Japanese (Hiraiwa &amp; Ishihara 2012 and references therein). Stripping is a relatively understudied type of elliptical construction (Fukaya 2007, Fukaya &amp; Hoji 2003, Fukui &amp; Sakai 2003, Sakai 2000see also Hankamer &amp; Sag 1976). As shown in (1)B, stripping consists of the NP Mary and the copula da, where case-marking of Mary is optional. This article presents novel data on partial case-marking in Japanese stripping/sluicing: only the final NP in multiple stripping/sluicing may lack a case particle. These data challenge previous works that assign radically distinct structures to stripping/sluicing depending on whether or not case-marking is involved. These case-marking patterns are reducible to incremental growth of semantic representation, formalised in Dynamic Syntax: each NP is parsed at an &apos;unfixed&apos; node, and this structural uncertainty must be resolved before another unfixed node is introduced.
A Corpus-Based Quantitative Study of Nominalizations across Chinese and British Media English Nominalization can refer to "the process of forming a noun from some other word-class (e.g. red + ness) or the derivation of a noun phrase from an underlying clause (e.g. Her answering of the letter… from She answered the letter)" (Crystal, 1997: 260). It has been approached by scholars from various perspectives, covering aspects of its form, meaning and use, as in the traditional grammar (e.g. Quirk et al., 1985), generative grammar (e.g. Lees, 1960;Chomsky, 1970), functional grammar (e.g. Halliday, 1994;Eggins, 2004) and cognitive grammar (e.g. Langacker, 1991). Among other things, nominalization is of close relevance to language variation studies due to its function to distinguish a nominal and compressed style from a colloquial one (e.g. Biber, 1986;Greenbaum, 1988, etc). However, in spite of numerous theoretical discussions, nominalization has only been touched upon sporadically in corpusbased studies, with notable exceptions of Biber (1986), Biber et al. (1998Biber et al. ( , 1999) and Leech et al. (2009). Due to an overwhelming word-based approach and a reliance on suffixes for identification, only a limited scope of nominalizations has been included in previous corpus-based studies. In addition, although these studies have revealed the discriminatory power of nominalization in language use with regard to spoken and written registers and genres, there are few attempts to investigate the use of nominalization across different language varieties except Leech et al. (2009).The research to be reported on in this paper attempts to bridge the afore-said gaps. It is exploratory and descriptive in nature and attempts to examine the cross-variety quantitative differences in the use of nominalizations across China English and British English in two comparable media corpora. Our study is different from previous corpus-based studies in several important respects. First, our study is not about variations of nominalization across registers and genres, but will explore variations across different English varieties, a different level of linguistic variation. The reason why we chose China English is that previous studies (Xu, 2008(Xu, , 2010 have shown that there are frequent uses of nominalization in China English. British English is chosen as the base for comparison. Second, the present study will adopt a syntactic approach to nominalizations, an approach that has not been undertaken in previous corpus-based studies. We will explain this further in Section 3. Third, there are some methodological innovations in the identification and retrieval of nominalizations in this study. We will not rely on the suffix-based This paper reports on a corpus-based quantitative study of the use of nominalizations across China English and British English in two comparable media corpora. In contrast to previous corpus-based studies of nominalizations, we start by using a syntactic approach and proceed with some methodological innovations incorporating large lexical databases and syntactically annotated corpora. The data show that there are significant differences in the use of nominalizations across these two English varieties. It is hoped that this research will offer useful insights on variations in nominalization across different English varieties and also on the understanding of the two English varieties in question.
Machine-Guided Solution to Mathematical Word Problems Mathematical word problems (MWP) constitute an integral part of a child's elementary schooling curriculum. Solving an MWP is a complex task involving critical aspects of reading comprehension (understanding the components of the problem), and generating a solution that agrees with the 'story' in the problem. Children are trained through the process of problem solving by the use of various strategies. In this study, we formulate solving an MWP as an NLP task involving text classification, discourse processing and information extraction. Our primary goal is to guide young learners through the important steps of mathematics comprehension and problem solving of arithmetic word problems commonly encountered in the elementary grades. We take a bottom-up approach, identifying the discourse structure of the MWP and then utilizing the semantic information contained in the components of the problem to generate a solution.In an MWP, significant background information is presented in text format. The ability to solve an MWP critically depends on the ability to detect the problem type and identify the components of the word problem as observed in studies in mathematics education and cognitive psychology (De Corte and Verschaffel, 1987;Cummins, 1991;Verschaffel et al., 2000).Motivated by these studies, we divide the overall problem solving process into stages: predicting the problem type, identification of the function of sentences (or sentence type) in each problem, and extracting the necessary information from the question to generate the corresponding mathematical equation. Since classification of the problem and sentence types involves a decision based on the textual representation, the classification tasks can be viewed as automatic text categorization problems (Yang and Liu, 1999) with domain-specific feature engineering. More broadly, a knowledge of the discourse structure of an MWP provides the human solver with a critical first step for information extraction and text summarization needed for mathematics problem comprehension and solving.A text classification perspective to MWP solu- Mathematical word problems (MWP) test critical aspects of reading comprehension in conjunction with generating a solution that agrees with the &quot;story&quot; in the problem. In this paper we design and construct an MWP solver in a systematic manner, as a step towards enabling comprehension in mathematics and teaching problem solving for children in the elementary grades. We do this by (a) identifying the discourse structure of MWPs that will enable comprehension in mathematics, and (b) utilizing the information in the discourse structure towards generating the solution in a systematic manner. We build a multistage software prototype that predicts the problem type, identifies the function of sentences in each problem, and extracts the necessary information from the question to generate the corresponding mathematical equation. Our prototype has an accuracy of 86% on a large corpus of MWPs of three problem types from elementary grade mathematics curriculum.
Taking Antonymy Mask off in Vector Space Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon and its identification is very challenging for computational models (Mohammad et al., 2008;Deese, 1965;Deese, 1964). Yet, antonymy is essential for many Natural Language Processing (NLP) applications, such as Information Retrieval (IR), Ontology Learning (OL), Machine Translation (MT), Sentiment Analysis (SA) and Dialogue Systems ( Roth and Schulte im Walde, 2014;Mohammad et al., 2013). In particular, the automatic identification of semantic opposition is a crucial component for the detection and generation of paraphrases (Marton et al., 2011), the understanding of contradictions (de Marneffe et al., 2008) and the detection of humor (Mihalcea and Strapparava, 2005).Several existing computational lexicons and thesauri explicitly encode antonymy, together with other semantic relations. Although such resources are often used to support the above mentioned NLP tasks, hand-coded lexicons and thesauri have low coverage and many scholars have shown their limits: Mohammad et al. (2013), for example, have noticed that "more than 90% of the contrasting pairs in GRE closest-to-opposite questions are not listed as opposites in WordNet". Automatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms. In this paper, we introduce APAnt, a new Average-Precision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the co-occurrence hypothesis.
Topic-based Multi-document Summarization using Differential Evolution for Combinatorial Optimization of Sentences As a general method of automatic multi-document summarization, we often use the important sentence extraction method which obtains the most proper combination of important sentences in target documents for a summary, avoiding redundancy in the generated summary. The explicit solution techniques, e.g., integer programming, branch and bound method, for optimal combination are often used under some constraints for the best combination of sentences. They have however a problem in terms of calculation costs. In general, if the size of target data sets is huge, the problem of combinatorial optimization becomes NP-hard. On the other hand, as an optimization method to obtain quasi-optimum solution in real time, it is reported that evolutionary computation is useful for realistic solutions. In this context, we employ differential evolution (DE) known as superior to other evolutionary computation algorithms in terms of calculation costs and the accuracy of solution, and apply it to multi-document summarization. Besides, under an assumption that multiple topics are included in documents, latent topics in documents are extracted by means of latent Dirichlet allocation, we make a summary, considering the latent topics. This paper describes a method of multi-document summarization with evolutionary computation. In automatic document sum-marization, the method to make a summary by finding the best combination of important sentences in target documents is popular approach. To find the best combination of sentences , explicit solution techniques such as integer linear programming, branch and bound method, and so on are usually adopted. However , there is a problem with them in terms of calculation efficiency. So, we apply evolutionary computation, especially differential evolution which is regarded as a method having a good feature in terms of calculation cost to obtain a reasonable quasi-optimum solution in real time, to the problem of combinatorial optimization of important sentences. Moreover, we consider latent topics in deciding the importance of a sentence, and define three fitness functions to compare the results. As a result, we have confirmed that our proposed methods reduced the calculation time necessary to make a summary considerably, although precision is more worse than the method with an explicit solution technique.
Mapping between Lexical Tones and Musical Notes in Thai Pop Songs Pitch is an important element in both language and music. In languages, pitch is used to convey different levels of meaning, e.g. lexical, sentential, attitudinal, emotional etc. In music, pitch serves the melodic structure, whether played on instruments or sung by voice, in order to express meaning to the listener. However, pitch in language and music differs with respect to how it is treated. While pitch in language is treated as a relative difference, pitch in music is treated as an absolute difference. Given their similarity and difference, it is important for our understanding of human cognition to examine the relationship between pitch in language and music. Of crucial relevance are languages that use patterns of relative pitch to convey lexical contrast. It is a puzzle how tonal languages relate their lexical tones to musical melody, which is made up of patterns of absolute pitch played on instruments or sung.One pertinent question is how contour tones are treated in the mapping between tone and melody. To answer this question, the Thai language is a great case study because its five tones, shown in Table 1, have been studied quite extensively both in terms of acoustics, perception, as well as phonology. However, little research has been done on the mapping between lexical tones and music in Thai, especially with respect to the treatment of contour tones. The aim of this paper is to examine the parallelism between tonal transitions and musical note transitions in Thai pop songs based on the data from 30 current pop songs. The results suggest that there is a statistically significant parallelism between tonal transitions and musical note transitions. Interestingly, the results show that both contour tones, RISING and FALLING, typically pattern with HIGH with respect to the mapping between tonal transitions and note transitions. Nevertheless, when two FALLING occur consecutively, the offset of the second one is used for mapping. Our results seem to find further support for decomposability of contour tones in Thai. Furthermore, they suggest that Thai pop music composition does not strive to maximize parallel transitions but prefer to avoid opposing transitions.
Emphasized Accent Phrase Prediction from Text for Advertisement Text-To-Speech Synthesis The introduction of corpus-based speech synthesis methods such as unit selection synthesis ( (Hunt, et al., 1996) etc.) and Hidden Markov Model speech synthesis ( (Zen, et al., 2009) etc.) makes expressive speech synthesis possible if an adequate speech database is prepared. However, the synthesized speech often fails to recreate emphasis or phrase boundary tone, even though both are key characteristics of expressive speech. The location markers of emphasis and phrase boundary tone have been confirmed useful in improving expressive speech synthesis qualities; they form part of the context information for speech synthesis (Meng, et al., 2012;Maeno, et al., 2014;Strom, et al., 2007;Yu, et al., 2010).For establishing Text-To-Speech (TTS) synthesis for expressive speech, it is necessary to predict locations of emphasis and phrase boundary tone from the input text. The phrase boundary tone occurs at the phrase end, and existence/non-existence of the tone can be accurately classified, from the text to be synthesized, by using machine learning approaches (Nakajima, et al., 2013;Ross, et al., 1996). Thus, this paper focuses on the remaining target of emphasis positions. In this work, we use the word "emphasis (emphasized)" to denote portions that are perceptually more salient to the listeners in a sentence.In human speech, emphasis can be regrouped at least into four functions based on analysis in conventional literatures as (Hovy et. al, 2013;Sridhar et. al, 2008) (bold portions show emphasized words and phrases). Realizing expressive text-to-speech synthesis needs both text processing and the rendering of natural expressive speech. This paper fo-cuses on the former as a front-end task in the production of synthetic speech, and investigates a novel method for predicting emphasized accent phrases from advertisement text information. For this purpose, we examine features that can be accurately extracted by text processing based on current Text-to-speech synthesis technologies. Among features , the word surface string of the main content and function words and the part-of-speech of main function words in an accent phrase are found to have higher potential on predicting whether the accent phrase should be emphasized or not through the calculation of mutual information between emphasis label and features of Japanese advertisement sentences. Experiments confirm that emphasized accent phrase prediction using support vector machine (SVM) offers encouraging accuracies for the system which requires emphasized accent phrase locations as context information to improve speech synthesis qualities.
Automatic News Source Detection in Twitter Based on Text Segmentation Recently, with the advent of social-media, it has become easy to express opinions or comment about experiences. In particular, Twitter 1 is a popular service used worldwide, and extremely large number of messages (tweets) is generated every day on it. It has been widely recognized that Twitter can potentially contain much useful information. Therefore, many researchers have conducted content analysis on Twitter ( Java et al., 2006;Krishnamurthy et al., 2008;Pennacchiotti and Gurumurthy, 2011;Mehro- tra et al., 2013).Twitter can be regarded as a news feeder (Zhao et al., 2011). News content distributed by other media are often re-distributed and diffused to more people through Twitter. For example, a user X posted a tweet as follows. Many people have a chance to know the details of Mario's fantastic goal 2 through t ex . Web content included in the URL http://example.football.com functions as an information source on t ex . It can be said that tweets, such as t ex , contain suitable information for news feeders. However, such cases are rare. Almost all tweets on Twitter are unsuitable due to a variety of reasons, e.g. (i) X did not write the information source in her stream of tweets, (ii) a tweet message and its information source (URL) were written in separate tweets, or (iii) X included a URL that was not related to the tweet message. In these cases, tweets do not function as the news feeders and people cannot obtain any additional information from them.We discuss news source detection (NSD), which involves finding additional information of a message generated on social media to understand the original message more deeply. In Twitter, given a tweet t i , the goal with NSD is to find another tweet t j (̸ = t i ) that includes a reference to its information source on t i . The details of NSD are described in Section 2. We propose an NSD method based on the text segmentation. It is difficult to straightforwardly resolve NSD because a search space of tweet pair combinations is exponentially large. Therefore, we simplify the NSD problem from the viewpoint of the text segmentation and provide an approximate solution. We also discuss two extension models of the proposed method using web content and post times. The rest of the paper is organized as follows. First, we define NSD and introduce some concepts and their notations for a formal description of NSD in Section 2. We then propose an NSD method that is based on the text segmentation and also discuss two extensions of the proposed method in Section 3. In Section 4, we introduce related work and discuss the differences between them. In Section 5, we describe the details of the experiments using realworld data and argue that the proposed method performs better than the baseline methods. We summarize the paper in Section 6. In this paper, we discuss news source detection (NSD), which involves finding additional information of a message generated in social media to understand the original message more deeply. We propose an NSD method based on the text segmentation and two extension models using web content and post times. Through the experiments using the real-world data, the proposed methods outperformed the baseline methods and exhibited an F-measure of 34.9.
Sentiment Lexicon Interpolation and Polarity Estimation of Objective and Out-Of-Vocabulary Words to Improve Sentiment Classification on Microblogging Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes and emotions from written language ( Liu, 2010). Recently, Twitter has become an important resource for sentiment analysis. People express their opinions and feelings using Twitter and these data can be grabbed publicly through Twitter API. There are two main approaches to sentiment analysis: lexicon-based and machine learningbased techniques. Several researchers have combined these two techniques ( Kumar et al., 2012;Mudinas et al., 2012;Saif et al., 2012;Fang et al., 2011;Hung et al., 2013). This study adopts a similar approach; we seek to combine the prior polarity knowledge from the lexicon-based method and the powerful classification algorithm from the machine learning-based method. Two main motivations of this approach are discussed below.The initial motivation is to revise the polarity of objective and out-of-vocabulary words in the public sentiment lexicon to improve Twitter sentiment classification. In the lexicon-based approach, sentiment classification is done by comparing the group of positive and negative words looked up from the public lexicon. For example, if the document contains more positive words than negative words, it will be classified as positive. Several public lexical resources such as ANEW 1 , OpinionFinder 2 , SentiStrength 3 , SentiWordNet 4 and SenticNet 5 lexicon are available for this type of analysis. SentiWordNet or "SWN" ( Esuli et al., 2010) has become one of the most fa- Sentiment analysis has become an important classification task because a large amount of user-generated content is published over the Internet. Sentiment lexicons have been used successfully to classify the sentiment of user review datasets. More recently, microblog-ging services such as Twitter have become a popular data source in the domain of sentiment analysis. However, analyzing sentiments on tweets is still difficult because tweets are very short and contain slang, informal expressions , emoticons, mistyping and many words not found in a dictionary. In addition, more than 90 percent of the words in public sentiment lexicons, such as SentiWordNet, are objective words, which are often considered less important in a classification module. In this paper, we introduce a hybrid approach that incorporates sentiment lexicons into a machine learning approach to improve sentiment classification in tweets. We automatically construct an Add-on lexicon that compiles the polarity scores of objective words and out-of-vocabulary (OOV) words from tweet corpora. We also introduce a novel feature weight-ing method by interpolating sentiment lexicon score into uni-gram vectors in the Support Vector Machine (SVM). Results of our experiment show that our method is effective and significantly improves the sentiment classification accuracy compared to a baseline uni-gram model.
How Mutual Knowledge Constrains the Choice of Anaphoric Demonstratives in Japanese and English Since Kuno (1973), it has been widely acknowledged in Japanese linguistics that the choice of demonstratives (the distal a-series, the medial soseries, and the proximal ko-series) in their anaphoric use is regulated by the rules concerned with the speaker's and the hearer's knowledge of the referent. In cross-linguistic discussions of anaphoric demonstratives (e.g., Diessel, 1999), on the other hand, the effect of the interlocutors' knowledge of the referent has not received such recognition.The purpose of the current work is three-fold. First, it critically reviews Kuno's seminal analysis of Japanese anaphoric demonstratives, and presents a modified version of it. Second, it argues that the interlocutors' knowledge of the referent is relevant to the choice of the English demonstratives this and that too. Third, it provides a formal semantic analysis of anaphoric demonstratives in the two languages couched in the Discourse Representation Theory (DRT) framework.It should be noted, before we proceed, that our discussion will focus on usage of anaphoric demonstratives in typical, two-agent conversations (dialogue); the question of whether and how the presented analysis can be extended to other discourse types, such as soliloquy (monologue) and nonfictional prose, will be left open. Also, our discussion will not cover the cases of demonstratives that do not refer to a specific entity (e.g., the "donkey anaphora" case, as in: If a man is in Rhodes, that man cannot be in Athens). It has been widely acknowledged that the choice of Japanese demonstratives (the distal a-series, the medial so-series, and the prox-imal ko-series) in their anaphoric use is regulated by the rules concerned with the inter-locutors&apos; knowledge of the referent. In cross-linguistic discussions of anaphoric demon-stratives, on the other hand, the effect of the interlocutors&apos; knowledge of the referent has not received such recognition. This paper has the following goals. First, it critically reviews Susumu Kuno&apos;s seminal analysis of Japanese anaphoric demonstratives, and presents a modified version of it. Second, it argues that the interlocutors&apos; knowledge of the referent is relevant to the choice of the English demonstratives this and that too. Third, it provides a formal semantic analysis of anaphoric demonstratives in the two languages.
Needle in a Haystack: Reducing the Costs of Annotating Rare-Class Instances in Imbalanced Datasets The advent of crowdsourcing as a cheap but noisy source for annotation labels has spurred the development of algorithms to maximize quality and minimize cost. Techniques can detect spammers (Oleson et al., 2011;Downs et al., 2010;Buchholz and La- torre, 2011), model worker quality and bias during label aggregation (Jung and Lease, 2012;Ipeirotis et al., 2010) and optimize obtaining more labels per instance or more labelled instances ( Kumar and Lease, 2011;Sheng et al., 2008). However, much previous work for quality maximization and cost limitation assumes that the dataset to be annotated is classbalanced.Class-imbalanced datasets, or datasets with differences in prior class probabilities, present a unique problem during corpus production: how to include enough rare-class instances in the corpus to train a machine learner? If the orginal class distribution is maintained, a corpus that is large enough for a machine learner to identify common-class (i.e., frequent class) instances may suffer from a lack of rare-class (i.e., infrequent class) instances. Yet, it can be cost-prohibitive to expand the corpus until enough rare-class instances are included.Content-based instance targeting can be used to select instances with a high probability of being rare-class. For example, in a binary class annotation task identifying pairs of emails from the same thread, where most instances are negative, cosine text similarity between the emails can be used to identify pairs of emails that are likely to be positive, so that they could be annotated and included in the resulting class-balanced corpus (Jamison and Gurevych, 2013). However, this technique renders the corpus useless for experiments including token similarity (or ngram similarity, semantic similarity, stopword distribution similarity, keyword similarity, etc) as a feature; a machine learner would be likely to learn the very same features for classification that were used to identify the rare-class instances during corpus construction. Even worse, Mikros and Argiri (2007) showed that many features besides ngrams are significantly correlated with topic, including sentence and token length, readability measures, and word length distributions. The proposed targetedinstance corpus is unfit for experiments using sentence length similarity features, token length similarity features, etc. Crowdsourced data annotation is noisier than annotation from trained workers. Previous work has shown that redundant annotations can eliminate the agreement gap between crowdsource workers and trained workers. Redundant annotation is usually non-problematic because individual crowdsource judgments are inconsequentially cheap in a class-balanced dataset. However, redundant annotation on class-imbalanced datasets requires many more labels per instance. In this paper, using three class-imbalanced corpora, we show that annotation redundancy for noise reduction is very expensive on a class-imbalanced dataset, and should be discarded for instances receiving a single common-class label. We also show that this simple technique produces annotations at approximately the same cost of a metadata-trained, supervised cascading machine classifier, or about 70% cheaper than 5-vote majority-vote aggregation.
Hybrid Approach to Zero Subject Resolution for multilingual MT -Spanish-to-Korean Cases Spanish is one of the so-called pro-drop languages where certain pronouns may be omitted. In Spanish, the pronominal subject can be deleted and is called a zero subject. A zero subject is the most frequent type of anaphoric expression in Spanish. 1 Spanish zero subject is one of the important issues that must be tackled in Spanish-to-Korean MT. This kind of pronoun is very important due to its high frequency in Spanish texts. In many cases its resolution is obligatory in Spanish-to-Korean MT. Palomar et al.(2001) reported that about 65.5% of the pronouns are the zero subject pronoun in pronoun occurrences in Spanish corpus.Let us consider the following example. In this example, the omitted subject is represented by the symbol ø.(1) Luis quiere que Ø vayamos[1 st plural] a la playa.Luis want that go to the beach lwuisunun wulituli haypyeney kakilul wunhanta "Luis wants us to go to the beach."In Spanish, the subject pronoun and the verb must agree in person and number. Even though the subject pronoun is not present in the sentence, the zero subject can be restored from the verb ending, as '-os' is a 1 st person plural morpheme. This gives us clues to resolve a Spanish zero subject. However, there is another case for which to use verb ending for Spanish zero subject resolution is not enough. In Spanish, the verb ending for the 3 rd person singular subject 'él(he)', 'ella(she)' and formal 2 nd person singular subject 'Usted(you)' is same and so are for the plural subjects. Also for some verbs in a specific tense like pretérito imperfecto, the verb endings for the 1 st and 3 rd person singular are identical. Even in other sentence mood, there are some verbs which conjugate in the same way. Therefore, there still exists a problem to select the one right subject among possible subjects for some verb endings. For these cases, we need to suggest another method to select the one right subject among other possible subjects. We introduce a machine learning method to resolve the case in which morphological information is not enough to resolve Spanish zero subject. Machine learning (ML) has already been successfully used in the computational linguistics for disambiguation and classification issues. Selecting one right subject among possible candidates can also be regarded as a disambiguation issue. In this paper we propose a hybrid approach to zero subject in Spanish, which combines linguistic knowledge and ML approach in one model. The hybrid approach can benefit from the strengths of both approaches. The related works about anaphora resolution and their limitation are presented in Section 2. In Section 3, we suggest the method for Spanish zero subject resolution. 11 features for ML method are also proposed. In Section 4, the effect of using ML is evaluated. Finally, the conclusion is presented in Section 5. The current paper proposes a novel approach to Spanish
Improving Statistical Machine Translation Accuracy Using Bilingual Lexicon Extraction with Paraphrases In statistical machine translation (SMT) (Brown et al., 1993), the translation model is automatically learned form parallel corpora in an unsupervised way. The translation model contains translation pairs with their features scores. SMT suffers from the accuracy problem that the translation model may be inaccurate, meaning that the translation pairs and their features scores may be inaccurate. The accuracy problem is caused by the quality of the unsupervised method used for translation model learning, which always correlates with the amount of parallel corpora. Increasing the amount of parallel corpora is a possible way to improve the accuracy, however parallel corpora remain a scarce resource for most language pairs and domains. 1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate.Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features ( Klementiev et al., 2012;Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word Statistical machine translation (SMT) suffers from the accuracy problem that the translation pairs and their feature scores in the translation model can be inaccurate. The accuracy problem is caused by the quality of the unsu-pervised methods used for translation model learning. Previous studies propose estimating comparable features for the translation pairs in the translation model from comparable corpora , to improve the accuracy of the translation model. Comparable feature estimation is based on bilingual lexicon extraction (BLE) technology. However, BLE suffers from the data sparseness problem, which makes the comparable features inaccurate. In this paper, we propose using paraphrases to address this problem. Paraphrases are used to smooth the vectors used in comparable feature estimation with BLE. In this way, we improve the quality of comparable features, which can improve the accuracy of the translation model thus improve SMT performance. Experiments conducted on Chinese-English phrase-based SMT (PBSMT) verify the effectiveness of our proposed method.
Incrementally Updating the SMT Reordering Model Parallel data for training statistical machine translation (SMT) models is being constantly generated, both by professional and by casual translators. Typically, large amounts of data are required to produce decent SMT models, yet training a model is an expensive process in terms of time and computational resources. Most often, and in particular when community effort is made to translate new content, it is desirable to keep the system up-to-date with the new data; yet, constant retraining is not feasible. The line of research concerning incremental training for SMT has been addressing this problem, aiming at updating the model given new parallel data, rather than retraining it.Typical phrase-based SMT models use a loglinear combination of various features that mostly represent three sub-models: a translation model (TM), responsible for the selection of a target phrase for each source phrase, a language model (LM), addressing target language fluency, and a reordering model (RM). The reordering model is required since different languages exercise different syntactic ordering. For instance, adjectives in English precede the noun, while they typically follow the noun in French (the blue sky vs. le ciel bleu); in Modern Standard Arabic the verb precedes the subject, and in Japanese the verb comes last. As a result, source language phrases cannot be translated and placed in the same order in the generated translation in the target language, but phrase movements have to be considered. This is the role of the reordering model. Estimating the exact distance of movement for each phrase is too sparse; therefore, instead, the lexicalized reordering model (Koehn, 2009) estimates phrase movements using only a few reordering types, such as a monotonous order, where the order is preserved, or a swap, when the order of two consecutive source phrases is inverted when their translations are placed in the target side. up-to-date requires retraining. Yet, refraining from updating this model is expected to yield inferior translation performance. An example is shown in Figure 1, comparing results with and without an updated reordering model. While not as important a component as the TM or the LM (see further results in Section 6), updating the RM does improve translation. We therefore seek to allow quick incremental updates of the RM within Moses ( Koehn et al., 2007). In this paper we outline several practical options to carry out this update, and describe an implementation of one of them. In a set of experiments we show both that RM updates help improving results and that is can be carried out much quicker than reconstructing the model from scratch.Next, we describe related work on SMT model updates (Section 2), and provide the details of the Moses reordering model and its relevant data structures (Section 3); we outline and analyze several options to perform RM updates in Section 4, and propose an method in Section 5. Section 6 includes evaluation in terms of translation performance and run-time, and Section 7 summarizes this work and suggests future research directions. This work is concerned with incrementally training statistical machine translation (SMT) models when new data becomes available. That, in contrast to retraining new models based on the entire accumulated data. In-cremental training provides a way to perform faster, more frequent model updates, enabling keeping the SMT system up-to-date with the most recent data. Specifically, we address incrementally updating the reordering model (RM), a component in phrase-based machine translation that models phrase order changes between the source and the target languages, and for which incremental training has not been proposed so far. First, we show that updating the reordering model is helpful for improving translation quality. Second , we present an algorithm for updating the reordering model within the popular Moses SMT system. Our method produces the exact same model as when training the model from scratch, but doing so much faster.
Frequency-influenced choice of L2 sound realization and perception: evidence from two Chinese dialects Second language speech has generally seen as function of linguistic experience. However, how experience shape the formation of phonetic category was understudied. This study addresses a case when speakers from two L1s with similar segmental layout may have different realizations of L2 categories. Although theoretic models in speech learning such were very rich in literature, such as Perceptual Assimilation Model (PAM [1]) and its another version for L2 learners (PAM-L2 [2]) as well as Speech Learning model (SLM, [3]) had addressed different L1 assimilation patterns in learning multiple L2s, few studies had found similar multiple L1s yielding different L2 learning outcomes.PAM and SLM suggest that second language learners will either assimilate the L2 sound categories (or sequence of sounds) to L1 sound categories according to different perceptual distances. Increased exposure to L2 will thus trigger distributive learning of L2 input by forming a new intermediate category between the L1 and L2 in the learner's common phonetic space [1]. In experience-based models, the positive effect of L2 exposure will increase the chance of distributive learning because the learnability of certain L2 categories should become stable if the input of L2 categories occurs in environments with similar frequency [3].This paper displays that similar L1 inventories may result in different learning outcomes and argues that this phenomenon is influenced by frequency in similar ways as the native language was (NLM, [5]). The two English sounds under current investigation are the voiceless interdental fricative (/θ/) and the central liquid (/r/). In a pilot study, it was found that Sichuanse speakers replace English /θ/ by /s/ but Cantonese speakers by /f/. The study of second language speech perception usually put L1-L2 phonological mapping as the rule of thumb in predicting learning outcome, and seldom included more fine-grained aspects such as frequency. This study examines how frequency of sounds in L1 may influence L2 segmental production and perception, with examples from English learners native to two Chinese dialects, Cantonese and Sichuanese. Although these two dialects (L1s) have very similar phonological inventory, they produce certain L2 sounds in drastic difference. Productions of English voiceless interdental fricative and central liquid in the onset position were obtained in free speech from the two dialects&apos; speakers in vast phonological environments. Then, perception tests, including AX and oddity tasks, were done for these two groups of speakers as well. Results showed that the two English sounds were respectively realized as different sounds in Cantonese and Sichuanese L1, which was reflected by both production and perception data. Findings suggest that L2 category formation is frequency-motivated instead of markedness-motivated, and is significantly influenced by the functional load of L1 sound input. Findings further imply that a quantitative and frequency-sensitive learning model is more suitable for L2 sound acquisition.
The L2 Acquisition of the Chinese Aspect Marking 1 In the early nineteen seventies, researchers carried out a number of studies on first language (L1) acquisition of the tense-aspect system, and their findings show a close relationship between the use of the verbal morphology and aspectual properties of verbs/situations like [ dynamic], [ telic] and [ punctual] ( Antinucci &amp; Miller, 1976;Bloom et al., 1980;Bronckart &amp; Sinclair, 1973;Li, 1989). Beginning L1 learners tend to restrict their use of the perfective past (simple past tense in English which indicates both past time location and perfective aspect (Smith, 1997) to telic verbs (Achievements and Accomplishments), and their use of the imperfective aspect to Activities. The same patterns have also been attested in second language (L2) acquisition (Andersen,1986(Andersen, , 1989(Andersen, , 1990Bardovi-Harlig, 1992, 1994Bardovi-Harlig &amp; Bergström, 1996;Bardovi-Harlig &amp; Reynolds, 1995;Flashner, 1989;Kaplan, 1987;Kumpf,1984;Robison, 1990;Shirai &amp; Andersen, 1995;etc.) There widely attested developmental patterns were first referred to as the Defective Tense Hypothesis ( Weist et al., 1984), and later came to be known as the Primacy of Aspect Hypothesis (Andersen, 1989;Robison 1990) or the Aspect Hypothesis (Andersen &amp; Shrai, 1994;Robison, 1995, Shrai &amp; Kurono, 1998). The Defective Tense Hypothesis attributes the observed patterns "to a cognitive inability of a young child to conceive of a notion of 'past event or situation'" (Andersen &amp; Shirai, 1996, p. 560), while the Aspect Hypothesis suggests that learners primarily use verbal morphology to mark lexical aspectual distinction rather than temporal distinction. The Aspect Hypothesis as summarized in its simplest form by Andersen (2002: 79) 
Readability of Bangla News Articles for Children News is the communication of selected information on current events (Shirky, 2009). This communication is shared by various mediums such as print, online and broadcasting. A newspaper is a printed publication that contains news and other informative articles. There are many newspapers that are also published online. Due to the rapid growth of internet use, it is very common that more people read news online nowadays than before. Newspapers try to target certain audience through different topics and stories. Children are also in their target audience. This target group is their future reader.Nowadays children also read news online. One third of children in developed countries such as Netherlands, United Kingdoms and Belgium browse internet for news (De Cock, 2012;De Cock and Hautekiet, 2012). Another study by Livingstone et al. (2010) showed that one fourth of the British children between age of nine and nineteen look for news on the internet. The ratio could be similar in other developed countries where most of the citizen have access over the internet.The number of internet users also increasing in developing countries such as Bangladesh and India. According to the English Wikipedia 1 , more than thirty three million people in Bangladesh use internet and many of them read news online. Also the Alexa index 2 shows that three Bangla news sites are in the list of ten most visited websites from Bangladesh.All newspapers contain a variety of sections. These sections are based on different news topics. Some of the them are specific to children. The news for children will vary linguistically and cognitively than news for adults. This characteristic is similar to the websites dedicated for children. De Cock and Heutekiet (2012) observed difficulties for children to navigate these websites. Readability of the texts is one of the reasons. There is no specific guideline for writing texts for this target group. Journalists use their experience and intuition while writing. However, a text that is very easy to understand for an adult reader could be very difficult for a child. This difficulty motivate children readers to skip the newspaper in future.The readability of a text relates to how easily human readers can process and understand a text. There are many text related factors that influence the readability of a text. These factors include very simple features such as type face, font size, text vocabulary as well as complex features like grammatical conciseness, clarity, underlying semantics and lack of ambiguity. Nielsen (2010) recommended font size of 14 for young children and 12 for adults. Many news papers publish articles for children. Journalists use their experience and intuition to write these. They might not aware of readability of articles they write. There is no evaluation tool or method available to determine how appropriate these articles are for the target readers. In this paper, we evaluate difficulty of Bangla news articles that are written for children.
Focusing on a Subset of Scripts Enhances the Learning Efficiency of Second Language Writing System There is a general assumption in many language textbooks that L2 learners are able to comprehend and produce scripts of any second language just because they have acquired the reading and writing ability in their first language. However, different processes and strategies are actually involved in L1 and L2 writing systems (L1WS and L2WS) and should not be ignored by the learning material designers or the language teachers (Bassetti, 2006). English is an international language learnt globally by most people and textbooks of English are perhaps the most convenient model being imitated by textbooks of other languages for L2 learners. In the first lesson of learning a L2WS, the first thing that comes to our mind is the 26 letters. Therefore, learning how to read and write kana and hangul are expected in the first chapter of Japanese or Korean textbooks, because they function like the 26 letters of English as the most basic building blocks in the writing system (WS, hereafter).Although it may take some time for learners to "swallow" graphemes like kana or hangul, it is not a daunting task for most people to master (at least to recognize) them well enough for general usage. However, there are also WSs which are less "learner-friendly" -Thai WS is one of them. In Hong Kong, we observed through classroom teaching that many L2 learners of Japanese or Korean can memorize kana or hangul reasonably well within weeks or even days; while L2 learners of Thai tend to give up learning to read and write for good, and confine their learning to verbal language.In the present thesis, we propose that (1) WSs that have relatively large inventory size could be learnt more efficiently through reordering the learning sequence; (2) the usage frequencies of the basic writing units are important criteria for deciding on the learning sequence; (3) learning materials designed for L2WS adult learners should be different from those for L1WS children learners, as two groups have had different experience before they learn the target WS.In our discussions in this paper, we would like to take learning Thai scripts as an illustration for Memorizing the whole set of graphemes is generally accepted as the first step of learning a phonogramic language. However, it is demanding for L2 learners to familiarize the whole inventory of graphemes in advance if the language has a relatively large inventory size. We propose that learning a subset of graphemes would largely enhance the learning efficiency by reducing the memory burden. With homophony minimized, effort of acquiring vocabulary in elementary stage can be greatly reduced. In this paper, the writing system of Thai is used to illustrate the main idea. Besides, the method may also be extendable to Japanese and Korean, which grapheme inventory sizes are smaller.
Retrieval Term Prediction Using Deep Belief Networks The current Web search engines have a very high retrieval performance as long as the proper retrieval terms are given. However, many people, particularly children, seniors, and foreigners, have difficulty deciding on the proper retrieval terms for representing the retrieval objects, 1 especially with searches related to technical fields. The support systems are in place for search engine users that show suitable retrieval term candidates when some clues such as their descriptive texts or relevant/surrounding words are given by the users. For example, when the relevant/surrounding words "computer", "previous state", and "return" are given by users, "system restore" is predicted by the systems as a retrieval term candidate.Our objective is to develop various domainspecific information retrieval support systems that can predict suitable retrieval terms from relevant/surrounding words or descriptive texts in Japanese. To our knowledge, no such studies have been done so far in Japanese. As the first step, here, we confined the retrieval terms to the computerrelated field and proposed a method to predict them using machine learning methods with deep belief networks (DBN), one of two typical types of deep learning.In recent years, deep learning/neural network techniques have attracted a great deal of attention in various fields and have been successfully applied not only in speech recognition ( Li et al., 2013) and image recognition ( Krizhevsky et al., 2012 This paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep belief networks (DBN), one of two typical types of deep learning. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using example-based approaches and conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), for comparison. The data for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo data was also used. A grid search was adopted for obtaining the optimal hyper-parameters of these machine learning methods by performing cross-validation on training data. Experimental results showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or SVM; (2) adding automatically gathered data and pseudo data to the manually gathered data as training data is an effective measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier training data than MLP, i.e., the prediction precision of DBN can be improved by adding noisy training data, but that of MLP cannot be.
An Example-Based Approach to Difficult Pronoun Resolution Consider the following pair of sentences: 1 (1) a. The outlaw shot the sheriff, but he did not shoot the deputy.b. The outlaw shot the sheriff, but he shot back.Suppose that the target pronoun is he, and its two candidate antecedents are the outlaw and the sheriff. The question is which of the two candidates is the correct antecedent for the target pronoun in each sentence? Most people resolve he to the outlaw in (1a) but to the sheriff in (1b) without noticing any ambiguity. However, for a computer program, this pronoun resolution becomes extremely difficult, requiring the use of world knowledge and the ability to reason. We refer to the pair of sentences like (1) as a Winograd schema (Levesque, 2011;Levesque et al., 2012). Note that the two sentences differ only in a few words and have a referential ambiguity that is resolved in opposite ways.A previous work by Rahman and Ng (2012) showed that two sources of world knowledge, including narrative chains (Chambers and Jurafsky, 2008) and page counts returned by a search engine, are useful for resolving Winograd schemas. However, these two knowledge sources have their own weaknesses and need some heuristics to bridge the gap. Narrative chains suffer from the lack of discourse relations. For example, both sentences in (1) have a contrast relation indicated by but. However, narrative chains rely only on temporal relations between two events (e.g., before and after). Page counts used for estimating n-gram statistics are unstable and vary considerably over time (Lapata and Keller, 2005;Levesque et al., 2012). Therefore the answer to the question "what kind of world knowledge does a computer program need to have to resolve Winograd schemas?" (Levesque, 2013) is still unclear.Rather than looking for new knowledge bases, we first examine whether existing sentences on the Web have sufficient evidence that could be applied to resolve Winograd schemas. If such evidence is available, we may be able to later generalize a collection of those sentences into a more abstract level of representation. A Winograd schema is a pair of twin sentences containing a referential ambiguity that is easy for a human to resolve but difficult for a computer. This paper explores the characteristics of world knowledge necessary for resolving such a schema. We observe that people tend to avoid ambiguous antecedents when using pronouns in writing. We present a method for automatically acquiring examples that are similar to Winograd schemas but have less ambiguity. We generate a concise search query that captures the essential parts of a given source sentence and then find the alignments of the source sentence and its retrieved examples. Our experimental results show that the existing sentences on the Web indeed contain instances of world knowledge useful for difficult pronoun resolution.
A Unified Analysis to Surpass Comparative and Experiential Aspect English has both comparative construction (1-a) and experiential perfect (1-b) sentence, which are marked by different morphemes. Cantonese has these two constructions too, only that it uses the same morpheme gwo3 to mark both. As a lexical verb, gwo3 means 'to cross' or 'to surpass'. In (2), gwo3 shows the standard of comparison (henceforth standard or std) in a comparative sentence. In (3), it shows that the event of 'going to England' has taken place at any point in the past. The correlation pattern between these two constructions, surpass-comparative (2) and experiential aspect marking (3), is reported to be common in typology literature (Ansaldo, 2010;Stassen, 1985) and is therefore not mere coincidence. The aim of this study is to provide a formal account to this wellobserved correlation.This study builds on the notion of scale structure (Kennedy &amp; McNally, 2005) that is primarily applied to adjectives and/or property-denoting predicates. Since scale structure is non-temporal by nature, this paper posits that verbal predicates can be conceptualized and formalized as scales measured by time, i.e. a 'temporal scale'. Under this view, both comparative and experiential sentences can be treated on a par as scalar predicates specified with a degree along the scale. This study compares two constructions in Cantonese which shares similar features in their syntax and semantics. Previous works observe that comparatives often appear after experiential aspect in the verbal domain historically. This study builds upon this observation and argues that the similarities between two constructions, comparatives and experien-tials, are of formal nature and that the similarities originate from the semantics of these constructions. This formal account means that there is a deep connection between the two constructions and therefore explains the pattern observed by typologists (Stassen, 1985; Ansaldo, 2010). The homomorphic approach also means a simpler syntax-semantics that applies to both event-denoting (&apos;verbs&apos;) and property-denoting (&apos;adjectives&apos;) predicates.
A Quantitative View of Short Utterances in Daily Conversation: A Case Study of That&apos;s right, That&apos;s true and That&apos;s correct Dialogue act (DA), defined as "communicative activity of a dialogue participant, interpreted as having a certain communicative function and semantic content" (ISO 24617-2, 2012: 2), plays a key role in the interpretation of the communicative behaviour of dialogue participants and offer valuable insight into the design of human-machine dialogue system (Bunt et al. 2010). With the goal of facilitating automatic DA tagging, this paper describes a corpus-based investigation into that's right, that's true, that's correct and their variations in the Switchboard Dialogue Act (SWBD) Corpus, in order to answer questions about the communicative functions they mainly perform in daily conversation. These utterances deserve our particular attention in research considering that, like other brief responses (e.g. Oh, Uh huh, Mm, Okay), they serve as important feedback to the main speaker and they usually occur as overlapping speech. They are particularly problematic to interpret because they demonstrate a drastically different functional or pragmatic meaning from the semantic meaning of the component tokens. Consider Ex This is one excerpt retrieved from the targeted corpus, which will be further illustrated in section 2. A and B, two speakers, are talking about books and literature, where B is describing one of her daughter's book, "real easy to follow". The last utterance Short utterances serve a multitude of different communicative functions in interactive speech and have attracted due attention in recent research in dialogue acts. This paper presents a quantitative description of three short utterances i.e. that&apos;s right, that&apos;s true, that&apos;s correct and their variations based on the Switchboard Dialogue Act Corpus. Particularly, it offers an overview to account for how they are deployed by native speakers in daily conversation. At the same time, it attempts to provide a comparative account of that&apos;s right and that&apos;s true, showing that while almost 75% of them are mutually exchangeable, they nonetheless exhibit preferences in interactive speech. This insight is expected to form a useful approach towards automatic dialogue act tagging.
A Listenability Measuring Method for an Adaptive Computer-assisted Language Learning and Teaching System Listening practice using authentic materials is necessary for learners of English as a foreign language (EFL) who have little or no chance to use English in their daily life because these materials let them immerse themselves in real-life settings. Since authentic materials are not usually constrained by the ease of listening comprehension or listenability (Chall &amp; Dial 1948), teachers have to select materials according to learner listening proficiency; otherwise, too difficult or easy materials reduce the learning effect or spoil learner motivation (Hubbard 2004, Petrides 2006). An adaptive computer-assisted language learning and teaching system can pick up news clips as authentic materials from the Internet according to the listening proficiency if it is equipped with a listenability measuring method that takes into both linguistic features of a news clip and the listening proficiency. Therefore, we propose an automatic method that statistically measures listenability for EFL learners. This method is useful for learning and teaching English by showing listenability levels of authentic listening materials such as news clips. It also helps to create a computer-based selflearning environment because EFL learners can select appropriate materials with this method.Although listenability of authentic listening materials can be measured with readability measuring methods using lexical, syntactic, and discourse features (Flesch 1950, Graesser et al. (2004, and Shen et al 2013), our listenability measuring method uses phonological features as well as lexical and syntactic features. Phonological feature accounts for listenability in terms of speech rate and phonological modification. The natural speech rate for native speakers reduces listenability for learners because learner processing speed is slow due to the lack of automation of mental language processing. Phonological modification refers to sound change such as the elision observed in the second vowel sound of "chocolate" (Roach 2001). Phonological modification has been reported to increase listenability for native speakers, but reduce it for learners (Henricksen 1984).In addition to linguistic features such as lexical, syntactic, and phonological, our method also uses learner features, which account for the listening proficiency. Unlike native speakers, the listening proficiency greatly differs among individuals (Saville-Troike 2006). That is, listening material can be appropriate for a learner but not for another. Therefore, it is necessary to measure listenability based not only on linguistic features but also learner features. In teaching and learning of English as a foreign language, the Internet serves as a source of authentic listening material, enabling learners to practice English in real contexts. An adaptive computer-assisted language learning and teaching system can pick up news clips as authentic materials from the Internet according to learner listening proficiency if it is equipped with a listenability measuring method that takes into both linguistic features of a news clip and the listening proficiency. Therefore, we developed a method for measuring listening proficiency-based listenability. With our method, listenability is measured through multiple regression analysis using both learner and linguistic features as independent variables. Learner features account for learner listening proficiency, and linguistic features explain lexical, syntactic, and phonological complexities of sentences. A cross validation test showed that listenability measured with our method exhibited higher correlation (r = 0.57) than listenability measured with other methods using either learner features (r = 0.43) or other linguistic features (r = 0.32, r = 0.36). A comparison of our method with other methods showed a statistically significant difference (p &lt; 0.003 after Bonferroni correction). These results suggest the effectiveness of learner and linguistic features for measuring listening proficiency-based listenability.
Prosodic Differences Between Declaratives and Polar Questions in Fataluku Fataluku is an underdocumented language spoken by approximately 37,000 individuals in island Southeast Asia, on the far eastern tip of the nation of East Timor ( Lewis et al., 2013). Fataluku is a member of the Timor-Alor-Pantar family of Papuan languages, which includes about twenty-five languages spoken on Timor and nearby islands (Klamer, 2014;Schapper et al., 2014). Relatively little has been published about any aspect of the phonology of Fataluku.The primary goal of the present paper is to describe the intonational differences between declaratives and polar questions (also known as yes-no questions) in Fataluku. This paper is part of a larger project to describe Fataluku segmental and suprasegmental phonology. I analyze Fataluku intonation within the framework of the autosegmentalmetrical (AM) theory of intonational phonology (Pierrehumbert, 1980;Ladd, 1996), which has become the standard for intonation research. In the AM model, the phonological structure of intonation is represented underlyingly as a sequence of discrete level tones, each of which is associated either with a prominent syllable (a "pitch accent") or with the edge of some prosodic constituent (a "boundary tone"). The surface intonation contour is a result of continuous interpolation between discrete level tones.My focus here is on behavior at the right edge of an Intonational Phrase (IP) in Fataluku. The IP-the largest prosodic constituent in the AM frameworkis a phrase that can stand alone and is generally accompanied by a final boundary tone and final lengthening (Jun and Fletcher, 2014). Typologically, IPfinal boundary tones are rich sources of linguistic information (Lindström and Remijsen, 2005), a generalization that holds for Fataluku as well.To lay the groundwork for the analysis of intonation, section 2 provides some background on the language, including a review of a previous study on Fataluku question intonation. After a brief discussion of methods, the results of the present study are given, describing the prosodic patterns of statements and polar questions. The discussion section proposes a phonological analysis to explain the observed prosodic differences. The paper concludes with a summary and some suggestions for future research.  The basic syllable structure of Fataluku is (C)V(V)(C). Consonant sequences are rare, especially within a morpheme. Fataluku has both long vowels and diphthongs, both of which are represented underlyingly as sequences of vowelsidentical in the case of long vowels and nonidentical in the case of diphthongs (Heston, 2014). The examples below are given in a phonemic practical orthography. 1 The primary goal of the present study is to describe the basic prosodic differences between declaratives and polar questions in Fataluku, an underdocumented Papuan language spoken in the island nation of East Timor. Two robust prosodic differences between statements and questions are observed, namely, the duration of the final vowel and the intonational tune at the right margin of the sentence. Declara-tives have a shorter final vowel that carries a low f0, while questions have a much longer final vowel that has a rising-falling f0 pattern. I postulate a L% boundary tone for declaratives and a L+HL% boundary tone for questions, proposing that the final syllables of questions are lengthened to accommodate the more complex sequence of final tones.
Recognition of Sarcasm in Tweets Based on Concept Level Sentiment Analysis and Supervised Learning Approaches Recognition of sarcasm is one of the most difficult tasks in natural language processing (NLP). It is a problem of determining if the actual meaning of a word is intended in a given context. Sarcasm is normally represented in a form of ironic speech in which the speakers convey an implicit message to criticize a particular person. Thus, tone of voice plays a significant role in the communication. There are many communication programs (e.g. Line, Facebook, Twitter), which allow to communicate together through only text characters. It is very difficult to determine the actual meaning by just looking at the text itself. Recognition of sarcasm prevents us from misinterpreting sentences whose meaning are opposite to their literal meaning. It is also a task that is potentially applicable for many other areas of NLP, for example, machine translation, information retrieval, information extraction and knowledge acquisition.Twitter is an online social networking service that allows users to post and read short messages, called "tweets". However, Twitter allows users to write short messages, i.e. 140 characters per tweet. Also, users usually post a lot of tweets in complex sentence structures. Regarding to these issues, a new method is created to detect sarcasm in tweets.Sarcasm is known as "the activity of saying or writing the opposite of what you mean, or of speaking in a way intended to make someone else feel stupid or show them that you are angry" (Macmil- lan, 2007). According to this definition, we can recognize sarcasm by evaluating the polarity of the sentences. In other words, a sarcastic sentence contains two or more words, which may cause conflict in sentiment polarities (both positive and negative) in a sentence, whereas a normal sentence should contain at most one polarity. Let us consider the example sentence "I love being ignored." The sentence contains both positive ("love") and negative word ("ignored") in a sentence. Therefore, it can be classified as a sarcastic sentence.In the identification of sarcasm based on the contradiction of the polarity, unknown words in the sentiment lexicon are serious problem. To tackle it, we try to consider the related concepts for each word to identify the sentence polarity. For example, let us consider the tweet "It's Wednesday and it's freezing! It's raining! How better can this day be??" This would be classified as a normal tweet since only Sarcasm is a form of communication that is intended to mock or harass someone by using words with the opposite of their literal meaning. However, identification of sarcasm is somewhat difficult due to the gap between its literal and intended meaning. Recognition of sarcasm is a task that can potentially provide a lot of benefits to other areas of natural language processing. In this research, we propose a new method to identify sarcasm in tweets that focuses on several approaches: 1) sentiment analysis, 2) concept level and common-sense knowledge 3) coherence and 4) machine learning classification. We will use support vector machine (SVM) to classify sarcastic tweet based on our proposed features as well as ordinary N-grams. Our proposed classifier is an ensemble of two SVMs with two different feature sets. The results of the experiment show our method outperforms the baseline method and achieves 80% accuracy.
CHULA TTS: A Modularized Text-To-Speech Framework A Text-to-Speech (TTS) system is a system which artificially produces human speech by converting a target text into its corresponding acoustic signal. TTS systems are crucial components to many kinds of computer applications, particularly applications in assistive technology, E.g. applications for assisting the visually-impaired to access information on the Internet ( Chirathivat et al. 2007), applications for automatically producing digital talking books (DTB) ( Punyabukkana et al. 2012), and etc., Over the past decades, several TTS systems had been developed to fulfill applications on various computing platforms including mobile devices ( Chinathimatmongkhon et al. 2008). Given specific domains, some applications of TTS systems require the systems to produce word pronunciations or generating speech signals that sound more natural to the listeners than ones generated with systems designed for texts of more general domains. For example, an application to read text from a social media web site might need a TTS system that performs a normalization of wordplays rather than attempting to pronounce them straightforwardly according to their exact spellings. While such a TTS system produced more naturally-sounded speech utterances ( Hirankan et al. 2014), the normalization process might degrade a TTS's performance on a domain involving more formal texts where wordplays are scarce. For a TTS system aiming for expressive speech utterances, with multiple handlers, each of which is responsible for handling a different expression, the system could produce better results as well as easier handler development. A TTS system that allows interoperation of components, such as Grapheme-To-Phoneme (G2P) or signal generation components, deploying different speech and text processing algorithms without re-compiling of the system is obviously desirable. Still, many TTS systems were not designed with such abilities.In this paper, we therefore reported our recent attempt on designing and implementing a modularized TTS framework, namely ChulaTTS. The goal of the design of ChulaTTS was to allow a TTS system to incorporate multiple speech and text processing components and allow them to work together with minimal development efforts. Components with similar classes of functionality must interoperate despite the differences in their underlying algorithms or the differences in phonetic units primitive to each of the components. With that goal in mind, ChulaTTS is suitable for conducting speech synthesis experiments to observe the performance of newly-developed algorithms in a complete TTS system conveniently. Furthermore, ChulaTTS can be easily configured into a TTS system expected to handle special phenomena appearing in the domain that it is deployed.The rest of the paper was organized as follows. Related works were reviewed and discussed in the Section 2. In Section 3, we reported the design of our modularized TTS framework, and described the details of an implementation of a TTS system based on the modularized framework in Section 4. Section 5 discussed real applications of ChulaTTS systems. Finally, we concluded the paper in the last section. Spoken and written languages evolve constantly through their everyday usages. Combining with practical expectation for automatically generating synthetic speech suitable for various domains of context, such a reason makes Text-to-Speech (TTS) systems of living languages require characteristics that allow extensible handlers for new language phenomena or customized to the nature of the domains in which TTS systems are deployed. ChulaTTS was designed and implemented with a modularized concept. Its framework lets components of typical TTS systems work together and their combinations are customized using simple human-readable configurations. Under .NET development framework, new text processing and signal synthesis components can be built while existing components can simply be wrapped in .NET dynamic-link libraries exposing expected methods governed by a predefined programming interface. A case of ChulaTTS implementation and sample applications were also discussed in this paper.
On the Functional Differences between the Discourse Particles Ne and Yone in Japanese The Japanese discourse particles (also called sentence-final particles) ne and yone each have a variety of functions, and both have the functions that can be roughly characterized as the ⟨shared information⟩ (SI) use and the ⟨call for confirmation⟩ (CFC) use. The semantic effect of ne/yone in their SI use is comparable to that of English reversed polarity tag interrogatives 1 with a falling tone (e.g. He was here, wasn't he↘); that is, it conveys that S (the speaker) assumes that H (the hearer) has been aware that the propositional content (e.g., Ito's having been sullen in (1)) holds. The semantic effect of ne/yone in their CFC use is comparable to that of English reversed polarity tag interrogatives with a rising tone (e.g. He was here, wasn't he↗); that is, it serves to form a polar question with expectation of the positive answer (e.g., "Yes, I am Arai." in (2)). 2 (1) Ito Some scholars treat yone as a sequence of the two discourse particles yo and ne. 3 I treat it as a single particle, however, based on the consideration that it is hard to compositionally derive the functions of yone from those of yo and ne. It should also be noted that, under the "sequence-of-two-particles" analysis, the different intonational properties of ne and yone cannot be easily explained (see Section 2).In the existing literature (e.g., Takubo and Kin- sui 1997, Miyazaki et al. 2002, Izuhara 2003, Ni- hongo Kijutsu Bunpo Kenkyukai 2003, Ohso 2005, McCready 2009), a satisfactory description has not been obtained as to how the choice between the two particles is made. This paper aims to clarify discourse conditions under which ne and yone can be felicitously used. Section 2 illustrates, as a preliminary, intonational contrasts between the two parti- The Japanese discourse particles (sentence-final particles) ne and yone both have the functions that can be roughly characterized as the ⟨shared information⟩ use and the ⟨call for confirmation⟩ use. In the literature, an adequate descriptive analysis has not been obtained as to how the choice between the two particles is made. This paper aims to clarify discourse conditions under which ne and yone can be felicitously used.
Adjacency Pair Recognition in Wikipedia Discussions using Lexical Pairs A growing cache of online information is contained inside user-posted forum discussions. Thread structure of the discussion is useful in extracting information from threads: Wang et al. (2013) use thread structure to improve IR over threads, and Cong et al. (2008) use thread structure to extract questionanswer pairs from forums. However, as Seo et al. (2009) point out, thread structure is unavailable in many forums, partly due to the popularity of forum software phpBB 1 and vBulletin 2 , whose default view is non-threaded.Thread reconstruction provides thread structure to forum discussions whose original thread structure is nonexistant or malformed, by sorting and reordering turns into a directed graph of adjacency (reply-to) relations. Pairs of adjacent turns (adjacency pairs) were first identified by Sacks et al. Turn1: This article has been gutted. I deleted a lot of the cruft that had taken over, but a lot of former material is missing. [...] Turn2: Good; the further this nest of doctrinaire obscurities is gutted, the better. Turn3: Wait, you changed it to say that English doesn't have a future tense or you're citing that as an error (which it would naturally be)? For what it matters, [...]  (1974) as the structural foundation of a discussion, and recognition of adjacency pairs is a critical step in thread reconstruction ( Balali et al., 2014;Aumayr et al., 2011). Figure 1 shows an excerpt from Ferschke's (2014) English Wikipedia Discussions Corpus. Thread structure is indicated by tab indents. Turn pairs (1,2), (1,3), and (3,4) are adjacency pairs; pairs (2,3) and (1,4) are not. Adjacency pair recognition is the classification of a pair of turns as adjacent or nonadjacent.Although most previous work on thread reconstruction takes advantage of metadata such as user id, timestamp, and quoted material (Aumayr et al., 2011;Wang et al., 2011a), metadata is unreliable in some forums, such as Wikipedia Discussion page forums, where metadata and user contribution is difficult to align (Ferschke et al., 2012). Wang et al. (2011b) find that joint prediction of dialogue act labels and adjacency pair recognition improves accuracy when compared to separate classification; dialogue act classification does not require metadata. However, existing dialogue act typologies are unapplicable for some forums (see Section 2.2).In this paper, we perform adjacency pair recognition on pairs of turns extracted from the English Adjacency pair recognition, a necessary component of discussion thread reconstruction, is the task of recognizing reply-to relations between pairs of discussion turns. Previously, dialogue act classification and metadata-based features have been shown useful in adjacency pair recognition. However, for certain forums such as Wikipedia discussions, metadata is not available, and existing dialogue act typologies are inapplicable. In this work, we show that adjacency pair recognition can be performed using lexical pair features, without a dialogue act typology or metadata, and that this is robust to controlling for topic bias of the discussions .
A Hierarchical Word Sequence Language Model Most language models used for natural language processing, such as n-gram approach proposed by Shannon (1948), are continuous. However, the assumption that a word depends upon the preceding n-1 words is too simple to cope with data sparsity problem.Thus, a number of useful smoothing techniques such as back-off (Katz,1987), Kneser-Ney (Kneser &amp; Ney,1995), modified Kneser-Ney (Chen &amp; Good- man,1999) have been developed to estimate the probabilities of unseen sequences. Yet even with 30 years worth of newswire text, more than one third of all trigrams are unseen ( Allison et al., 2005). It is still important to make full use of contextual information hidden in training data.D. Guthrie. et. al. (2006) proposed using skipgram (Huang et. al., 1993) to overcome the data sparsity problem. The skip-gram model using discontinuous sequences to model languages has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings the greatly increase of processing time and redundant contexts. D. Guthrie. et. al. (2006) examined the coverage of skip-gram, but didn't analyze the efficiency of them, which will be discussed in section 4.3 and section 4.4 in this paper.Taking into account of the balance between coverage and usage, we present a hierarchical word sequence model to relieve the data sparsity problem. Differing from other hierarchical language models, such as hierarchical phrase-based model (Chiang, 2007) used in SMT systems, our model is essentially a n-gram language model whose modeling assumption is determined by tree structures.We introduce our main idea in Section 2. In Section 3, we propose the hierarchical word sequence model. We show the effectiveness of this model by several experiments in Section 4 and conclude in Section 5. Most language models used for natural language processing are continuous. However, the assumption of such kind of models is too simple to cope with data sparsity problem. Although many useful smoothing techniques are developed to estimate these unseen sequences, it is still important to make full use of contex-tual information in training data. In this paper, we propose a hierarchical word sequence language model to relieve the data sparsity problem. Experiments verified the effectiveness of our model.
An Analysis of Radicals-based Features in Subjectivity Classification on Simplified Chinese Sentences In sentiment analysis, an important task is subjectivity classification on sentences, which means classify sentences as subjective or objective. This step's performance greatly affects the following processing that is related with polarity or emotion etc. Here 1. !C![WI!å‹©!"øè""èà¢" Most classrooms of Yongle elementary school nearby are also classified as dangerous buildings. Chinese radicals are linguistic elements smaller than Chinese characters 1. Normally, a radical is a semantic category and almost all characters contain radicals or are radicals themselves. In subjectivity classification on sentences, we can use radicals to represent characters, which reduce the scale of word space while keep the subjectivity information. In this paper, we manually labeled a character set to build a high-quality radical-character mapping, and then the mapping is used to generalize character-based features with radicals. In experiments, we at first evaluated the performance when directly generalizing characters with radicals, and then offer a hypothesis that can reduce noises. Experiments show that this approach based on our hypothesis can reduce feature space while keep or improve the performance, which is especially useful when the training samples are scarce.
A Semantics for Honorifics with Reference to Thai The phenomena of honorification and politeness register have received extensive attention in linguistics, both from formal and informal perspectives. Most of this work has focused on three general topics. First, from a formal perspective, researchers have been concerned with the way in which semantic composition with honorific expressions takes place, and with the kinds of denotations which they have; some main results of these investigations will be summarized later in this paper. 1 A second line of research is found within the sociolinguistic tradition (and also within discourse analysis), and looks at ways in which speakers use politeness expressions 1 Work on syntactic aspects of honorification is closely related (Niinuma, 2003), but since morphological axes with honorific meanings will not be my focus here, I will not consider this aspect of honorification further in the present paper.to indicate aspects of their social identities and further their general societal goals (Brown and Levin- son, 1987;Watts, 2003). Finally, there is a tradition which attempts to situate the use of politeness, including honorifics, within a general theory of rational linguistic behavior; this work begins with Brown and Levinson (1987) and continues to gametheoretic accounts like that of van Rooy (2003).Given the amount of research done in this area, it is no surprise that significant results have been obtained. However, a problematic feature of the literature is that the three strands of research mentioned above do not engage extensively with each other. Research on honorific meanings tends not to consider observations made within discourse analysis; game-theoretic accounts try to predict rational honorific use without a serious semantics for honorific content. A theory which can bring the various aspects of politeness together seems necessary, especially given the current interest in honorification in formal circles, and further is essential for the automatic generation of appropriate speech in computational pragmatics. The aim of the present paper is to propose a theory of the requisite sort. That said, space limitations preclude doing more than laying the formal groundwork needed; modeling substantial sociolinguistic observations and tying the result to game-theoretic calculation is left for future work.The paper is structured as follows. I will take the system of politeness marking found in Thai as the empirical domain of the analysis. This system is introduced in §2, though this introduction is necessarily non-exhaustive for reasons of space. Some lessons are drawn here for formal theories of polite- This paper proposes a general framework for the semantics of honorific expressions, including honorific pronouns, morphology, and discourse particles. Such expressions are claimed to indicate a level of politeness which must be compatible with a level of formality fixed by the discourse context together with sociolin-guistic factors, and, with their use, to change the range of formality the context specifies. Specific honorifics are taken to introduce expressive content of a kind modeled by real-numbered intervals. This general picture is exemplified with the honorific system of Thai.
Disunity in Cohesion: How Purpose Affects Methods and Results When Analyzing Lexical Cohesion The purposes of text analysis research can be divided into two main categories: applications and descriptions. The difference between these two areas is that applications produce results that are useful to end users who are outside of the field of linguistics, while descriptions of language are used internally by the linguistic community (Sinclair, 2004a). Many text analysis applications created for those outside of linguistics use automated tools, and therefore they focus on features that can be identified and analyzed with computers. One linguistic feature that can be analyzed to varying degrees of success using computers is lexical cohesion, since lexical cohesion can be found in the surface features of text. The analysis of lexical cohesion has been used in many text analysis applications, such as discourse analysis (Morris &amp; Hirst, 1991), automatic text summarization ( Barzi- lay &amp; Elhadad, 1999), text segmentation (Stokes, Carthy, &amp; Smeaton, 2004), word sense disambiguation (Okumura &amp; Honda, 1994), and evaluation of machine translations (Wong &amp; Kit, 2012).Lexical cohesion was defined by Halliday and Hasan (1976, p. 274) as "the cohesive effect achieved by the selection of vocabulary" and is one of five types of cohesion (the other four being reference, substitution, ellipsis, and conjunction). A cohesive text is held together by explicit relationships found in the lexis and grammar of the text. These lexico-grammatical relationships are called cohesive ties as they connect one sentence to another. Multiple ties can, in turn, be combined into longer lexical chains which can span large portions of the text.Current technology can identify lexical cohesion ties and lexical chains of ties with varying degrees of accuracy. Some cohesive ties are very easy to identify, such as the exact repetition of a lexical unit in an adjacent sentence, while others can be more difficult to correctly identify, such as the relationship of a pronoun to a noun in a previous sentence. Hoey (1991) outlined six types of lexical cohesion which are ordered by ease of identification from easiest to most difficult, along with some examples in Table 1.As stated earlier, lexical cohesion has been used in text analysis research for many different purposes. This paper will look at four main purposes: text evaluation, text segmentation, text summarization, and text criticism. The first three of these can be analyzed using automated computerized tools, while the fourth is a qualitative analysis that is beyond the capabilities of today's computers. These four purposes can be described as follows. The first purpose, text evaluation, especially of student writing, has often focused on the lexical cohesion of the text as a marker of the quality of the text, with the assumption being that features such as referential cohesion correlate with human evaluations of high quality text (Weston, Crossley, &amp; McNamara, 2010). The second purpose, text segmentation, finds breaks in the text where there are no lexical chains. The lack of lexical chains in a span of text shows that the topic might have changed (Şimon, Gravier, &amp; Sébillot, 2013). The third purpose, text summarization, tries to identify the important topics in the text in order to create a summary of the text. Lexical cohesion aids this task by showing which topics are repeated throughout the text (Barzilay &amp; Elhadad, 1999). The fourth purpose, text criticism, looks at the lexical cohesion in a text and attempts to understand the meaning behind the lexical choices, for example to find metaphors in political speeches that support the speaker's public image (Klebanov, Di- ermeier, &amp; Beigman, 2008).A key issue for lexical cohesion analysis is that the unit on which the analysis is conducted differs depending on the purpose of the research. Each of the four purposes discussed in this paper investigate a different unit. For the first purpose, a text evaluation is an evaluation of the cohesiveness of the text as a whole, and therefore should be based on the entire text. This can be done, for example, by computing the average cohesion between all of the sentences in the text. Text segmentation is an attempt to segment the whole text into smaller units and therefore the analysis must be based on units that are smaller than the whole text, such as measuring the lexical cohesion between individual adjacent pairs of sentences. Text summarization is focused on the lexical items of the text in order to find the important concepts, so the cohesive lexical items take priority over the whole text itself. Text criticism is not only looking at the lexical choices made by the writer or speaker but also at the potential meaning behind these choices. Therefore, the unit of investigation can vary in length as needed.Even though all of these purposes are using lexical cohesion as the subject of research, the results of the research may be very different. The purpose of this paper, then, is to illustrate how different purposes require different methods, and how Lexical Cohesion is a commonly studied linguistic feature as it is easily identified from the surface of a text. However, the purposes for studying lexical cohesion are varied, and each purpose requires different methods. This study analyzes two short movie review texts for four different research purposes using lexical cohesion: text evaluation, text segmenta-tion, text summarization, and text criticism. The analysis shows that these four different purposes produce very different results concerning the lexical cohesion of the two texts, suggesting that the apparently straightforward construct of lexical cohesion is actually complex .
Topics are conditionals: A case study from exhaustification over questions Similarities between conditionals and topics are identified by many linguists (Haiman, 1978(Haiman, , 1993Collins, 1998;Bittner, 2001;Bhatt &amp; Pancheva, 2006;Ebert et al., 2008). Some languages use an identical morpheme to mark topics and conditionals. In Japanese, for instance, a conditional suffix nara is used for both conditional and topic constructions. When nara follows a clause as in (1-a), the clause serves as an antecedent of a conditional sentence. When nara attaches to a NP as in (1-b), the NP is the topic of the sentence.(1) a. Taro This paper offers another piece of evidence for the virtual identity of topics and conditionals. In particular, I argue that topics have the same semantics as conditional antecedents in that both serve as contextshifters. In dynamic semantics, conditionals are defined in terms of a two-step (Stalnaker, 1968;Kart- tunen, 1974;Heim, 1982) or three-step (Kaufmann, 2000;Isaacs &amp; Rawlins, 2008) update procedure: (2) c+ 'if P , Q' = (c ∩ P ∩ Q) ∪ (c ∩ P ), where a context c and propositions P and Q are sets of possible worlds. To illustrate briefly, in (4-a), the initial context is assertively updated by the antecedent 'Max comes', that is, the worlds that make the proposition false are deleted. The derived context is then assertively updated by the consequent 'we'll play poker'. Finally, the worlds removed in the second step are also removed from the original context. The idea of context-shifting nature of conditionals might be clearer with so-called biscuit conditionals like (4-b). In (4-b), the antecedent 'if you're hungry' shifts the context so that the assertive update of the consequent 'There's food in the fridge' becomes relevant or optimal (Franke, 2007(Franke, , 2009). This paper argues in favor of Haiman&apos;s (1978) idea that conditionals and topics are analogous. The evidence comes from exhaustifi-cation over topicalized questions, which have the same semantics as conditional questions (Isaacs &amp; Rawlins, 2008).
A Keyword-based Monolingual Sentence Aligner in Text Simplification Many articles are posted on the Web every day, and an increasing number of educational websites specifically provide articles for audiences with different needs. For example, NewsInLevels (www.newsinlevels.com)  and EasierEnglishWiki (eewiki.newint.org) contain articles easier to read with simpler vocabulary and syntactic structure than English Wikipedia and New Internationalist for people with low literacy. And SoundReading (www.soundreading.com) even has audio recording for those with learning disabilities such as dyslexia.Language learning websites such as NewsInLevels and EasierEnglishWiki typically simplify original articles into easier ones and present the original and easier articles as pairs to non-native speakers, children, or lay people. However, language learners may want to compare the article pairs conveying the same information at sentence level, and most text simplification systems build on top of original and simplified sentence pairs. Unfortunately, current monolingual sentence alignment methods treat article sentences as bags of words, equally weight words, and align sentences with high word-overlap ratios. These article pairs could be sentence aligned more accurately if a system distinguished words of different importance and leveraged their importance levels in articles while aligning.Consider the original-simplified article pair in Figure 1. The best sentence alignment methods are probably not the ones with equal word weights (i.e., weights are the same with "the" and "gorilla" and the same with "everything" and "project"). A good aligning approach might take into account the words' significance in the pair. Intuitively, word significance can be evaluated by keyword extraction methods and by leveraging word significance, sentence aligners can be biased towards aligning sentences with more words that are more essential.We present a new system, KEA (keyword extraction based sentence aligner), that automatically learns to align sentences, considering word keyness, of monolingual parallel articles. That is, KEA aligns texts in the same language at sentence level that are "translation" of each other with different readability. An example KEA sentence alignment for an article pair is shown in Figure 1. KEA has determined the keyness scores of the words in the article pair. KEA learns these scores automatically during training by using TF*IDF and PageRank with semantic information (see details in Section 2). Both are famous keyword extraction methods.At run-time, KEA starts with a pair of monolingual parallel articles. KEA then computes similarity scores among sentences in the original and simplified article based on words' keyness scores from TF*IDF and PageRank. Cosine similarity is adopted to evaluate sentence-wise similarity with the help of alignment ratio of content words and differences of relative aligned word positions. Based on sentence-level similarity, KEA employs global dynamic programming with deletion and insertion operation to generate the optimal sentence alignment for the pair. In our prototype, KEA returns sentence pairs for evaluation and language learning directly (see Figure 1); alternatively, the sentence pairs returned by KEA can be used as input to a text simplification system. We introduce a method for learning to align sentences in monolingual parallel articles for text simplification. In our approach, word keyness is integrated to prefer aligning essential words in sentences. The method involves estimating word keyness based on TF*IDF and semantic PageRank, and word nodes&apos; parts-of-speech and degrees of reference. At run-time, the keyword analyses are used as word weights in sentence similarity measure. And a global dynamic programming goes through sentence similarities further weighted by aligned content-word ratios and positions of aligned words to determine the optimal candidates of parallel sentences. We present a prototype sentence aligner, KEA, that applies the method to monolingual parallel articles. Evaluation shows that KEA pays more attention to key words during sentence aligning and outperforms the current state-of-the-art in alignment accuracy and f-measure. Our pilot study also indicates that language learners benefit from our sentence-aligned parallel articles in reading comprehension test.
Automatic Detection of Comma Splices English text consists of a sequence of clauses linked and separated by punctuation and conjunctions. To separate two independent clauses, one uses a fullstop (period); to link together two related clauses, one typically uses a semicolon or a comma with an appropriate conjunction, which can be either coordinate ("and", "but", "or") or subordinate ("because", "so"). For example, to link the two related clauses "it was raining" and "we stayed home", one may use a comma and the conjunction "so", yielding the complex sentence "It was raining, so we stayed home". When a comma is used instead of a fullstop, or when it is used without a conjunction (e.g., "It was raining, we stayed home"), the result is a comma splice 1 . 1 Note that a list of noun phrases with a missing conjunction (e.g., "I like apples, oranges.") is not a comma splice.Our use of the term comma splice also includes improper linking of verb phrases. This occurs when a comma is used without a conjunction (e.g., "We stayed home, watched TV."); without a relative pronoun (e.g., "The boy chased after the rat, fled into the sewer"); or with the wrong verb form (e.g., "Waterborne pathogens are the pathogenic microorganisms, includes bacteria"). Comma splices are not only considered poor writing style, but they also compromise the readability of a text.Although native speakers have been found to commit a substantial number of common splice errors ( Connors and Lunsford, 1988;Lunsford and Lunsford, 2008), non-native speakers appear to be especially prone to producing them, possibly due to interference from syntactic differences in L1 ( Tseng and Liou, 2006;Bennui, 2008;Rahimi, 2009). This may be especially true for L1s where comma splices are frequently found and are not considered mistakes, such as in Chinese (Lin, 2002). Comma splices are one of the errors addressed in the 2014 CoNLL Shared Task on Grammatical Error Correction ( Ng et al., 2014). They are annotated in many learner corpora, including the NUS Corpus of Learner English ( Dahlmeier et al., 2013) and the EFCambridge Open Language Database (Geertzen et al., 2013). This paper addresses the task of detecting comma splices. We report human agreement in detecting these errors and propose a CRF model to automatically detect them. Our best model, which uses features derived from parse trees produced by the Stanford parser (Klein and Manning, 2003) In English text, independent clauses should be demarcated with full-stops (periods), or linked together with conjunctions. Non-native speakers are often prone to linking them improperly with commas instead of conjunctions, producing comma splices. This paper describes a method to detect comma splices using Conditional Random Fields (CRF), with features derived from parse tree patterns. In experiments, our model achieved an average of 0.91 precision and 0.28 recall in detecting comma splices, significantly outperform-ing both a baseline model using only local features and a widely used commercial grammar checker.
  
&quot;Guo1&quot; and &quot;Guo2&quot; in Chinese Temporal System Tense and aspect, which share certain similarity but significantly differ in nature, are crucial * Corresponding author concepts in the temporal system of a language. Compared with English grammar, the temporal system of Chinese grammar has a short history and the concept of tense and aspect in Chinese have been confused with each other even by some renowned scholars. This has caused negative consequences in the study related to the grammaticalization of time. It has been widely accepted that three Chinese particles, "zhe" "le" and "guo", are aspect markers in Chinese. And "guo" can be subdivided into two semantic variants called "guo1" and "guo2", of which "guo1" has been regarded as expressing a sense of "completeness" and "guo2" has been regarded as the marker of the experiential aspect, which also means the completeness of an action. However, the traditional theory fails to answer questions like "what is the difference between "guo1" and "guo2" if they both mean "completeness" ", "what is the relation between "guo1" and "guo2" " and "what is the nature of "the experiential aspect" in Chinese". This paper attempts to provide answers to all these questions. This paper aims to investigate the subtle nuances of meaning of two Chinese particles &quot;guo1&quot; and &quot;guo2&quot; as well as their different functions in Chinese temporal system. Two technical terms, &quot;tense&quot; and &quot;aspect&quot;, in traditional Chinese grammar are reconsidered in terms of the nature of these two concepts and the criteria to distinguish them. It is argued that in traditional Chinese grammar, &quot;tense&quot; and &quot;aspect&quot; are often mixed up by scholars, which has misled the study of &quot;guo1&quot; and &quot;guo2&quot;. Contrast to the traditional theory, this paper argues that &quot;guo1&quot; is the marker of the terminative aspect, while &quot;guo2&quot; is the marker of the past tense. Moreover, based on the markedness theory, the semantic and functional differences between &quot;guo1&quot; and &quot;guo2&quot; can be regarded as different usage of the particle &quot;guo&quot; in the unmarked or the marked sense.
A Non-local Attachment Preference in the Production and Comprehension of Thai Relative Clauses We investigated the role of locality (or proximity) in processing decisions by comparing two languages (Thai and English) that have evolved largely independently but share grammatical features that have been claimed to be crucial in sentence comprehension.A preference to associate words locally has been reported at least since the 1970s (Kimball, 1973;Gibson, 1998;inter alia). For example, in (1), the underlined relative clause (RC) can be attached to the non-local noun (N1, daughter) or to the local noun (N2, colonel).(1) The journalist interviewed the daughter of the colonel who had the accident.English readers prefer the RC to modify N2, whereas N1 is preferred in the corresponding construction in Spanish ( Cuetos and Mitchell, 1988). Various typological differences have been used to predict which languages violate locality by favoring N1 in such complex NPs (i.e., N1 of N2 RC).(2) A language L favors N1 attachment if:a. L has no alternative construction for expressing the N1 interpretation (Frazier and Clifton, 1996); b. L has flexible word order ( Gibson et al., 1996); c. L allows constituents (e.g., adverbs) to intervene between a verb and its direct object (Miyamoto, 1999); d. L exhibits consistent use of relative pronouns ( Hemforth et al., 2000); e. L has pseudo-RCs ( Grillo, 2012); f.L allows constituents (e.g., adjectives) to intervene between the modified noun and the RC (schematically: N adjective RC, the modifier-straddling hypothesis, MSH, Cuetos and Mitchell, 1988).All those competing proposals correctly predict that English does not violate locality as it favors N2.Thai is similar to English in a number of aspects. Word order is the same in the target construction (N1 of N2 RC) and a complementizer comparable to that (thî:) can be used as RC marker (there are two other RC markers, but thî: is the most frequent and has relatively few stylistic restrictions; Iwasaki and Ingkaphirom, 2009). The following properties are particularly relevant in the discussion on RC attachment.(3) a. Thai has at least two alternative unambiguous constructions to modify N1, namely, an RCpreposing construction (N1 RC of N2) and a compound-like structure (N1 N2 RC) resulting from the omission of the preposition. b. Thai is a rigid SVO language, in particular, verb and direct object have to be adjacent. c. The RC marker thî: has been claimed to be omissible in some environments (Iwasaki and Ingkaphirom, 2009;Kullavanijaya, 2010). d. Pseudo relative clauses are not available in Thai.The features in (3) together with the proposals in (2a-e) predict Thai to pattern with English in the comprehension of (1), thus resulting in a preference for N2 attachment.In contrast, according to the MSH (see (2f)), if a language allows the sequence N adjective RC, the adjective can be generalized to other types of modifiers (e.g., of N2), hence weakening the adjacency bias and increasing the likelihood that the RC will skip the intervening modifier and attach to N1 (Cuetos and Mitchell, 1988; see the general discussion on some possible counter-examples). Unlike English, adjectives are postnominal in Thai and can intervene between the noun and the RC. This should lead Thai readers to favor N1 according to the MSH. Therefore, the goal of this paper is to test the MSH against the proposals in (2a-e), which predict Thai to be an N2-attachment language.We report a corpus count and a self-paced reading experiment confirming the predictions of the MSH for thî:-marked RCs in Thai. In parsing, a phrase is more likely to be associated with an adjacent word than to a non-adjacent one. Instances of adjacency violation pose a challenge to researchers but also an opportunity to better understand how people process sentences and to improve parsing algorithms by, for example, suggesting new features that can be used in machine learning. We report corpus counts and reading-time data for Thai to investigate an adjacency violation that has been reported in other languages for ambiguous relative clauses that can be attached to either of two nouns, namely, the local noun (which is adjacent to the relative clause) or the non-local noun (which is farther from the relative clause). The results indicate that, unlike English, Thai violates adjacency by favoring non-local attachment even though the two languages share many grammatical features that have been linked to a local-attachment preference (e.g., rigid SVO word order). We re-interpret previous proposals to suggest that a language favors the non-local noun if it passes at least one of two tests. (1) Modifiers can intervene between noun and relative clause. (2) Adverbs can intervene between transitive verb and direct object.
Encoding Generalized Quantifiers in Dependency-based Compositional Semantics Dependency-based Compositional Semantics (DCS) provides a formal yet intuitive way to model natural language semantics. It was initially proposed in Liang et al. (2011) as a relational database querying protocol, and later used for logical inference in Tian et al. (2014a). Although the DCS inference framework provided decent support for both quantifiers all (universal quantifier) and no (negated existential quantifier), attention is required for an RTE system to cope with generalized quantifiers (GQ), including "at most n", "at least n", "most", etc., which can affect the direction or even the existence of an entailment relation, as demonstrated in Examples 1 to 3. ⇤ This work was conducted during an internship at the National Institute of Informatics, Japan. Example 1. P ) H but H ; P , where P At most 5 students like noodles. H At most 5 Japanese students like udon noodles.Example 2. P ) H but H ; P , where P At least 5 Japanese students like udon noodles. H At least 5 students like noodles.Example 3. P ; H and H ; P , where P Most Japanese students like udon noodles. H Most students like noodles.In this paper, we explore ways of encoding GQs in a recent framework of Dependency-based Compositional Semantics (DCS) ( Liang et al., 2013;Tian et al., 2014a), especially aiming to correctly handle linguistic knowledge like hyponymy when GQs are involved. We use selection operators, an extension mechanism described in Tian et al. (2014a), to implement a sub-type of GQs (Section 3.1). To deal with downward monotonicity of the predicate argument, we also propose a simple extension called "relation" to the framework (Section 3.2). This approach does not encode the exact semantics of every specific GQ, but instead captures some major properties that are both easily implementable with the current technology and useful in many cases.As in Tian et al. (2014a), we empirically tested the extended system on the "Generalized Quantifiers" section of the FraCaS corpus (The Fracas Consor- tium et al., 1996), and reduced 69% of the previous errors. A further error analysis reveals some limitations of the current approach, suggesting extensions towards more powerful logical systems. We hope this research could make linguistic knowledge like hyponymy a more effective resource for textual entailment tasks, and also shed some light on the handling of more complicated natural language inference phenomena. The extended system is publicly released at https://github.com/tomtung/ tifmo. For textual entailment recognition systems, it is often important to correctly handle Generalized quantifiers (GQ). In this paper, we explore ways of encoding GQs in a recent framework of Dependency-based Composi-tional Semantics, especially aiming to correctly handle linguistic knowledge like hy-ponymy when GQs are involved. We use both the selection operator mechanism and a new relation extension to implement some major properties of GQs, reducing 69% errors of a previous system, and a further error analysis suggests extensions towards more powerful logical systems.
On the Argument Structures of the Transitive Verb fan &apos;annoy; be annoyed; bother to do&apos;: A study based on two comparable corpora  This paper investigates the transitive uses of the verb fan &quot;
The Semantics of khɨn3 and loŋ1 in Thai Compared to up and down in English: A Corpus-Based Study Expressions of spatial directions are common in the world's languages. Given that spatial direction is a basic concept of humans (Langacker, 1987), spatial terms are expected to be of high frequency in language use. This study examines spatial terms for vertical directions in Thai and English. In particular, we focus on khɨn3 'ascend' and loŋ1 'descend' in Thai in comparison with up and down in English.The words khɨn3 'ascend' and loŋ1 'descend' in Thai are high-frequency words whose fundamental meanings are about vertical movement of upward and downward directions, respectively. Similarly, the words up and down in English have the basic senses of vertical directions. Moreover, both khɨn3 and up can be used to denote non-directional meanings (such as man4 caj1 khɨn3 'be more confident' and speed up), and this is also true with the pair loŋ1 and down (such as sin3sut2 loŋ1 'end' and close down). However, while khɨn3 and loŋ1 occur as main verbs or subsidiary verbs in serial verb constructions in Thai, up and down rarely occur in verb slots in English; they usually appear as satellites accompanying verbs. It is therefore interesting to investigate to what extent these vertical spatial expressions, which belong to different grammatical categories, overlap in terms of senses.To obtain objective, up to date and naturally occurring language data produced by various native speakers, this study utilized data from three corpora. The English data came from the British National Corpus (BNC), and the Thai data were drawn from the Thai National Corpus (TNC) ( Aroonmanakun et al., 2009). A parallel corpus, the English-Thai Parallel Concordance This corpus-based study analyzes meanings of khɨn3 &apos;ascend&apos; and loŋ1 &apos;descend&apos; in Thai in comparison with up and down in English. Data came from three corpora: the Thai National Corpus (TNC) (Aroonmanakun et al., 2009), the British National Corpus (BNC), and the English-Thai Parallel Concordance (Aroonmanakun, 2009). Results of the analyses show that there are senses of the vertical spatial terms khɨn3 and loŋ1 in Thai that overlap with those of up and down in English. This reflects a universal image schema of vertical movement and similar semantic extension processes in the two languages. Data from the parallel corpus also reveal that the vertical spatial terms khɨn3 and loŋ1 do not always occur in the same contexts with up and down. But, when they do, the frequently shared meaning involves vertical movement, which is the basic sense of the terms. The use of corpora as a tool to study the semantics of vertical spatial terms in Thai and English makes it possible to obtain objective and naturalistic data as well as to observe frequency of various senses that are in use.
Noun Paraphrasing Based on a Variety of Contexts Although extensive and various forms of text data are easily available in the present age, in order for readers to gather information effectively, they need technology that overcomes any differences in their linguistic competence. For example, technology that buries the difference in the linguistic competence of foreign language learners, children, the elderly, and disabled persons is useful (Inui and Fujita, 2004). We present our research on paraphrasing to control language at the elementary school level in order to simplify texts for children. We believe that vocabulary simplification for children can be realized by paraphrasing text according to Basic Vocabulary to Learn (BVL) (Kai and Matsukawa, 2002) . BVL is a collection of words selected on the basis on a lexical analysis of elementary school textbooks. It contains 5,404 words that can help children write expressively.As previous work indicated, there are lexical paraphrases that define statements from a Japanese dictionary ( Kajiwara et al., 2013). The definition statements from the Japanese dictionary explain a given headword in several easy words. Therefore, lexical simplification and paraphrasing that conserves a particular meaning are expected by paraphrasing the headword with the words in the definitions. However, definition statements are short sentences that consist of several words. Consequently, there are few paraphrase candidates, and natural paraphrasing is difficult even if we use certain dictionaries together. In addition, the definition statement as a whole is equivalent to the headword; there is no guarantee that any individual word extracted from the definition statement can paraphrase the headword.We propose lexical paraphrasing based on a variety of contexts obtained from a large corpus without depending on existing lexical resources from such a background. The proposed method is not dependent on language, thus it can perform lexical paraphrases using a corpus of arbitrary languages. In this paper we examine and report on Japanese nouns. We paraphrase nouns along the contexts of sentence input on the basis of a variety of contexts obtained from a large-scale corpus. The proposed method only uses the number of types of context, not word frequency or co-occurrence frequency features. This method is based on the notion that paraphrase candidates appear more commonly with target words in the same context. The results of our experiment demonstrate that the approach can produce more appropriate paraphrases than approaches based on co-occurrence frequency and pointwise mutual information.
K-repeating Substrings: a String-Algorithmic Approach to Privacy-Preserving Publishing of Textual Data The increasing amount of electronically available and searcheable texts poses an increasing need for privacy protection. Adversaries may extract previously unconnected information about a person by aggregating different data sources. IDs such as social security numbers in the United States are obvious means to aggregate data sources, but full names, residential addresses, and other attributes about individuals and their combinations may also work as pseudo identifiers from which one may be able to identify persons, or to raise the probability of successful identification (Sweeney, 2002)( Fung et al., 2010). While researchers and service providers wish to publish and share such textual data with the community to help facilitate further research, it is costly to do so while preserving utility of non-sensitive part of the data.In response to the demand for efficient and accurate automatic methods to help removing sensitive material from textual data, the last decade has seen progress in automatic anonymization and deidentification of text ( Liu, 2012;Fung et al., 2010;Uzuner et al., 2007). For example, health care industry puts efforts in utilizing electronic health record data that is accumulated daily while ensuring patients' privacy ( Kushida et al., 2012;Meystre et al., 2010). Nevertheless, two major problems remain unaddressed: 1) How to reduce human labor to prepare resources for automatic methods, including pattern-matching rules and training data for supervised-learning systems. 2) How to increase utility of published text by requiring less preprocessing of the input text.Our work explores aplicability of string algorithms into privacy-preserving publishing of textual data that reduce resource requirements. We propose using new variations of maximum repeats algorithms to unsupervisedly suggest spans to be hidden. We argue that our approach brings new assets to previous studies in text anonymization by requiring less linguistic resources and preprocessing; these points will be discussed in more detail in Section 3.Contributions of this paper are as follows. De-identifying textual data is an important task for publishing and sharing the data among researchers while protecting privacy of individuals referenced therein. While supervised learning approaches are successfully applied to the task in the clinical domain, existing methods are hard to transfer to different domains and languages because they require a considerable cost and time for preparation of linguistic resources. This paper presents an efficient unsupervised algorithm to detect all substrings occurring less than k times in the input string, based on the assumption that such rare sequences are likely to contain sensitive information such as names of people and rare diseases that may identify individuals. The proposed algorithm works in asymptotically and empirically linear time against the input size when k is a constant. Empirical evaluation on the i2b2 (Informatics for Integrating Biology and Bedside) dataset shows the effectiveness of the algorithm in comparison to baselines that use simple word frequencies.
PACLIC 2015 29th Pacific Asia Conference on Language, Information and Computation Proceedings of PACLIC 2015: Oral Papers Program chair: Talk Title:Research Activities for Translating Asian Languages  The PACLIC series of conferences emphasize the synergy of theoretical analysis and processing of language, and provide a forum for researchers in different fields of language study in the Pacific-Asia region to share their findings and interests in the formal and empirical study of languages. For the past years since its establishment, the PACLIC conferences have gained more and more interests and participations from linguistic researchers, as evidenced by the increasing number of papers and by the wider range of topics. Organized under the auspices of the PACLIC Steering Committee, it is the latest installment of our long standing collaborative efforts among theoretical and computational linguists in the Pacific-Asia region. PACLIC conference has received an overwhelming response of 221 papers from 104 countries or regions (87.50% from 10 regions in Asia, 6.73% from 4 regions in Europe, 3.85% from Africa, 1.92% from New Zealand). To ensure that all accepted papers meet the high quality standard of the PACLIC conference, each submission was reviewed by 2-4 reviewers. As a result, only approximately 63 (28.5%) of top-notch academic papers were accepted for oral presentations and 41 (18.5%) for poster sessions. From these accepted papers, 104 (47.0%) papers were presented and published in this proceedings. Abstract: This talk will introduce automatic translation projects for Asian languages, wherein we intend to seek greater cooperation.
Translation of Unseen Bigrams by Analogy Using an SVM Classifier Over the last decade, phrase-based statistical machine translation ( Koehn et al., 2003) systems have demonstrated that they can produce reasonable quality when ample training data is available, especially for language pairs with similar word order. However, the PB-SMT model has not yet been capable of satisfying the various translation tasks for very different languages ( Isozaki et al., 2010). The existence of translation divergences makes the straightforward transfer from source sentences into target sentences hard. Though many previous pieces of work (Dorr, 1994;Habash et al., 2002;Dorr et al., 2004) have attempted to take account for divergences and to deal with this linguistic problem using various translation approaches. This paper further inquires the topic.Since sentence consists of bigrams, instead of analysing the syntactic structures of the whole sentence or part of the sentence as in (Ding and Palmer, 2005), we explore the possibilities of translating unseen bigrams based on an analogy learning method. We investigate the coverage of translated bigrams in the test set and inspect the probability of translating a bigram using analogy. Analogical learning has been investigated by several authors. To cite a few, Lepage et al. (2005) showed that proportional analogy can capture some syntactic and lexical structures across languages. Langlais et al. (2007) investigated the more specific task of translating unseen words. Bayoudh et al. (2007) explored generating new learning examples from very scarce original learning data using analogy to train an SVM classifier. Dandapat et al. (2010) performed transliteration by analogical learning for English-to-Hindi.In the issue of translation using analogy, one of the main drawbacks should be addressed is the problem of "over-generative". Analogy is able to capture the most divergences of translation in the most cases, yet it generates a great number of solutions that are ungrammatical and incorrect. In this paper, we propose to translate useen bigrams as reconstructing with the principle of analogy learning. In machine learning, SVMs have been shown that it is efficient in performing a non-linear classification. By specifying features used in experiment, we employ an SVM classifier to fast filter the solutions output by the analogy solver. The final goal of this research is to explore the possibility of translation using analogy and point out a feasible way to solve the problem of "over-generative".The remainder of this paper is organized as follows: Section 2 describes basic notions in alignment and analogy. In Section 3, we explore the classification of bigrams and their contributions to the whole corpus and report some profiling results. Section 4 presents our approach, depending on the analogous, and describes how to processing the data and extract examples for training an SVM classifier. We also evaluate the result using the some standard measures. Finally, in Section 5, conclusions and perspectives are presented. Detecting language divergences and predicting possible sub-translations is one of the most essential issues in machine translation. Since the existence of translation divergences, it is impractical to straightforward translate from source sentence into target sentence while keeping the high degree of accuracy and without additional information. In this paper, we investigate the problem from an emerging and special point of view: bigrams and the corresponding translations. We first profile corpora and explore the constituents of bigrams in the source language. Then we translate unseen bigrams based on proportional analogy and filter the outputs using an Support Vector Machine (SVM) classifier. The experiment results also show that even a small set of features from analogous can provide meaningful information in translating by analogy.
Distant Supervision for Entity Linking To build the "Digital Alexandria Library" for our human race, researchers in the NLP community have dedicated themselves to Information Extraction (Sarawagi, 2008) over the past decades. Information extraction focuses on processing natural language text to produce structured knowledge, which is usually represented as triples (two entities and their relation) for the convenience of storage in a database, retrieval, or even automatic reasoning. For example, if we send a natural language sentence, Michael Jordan visited CMU yesterday, to the pipeline of information extraction machine, it will be processed by three operations in advance, i.e.,• Named Entity Recognition ( Nadeau and Sekine, 2007): Entities should firstly be identified and classified into predefined categories, such as person (PER), location (LOC) and organization (ORG). The sentence will be annotated as [Michael Jordan]/PER visited [CMU]/ORG yesterday, after being processed by this operation.• Coreference Resolution (Ng, 2010): Some entities may have alias or abbreviations. It is well known that CMU is the abbreviation for Carnegie Mellon University. The knowledge repository may only store the regularized name, e.g., Carnegie Mellon University, for this named entity, so coreference resolution is indeed necessary.• Relation Extraction ( Bach and Badaskar, 2007): After both of the named entities ([Michael Jordan]/PER and [Carnegie Mellon University]/ORG) are recognized and regularized, we begin to study on the relation between them. In this case, we extract the verb visited and map it to the relation visit. Then the output will be a triple, i.e., (Michael Jordan [PER], visit, Carnegie Mellon University [ORG]).So far, we only abstract the triple as the structured knowledge from the natural language sentence. However, it devotes nothing to increasing the scale of the knowledge repository such as Free-base ( Bollacker et al., 2007) which is a huge 1 , public 2 , collaborative 3 (Bollacker et al., 2008) and online knowledge base with billions of triples and millions of disambiguated entities, and is primarily maintained by Google Inc., because we even do not know which exact Michael Jordan the triple (Michael Jordan [PER], visit, Carnegie Mellon University [ORG]) refers to in Freebase. As illustrated in Figure 1, there are three different persons named Michael Jordan in Freebase and each of them may be the protagonist of that news. Therefore, to populate knowledge repositories (Ji and Grishman, 2011), we need the fourth operation:• Entity Linking ( Rao et al., 2013): It concerns about the study of aligning a textual entity mention to the corresponding disambiguated entry in a knowledge repository. More specifically, since there are several Michael Jordan disambiguated by different MIDs (machine identifiers) as illustrated in Figure 1, we may build a classifier that can help assign the Michael Jordan in the extracted triplet (Michael Jordan [PER], visit, Carnegie Mellon University [ORG]) to the exact named entity in Freebase or find out that this Michael Jordan is a newly discovered named entity (NIL).  Mihalcea and Csomai, 2007;Cucerzan, 2007;Milne and Witten, 2008;Ratinov et al., 2011) and the entity linking tracks 4 in TAC-KBP (McNamee and Dang, 2009;Ji et al., 2010) concentrate on linking ambiguous entities to the entries in Wikipedia, whereas our ultimate goal is to populate the structured knowledge repository, e.g., Freebase. However, to the best of our knowledge, few works ( Zheng et al., 2012) concern about disambiguating named entities using Freebase which contains much more entries but less text information for each entry than Wikipedia.Overall ing Wikipedia and Freebase, respectively. As both of the two collaborative web resources have their respective superiorities, i.e., more context information and more disambiguated entities, we begin to study a new paradigm that could bridge the gap between those two separated repositories and benefit from their respective advantages. From the perspective of supervised learning, entity linking can be naturally regarded as a classification problem. To build a training dataset for disambiguating a set of entities with the same name, we can firstly collect the sentences that mention that name from webpages, such as Wiki pages 5 , and then manually annotate each entity mention with its unique machine identifier (MID) in Freebase given the contexts of sentences that it occurs in. However, hand-labeled data is time consuming and usually applicable to some specific classes of entities, such as person (PER), location (LOC) and organization (ORG). Therefore, we look forward to an approach that averts the tedious and laborious work.Inspired by the idea of weak labeling (Fan et al., 2014;Craven et al., 1999), we contribute a new paradigm called distantly supervised entity linking (DSEL) without manual annotation in this paper. More specifically, we take advantage of a heuristic alignment assumption based on crowd sourcing to connect a certain disambiguated entity in Freebase with its related webpages. In these webpages, feature vectors can be extracted from the sentence-level textual contexts of that entity mention, and be labeled by its corresponding MID in Freebase. Then we can produce a large scale of weakly labeled 6 dataset in this way. Moreover, it is unrealistic to learn a specific classifier for each entity, as there are about 43 million disambiguated entities in Freebase. To tackle with those challenges, we propose a strategy of training a general classifier for disambiguating multiple entities and select a well known classifier, i.e., liblinear (Fan et al., 2008) to self-learn the weights among the high-dimensional sparse and noisy features. Experiments are conducted on a dataset of 140,000 items and 60,000 features. DSEL achieves a baseline F1-measure of 0.517. Furthermore, we analyze the performance influenced by other different features, and finally the F1-measure is improved to 0.545. Entity linking is an indispensable operation of populating knowledge repositories for information extraction. It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL benefits more via jointly leveraging the respective advantages of Freebase and Wikipedia. Specifically, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedi-a) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545.
Reducing Lexical Features in Parsing by Word Embeddings Lexical features are powerful machine learning ingredients for many NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential.Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003;Nivre et al., 2006;Zhang and Clark, 2008;Huang and Sagae, 2010; Zhang  and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s 0 w and q 0 w, respectively) are typically used as features to calculate scores of transitions. When s 0 w is used as a feature template, the features in this template (e.g. s 0 w saw and s 0 w look ) can be viewed as one-hot vectors of a dimension of the lexicon size ( Figure 1). Corresponding to s 0 w, a weight is assigned to each word (e.g. W (s 0 w saw ) and W (s 0 w look )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s 0 w by d features, namely s 0 e 1 , . . . , s 0 e d . Given the vector representation of a word (e.g., e saw = (0.6, . . . , 0.2)), we replace the lexical feature (e.g. s 0 w saw ) by a linear combination of the d features (e.g., s 0 e saw := 0.6s 0 e 1 + . . . + 0.2s 0 e d ). Then, instead of the weights in a number of lexicon size assigned to s 0 w, now we use d weights (i.e., W (s 0 e 1 ), . . . , W (s 0 e d )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s 0 wq 0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings ( Koo et al., 2008;Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among in-vocabulary words.
High-order Graph-based Neural Dependency Parsing There have been two classes of typical approaches for dependency parsing: transition-based parsing and graph-based parsing. The former parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003;Nivre, 2003), while the latter searches for a tree through graph algorithms by decomposing trees into factors. This paper will focus on graph-based methods, which are based * Correspondence author. † This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248 on dynamic programming strategies (Eisner, 1996;McDonald et al., 2005;McDonald and Pereira, 2006). In this recent decade, extensions have been made to use high-order factors (Carreras, 2007;Koo and Collins, 2010) in graph models and the highest one considers fourth-order ( Ma and Zhao, 2012). However, all those methods usually use sparse indicator features as inputs and linear models to get the scores for later inference process. They are easy to suffer from the problem of sparsity, and linear models can be insufficient to effectively integrate all the sparse features in spite of various rich context that can be potentially exploited.Distributed representations and neural network provide a way to alleviate such a drawback ( Bengio et al., 2003;). Instead of highdimensional sparse indicator feature vectors, distributed representations use low-dimensional dense vectors (also known as embeddings) to represent the features, and then they are usually used in a neural network. For example, in the traditional methods, a word is usually expressed by a one-hot vector; while distributed representations use a dense vector. By appropriate representation learning (usually by back-propagations in neural network), these embeddings can replace traditional sparse features and perform quite well together with neural network.In recent years, using distributed representations and neural network has gradually gained popularity in natural language processing (NLP) since the pioneer work of ( Bengio et al., 2003). Several neural network language models have reported exciting results for the tasks of machine translation and speech recognition (Schwenk, 2007;Mikolov et al., 2010;Wang et al., 2013;Wang et al., 2014;). Many other tasks of NLP have also been reconsidered using neural network, the SENNA system 1 ) solved the tasks of partof-speech (POS) tagging, chunking, named entity recognition and semantic role labeling.In this work, we utilize neural network for firstorder, second-order and third-order graph-based dependency parsing, with the help of the existing graph-based parsing algorithms. For high-order parsing, it is performed after the first-order parser prunes unlikely parts of the parsing tree. We use neural network to learn dense representations for word, POS and distance information, and predict how likely the dependency relationships are for a sub-tree factor in the dependency tree. For unlabeled projective dependency parsing, we have put a free distribution of our implementation on the Internet 2 .The remainder of the paper is organized as follows: Section 2 discusses related work, Section 3 gives the background for graph-based dependency parsing, Section 4 describes our neural network model and how we utilize it with graph-based parsing and Section 5 presents our experiments, results and some discussions. We summarize this paper in Section 6. In this work, we present a novel way of using neural network for graph-based dependency parsing, which fits the neural network into a simple probabilistic model and can be furthermore generalized to high-order parsing. Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efficient inference.
A Dynamic Syntax Modelling of Postposing in Japanese Narratives 1 Japanese is prescriptively verb-final as in (1)a, but elements may be placed after a verb in colloquial register. In (1)b, sushi-o appears after tabe 'eat.'(1) a. Ken-ga sushi-o tabe-ta-yo K-NOM sushi-ACC eat-PAST-FP 'Ken ate sushi.' b. Ken-ga Δ tabe-ta-yo, sushi-o K-NOM eat-PAST-FP sushi-ACCThe postposed item sushi-o is underlined in (1)b, and the gap is notated as Δ without any theoretical implications. Finally, yo is a final particle (FP) that appears in colloquial register. Japanese postposing has been explored in formal syntax ( Takano 2014, Takita 2014) as well as in dialogue/discourse studies ( Nomura 2008, Ono 2006). Except for Fujii (1995: 169), grammatical properties of Japanese postposing have not been examined based on naturally-occurring materials.We provide narrative data to set out an empirical ground of a grammatical study of postposing:o It seems postposing may occur at an embedded level, contrary to the prevalent, opposing view. o A wider variety of syntactic element may be postposed than has been held in the literature.These syntactic flexibilities pose a challenge for grammar modelling, and we propose a solution in Dynamic Syntax (DS) ( Cann et al. 2005). DS has been employed for postposing in several languages (Section 4); still, no analysis has been developed for Japanese presumably because it allows a wider range of items to be postposed. The application to Japanese advances formal aspects of the theory and broadens empirical coverage. Japanese is prescriptively said to be verb-final, but it exhibits postposing in colloquial register, where an element is placed after a verb. Based on narrative data, we show that the syntactic type of postposed element is quite diverse and that, contrary to the prevalent, opposing view, Japanese postposing is not restricted to a matrix clause. These issues are addressed in Dynamic Syntax, with the outcome of developing some formal aspects of the framework.
Unsupervised and Lightly Supervised Part-of-Speech Tagging Using Recurrent Neural Networks Nowadays, Natural Language Processing (NLP) tools (part-of-speech tagger, sense tagger, syntactic parser, named entity recognizer, semantic role labeler, etc.) with the best performance are those built using supervised learning approaches for resourcerich languages (where manually annotated corpora are available) such as English, French, German, Chinese and Arabic. However, for a large number of resource-poor languages, annotated corpora do not exist. Their manual construction is labor intensive and very expensive, making supervised approaches not feasible.The availability of parallel corpora has recently led to several strands of research work exploring the use of unsupervised approaches based on linguistic annotations projection from the (resourcerich) source language to the (under-resourced) target language. The goal of cross-language projection is, on the one hand, to provide all languages with linguistic annotations, and on the other hand, to automatically induce NLP tools for these languages. Unfortunately, the state-of-the-art in unsupervised methods, is still quite far from supervised learning approaches. For example, Petrov et al. (2012) obtained an average accuracy of 95.2% for 22 resource-rich languages supervised POS taggers, while the state-of-the-art in the unsupervised POS taggers achieved by Das and Petrov (2011) and Duong et al. (2013) with an average accuracy reaches only 83.4% on 8 European languages. Section 2 presents a brief overview of related work.In this paper, we first adapt a similar method than the one of Duong et al. (2013)  1 , to build an unsupervised POS tagger based on a simple cross-lingual projection (Section 3.1). Next, we explore the possibility of using a recurrent neural network (RNN) to induce multilingual NLP tools, without using word alignment information. To show the potential of our approach, we firstly investigate POS tagging.In our approach, a parallel corpus between a resource-rich language (having a POS tagger) and a lower-resourced language is used to extract a common words representation (cross-lingual words representation) based only on sentence level alignment. This representation is used with the source side of the parallel corpus (tagged corpus) to learn a neural network POS tagger for the source language. No word alignment information is needed in our approach. Based on this common representation of source and target words, this neural network POS tagger can also be used to tag target language text (Section 3.2).We assume that these two models (baseline crosslingual projection and RNN) are complementary to each other (one relies on word-alignment information while the other does not), and the performance can be further improved by combining them (linear combination presented in Section 3.3). This unsupervised RNN model, obtained without any target language annotated data, can be easily adapted in a weakly supervised manner (if a small amount of annotated target data is available) in order to take into account the target language specificity (Section 4).To evaluate our approach, we conducted an experiment, which consists of two parts. First, using only parallel corpora, we evaluate our unsupervised approach for 4 languages: French, German, Greek and Spanish. Secondly, the performance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. In this paper, we propose a novel approach to induce automatically a Part-Of-Speech (POS) tagger for resource-poor languages (languages that have no labeled training data). This approach is based on cross-language projection of linguistic annotations from parallel corpora without the use of word alignment information. Our approach does not assume any knowledge about foreign languages, making it applicable to a wide range of resource-poor languages. We use Recurrent Neural Networks (RNNs) as multilingual analysis tool. Our approach combined with a basic cross-lingual projection method (using word alignment information) achieves comparable results to the state-of-the-art. We also use our approach in a weakly supervised context, and it shows an excellent potential for very low-resource settings (less than 1k training utterances).
Japanese Sentiment Classification with Stacked Denoising Auto-Encoder using Distributed Word Representation As the popularity of social media continues to rise, serious attention is being given to review information nowadays. Reviews with positive/negative ratings, in particular, help (potential) customers with product comparisons and to make purchasing decisions. Consequently, automatic classification of the polarities (such as positive and negative) of reviews is extremely important.Traditional approaches to sentiment analysis utilize polarity dictionaries or classification rules. Although these approaches are fairly accurate, they depend on languages that may require significant amounts of manual labor. Further, dictionary-based methods have difficulty dealing with new or unknown words.Machine learning-based methods are widely adopted in sentiment classification in order to mitigate the problems associated with the making of dictionaries and/or rules. One of the most basic features used in machine learning-based sentiment classification is the bag-of-words feature ( Wang and Man- ning, 2012;Pang et al., 2002). In machine learningbased frameworks, the weights of words are automatically learned from a training corpus instead of being manually assigned.However, the bag-of-words feature cannot take syntactic structures into account. This leads to mistakes such as "a great design but inconvenient" and "inconvenient but a great design" being deemed to have the same meaning, even though their nuances are different; the former is somewhat negative whereas the latter is slightly positive. To solve this syntactic problem, Nakagawa et al. (2010) proposed a sentiment analysis model that used dependency trees with polarities assigned to their subtrees. However, their proposed model requires specialized knowledge to design complicated feature templates.In this study, we propose an approach that uses distributed word representation to overcome the first problem and deep neural networks to alleviate the second problem. The former is an unsupervised method capable of representing a word s meaning without using hand-tagged resources such as a polarity dictionary. In addition, it is robust to the data sparseness problem. The latter is a highly expressive model that does not utilize complex engineering features or models.Our research makes the following two main contributions:• We show that distributed word representation learned from a large-scale corpus and multiple layers (more than three layers) contributes significantly to classification accuracy in sentiment classification tasks.• We achieve state-of-the-art performance in Japanese sentiment classification tasks without designing complex features and models. Traditional sentiment classification methods often require polarity dictionaries or crafted features to utilize machine learning. However , those approaches incur high costs in the making of dictionaries and/or features, which hinder generalization of tasks. Examples of these approaches include an approach that uses a polarity dictionary that cannot handle unknown or newly invented words and another approach that uses a complex model with 13 types of feature templates. We propose a novel high performance sentiment classification method with stacked denoising auto-encoders that uses distributed word representation instead of building dictionaries or utilizing engineering features. The results of experiments conducted indicate that our model achieves state-of-the-art performance in Japanese sentiment classification tasks.
Is Wikipedia Really Neutral? A Sentiment Perspective Study of War-related Wikipedia Articles since 1945 Wikipedia is the largest and most widely used encyclopaedia in collaborative knowledge building (Medelyan et al., 2009). Since its start in 2001, it contains more than 33 million articles in more than 200 languages, while only about 4 million articles are in English 1 . Possible sources for the content include books, journal articles, newspapers, webpages, sound recordings 2 , etc. Although a "Neutral point of view" (NPOV) 3 is Wikipedia's core content policy, we believe sentiment expression is inevitable in this user-generated content. Already in (Green- stein and Zhu, 2012), researchers have raised doubt about Wikipedia's neutrality, as they pointed out that "Wikipedia achieves something akin to a NPOV across articles, but not necessarily within them". Moreover, people of different language backgrounds share different cultures and sources of information. These differences have reflected on the style of contributions ( Pfeil et al., 2006) and the type of information covered (Callahan and Herring, 2011). Furthermore, Wikipedia webpages actually allow to contain opinions, as long as they come from reliable authors 4 . Due to its openness to multiple forms of contribution, the articles on Wikipedia can be viewed as a summarisation of thoughts in multiple languages about specific topics. Automatically detecting and measuring the differences can be crucial in many applications: public relation departments can get some useful suggestions from Wikipedia about topics close to their hearts; Wikipedia readers can get some insights about what people speaking other languages think about the same topic; Wikipedia administrators can quickly locate the Wikipedia articles that express extreme sentiment, to better apply the NPOV policy, by eliminating some edits.In order to further gain insight on these matters, and especially, on the degree of neutrality on given topics presented in different languages, we explore an approach that can perform multiple levels of sentiment analysis on multilingual Wikipedia articles. We generate graded sentiment analysis results for multilingual articles, and attribute sentiment analysis to concepts, to analyse the sentiment that onespecific named entity is involved in. For the sake of simplicity, we restrict our scenario within the war-related topics, although our approach can be easily applied on other domains. Our results show that even though the overall sentiment polarities of multilingual Wikipedia articles on the same war-related topic are consistent, the strengths of sentiment expression vary from language to language.The remainder of the paper is structured as follows. In Section 2, we present an overview of different approaches of sentiment analysis. Section 3 describes the approach selected in this research to perform article level and concept level sentiment analysis on multilingual Wikipedia articles. In Section 4, experimental results are presented and analysed, and in Section 5, we conclude the major findings and remarks for further research. Wikipedia is supposed to be supporting the &quot;Neutral Point of View&quot;. Instead of accepting this statement as a fact, the current paper analyses its veracity by specifically analysing a typically controversial (negative) topic, such as war, and answering questions such as &quot;Are there sentiment differences in how Wikipedia articles in different languages describe the same war?&quot;. This paper tackles this challenge by proposing an automatic methodology based on article level and concept level sentiment analysis on multilingual Wikipedia articles. The results obtained so far show that reasons such as people&apos;s feelings of involvement and empathy can lead to sentiment expression differences across multilingual Wikipedia on war-related topics; the more people contribute to an article on a war-related topic, the more extreme sentiment the article will express; different cultures also focus on different concepts about the same war and present different sentiments towards them. Moreover, our research provides a framework for performing different levels of sentiment analysis on multilingual texts.
Sentiment Analyzer with Rich Features for Ironic and Sarcastic Tweets Whenever a message is encoded into linguistic form for being communicated -either in a spoken or written text 1 -information revealing judgments, evaluations, attitudes and emotions is also encoded (Mar- tin and White, 2005). This is true for both informal and formal texts, independently of how much attention the writer pays in cleaning such information out. This is also true for texts posted on social networks (i.e. Facebook, Twitter, etc.), where judgments, evaluations, attitudes and emotions constitute an important part of the message (Pak and Paroubek, 2010).Sentiment analysis (also known as opinion mining and subjectivity analysis) is a Natural Language Processing (NLP) task that focuses on identification of such judgments, evaluations, attitudes and emotions. It can be compared to other classification tasks, as it consists in associating the analyzed texts with a label that represents the sentiment of the message or the affective state of the writer (Hart, 2013).In its earliest incarnations, sentiment analysis was limited to the identification of the polarity of the texts, and the classification label was either positive or negative. Later on, the task was extended to address more challenging and complex goals, such as the identification of the sentiment of the messages or the writer's affective state in a more fine scale, with labels including anger, happiness or depression.Such extension could not avoid considering one of the most pervasive tools used in communication, namely figurative language. In fact, this expressive tool is not only very frequent in various kinds of texts, but it also strongly affects the sentiment expressed in the text, often completely reversing its polarity ( Xu et al., 2015;Ghosh et al., 2015).Because figurative language is used in unpredictable ways in communication (i.e. either in crystallized forms or in creative ways) and it can involve several linguistic and extra-linguistic levels (i.e. from syntax to concepts and pragmatics), its identification and understanding is often difficult, even for human beings. If humans are able to rely on prosody (e.g. stress or intonation), kinesis (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background, machines cannot access the same type of information. These difficulties pose a major challenge in sentiment analysis.Currently, a large number of studies have been devoting to the problem. Most of them focus on microblogging, especially Twitter, because i) social networks are rich of spontaneous public messages written by several users in different styles; ii) tweets are short (i.e. a tweet can contain maximum 140 characters) and containing a lot of unconventional textual elements (e.g. emoticons, abbreviations, slang, emphasized capitalization and punctuation, etc.), which pose another interesting challenge; iii) social networks provide a precise picture of peoples' sentiments about a topic or product in a specific moment. This third point, in particular, is relevant for companies, political parties and other public entities in order to adapt and improve their marketing strategies and decisions (Medhat et al., 2014;Pang and Lee, 2008).In this paper, we introduce a sentiment analysis system created with a particular focus on the identification and proper elaboration of irony and sarcasm in tweets. The system is developed by combining and improving two previous algorithms (Tungth- amthiti et al., 2014;Xu et al., 2015). In particular, we propose a new method for coherence identification across sentences, some additional features indicating the strong emotion of the Twitter user, and several features of punctuations &amp; special symbols that contribute to the final sentiment score. Sentiment Analysis of tweets is a complex task, because these short messages employ unconventional language to increase the ex-pressiveness. This task becomes even more difficult when people use figurative language (e.g. irony, sarcasm and metaphors) because it causes a mismatch between the literal meaning and the actual expressed sentiment. In this paper, we describe a sentiment analysis system designed for handling ironic and sarcastic tweets. Features grounded on several linguistic levels are proposed and used to classify the tweets in a 11-scale range, using a decision tree. The system is evaluated on the dataset released by the organizers of the SemEval 2015, task 11. The results show that our method largely outperforms the systems proposed by the participants of the task on ironic and sarcastic tweets.
Sentiment Classification of Arabic Documents: Experiments with multi-type features and ensemble algorithms With the expanding growth of social networks services, user generated content web has emerged from being a simple web space for people to express their opinions and to share their knowledge, to a high value information source for business companies to discover consumer feedbacks about their products or even to decide future marketing actions. Therefore, opinion mining is becoming a potential research domain interesting more and more researchers who attempt to improve current results and to solve more advanced and complex issues in the domain. Typically, mining opinions is viewed as a classification problem called sentiment classification. Sentiment classification aims to determine whether the semantic orientation of a text is positive, negative or neutral. It can be tackled at many levels of granularity: expression or phrase level, sentence level, and document level. Expression sentiment classification aims to determine the prior sentiment class or valence of an expression. As for sentence level, the objective is to calculate the contextual polarity of a sentence. Concerning document level, which is our focus in this research, the main goal is to mine the overall polarity of a document with the hypothesis that is expressed by a single author towards a single target.Document sentiment classification is often processed by applying machine learning techniques, in particular supervised learning which consists basically of two major steps: feature extraction and training the learning model. In the literature, most existing researches rely on ngrams as selected features, and on a simple basic classifier as learning model. The limit of these two choices is revealed when shifting from one domain to another. As a matter of fact, in one hand, each domain has generally his specific vocabulary. So, n-grams features produced from one domain fail to be discriminative in another. In the other hand, numerous studies showed that the performance of classification algorithms is domain dependent (Xia et al., 2011).In the context of our work, we try to improve document classification findings in Arabic sentiment analysis by (i) combining different types of features such as opinion and discourse features; and by (ii) proposing an ensemble-based classifier consisting of a set of accurate basic classifiers to investigate its contribution in Arabic sentiment classification similarly to some other languages such as Chinese ( Wang et al., 2014).The rest of the paper is organized as follows. In section 2, we review a selection of related work to document sentiment classification for English and Arabic languages. In section 3, we detail our proposed approach and focus on the feature extraction and the classification model selection steps. In section 4, we describe the conducted experiments and discuss the obtained results. Finally, we summarize our conclusions and provide some perspectives. Document sentiment classification is often processed by applying machine learning techniques, in particular supervised learning which consists basically of two major steps: feature extraction and training the learning model. In the literature, most existing researches rely on n-grams as selected features, and on a simple basic classifier as learning model. In the context of our work, we try to improve document classification findings in Ara-bic sentiment analysis by combining different types of features such as opinion and discourse features; and by proposing an ensemble-based classifier to investigate its contribution in Arabic sentiment classification. Obtained results attained 85.06% in terms of macro-averaged F-measure, and showed that discourse features have moderately improved F-measure by approximately 3% or 4%.
Pan&apos;s (2001) puzzle revisited It has been widely noted that what licenses the longdistance binding is closely related to the logophoric property of reflexives. More specifically, since Sells' (1987) logophoric approach on Icelandic and Japanese, many researchers (Yoon 1989, Huang and Liu 2001, among others) have argued that the binding behaviors of long-distance anaphors, such as those in Korean and Chinese, are attributed to the logophoric use of reflexives and that they carry the de facto identical function.Huang and Liu (2001) point out that the three distinct roles in discourse, which are source, self, and pivot originally coined by Sells (1987), for the logophoric use of the Chinese long-distance ziji are a necessary but not a sufficient condition for longdistance anaphors. For this reason, they suggest that the notion of attitude de se be introduced to the longdistance anaphor ziji. 1 However, despite a close link between the longdistance anaphor and logophoricity as a licensing condition for the referent it refers to, it has been repeatedly observed that logophoric accounts of longdistance anaphors have not been fully successful, facing a variety of counterexamples. In addition, in contrast to logophoric accounts for ziji binding, Pan (2001) strongly argues that the long-distance anaphor ziji should not be treated with logophoric accounts since some properties of ziji are not compatible with logophoricity. Pan's view is not incorrect. Indeed, the definition that lies at the heart of logophoricity is not satisfactory to cover every aspect of long-distance anaphors, especially in Chinese, since they are used as a versatile tool.This paper revisits Pan's (2001) puzzle, which arises from the ability of ziji to serve as a logophor, in order to call attention to what the alternative to this view might be, and proposes a solution to it through the notion of empathy, in Kuno and Ka- buraki's (1977) sense of the term, so that the longdistance anaphors, which are not fully covered in terms of logophoricity, can be reconciled with other East Asian languages, such as Japanese zibun and Korean caki, in terms of a unified treatment.The structure of this paper is as follows. We discuss Pan's puzzle in Section 2, describing which kinds of binding behaviors in Chinese are not compatible with the properties of logophoricity. Section 3 argues that the term empathy should be accepted in order to complement the logophoric accounts of the long-distance bound anaphor ziji. Section 4 revisits Pan's puzzle and describes that his claim is partly the case in certain environments, and that it can be accounted for with the empathic accounts. Thus, we argue that the long-distance anaphor ziji in Chinese should be divided into two categories of logophor and empathy. Finally, we conclude our work in Section 5. The notion of logophoricity has long played a crucial role in understanding the co-referential relations between certain anaphoric expressions cross-linguistically, especially for long-distance anaphors violating a locality constraint and syntactic prominence conditions within the framework of pure syntactic accounts. However, Pan (2001) has shown that the long-distance binding of Chinese ziji should not be treated with the logophoric accounts in some aspects. This paper revisits Pan&apos;s (2001) puzzle, which arises from the ability of ziji to serve as a logophor, in order to call attention to what the alternative to this view might be, and proposes a solution to it through the notion of empathy, in Kuno and Kaburaki&apos;s (1977) sense of the term, so that long-distance anaphors, which are not fully covered in terms of logophoricity, can be reconciled with other East Asian languages, such as Japanese zibun and Korean caki, in terms of a unified treatment.
English Right Dislocation The Right Dislocation Construction (RDC) is a construction in which a dislocated NP appearing in sentence-final position refers to a pronoun, as observed in example (1), with the relevant pronoun in italics and the dislocated NP in boldface.(1) He is real smart, John.As (2) shows, the dislocated NP cannot occur outside the embedded clause that contains the relevant pronoun. This seems to suggest that the dislocated NP is derived by movement, because a violation of a movement constraint-namely, the Right Roof Constraint (RRC)-appears to be present (Ross, 1986: 179). 1 (2) *That they spoke to the janitor about that robbery yesterday is terrible, the cops. (Ross, 1986: 258) However, there is a construction that violates the RRC but is still acceptable, as seen in (3).(3) [That they spoke to the janitor about that robbery yesterday] is terrible, I mean, the cops. (Whitman, 2000: 450)The sentence in (3) differs from that in (2) only in that it has I mean inserted between the preceding clause and the dislocated element. This suggests that the derivation of the RDC should at least not involve rightward movement. 2 Note that the relevant pronoun is not a "resumptive" pronoun that repairs an island violation; it would otherwise be difficult to account for the unacceptability of the example in (2), in which the pronoun seems to play no role in repairing the violation of the RRC. 3 Further acceptable examples that appear to violate movement constraints exist, as in (4). In (4), it is possible to connect the dislocated NPs with him and his, respectively. If the dislocated NP in (4a) were extracted from the position occupied by the pronoun him, a conjunct could be moved. Likewise in (4b), if the dislocated NP were extracted from the position occupied by his, an 2 An example of the type in (3) was originally provided by Tsubomoto (1995), who argues against a movement analysis for the RDC and accounts for some of its properties in terms of information structure. 3 If movement were involved in the derivation of the RDC and the relevant pronoun were a resumptive pronoun, the RRC would be a condition on a representation. element could be moved out of the specifier position of the NP. Irrespective of whether an element moves rightward or leftward, however, English observes the Coordinate Structure Constraint (CSC) and the Left Branch Condition (LBC), as shown in (5)   Ross, 1986: 260) If the derivation of the RDC involved rightward movement in any way, the examples in (4) would violate the movement constraints, resulting in unacceptability-contrary to the actual situation. Furthermore, the examples in (4) suggest that the derivation of the RDC involves no rightward movement. 4 This paper is structured as follows. In section 2, I argue that the derivation of the RDC involves no movement, by pointing out empirical problems with the argument by Whitman (2000), who claims that the derivation of the RDC in English involves the operation of deletion after leftward movement. In section 3, I first set out a number of independently motivated principles, such as parsing principles and a licensing condition for adjoined elements, and then I demonstrate that the interaction of the licensing condition with these principles can account for the cases with which movement analyses fail to cope. Section 4 concludes the paper. A number of researchers claim that the derivation of the Right Dislocation Construction (RDC) involves movement (e. English, Japanese, and Korean). However, the RDC in English does not obey movement constraints such as the Coordinate Structure Constraint and the Left Branch Condition; that is, there are acceptable sentences that seem to violate these movement constraints. This suggests that the derivation of the English RDC should not involve movement. The present paper demonstrates that some syntactic properties of the English RDC can be explained instead through the interaction of independently motivated parsing strategies with a licensing condition for adjoined elements.
Complex-NP Islands in Korean: An Experimental Approach Since Ross's identifications of island constraints in English (Ross, 1967), there have been a lot of debates on the existence of island constraints in other languages. For example, Nishigauchi (1990) and Watanabe (1992) claimed that there were island constraints in Japanese, but Ishihara (2002) and Sprouse et al. (2011) mentioned that this language had no island constraint. Likewise, there have been controversies on the existence of island constraints in Korean. Some have argued for the presence of island effects ( Lee 1982, Han 1992, Hong 2004), while others have argued against it ( Sohn 1980, Kang 1986, Suh 1987, Hwang 2007.This paper investigated the island constraints in Korean. Our questions were (i) if Korean also has the Complex NP island constraints and (ii) if there are, why there have been so many controversies on the existence of island constraints.In order to answer these questions, this paper took an experimental approach and examined the island properties in Korean. The target sentences were constructed with three factors, and native speakers' intuition was measured with Magnitude Estimation (ME). After the experiment, all the data for the target sentences were extracted and they were statistically analyzed with R.Through the analysis, it was found that that the presence/absence of Complex NP island did not play a role by itself in Korean but that it made distinctions through the interactions with other factor such as matrix vs. embedded clause. These examples provided an account for why there have been so many controversies on the existence of island constraints in Korean.This paper is organized as follows. In Section 2, previous studies were reviewed especially focused on the experimental approaches. Section 3 includes the accounts for experimental design (research materials and research methods), and Section 4 enumerates the analysis results. Section 5 contains discussions, and Section 6 summarizes this paper. This paper took an experimental approach and examined island constraints in Korean. Among many island constraints, this study took a Complex NP island constraint, and the experiment was designed with 3 related factors: presence vs. absence of island, matrix clause vs. embedded clause, and scrambling. The analysis results illustrated that the presence/absence of complex NP island did not play a role by itself in Korean but that it made distinctions through the interactions with other factor such as matrix vs. embedded clause.
Two Types of Multiple Subject Constructions (MSCs) in Korean Korean has a type of sentence where more than one nominative-marked NP occurs in a single clause, as shown in (1). These sentences are called Multiple Nominative Constructions (MNCs) or Multiple Subject Constructions (MSCs): (1a) and (1c) show a sentence with two nom-marked NPs, and (1b) and (1d) show a sentence with more than two nom-marked NPs.(1) a. Cheli-ka kho-ka khu-ta Cheli-nom nose-nom is-big-decl 'It is Cheli whose nose is big.' b. Cheli-ka apeci-ka kho-ka Cheli-nom father-nom nose-nom khu-ta is-big-decl 'It is Cheli whose father's nose is big.' c. Yelum-i maykcwu-ka coh-ta summer-nom beer-nom good-decl 'In summer, beer is good.' d. I cip-i kyewul-i this house-nom winter-nom ohwu-ka ttattusha-ta afternoon-nom is-warm-decl 'This house is warm in winter afternoon.'In the literature, MSCs have been classified according to certain interpretive relationships between the multiple nom-marked NPs. For example, the two NPs in (1a, b) stand in a Possessor-Possessee (Part-Whole relation) as indicated by the paraphrases (2a, b), whereas the first NP in (1c, d) functions as a scene-setting or temporal Adjunct with respect to which the event denoted by the second NP and its predicate is interpreted.(2) a. Cheli-ka/uy kho-ka khu-ta Cheli-nom/gen nose-nom is-big-decl 'It is Cheli whose nose is big.' b. Cheli-ka/uy apeci-ka/uy kho-ka Cheli-nom/gen father-nom/gen nose-nom khu-ta is-big-decl 'It is Cheli whose father's nose is big.' c. Yelum-i/ey maykcwu-ka coh-ta summer-nom/gen beer-nom good-decl 'In summer, beer is good.' d. I cip-i/eyse (-nun) kyewul-i/ey this house-nom/loc(-top) winter-nom/loc ohwu.sikan-i ttattusha-ta afternoon-nom is-warm-decl 'This house is warm in winter afternoon.'Based on the possible paraphrases, the first type of MSCs (cf. 1a, b) is called 'Possessor-type MSCs', while the second (cf. 1c, d) is dubbed 'Adjuncttype MSCs'. Whether or not this classification is theoretically significant depends on a host of interrelated questions. 1 In this paper, we follow the view on MSCs that Yoon (2004,2007,2009,2015) endorsed, where MSCs are viewed as containing multiple Subjects, with the rightmost NP functioning as the Grammatical Subject that takes the VP as predicate, and the outer NPs functioning as Major Subjects that take a Sentential Predicate (SP) constituted of the Grammatical Subject and its predicate (Teng, 1974;B-S Park, 1973B-S Park, , 2001I-H Lee, 1987;Heycock and Lee, 1989;Chae and Kim, 2008 2 ;Yoon, 2004Yoon, , 2007Yoon, , 2009Yoon, , 2015).According to Yoon (2004Yoon ( , 2007Yoon ( , 2009Yoon ( , 2015, the licensing conditions on MSCs are as follows:Properties of MSCs a. Outer nom-marked NPs in MSCs are licensed syntactically by being assigned nominative case, as multiple Case assignment is possible in Korean. b. Outer nom-marked NPs in MSCs are licensed semantically through predication from the Sentential Predicate (SP) as Major Subject (MS), by binding a predicate variable within the SP. c. MS and SP in MSCs have restricted interpretive properties compared to Grammatical Subjects (GS) and VP. d. Sentential Predicates (SPs) are felicitous if they can be construed as denoting a salient 1 Possessor vs. adjunct classification is theoretically meaningful if one derives the first NP in a P-type MSC through Possessor Raising (Chun, 1985;Youn, 1990). Under this analysis, P-type MSCs have a unique subject (1 st ) NP, while in A-type MSCs, the unique subject is the 2 nd NP, with the 1 st NP functioning as topic/focus (Chun, 1985;Youn, 1990). The distinction is without significance if the two types are licensed in the same way. 2 Chae &amp; Kim (2008) admitted the clausal analysis of MSCs, but did not agree on the distinct functions of MS and GS in MSCs.characteristic property of the referent of the MS. e. MSs are felicitous if they can be construed denoting a newsworthy entity.In particular, the properties (3c-e) distinguish MSCs from single subject constructions (SSCs). 3 In this approach to MSCs, all MSCs are licensed in the same way. Thus, classificatory distinctions such as P-type vs. A-type do not carry theoretical significance. This is an important point that we return to in the discussion.Though there is a great deal of previous research on MSCs in Korean and other languages (Teng, 1974;B-S Park, 1973B-S Park, , 2001I-H Lee, 1987;Heycock and Lee, 1989;Heycock, 1993;Chae and Kim, 2008;Yoon, 2004Yoon, , 2007Yoon, , 2009Yoon, , 2015Ryu, 2010Ryu, , 2013Ryu, , 2014, they were based on the intuition of researchers, and not validated through experimental investigation. The current study is one of the few studies that adopt experimental methods to investigate the properties of MSCs. In particular, this study focuses on two different types of MSCs mentioned in the literature -Possessortype and Adjunct-type MSCs. Using Magnitude Estimation (ME), we examined native Korean speakers' knowledge of these two types of constructions.The organization of the paper is as follows. The next section will introduce some relevant previous studies done on the matter. The following section will explain the methodology of our experiment and present the results. Finally, we will discuss the results and conclude with the future direction of the study. Although Multiple Subject Constructions in Korean have received significant attention in theoretical literature, few experimental investigations of various syntactic and semantic properties of these constructions have been conducted. In this study, we administered a Magnitude Estimation (ME) experiment in order to compare the acceptability of Multiple Subject and related Single Subject Constructions (MSCs vs. SSCs) and that of two types of MSCs (Possessor-type vs. Adjunct-type MSCs). The results showed that MSCs received lower acceptability than SSCs. In addition, the Adjunct-type MSCs received higher acceptability scores than the Possessor-type MSCs. Possible reasons for these results are discussed.
Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features Pivot-based statistical machine translation (SMT) ( Wu and Wang, 2007) has been shown to be a possible way of constructing a dictionary for the language pairs that have scarce parallel data ( Tsunakawa et al., 2009;Chu et al., 2015). The assumption of this method is that there is a pair of large-scale parallel data: one between the source language and an intermediate resource rich language (henceforth called pivot), and one between that pivot and the target language. We can use the source-pivot and pivot-target parallel data to develop a source-target term 1 translation model for dictionary construction.Pivot-based SMT uses the log linear model as conventional phrase-based SMT ( Koehn et al., 2007) does. This method can address the data sparseness problem of directly merging the source-pivot and pivot-target terms, because it can use the portion of terms to generate new terms. Small-scale experiments in ( Tsunakawa et al., 2009) showed very low accuracy of pivot-based SMT for dictionary construction. 2 This paper presents our study to construct a largescale Japanese-Chinese (Ja-Zh) scientific dictionary, using large-scale Japanese-English (Ja-En) (49.1M sentences and 1.4M terms) and English-Chinese (En-Zh) (8.7M sentences and 4.5M terms) parallel data via pivot-based SMT. We generate a large pivot translation model using the Ja-En and En-Zh parallel data. Moreover, a small direct Ja-Zh translation model is generated using small-scale Ja-Zh parallel data. (680k sentences and 561k terms). Both the direct and pivot translation models are used to translate the Ja terms in the Ja-En dictionaries to Zh and the Zh terms in the Zh-En dictionaries to Ja to construct a large-scale Ja-Zh dictionary (about 3.6M terms).We address the noisy nature of pivoting large phrase tables by statistical significance pruning (Johnson et al., 2007). In addition, we exploit linguistic knowledge of common Chinese characters ( Chu et al., 2013) shared in Ja-Zh to further improve the translation model. Large-scale experiments on scientific domain data indicate that our proposed method achieves high quality dictionaries which we manually verify to have a high quality.Reranking the n-best list produced by the SMT decoder is known to help improve the translation quality given that good quality features are used ( Och et al., 2004). In this paper, we use bilingual neural network language model features for reranking the n-best list produced by the pivot-based system which uses significance pruning, and achieve a 2.5% (absolute) accuracy improvement. Compared to a setting which uses neither significance pruning nor n-best list reranking the improvement in accu-racy is about 5% (absolute). We also use character based neural MT to eliminate the out-of-vocabulary (OOV) terms, which further improves the quality.The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 presents our dictionary construction using pivot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking.
Annotation and Classification of French Feedback Communicative Functions Positive feedback tokens (yeah, yes, mhm ...) are the most frequent tokens in spontaneous speech. They play a crucial role in managing the common ground of a conversation. Several studies have attempted to provide a detailed quantitative analysis of these tokens in particular by looking at the form-function relationship (Allwood et al., 2007;Petukhova and Bunt, 2009;Gravano et al., 2012;Neiberg et al., 2013). About form, they looked at lexical choice, phonology and prosody. About communicative function, they considered in particular grounding, attitudes, turn-taking and dialogue structure management.Despite the previous attempts to quantify that form-function relationship of feedback, we think that more work needs to be done on the conversational part of it. For example, Gravano et al. (2012) used automatic classification of positive cue words, however the underlying corpus consists of games, that are far off being "conversational" and therefore do not permit to draw any conclusions on how feedback is performed in conversational talk or talkin-interaction. What concerns the selection of the feedback units, i.e. utterances, more work that clarifies what consists of feedback is also needed, as an approach that purely extracts specific lexical forms ("okay", "yeah", etc.) is not sufficient in order to account for feedback in general. Also, the question of what features to extract (acoustic, prosodic, contextual, etc.) is far from being answered. The aim of this paper is to shed some more light on these issues by taking data from real conversations, annotating communicative functions, extracting various features and using them in experiments to classify the communicative functions.The study reported in this paper takes place in a project (Prévot and Bertrand, 2012) that aims to use, among other methodologies, quantitative clues to decipher the form-function relationship within feedback utterances. More precisely, we are interested in the creation of (large) datasets composed of feedback utterances annotated with communicative functions. From these datasets, we conduce quantitative (statistical) linguistics tests as well as machine learning classification experiments.After presenting feedback phenomena and reviewing the relevant literature (Section 2), we introduce our dataset (Section 3), annotation framework and annotation campaign (Section 4). After discussing the evaluation of the campaign (Section 5), we turn to the feature extraction (Section 6) and our first classification experiments (Section 7). Feedback utterances are among the most frequent in dialogue. Feedback is also a crucial aspect of all linguistic theories that take social interaction involving language into account. However, determining communicative functions is a notoriously difficult task both for human interpreters and systems. It involves an interpretative process that integrates various sources of information. Existing work on communicative function classification comes from either dialogue act tagging where it is generally coarse grained concerning the feedback phenomena or it is token-based and does not address the variety of forms that feedback utterances can take. This paper introduces an annotation framework, the dataset and the related annotation campaign (involv-ing 7 raters to annotate nearly 6000 utterances). We present its evaluation not merely in terms of inter-rater agreement but also in terms of usability of the resulting reference dataset both from a linguistic research perspective and from a more applicative viewpoint .
Automatic conversion of sentence-end expressions for utterance characterization of dialogue systems Dialogue agents, which can carry out various tasks according to user demand, have been gaining in popularity due to their convenience and potential in casual conversations with humans. To make the agents more attractive as conversation partners, characterization is important since it makes the agents more friendly and human-like. Characterization here means adding particular personal characteristics to agent utterances; for example, adding the characteristics of a particular person ( Mizukami et al., 2015), Big Five personalities ( Mairesse and Walker, 2007), or personal attributes such as gender, age and area of residence (which is closely related to dialects). To characterize agents, utterances suitable for the designated characteristics are usually manually prepared. However, it is expensive to do this for a large number of utterances.To reduce this cost, we propose a method for automatically converting utterances into those that are suitable for various characters. In particular, the method automatically modifies 'how to say it' (i.e., linguistic expressions) without changing 'what to say' (i.e., contents of the utterances). Conversion is done by (i) collecting conversion candidates from various utterances on the Web (e.g., Twitter postings), which are annotated with their authors' personal attributes (this paper deals especially with gender, age, and area of residence), and (ii) using syntactic and semantic filters to suppress the generation of ill-formed utterances.The rest of the paper is organized as follows. Section 2 introduces studies related to characterization, Section 3 discusses the features of Japanese sentence-end expressions, Section 4 presents our method for converting sentence-end expressions, Section 5 shows our experimental results, and Section 6 concludes the paper and refers to future work. Building characters for dialogue agents is important in making the agents more friendly and human-like. To build such characters, utterances suitable for the designated characters are usually manually prepared. However, it is expensive to do this for a large number of utterances for various types of characters. We propose a method for automatically converting system utterances into those that are characteristic of designated personal attributes, such as gender, age and area of residence , to characterize agents. In particular, we focus on converting sentence-end expressions , which are considered to greatly affect personal attributes in Japanese. Conversion is done by (i) automatically collecting conversion candidates from various utterances on the Web (e.g., Twitter postings), and (ii) using syntactic and semantic filters to suppress the generation of ill-formed utterances. Experimental results show that our method can convert approximately 95% of utterances into those that are grammatically and semantically acceptable and approximately 90% of utterances into those that are perceived to be acceptable for designated personal attributes.
System Utterance Generation by Label Propagation over Association Graph of Words and Utterance Patterns for Open-Domain Dialogue Systems Chatting plays a lot of important roles in human communications for naturally exchanging diverse information, facilitating collaborative tasks, or even enhancing the quality of the conversations themselves. For dialogue systems as well, the functionality of being able to create chats is considered to have a significant importance regardless of whether task-oriented or non-task-oriented. There are currently many types of smart devices in our daily life and most of them have spoken dialogue interfaces, although they are basically limited to questionanswering. However, there are cases where people do not always have the clear intent on searching for something but they just want to know whether there is anything interesting they should know. In such cases, if the systems could offer a chats function instead, people may be able to make such unconscious or potential intentions clear by themselves through chats with these systems.However, it is quite challenging for dialogue systems to automatically generate chat responses because of the wide variety of topics in user utterances. In ordinary dialogue systems, i.e., rule-based systems, a very large number of hand-crafted rules and utterance patterns, or templates, would need to be prepared for extending the coverage of topics they can handle. However, this would be a very formidable task both to create them and to maintain them while keeping them up to date. Thus, a datadriven approach that makes use of the huge amount of conversational resources currently on the web, such as microblogs or social network media, as corpora have been recently investigated ( Shibata et al., 2009;Sugiyama et al., 2013). These corpora contain a large number of sentences that cover a wide range of topics, but there are many noisy sentences that do not contain meaningful content themselves. Another issue with this approach is that it basically selects sentences that are similar to the user utterances on the surface-level. Thus, the generated responses tend to be monotonous and the topic of conversation is not naturally changed by these systems.We propose a graph-based approach to address these issues. It is based on a dialogue corpus with a considerably large number of utterances. Out of a corpus, we construct an association graph, which is a bipartite graph with word and utterance pattern nodes, where a word represents a named entity and an utterance pattern represents a template of utterances reduced by replacing their named entities with slots holding the type of named entities that are originally placed there. The association graph is used for finding words and utterance patterns that belong to the same semantic category, or topic of conversation, and formed dynamically using label propagation over the association graph with the words and utterance patterns of previous utterances. The system utterances are synthesized out of those words and utterance patterns. This paper is organized as follows. First, we explore the use of crowdsourcing for efficiently constructing a dialogue corpus in Sec. 2. The details of the proposed method are described in Sec. 3. We discuss the results from a subjective evaluation in Sec. 4. These results support the concept that the proposed method can create responses with significant and interesting information and that it can appropriately expand the topics. We introduce the related works in Sec. 5. Finally, we give a summary and present some future prospects for the present study in Sec. 6 A novel graph-based utterance generation method for open-domain dialogue systems is proposed in this paper. After an association graph of words and utterance patterns from a dialogue corpus is constructed, a label propagation algorithm is used for generating system utterances from the words and utterance patterns in the association graph that are found to strongly correlate with the words and utterance patterns that appeared in previous user utterances. We also propose a crowdsourcing framework for collecting annotated chat data so that we can implement our method in a cost effective manner. Crowdsourcing is also used for conducting subjective evaluations and the results will show that the proposed method can not only provide interesting and informative responses but it also can appropriately expand the topics by comparing them to a well-known chat system in Japanese.
Design of a Learner Corpus for Listening and Speaking Performance The linguistic properties of learners of English as a foreign language (EFL), which are different from those of native speakers, have been identified through the analysis of output compiled in learner corpora ( Sugiura et al. 2007, Friginal et al. 2013, Barron and Black 2014. These properties have been used to statistically classify learners' output into a range of proficiency levels (Thewissen 2013). Thus, a learner corpus is a useful linguistic resource for developing assessment techniques that are implementable in a computer-assisted language learning (CALL) system.Although the contribution of learner corpora is well acknowledged (Granger 2009), previous learner corpora are limited in that learners' outputs have only been examined in terms of linguistic accuracy. As noted by Housen et al. (2012), learners' performance should be analyzed in terms of both accuracy and fluency. On the one hand, it is true that a proficient learner uses a target language accurately; however, on the other hand, a trade-off is often observed, as a learner speaks grammatically correct sentences (high accuracy), but does so at an unnaturally slow speech rate (low fluency) (Brand andGötz 2011, Chang 2012).Another limitation is typically seen in the target skill. Most previous learner corpora cover output skills in spoken or written language. From the viewpoint of communicative competence in spoken language, learners need to be proficient not only in speaking (output), but also in listening (input). Although speaking proficiency is well correlated with listening proficiency, a gap between these proficiencies is also known to exist ( Liao et al. 2010, Liu andCostanzo 2013), as a learner may comprehend some sentences containing lexical and syntactic items that are difficult for them to actually articulate.Because previous learner corpora have been limited in terms of speaking, a spoken learner corpus that demonstrates accuracy and fluency in speaking and listening is needed. The present study proposes to build a spoken learner corpus by annotating relevant information on sentences that learners spoke and listened to, respectively.Another limitation is seen in the scope of corpus data analysis. Although the learner corpus of Kotani et al. (2015) addressed listening, it was only capable of providing listening comprehension data for analysis at the text level, because that was the level at which comprehension was examined. However, identifying which linguistic properties affect listening comprehension through text-level analysis is difficult. To identify learners' linguistic problem areas, language use in local domains such as sentences needs to be analyzed, similar to machine translation evaluation at the sentence level ( Gamon et al. 2005, Stanojević andSima'an 2014). Therefore, the present study proposes to annotate listening comprehension data for individual sentences, which is expected to offer a finergrained analysis for the identification of learners' linguistic problem areas. A learner corpus is a useful resource for developing automatic assessment techniques for implementation in a computer-assisted language learning system. However, presently, learner corpora are only helpful in terms of evaluating the accuracy of learner output (speaking and writing). Therefore, the present study proposes a learner corpus annotated with evaluation results regarding the accuracy and fluency of performance in speaking (output) and listening (input).
Understanding Infants&apos; Language Development in Relation to Levels of Consciousness: An Approach in Building up an Agent-based Model Understanding how language is acquired by infants has remained to be a challenging task. Previous attempts, such as the behavioral approach (e.g. Skinner, 1957;Roediger, 2004;Ramscar &amp; Yarlett, 2007), relational frame theory (e.g. Hayes et al., 2001), nativist theories (Chomsky, 1967(Chomsky, , 1975, social interactionist theories (Bruner, 1983;Carpenter et al., 1998;Tomasello, 2003), etc. all have achieved remarkable results that have shed light on future directions in research in child language acquisition.More recent views emphasize that child language emerges through imitation and social interaction with the support of built-in learning mechanisms (Tomasello &amp; Bates, 2001;Tomasello, 2003;Snow, 1999;MacWhinney, 2004;Bates &amp; Goodman, 1999). For example, emergentist theories, represented by MacWhinney's competition model (1986), argue that language acquisition emerges from the interaction of biological pressures and the environment through a cognitive process. These theories emphasize that nature and nurture need to be jointly involved to trigger the language learning process.In psychology, Jean Piaget's experimental studies on cognitive development revealed stagedevelopment in children. Children's speech was discussed in terms of thought and reasoning (Piaget, 1926). Following Piaget, psychologists and linguists (e.g. Bowerman, 1990Bowerman, , 2004Bates, 1975Bates, , 1999Bates &amp; Goodman, 1997Mandler, 2004Mandler, , 1998) made data-based assumptions that there could be many learning processes involved in language acquisition. Evolutionarily, some wired-in help supplied by a long evolutionary history is assumed to exist in supporting this task. For example, infants can imitate facial gestures between 12 and 21 days of age, an age much earlier than predicted by stage development theory (e.g. Piaget). Such imitation implies that human neonates equate their own behaviors with gestures they see others perform (Gopnik &amp; Meltzoff, 1997;Meltzoff &amp; Borton, 1979;Meltzoff &amp; Moore, 1977). But how does the newborn go on from there to make sense of the torrent of novel input? In particular, how does the newborn travel the long distance from very limited initial abilities to full language acquisition? Although we have large collections of relevant data, we have little theory of the dynamics of this process. These questions remain to be answered (Gao &amp; Holland, 2013).Our objective of this study is to apply the agentbased model (Holland, 1995) to explore language development in early infants. Our approach has substantial differences from the previous attempts. We will take Gao &amp; Holland's (2008, 2013 statements of levels of consciousness (LoC) for language development as the theoretical guideline to build up a model that can reveal the dynamics of language development in early infants. By incorporate development observations into a theoretical framework, we will illustrate the mechanisms underlying LoC transitions and introduce an interdisciplinary approach to new experiments. Language development in infants is a dynamic process that involves the emergence and increase of consciousness, with which built-in learning mechanisms make infants&apos; imitation and interaction with their surroundings become socially meaningful. Taking Gao &amp; Holland&apos;s (2008, 2013) statements of levels of consciousness for language development as the theoretical guideline, this study proposes a rule-based, signal-processing agent-based model to explore the dynamics of language development in early infants. In this model, we assume that an infant&apos;s rule-based learning behaviors can be featured by different levels of consciousness and that its adaptation processes can be explained in relation to levels of consciousness. In this paper we will discuss properties of consciousness at different levels and identify the influencing factors for reaching them. Our ultimate goal in building up the model is to understand the processes of language development with an approach that can better reflect reality.
Pivot-Based Topic Models for Low-Resource Lexicon Extraction Data-driven approaches to natural language processing have been shown to be greatly effective, and the case of bilingual lexicon extraction is no exception. Recent advances in this area have enabled the construction of large, highquality bilingual lexicons, requiring less parallel data by making use of comparable corpora.While such comparable corpora are readily available for many language pairs, particularly when one of those languages is English, previous direct approaches fail when there is no such data available. For many language pairs there simply does not exist comparable (and even less so parallel) data. Even for languages with a large volume of available parallel data, most corpora cover only limited domains.There are two natural methods to deal with this problem: constructing or mining new data for the direct approach, and finding new ways to make better use of what data is already available. For an example of the construction of comparable corpora, see Zhu et al. (2013). We take the second approach and design pivot-based models for bilingual lexicon extraction. The major advantage of using a pivot language is that it is possible to take advantage of the large volume of comparable data sharing a common language such as English.In this paper we develop pivot-based approaches to make use of modern bilingual lexicon extraction methods that can be trained on comparable corpora. We present a selection of efficient algorithms using the framework of topic modelling ( Blei et al., 2003). Topic modelling has been a popular approach for bilingual lexicon extraction, however its use as a pivot model has yet to be explored. The use of topic models as a semantic similarity measure is a scalable method for low-resource languages because document-aligned comparable pivot training data (such as for English and a low-resource language) is growing ever more widely available. Examples of such sources are Wikipedia, multilingual newspaper articles and mined Web data.While there have been many studies on bilingual lexicon extraction, there has been little focus on the important problem of resource construction for low-resource language pairs. We present a variety of solutions to this problem, demonstrating their application to a practical scenario, and compare their effectiveness to mainstream approaches. This paper proposes a range of solutions to the challenges of extracting large and high-quality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivot-based approaches inspired by the state-of-the-art technique of bilingual topic modelling , extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an Icelandic-Korean lexicon from Wikipedia.
Self Syntactico-Semantic Enrichment of LMF Normalized Dictionaries Natural Language Processing (NLP) tasks require reliable linguistic resources such as lexicons. The latter represent lexical resources that should define for each lemma a highly valuable knowledge such as morphological features, syntactic behaviors and semantic knowledge like meanings, contexts, semantic classes and thematic roles. The availability of such knowledge favors the efficiency of NLP tools. For example, (Briscoe and Carroll, 2002) estimate that about half of the errors of parsers are based on the insufficient amount of knowledge concerning the syntactic argument structure in the used lexicons; on the other hand, (  show that the use of syntactic lexicons by a syntactic parser improves its performance. Furthermore, the lexicon is the core component for machine translation and information extraction ( Surdeanu et al., 2011). Unfortunately, we find that the lexicons that combine syntactic and semantic knowledge (i.e., representing semantic predicates) are shallow for some languages and unavailable for many others. Among the first lexicons dealing with the syntactico-semantic knowledge, we note the framework of (Gross, 1975), which was a revelation in this field. However, the enrichment of the proposed structure and even that of the lexicons proposed thereafter was such a hard task that it could not be accomplished due to the varied and abundant knowledge to be represented and requiring a high linguistic expertise. Thus, this enrichment task is an expensive and timeconsuming process. Some other researchers like ( Medelyan et al., 2013) have proposed to enrich such lexicons automatically using statistical methods. Nevertheless, the obtained content of such lexicons lacks reliability compared to the expert enrichment work.We feel that the enrichment issue of syntacticosemantic lexicons cannot be dealt with independently of their models. In this context, the International Organization of Standardization (ISO) has published the LMF-ISO 24613 (Lexical Markup Framework) standard (Francopoulo and George, 2008). LMF provides a unified model for constructing lexical resources covering all linguistic levels and dealing with the majority of languages. It offers a finely-structured model including the syntactico-semantic part. Many compliant lexicons to the LMF standard have been developed such as Wordnet-LMF (Henrich and Hinrichs, 2010), LG-LMF ( Laporte and Matthieu-Constant, 2013) and the El-Madar Arabic dictionary ( Khemakhem et al., 2013). Considering the richness and the fine structure of LMF lexicons, we propose in this paper an automatic approach for enriching LMF lexicons with syntactico-semantic links. This approach uses the available syntactic and semantic knowledge (already enriched) and operates the "Context" fields that explain each meaning with reference sentences.The proposed approach was experimented on an available Arabic dictionary named El-Madar ( Khemakhem et al., 2013). This dictionary offered us a good framework for experimentation because it covers, among others, syntactic, semantic and syntactico-semantic levels. The content of this dictionary has been enriched regarding syntactic behaviors ( Elleuch et al., 2015). Also, it contains the semantic classes of each meaning of a given lexical entry ( Elleuch et al, 2014).The remainder of this paper is devoted primarily to the presentation of some related works. Secondly, the proposed approach to enrich LMF normalized dictionaries with syntactico-semantic links is detailed. Then, the experimentation carried out on an available Arabic normalized dictionary is described and the obtained results are commented upon. Finally, some future works and perspectives are announced in the conclusion. The main challenge of this paper is the syntactico-semantic enrichment of LMF normalized dictionaries. To meet this challenge, we propose an approach based on the content of these dictionaries, namely the &quot;Context&quot; fields and the syntactic and semantic knowledge. The proposed approach is composed of three phases. The first one deals with the data set concerning the syntactic arguments of the &quot;Context&quot; fields. The second consists in connect semantic arguments to the syntactic ones. The last phase links syntactic and semantic arguments. In order to evaluate the proposed approach, we have applied it to an available Arabic normalized dictionary. The results are encouraging with respect to the measurement evaluation.
Not Voice but Case Identity in VP Ellipsis of English According to Merchant (2008), VP ellipsis (VPE) in English allows mismatch between the voice of an elided constituent and that of its antecedent, whereas Sluicing and Pseudogapping do not. This holds for either direction of voice alternation between an elided constituent and its antecedent. This is illustrated in (1) through (3) ( (1) and (3), taken from Merchant (2008: 169-170); (2), taken from Merchant (2013: 81)).(1) Active antecedent, passive ellipsis (VPE) a. The janitor must &lt;remove the trash 1  This paper examines the very issue of voice mismatch in the above three types of ellipsis in English. The next section reviews Merchant's (2007Merchant's ( , 2008 analysis of voice mismatch in ellipsis by postulating the functional category of Voice in the syntactic structure of a clause, and the subsequent rebuttal of Merchant's analysis by Tanaka (2011). Departing from the empirical generalization made by Tanaka, section 3 proposes a not Voice-but Case/case-theoretic account for apparent voice mismatch in VP ellipsis andPseudogapping. Section 4 investigates argument structure mismatch and its interaction with Pseudogapping. Section 5 explores a Case/casetheoretic account for voice mismatch in Sluicing. Section 6 wraps up with a conclusion. This paper develops a Case/case-theoretic acco unt for what Merchant (2008) calls voice mism atch in ellipsis constructions of English. Merch ant (ibid.) reports that VP ellipsis as an elision o f smaller size VP allows voice mismatch, but Ps eudogapping and Sluicing as an elision of bigge r size vP/TP do not. However, Tanaka (2011) ar gues against Merchant&apos;s dichotomy in voice mis match between VP ellipsis and Pseudogapping, reporting that voice mismatch in both types of e llipsis is permissible or not while interacting wi th what Kehler (2000) calls discourse coherence relations between ellipsis and antecedent clause s. Departing from Kehler&apos;s (2000) insight, we s uggest that vP undergoes ellipsis in a resemblan ce discourse relation, but VP does so in a cause/ effect discourse relation. Given the asymmetry i n the size of ellipsis in tandem with discourse re lations, we argue that since Accusative as well as Nominative Case is checked outside VP, the VP to be elided can meet the identity condition on ellipsis with its antecedent VP as the object element in the former and the subject one in the latter or vice versus have not been Case-checke d yet, thus being identical in terms of Case-feat ure at the point of derivation building a VP.
A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners The role of memory in language learning has long received ample attention from researchers in first and second language acquisition (SLA) (Baddeley (1999), Ellis (2001), Juffs (2006)). At an intuitive level, it seems right to reason that individual differences among adult learners in their successful attainment of a second language (L2) are attributable to individual differences in memory capacity. In SLA, researchers have focused on short-term or working rather than long-term memory differences because they think short-term or working-memory (WM) plays a more instrumental role for individual differences in language development. The rationale for this belief is that WM is an on-line capacity for processing and analyzing new information (words, grammatical structures and so on). As a consequence, the bigger the on-line capacity an individual has for new information, the more information will settle into off-line, long-term memory.In this paper we concentrate on Korean leaners of English (KLEs) to examine the correlation between their individual WM capacity and their knowledge of island constraints on whdependencies in English. To this end we adopt the methodology that Sprouse, Wagers, and Phillips (SWP) (2012a, b) use for L1 speakers. The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.
De-verbalization and Nominal Categories in Mandarin Chinese: A corpus-driven study in both Mainland Mandarin and Taiwan Mandarin This paper starts from examining two newlyemergent uses in Mainland Mandarin (MM) and Taiwan Mandarin (TM) and moves to the investigation of de-verbalization in Mandarin Chinese in section 2. In section 3 and 4, we probe into the grammatical behaviors and the ontological foundations of various nominal categories, respectively. This paper probes into the issue of de-verbalization in Chinese by starting from two potential and innovative uses of de-verbalization in Mainland Mandarin and Taiwan Mandarin, respectively. Then, we move to the exploration of various nominal categories in Chinese, with regard to their grammatical behaviors as well as their ontological differences. Crucially, we find that nominal categories in Chinese diverge upon individualization, which can be realized along either spatial or temporal dimension, as evidenced by the application of different types of classifiers. Specifically, event nouns and deverbal nouns allow temporal individualization only, while xingwei-marked nouns are exclusively compatible with spatial individualization. By contrast, entity nouns and dongzuo-marked nouns allow both spatial and temporal individualization. Hence, individualization is the key to our understanding of nominal categories in Chinese.
An Improved Hierarchical Word Sequence Language Model Using Directional Information Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in many applications such as machine translation (Brown et al., 1990), spelling correction ( Mays et al., 1991), speech recognition (Ra- biner and Juang, 1993), word prediction ( Bickel et al., 2005) and so on.Most research about Probabilistic Language Modeling, such as back-off (Katz,1987), Kneser-Ney ( Kneser and Ney, 1995), and modified Kneser-Ney ( Chen and Goodman, 1999), only focus on smoothing methods because they all take n-gram approach (Shannon, 1948) as a default setting for extracting word sequences from a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen ( Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modified Kneser-Ney (abbreviated as MKN). It is better to make these unseen sequences actually be observed rather than to leave them to smoothing method directly.For the purpose of extracting more valid word sequences and relieving data sparsity problem, Wu and Matsumoto (2014) proposed a heuristic approach to convert a sentence into a hierarchical word sequence (abbreviated as HWS) structure, by which special ngrams can be achieved. In this paper, we improve HWS models by adding directional information for achieving higher performance. This paper is organized as follows. In Section 2, we give a complete review of the HWS language model. We present our improved HWS model in Section 3. In Section 4, we show the effectiveness of our model by several experiments. Finally, we summarize our findings in Section 5. For relieving data sparsity problem, Hierarchical Word Sequence (abbreviated as HWS) language model, which uses word frequency information to convert raw sentences into special n-gram sequences, can be viewed as an effective alternative to normal n-gram method. In this paper, we use directional information to make HWS models more syntactically appropriate so that higher performance can be achieved. For evaluation, we perform intrinsic and extrinsic experiments, both verify the effectiveness of our improved model.
Neural Network Language Model for Chinese Pinyin Input Method Engine Input method engine (IME) is the encoding method to input a variety of characters into computer or other information processing and communication devices, such as mobile phone. People working with computer cannot live without IMEs. With the development and improvement of IMEs, people are paying more and more attention to its efficiency and humanization. Since there are thousands of Chinese characters and only 26 English letters on standard keyboard, we cannot directly input the Chinese characters to the computer by simply hitting keys. A mapping or encoding from Chinese characters to English letters is required, and IME is such a system software to do the mapping (Chen and Lee, 2000;Wu et al., 2009;Zhao et al., 2006). Among various encoding schemes, most IMEs adopt Chinese However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014;Xu and Zhao, 2012;Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000;Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques.Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996;Chen and Goodman, 1999;Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) ( Bengio et al., 2003;Schwenk, 2007;Le et al., 2010) are used in more and more natural language processing tasks, and practically outperform BNLMs in prediction accuracy. However, NNLMs are still out of consideration for IMEs according to our best knowledge. The main obstacle about using NNLMs in IME is that it is too timeconsuming to meet the requirement from IME that needs a real-time response as human-computer interface.Actually, too long computational time makes the direct integration of NNLMs infeasible for pinyin IMEs. We can hardly image that users have to wait for over 10 seconds to get the result after typing the pinyin sequence. So we have to find another way. Although some work have reduced the decoding time of NNLMs, such as ( Arisoy et al., 2014), (Vaswani et al., 2013) and (Devlin et al., 2014), these methods are mainly designed for speech recognition or machine translation and can not be integrated into IME directly.Instead of replacing BNLMs with NNLMs in IME, we propose to use NNLMs to enhance BNLMs, which means recalculating the probabilities of ngrams in the BNLMs with NNLMs. Thus we take advantage of the probabilities provided by NNLMs without increasing its on-site computational cost. Furthermore, we will also demonstrate that our method may indeed improve the prediction performance of pinyin IMEs.In Section 2, we introduce the typical pinyin IME model and explain how language models work on the process of candidate sentence generation. In Section 3, we describe the basic concept of BNLMs and NNLMs, then we describe our approach of converting the NNLMs into BNLMs. In section 4 and 5, we do experiments and conclude. Neural network language models (NNLMs) have been shown to outperform traditional n-gram language model. However, too high computational cost of NNLMs becomes the main obstacle of directly integrating it into pinyin IME that normally requires a real-time response. In this paper, an efficient solution is proposed by converting NNLMs into back-off n-gram language models, and we integrate the converted NNLM into pinyin IME. Our experimental results show that the proposed method gives better decoding predictive performance for pinyin IME with satisfied efficiency.
Trouble information extraction based on a bootstrap approach from Twitter The World Wide Web contains a huge number of online documents that are easily accessible. Analysis of the documents has an important role for natural language processing. One of the important information for business companies is trouble information of a product as the risk management. If they can monitor the information about products and the troubles from the Web automatically, they might be able to avoid critical damages by realizing the risk in advance. Therefore trouble information extraction is a significant task in business. There are many studies which handled news articles ( Sakai et al., 2006), review documents ( Ivanov and Tutubalina, 2014), financial documents ( Leider and Schilder, 2010), daily reports ( Kakimoto and Yamamoto, 2008), a failure database on the Web ( Awano et al., 2012) and so on, as the target data. However, these information sources are not usually instantaneous and exhaustive. To solve this problem, we focus on Twitter. It is one of the most famous microblogging services and text-based posts of up to 140 characters. The posted sentences are described as "tweets." We suppose users on Twitter often post tweets with trouble information because they tend to post tweets as lifelog data in real time. Some researchers focused on the characteristic (Aramaki et al., 2011;Sakaki et al., 2010;Shimada et al., 2012).In this paper, we propose a method to extract trouble information from Twitter. One of the most common approaches is to classify an input into trouble information and non-trouble information by using a machine learning technique. However, most of the tweets do not relate to trouble information. In other word, the ratio of trouble tweets and non-trouble tweets is biased. Such biased data generally generate a unsuitable classifier. Another approach is to extract trouble information by using handwritten rules. However, constructing high coverage rules by handwork is usually a difficult task. In this paper, we investigate these problems through a preliminary experiment. On the basis of the result, we introduce a bootstrapping approach to our trouble information extraction task. Methods based on bootstrapping techniques are one of the effective approaches to extract information ( Riloff and Jones, 1999;Et- zioni et al., 2004). Riloff et al. (2013) have pro-posed a method to identify sarcastic tweets by using a bootstrapping algorithm. Ohmori and Mori (2010) have proposed a method based on a bootstrapping approach with words and phrases for searching for failure cases among products. We focus on trouble expressions which indicate the malfunction and failure of products. We apply the trouble expressions as seeds into a bootstrapping approach. By the iteration process, our method obtains more trouble expressions, and then extracts tweets with trouble information. In this paper, we propose a method for extracting trouble information from Twitter. One useful approach is based on machine learning techniques such as SVMs. However, trouble information is a fraction of a percent of all tweets on Twitter. In general, imbalanced distribution is not suitable for machine learning techniques to generate a classifier. Another approach is to extract trouble information by using handwritten rules. However, constructing high coverage rules by handwork is costly. First, we verify these problems in a preliminary experiment. Then, to solve these problems, we apply a bootstrapping method to our trouble information extraction task. We introduce three characteristics and a scoring method to the bootstrapping. As a result, the iteration process on the bootstrapping increased the number of tweets and patterns for trouble information dramatically.
Distant-supervised Language Model for Detecting Emotional Upsurge on Twitter Twitter is one of the most popular micro-blogging platforms in recent days. There are over 500 million tweets posted per day 1 including real-world events described on Twitter which range from short and daily life events (e.g. falling to the ground) to long and widely-broadcasted events (e.g. a match in World Cup). Such tweets are good sources to detect users' reactions toward real-world events. * This work was done while the first author was at the University of Tokyo and National Institute of Informatics. The first author is currently at Amazon Japan K.K. People behave unusually when they encounter exciting moments in an event, for example, yell out or dance with each other after their favorite soccer team scores a goal. On Twitter, this action is often reflected by a large number of posts within a short time period. When Japan scored a goal against Cameroon in World Cup 2010, there were a maximum of 2, 940 tweets per second (TPS), which marked the record TPS for goals at that time. 2 It is significantly larger than the average of 750 TPS. 2 In this paper, we call such bursty traffic as "numerical spikes". Figure 1 shows the number of tweets per minute during the match of Cameroon vs. Japan, and Table 1 shows the examples of tweets sampled from both numerical spikes and other parts.Detecting emotional upsurge is important for both extracting emerging important real-world events and important moments of them. We call an upsurge that are caused by Twitter users' emotional spike as Event-specific twitter streams often reveal sudden spikes triggered by users&apos; upsurge of emotions to crucial moments in the real world. Although upsurge of emotion is usually identified by a sudden rise in the number of tweets, the detection for diverse event streams is not a trivial task. In this paper, we propose a new method to extract spiking tweets with upsurge of emotions based on characteristic expressions used in tweets. The core part of our method is to use a distant-supervised language model (Spike LM) built from tweets in spikes to capture such expressions. We investigate the performance of detecting emotional spik-ing tweets using language models including Spike LM. Our experimental results show that the natural language expressions used in emotional upsurge fit specifically well to Spike LM.
Multi-aspects Rating Prediction Using Aspect Words and Sentences As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. Finding information relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user's opinions for products. Buying products, users usually survey the product reviews. More precise and effective methods for evaluating the products are useful for users. Many researchers have recently studied extraction and classification of opinions, namely sentiment analysis ( Pang and Lee, 2008).For sentiment analysis, one of the most primitive studies is to classify a document into two classes; positive and negative opinions ( Pang et al., 2002;Turney, 2002). One simple extension of p/n classification is a rating prediction task. It is a finer-grained task, as compared with the p/n classification. Several researchers have challenged rating prediction tasks in reviews ( Goldberg and Zhu, 2006;Li et al., 2011;Okanohara and Tsujii, 2005;Pang and Lee, 2005). They are called "seeing stars." These tasks handled an overall rating in the prediction. However, each review contains many descriptions about several aspects of a product. For example, they are "performance", "user-friendliness" and "portability" for laptop PCs and "script", "casting" and "music" for movies. Since reviewers judge not only the overall polarity for a product but also details for it, predicting stars of several aspects in a review is also one of the most important tasks in sentiment analysis, instead of a single overall rating. There are several studies to predict some stars in a review, namely "seeing several stars" or "aspect ratings" (Gupta et al., 2010;Pappas and Popescu-Belis, 2014;Shimada and Endo, 2008;Snyder and Barzilay, 2007).In this paper we propose a method for a rating prediction task with some aspects. In other words, we focus on multi-scale and multi-aspects rating prediction for reviews. We handle video game reviews with seven aspects and zero to five stars. Here we also focus on feature extraction for the prediction. The most common approach is usually based on feature extraction from all sentences in each review. However, all sentences in a review do not always contribute to the prediction of a specific aspect in the review. In other words, the methods handling a review globally are not always suitable to gener-ate a model for rating prediction. In addition, Pang and Lee (2004) mentioned that classifying sentences in documents into subjective or objective was effective for p/n classification. In a similar way, for the aspect rating tasks, aspect identification of each sentence and use of aspect sentences for feature extraction might contribute to the improvement for rating prediction. Therefore, the proposed method identifies the aspect of each sentence in each review first. Then, it extracts features for prediction models of seven aspects from all sentences and aspect sentences, on the basis of the variance of words. Finally, it generates prediction models based on Support Vector Regression (SVR) for seven aspects. In this paper we propose a method for a rating prediction task. Each review consists of several ratings for a product, namely aspects. To predict the ratings of the aspects, we utilize not only aspect words, but also aspect sentences. First, our method detects aspect sentences by using a machine learning technique. Then, it incorporates words extracted from aspect sentences with aspect word features. For estimating aspect likelihood of each word, we utilize the variance of words among aspects. Finally, it generates classifiers for each aspect by using the extracted features based on the aspect likelihood. Experimental result shows the effectiveness of features from aspect sentences .
Understanding Rating Behaviour and Predicting Ratings by Identifying Representative Users With the advent of Web 2.0, a large number of platforms including e-commerce sites, discussion forums, blogs etc. have emerged that allow users to express their opinions regarding various businesses, products and services. These opinions are usually in the form of reviews, each consisting of text feedback describing various aspects of the product along with a single numeric rating representing the users' overall sentiment about the same ( McAuley et al., 2012).Such user review ratings are normally aggregated to provide an overall product rating, which help other people form their own opinion and help them make an informed decision during purchase. However, in case of new products, there is a time delay till a sufficient number of ratings that give a 'complete picture' of the product can be obtained. In such a scenario, the seller of the product may find it useful to identify a few people whose ratings, when combined together, reflect this 'complete picture'. The seller may then invite these people to review the product and, as a result, reduce the time delay involved in getting the 'true' product rating.Review text is unstructured and inherently noisy. But it can be a valuable source of information since users justify their ratings through such text (McAuley and Leskovec, 2013). Users tend to express their sentiments about different aspects of a product in the review text and provide a rating based on some combination of these sentiments ( Ganu et al., 2009). However, some users are influenced heavily by one particular aspect of the product and this is reflected in their ratings. For example: While reviewing smartphones, the ratings provided by a user may be influenced heavily by just the batterylife, irrespective of the quality of other aspects of the phone. Similarly, while reviewing restaurants, some users' ratings may correlate with the ambience of the restaurant or the level of service provided. We call such users as 'representative users' since their ratings tend to 'represent' one particular dimension of the product.Although latent factors obtained from ratings data have been used extensively for rating prediction, very few previous works have attempted to combine both review text and ratings. Our approach combines latent topics obtained from review text with users' rating data to learn representative users for each product. This enables us to predict ratings for new products by just looking at the ratings of a small set of users, even when no review text is available. In traditional methods, product ratings are obtained by modelling the product factors from ratings data. However, (McAuley and Leskovec, 2013) suggest that this approach is not accurate in case of new products due to the lack of sufficient number of ratings. They, in turn, propose a model which fits product factors from a few review texts. Our approach is free from both these constraints.In this study, we use the topic model Multi-Grain Latent Dirchlet Allocation (MG-LDA) described in (Titov and McDonald, 2008a) on restaurant reviews obtained from Yelp 1 to obtain latent topics that correspond to ratable aspects of the restaurants. Since we segregate the reviews on the basis of restaurant category, we notice some interesting variations across different cuisines. The words associated with the extracted topics are then used to perform review segmentation where we identify the sentences that describe each topic. This also enables us to analyse the sentiment expressed regarding each topic in a review. We then capture the intuition of represen-1 http://www.yelp.com tative users to learn a set of users who best represent each topic. Latent topic ratings for restaurants are then obtained by aggregating the ratings of those users who represent that topic. The overall ratings of new restaurants are then predicted using a regression model. An overview of the proposed method is shown in Figure 1.We also show how this concept could be used to better understand rating behaviour across different cuisines. For example: What do people who visit French restaurants care most about -food, service or value for money? How is this different from people who visit Italian restaurants?The rest of the paper is structured as follows. Section 2 provides a review of related work. Section 3 describes our proposed method. In Section 4, we describe the experiments performed and report the results of our evaluation. Section 5 concludes the paper with a summary of the work and the scope for future work. Online user reviews describing various products and services are now abundant on the web. While the information conveyed through review texts and ratings is easily comprehensi-ble, there is a wealth of hidden information in them that is not immediately obvious. In this study, we unlock this hidden value behind user reviews to understand the various dimensions along which users rate products. We learn a set of users that represent each of these dimensions and use their ratings to predict product ratings. Specifically, we work with restaurant reviews to identify users whose ratings are influenced by dimensions like &apos;Service&apos;, &apos;Atmo-sphere&apos; etc. in order to predict restaurant ratings and understand the variation in rating behaviour across different cuisines. While previous approaches to obtaining product ratings require either a large number of user ratings or a few review texts, we show that it is possible to predict ratings with few user ratings and no review text. Our experiments show that our approach outperforms other conventional methods by 16-27% in terms of RMSE.
Corpus annotation with a linguistic analysis of the associations between event mentions and spatial expressions Every event is situated within some real-world space and textual descriptions that refer to events in documents also convey such spatial information. Such information is important not only for the interpretation of single events but also for the understanding of the relations among them. Spatial information can be used for various applications such as information extraction, textual entailment, and question answering. For instance, if we want to answer the question "Where did traffic accidents happen most frequently in 2014?," we would need a * Corresponding author method to access and collect spatial information associated with all traffic accidents from the relevant textual descriptions. However, such information is usually not provided explicitly in text since humans can intuitively understand from the context where each event occurs.In general, two factors make it difficult to automatically recognize the location of events in text. First, there are usually more event mentions in text than expressions containing information about the location of events. A system must thus choose the most appropriate spatial expression for a given event mention. Second, such expressions are not always syntactically close to event mentions, which may make it less obvious to recognize their semantic association. The following three sentences exemplify different levels of difficulties in determining whether particular event mentions and spatial expressions are associated, i.e., whether a spatial expression refers to the space where an event occurred.(1) A fire [ Sentence (1) shows that the event mention and the spatial expression are syntactically connected in a single clause, which can probably be identified in a straightforward manner with a conventional semantic role labeler. Sentence (2) shows that they exist in the same sentence but not in the same clause, and that there must be some inference in order to find out that fired is likely to occur around the Northern Limit Line; for example, intruded and fired can occur in a similar place and the time interval between them may not be too long. Sentence (3) shows that, even though an event mention and a strong candidate for its spatial expression exist in the same sentence, their association may or may not hold depending on the context; there may be another place where the missing event actually happened. In this case, the system may have to search the text backwards to find out where missing or other relevant events are mentioned. Such information may, however, not have been stated at all in the available text.In this paper, we present a linguistic analysis of how event mentions and spatial expressions are associated in text with respect to a corpus annotation process. More specifically, we discuss the following four issues:• which information should be included in the guidelines in order to recognize spatial information about events in text, • what kind of difficulties and issues arise during the annotation process, • what trends are found in the corpus, and • which factors could be of help to achieve a high inter-annotator agreement and to build an automated system.The rest of this paper is organized as follows. Section 2 presents previous work on analyzing properties of events in text. Section 3 shows the proposed annotation framework for creating a corpus. Section 4 gives an analysis of the corpus and disagreements between annotators. Section 5 discusses issues on improving the proposed annotation framework, with concluding remarks. Recognizing spatial information associated with events expressed in natural language text is essential for the proper interpretation of such events. However, the associations between events and spatial information found throughout the text have been much less studied than other types of spatial association as looked into in SpatialML and ISO-Space. In this paper, we present an annotation framework for the linguistic analysis of the associations between event mentions and spatial expressions in broadcast news articles. Based on the corpus annotation and analysis, we discuss which information should be included in the guidelines and what makes it difficult to achieve a high inter-annotator agreement. We also discuss possible improvements on the current corpus and annotation framework for insights into developing an automated system.
Topic Model for Identifying Suicidal Ideation in Chinese Microblog Suicide is a severe health problem worldwide, which is one of leading causes of youth death in the world, especially in China. In the latest report (Organiza- tion and others, 2014) from World Health Organization (WHO), over 800, 000 people committed suicide in 2012, including 120, 730 Chinese; and it is very likely that the data is underestimated. Indeed, there are many more people who attempt suicide every year. Instead of calling health services or seeking for help in-person, choosing social networks is a preferable choice for some suicide because of privacy and facilitating sharing similar experiences among peers (Luxton et al., 2011).Social network sites (SNS), such as Twitter, Sina Weibo, have become popular platforms for people to express themselves. Sina Weibo is a Chinese leading social network akin to Twitter. According to the latest Sina Weibo User Activity Report (http://data.weibo.com/report/ reportDetail?id=215), Weibo now has more than 70 million active users per day, and over 160 million active users per month. It becomes a great platform for sharing opinions, emotions, and even to breaking news or public events. Recent work ( Fu et al., 2013) showed that SNS not only enhanced our connections with others, but also facilitated selective self-presentation of undesirable behaviors, such as suicide.The association between social media and suicide has drawn public attention recently, since several actual suicidal cases were reported in Sina Weibo, e.g.,(http://news.sina.com.cn/zl/ zatan/2014-12-02/18032759.shtml). However, new approaches towards online suicide ideation monitoring and prevention are still under development. ( Fu et al., 2013) suggested that diffusion of microblogs about one's suicidal ideation or behaviors on social networks might serve as an early indication of a person's mental state. These indicators include one's writing through style, format, selection of specific words, and general structure. It would therefore be desirable to build an appropriate suicide-monitoring system, to identify people who gave expressed suicidal ideation on SNS and provide follow-up support and services.In this paper, we propose to detect suicide ideation in Chinese SNS and explore the possibility of using Topic Model ( Blei et al., 2003). In particular, we first collect and evaluate suicidal microblogs by psychological standards. Second, we construct our psychological lexicons using word embedding techniques and explore the differences of online behaviors between suicide and none-suicide Weibo users. Then, in order to avoid the adverse outcomes that high dimensions of lexicon feature, which could weaken both efficiency and accuracy of classifiers, we model features of microblog in social networks and utilize the popular unsupervised model, Topic Model. Finally we design, develop, and test a model that can effectively identify suicidal ideation in SNS.To summarize, our research has three main contributions: first, we use word embedding technique to construct psychological lexicons to enable utilization of suicide online behaviours; second, we employ Topic Model with lexicon knowledge and hybrid approaches for suicidal ideation identification on real-world datasets; third, a real-time application of suicide ideation prototype monitoring system is deployed online. Suicide is one of major public health problems worldwide. Traditionally, suicidal ideation is assessed by surveys or interviews, which lacks of a real-time assessment of personal mental state. Online social networks, with large amount of user-generated data, offer opportunities to gain insights of suicide assessment and prevention. In this paper, we explore potentiality to identify and monitor suicide expressed in microblog on social networks. First, we identify users who have committed suicide and collect millions of microblogs from social networks. Second, we build suicide psychological lexicon by psychological standards and word embedding technique. Third, by leveraging both language styles and online behaviors, we employ Topic Model and other machine learning algorithms to identify suicidal ideation. Our approach achieves the best results on topic-500, yielding F 1 − measure of 80.0%, Precision of 87.1%, Recall of 73.9%, and Accuracy of 93.2%. Furthermore , a prototype system for monitoring suicidal ideation on several social networks is deployed.
PACLIC 2015 29th Pacific Asia Conference on Language, Information and Computation Proceedings of PACLIC 2015: Poster Papers Program chair  
Identification of Sympathy in Free Conversation Dialog systems could be broadly divided into two categories. One is a task oriented dialog system. It focuses on a specific task such as guidance on sightseeing, hotel reservation or promotion of products, and communicates with a user to achieve a goal of the task. The other is a non task oriented dialog system or chat system. It does not suppose any specific tasks but can handle a wide variety of topics to freely chat with the user. Most of the past researches focus on task oriented dialog systems. In recent years, however, non task oriented dialog systems become more important since robotic pets or nursing care robots are paid much attention ( Libin and Libin, 2004).One of important characteristics in free conversation is sympathy of a speaker for the topics in the conversation ( Anderson and Keltner, 2002;Hi- gashinaka et al., 2008). The topics in free conversation are not fixed but could be changed by the speakers at any time. To make the conversation natural and smooth, however, a non task oriented dialog system can not arbitrarily change the topics. It is uncomfortable for the user if the system would suddenly change the topic when the user wants to continue to talk on the current topic, or if the system would keep the same topic when the user is bored and does not want to talk on the topic any more. If the system fails to shift the topic at appropriate time, the user may break the conversation.The sympathy of the user is one of the useful clues to guess good timing for changing the topic. If the user shows the sympathy for the current topic, the system should continue the conversation with the same topic. On the other hand, if the user does not display the sympathy, the system should provide other topics. Therefore, it is essential for the chat system to guess the sympathy of the user. This paper proposes a method to automatically judge whether the user displays the sympathy in his/her utterance as a fundamental technique in a non task oriented dialog system. In this paper, we define 'sympathetic utterance' as the utterance where the speaker expresses the sympathy or approval especially when he/she replies to subjective utterance of the other participant. Note that the utterance just showing agreement is not defined as sympathetic. Various kinds of clues could be applicable for identification of the sympathy, such as facial expressions, gesture or the contents of the utterance. Since we focus on a text based chat system, our method only considers the content and detects the user's sympathy in a transcript of the utterance. In addition to ordinary n-gram features, new features for the sympathy identification are introduced. The effectiveness of our proposed features will be proved via empirical evaluation.The remaining parts of this paper are organized as follows. Section 2 discusses related work for the sympathy identification. Section 3 presents our proposed method. Section 4 reports results of evaluation experiments. Finally, Section 5 concludes the paper. Dialog systems are generally categorized into two types: task oriented and non task oriented systems. Recently, the study of non task oriented dialog systems or chat systems becomes more important since robotic pets or nursing care robots are paid much attention in our daily life. In this paper, as a fundamental technique in a chat system, we propose a method to identify if a speaker displays sympathy in his/her utterance. Our method is based on supervised machine learning. New features are proposed to train a classifier for identifying the sympathy in user&apos;s utterance. Results of our experiments show that the proposed features improve the F-measure by 3-4% over a base-line.
Learning Sentential Patterns of Various Rhetoric Moves for Assisted Academic Writing The British Council estimated that roughly a billion people are learning and using English around the world (Graddol, 1997), mostly as a second language, and the number has been growing. For advanced learners in university, English for Academic Purposes (EAP) plays an important role in English Specific Purposes (ESP) study. More and more Computer Assisted Language Learning (CALL) systems have been developed to help learners in academic writing, including concordancers, grammar checkers, and essay raters. Typical CALL systems assist learners before and after the writing process by providing corpus-based reference services, or returning a grade and corrective feedback (e.g., Cambridge English Write &amp; Improve).However, researchers have shown that non-native student writers may have difficulties in composing sentences and lack knowledge at discourse level (Hinds, 1990;Swales, 1990or Connor,1996) in academic writing. For example, (Antony, 2003) indicated that many Japanese scientists and engineers lack sufficient knowledge of commonly used structural patterns at the discourse level.The rhetorical organization has been considered to be one of the most effective strategies of teaching technical writing and reading. The American National Standard Institute (ANSI) recommends editors or writers to state the purpose, methods, results, and conclusions in the document (Weil, 1970). That is, an article usually begins with a description of background information, and is followed by the target problem, solution to the problem, evaluation of the solution, and conclusion, by analyzing annotated dictionary examples and automatically tagged sentences in a corpus. As will be described in Section 4, we used the information on collocation and syntax (ICS) for example sentences from online Macmillan English Dictionary, as well as in the Citeseer x corpus, to develop WriteAhead. Along the same line, the second edition of the Macmillan English Dictionary provides a 29-page Improve your Writing Skills Writing Section with instruction on how to write fluently by : adding information, comparing and contrasting, exemplifying, expressing cause and effect, expressing personal opinions, possibility and certainty, introducing a concession or introducing topics, listing items, paraphrasing or clarifying, quoting and reporting, summarizing and concluding.Although there are much information (such as dictionary examples) that could help to write a paper, learners may fail to generalize from examples and apply to their own situations. Often, there are too many examples to choose from and to adapt to match the need of the learner writers. Learners could be more effectively assisted with a tool that provides concise, relevant, genre-specific suggestions as learners type away, when writing a draft. In our research, we automatically extract rhetorical patterns to assist learners in academic writing. For example, in Figure 1, the learner has already typed a sentence describing the background and problem, and then the learner types the move tag AIM. Figure 2 shows the implementation of WriteAhead in the Google Doc environment. With this Google Docs Add-on, the user can conveniently access the WriteAhead functionalities, as well as common editing functions. According to the information, WriteAhead displays the appropriate sentential patterns and examples for the "method" extracted from a corpus, to help the learner continue writing:• Our ALGORITHM be BASE on (Our approach/method is based on), • We ILLUSTRATE the ALGORITHM (We illustrate the approach/method/technique), • The ALGORITHM be BASE on (The method/approach/model is base on). In this paper, we present a prototype system, WriteAhead, that extracts patterns that cover extensively most semantic categories in academic writing (e.g. Teufel, 2000) from an academic corpus.Writing suggestions are given to assist student writers. WriteAhead extracts these sentential patterns and examples automatically by tagging and analyzing sentences in a corpus. As will be described in Section 3, we used the Citeseer x corpus as our source to extract sentential patterns.These patterns are then used at run-time in WriteAhead for assisted writing. WriteAhead takes the move tag the user types in, and then retrieves, and displays patterns related to the tag to help the user write or edit a draft (Figure 1). We present a new methodology for automatically deriving patterns. WriteAhead is also the first interface that suggests patterns for learners while they type.The rest of the paper is organized as follows. The related work is reviewed in the next section. Then we present our method for automatically extracting sentential patterns and examples (Section 3). As part of our evaluation, we measured the accuracy rate of suggestions generated by WriteAhead using published research papers unrelated to the training data (Section 4). Section 5 reports on the experiment results and we summarize our conclusion in Section 6. We introduce a new method for extracting representative sentential patterns from a corpus for the purpose of assisting ESL learners in academic writing. In our approach, sentences are transformed into patterns for statistical analysis and filtering, and then are annotated with relevant rhetoric moves. The method involves annotating every sentence in a given corpus with part of speech and base phrase information, converting the sentence into for-mulaic patterns, and filtering salient patterns for key content words (verbs and nouns). We display the patterns in the interactive writing environment, WriteAhead, to prompt the user as they type away.
Idioms: Formally Flexible but Semantically Non-transparent Although idioms are generally assumed to be noncompositional and, hence, non-flexible, it has been well attested that they are not fixed expressions formally. Even one of the most fixed idioms like [kick the bucket] show morphological flexibility in the behavior of the verb kick. Many other idioms show some degree of syntactic flexibility with reference to various types of syntactic behavior. Even the non-compositionality of them has been challenged, especially by those who are working under the framework of cognitive linguistics (cf. Croft &amp; Cruse 2004: Ch. 9, andGibbs 2007). Reflecting this trend, Wasow et al. (1983) and Nunberg et al. (1994), for example, argue that syntactic flexibility is closely related to semantic transparency. In this paper, however, we are going to show that idioms can better be analyzed as semantically non-transparent although they are formally flexible, providing further evidence for the analysis in Chae (2014).Adopting Culicover's (2009) definition of construction, 1 Chae (2014) assumes that all and only idioms are represented as constructions. Under this view, grammar consists of three components: the set of lexical items (i.e. the lexicon), the set of rules and the set of constructions. He introduces some "notations/ conventions," which apply to regular phrase structures, to represent the restrictions operating on idioms. Employing these notations, he provides representations of various types of formal properties of idioms (in English and Korean): from the least flexible ones to the most flexible ones. However, the meanings of idioms are supposed to come from the whole idioms/constructions rather than from their component parts compositionally.In section 2, we will introduce a framework to represent the syntactic flexibility of idioms, which is developed in Chae (2014). We will also observe some consequences of the framework on the lexicon and the set of rules. Then, in section 3, we will examine some phenomena which seem to be handled only by assuming that the component parts of idioms have their own separate meanings. It will be shown, however, that all the phenomena can be accounted for effectively without assuming 1 The definition is as follows (Culicover 2009: 33): "A construction is a syntactically complex expression whose meaning is not entirely predictable from the meanings of its parts and the way they are combined in the structure." separate meanings of parts. We will focus on the behavior of idiom-internal adjectives, which is the most difficult to treat properly under the assumption of semantic non-transparency of idioms. Contrary to popular beliefs, idioms show a high degree of formal flexibility, ranging from word-like idioms to those which are like almost regular phrases. However, we argue that their meanings are not transparent, i.e. they are non-compositional, regardless of their syntactic flexibility. In this paper, firstly, we will introduce a framework to represent their syntactic flexibility, which is developed in Chae (2014), and will observe some consequences of the framework on the lexicon and the set of rules. Secondly, there seem to be some phenomena which can only be handled under the assumption that the component parts of idioms have their own separate meanings. However, we will show that all the phenomena, focusing on the behavior of idiom-internal adjectives, can be accounted for effectively without assuming separate meanings of parts, which confirms the non-transparency of idioms.
Detecting an Infant&apos;s Developmental Reactions in Reviews on Picture Books Generally, educational books focus on a specific subject to be learned such as science, sociology, etc. Picture books are exceptions, in terms of their efficiency for infants' cognitive developments (Pardeck, 1986) without any intention on specific educational subject with their style of expressions, i.e., funny stories and pictures. Additionally, picture books are outstanding in that those who read them are separated from those who perceive them. Readers are parents or child care persons who make book talks for infants who do not have sufficient literacy yet. Infants perceive and interpret incoming stimuli of the book talks and the pictures.According to the research in the developmental psychology, infants are found to express variety of cognitive reactions to the external stimuli in accordance with their developmental stage. If picture books work as those kinds of stimuli, infants might express the cognitive reactions when the stimuli of picture books are perceived. Furthermore, this tendency might be amplified, because infants are free from understanding the printing letters of picture books.In order to examine how the stimuli of picture books induces varieties of infants' reactions, we take an approach of applying a text mining technique to a large amount of the reviews on picture books written by their parents or the childcare persons. More specifically, this paper proposes to detect an infant's developmental reactions in reviews on picture books and shows effectiveness of the proposed method through experimental evaluation. This paper is the first attempt to solve the task of detecting an infant's developmental reactions in reviews on picture books. We extract the book reviews on picture books written on the Web site specialized in picture books, and found that those reviews reflect in-fants&apos; behavioral expressions as well as their parents&apos; reading activities in detail. Analysis of the reviews reveals that infants&apos; reactions written on the reviews are coincident with the findings of developmental psychology concerning infants&apos; behaviors. In order to examine how the stimuli of picture books induces varieties of infants&apos; reactions, this paper proposes to detect an infant&apos;s developmental reactions in reviews on picture books and shows effectiveness of the proposed method through experimental evaluation.
Finding the Origin of a Translated Historical Document Translation is a process of rewriting an original text in a different language (Lefevere, 2002). It is one of the oldest text manipulation related processes. Gospels are historical documents that were translated centuries ago. There are many versions of the same Gospel, translated from the original, or from another Gospel that had already been translated into a different language. Nowadays, it is unclear what was the language of the original Gospel from where these were translated. Historians and linguists are uncertain as to the origin of such historical documents. The Georgian Gospels are translated from Armenian or Greek Gospels (Lang, 1957). There are about 300 manuscripts of the four Gospels in Georgian that are translated from different languages (Kharanauli, 2000). Linguists are able to narrow down potential origins by looking at different linguistic properties, but skeptical to choose a single origin. We have three such Gospels in Georgian, Armenian and Greek, where linguists believe that Armenian or Greek are the potential origin. In this paper we use a supervised machine learning technique to find out the correct origin of a version of the Georgian Gospel.One of the challenges of dealing with historical data is the requirement of specific knowledge of languages that are not spoken at present day. If the language is currently spoken, it is likely that many properties have changed due to language evolution. Due to this issue, the available historical data set is very small in size, which proves a challenge for machine learning algorithms.From the early stage of translation studies research, translation scholars proposed different kinds of properties of source text and translated text. Recently, scholars in this area identified several properties of the translation process with the aid of corpora (Baker, 1993;Baker, 1996;Olohan, 2001;Laviosa, 2002;Hansen, 2003;Pym, 2005;Toury, 1995). These properties are subsumed under four keywords: explicitation, simplification, normalization, levelling out and interference.In this paper, we use texts from modern language to train a Support Vector Machine (SVM) that can be used to identify the original source of the Georgian Gospel.The paper is organized as follows: Section 2 introduced the historical documents that we are dealing with here, Section 3 discusses related work, followed by a discussion of the nature of a translated text in Section 4. The methodology is described in Section 5. The corpus of modern languages is described briefly in Section 6 followed by a discussion of different features we used in this paper in Section 7. The experiment and evaluation in Section 8 and finally, we present conclusions in Section 9. Gospels are one type of translated historical document. There are many versions of the same Gospel that have been translated from the original, or from another Gospel that has already been translated into a different language. Nowadays, it is difficult to determine the language of the original Gospel from where these Gospels were translated. In this paper we use a supervised machine learning technique to determine the origin of a version of the Geor-gian Gospel.
Improving the Performance of an Example-Based Machine Translation System Using a Domain-specific Bilingual Lexicon There are mainly two approaches for Machine Translation (MT): rule-based and corpus-based (Trujillo, 1999;Hutchins, 2003). Rule-Based MT (RBMT) approaches require manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based machine translation approaches are effective only when large amounts of parallel corpora are available. However, parallel corpora are only available for a limited number of language pairs and domains. In several fields, available corpora are not sufficient to make Statistical Machine Translation (SMT) approaches operational. Most previous works addressing domain adaptation in machine translation have proven that a SMT system, trained on general texts, has poor performance on specific domains. In this paper, we study the impact of using a domain-specific bilingual lexicon on the performance of an Example-Based Machine Translation (EBMT) system, and we compare the results of the EBMT system against those of the SMT system Moses on in-domain and out-ofdomain texts.The rest of the paper is organized as follows. In Section 2, we present previous research in the field of domain adaptation in SMT. Section 3 describes the translation process and the main components of the EBMT system. Section 4 presents the experimental setup and inspects the results of the EBMT system in qualitative and quantitative evaluations. Section 5 concludes our study and presents our future research directions. In this paper, we study the impact of using a domain-specific bilingual lexicon on the performance of an Example-Based Machine Translation system. We conducted experiments for the English-French language pair on in-domain texts from Europarl (European Parliament Proceedings) and out-of-domain texts from Emea (European Medicines Agency Documents), and we compared the results of the Example-Based Machine Translation system against those of the Statistical Machine Translation system Moses. The obtained results revealed that adding a domain-specific bilingual lexicon (extracted from a parallel domain-specific corpus) to the general-purpose bilingual lexicon of the Example-Based Machine Translation system improves translation quality for both in-domain as well as out-of-domain texts, and the Example-Based Machine Translation system outperforms Moses when texts to translate are related to the specific domain.
A Multifactorial Analysis of English Particle Movement in Korean EFL Learners&apos; Writings Linguistic alternation is one of the interesting areas in the linguistic investigations. Particle Movement is one of the syntactic alternations. It refers to the phenomenon where a particle goes behind the direct object (DO) in the phrasal verb constructions. Let's see the following example (Gries, 1999:1).(1) a. John picked up the book. b. John picked the book up.In (1a), the order is 'verb + particle + DO'. However, the particle is separated from the verb in (1b). That is, (1b) has an order of 'verb + DO + particle'. There have been a lot of theoretical studies to investigate what linguistic factors determine the choice of the alternation, in traditional grammar and generative grammar. Nowadays, as computer technology and statistics develop, there have been a few studies to explain these syntactic phenomena with corpus data. Gries (1999Gries ( , 2001Gries ( , 2003) adopted a multifactorial analysis to examine the Particle Movement in the native speakers' writings. These studies proposed several linguistics factors and it was demonstrated that these factors and their interactions significantly influenced the choice of the constructions This paper also adopted a multifactorial analysis to examine the Particle Movements in Korean EFL learners' writings. The Korean part of TOEFL11 corpus was used, and all the relevant sentences were extracted using the C7 tag information. The relevant factors were encoded to these sentences, and each factor was statistically analyzed with R. Through the analysis, it was demonstrated that Korean EFL learners employed a different strategy in Particle Movement and that only some factors were used for the selection of alternation.This paper is organized as follows. In Section 2, previous studies are reviewed with focused on corpus-based approaches. Section 3 enumerates research methods, and Section 4 contains analyses results. Section 5 is for discussions, and Section 6 summarizes this paper. This paper investigated Particle Movement in Korean EFL learners&apos; writings. Gries (1999, 2001, 2003) adopted a multifactorial analysis to examine Particle Movement of native speakers. Several linguistics factors were proposed in the studies, and it was demonstrated that these factors influenced the choice of the constructions. This paper also employed a multifactorial analysis to examine Particle Movement in Korean EFL learners&apos; writings. The analysis results illustrated that the Korean EFL learners were slightly different from native speakers in that only some factors were used for the selection of constructions.
Chinese word segmentation based on analogy and majority voting Words are usually considered a basic unit in natural language processing (NLP) studies. As natural language texts are continuous sequences of characters, it is generally agreed that word segmentation is the initial step of NLP. The performance of the best Chinese segmenters for F-score has reached 95%, as reported in the second SIGHAN Chinese segmentation bakeoff (Emerson, 2005). These best existing methods rely on massive training data.How to utilize as much information as possible from the training corpus to adapt a segmentation system towards a segmentation standard has been the main issue (Kit et al., 2005). Most of existing methods can be roughly classified as either dictionarybased or statistical-based methods.Dictionary-based methods usually rely on largescale lexicons and are built upon a few basic "mechanical" segmentation methods based on string matching. Without a large, comprehensive dictionary, the success of such methods degrade.Statistical-based methods consider the segmentation problem as a classification problem on characters and usually involve complicated language models trained on large-scale corpora.All of these methods require pre-training data and prior lexical knowledge. All current methods assume comprehensive lexical knowledge. How to model human cognition and acquisition it to segment words efficiently without using knowledge of wordhood is still a challenge in CWS ( Huang et al., 2007).After this introduction, we shall introduce the notion of proportional analogy in section 2 on which our proposal relies. In section 3, we shall describe the main idea of our new method for CWS using proportional analogy. Section 4 shall present the details of our implementation of our method. Section 5 shall detail some experiments done to evaluate our method with other state-of-the-art methods. This paper proposes a new method of Chi-nese word segmentation based on proportional analogy and majority voting. First, we introduce an analogy-based method for solving the word segmentation problem. Second, we show how to use majority voting to make the decision on where to segment. The preliminary results show that this approach compares well with other segmenters reported in previous studies. As an important and original feature , our method does not need any pretraining or lexical knowledge.
Enhancing Root Extractors Using Light Stemmers Natural Languages (NLs) are the medium that allow two or more parties to communicate and interact. Linguistics have captured NLs as a set of sophisticated rules that describe the usage of a language.The merger between linguistics and computer science began to formalize into Natural Language Processing (NLP) in mid 1950s (Nadkarni et al., 2011). Machine Translation (MT) (Hutchins, 2004) was one of the first tasks of NLP. MT takes one language as an input then predicts the output in another language. Linguistic complexity limited the development of MT, and other NLP tasks (Nadkarni et al., 2011).There exists various NLP tasks. For example, Text Summarization (Jing and McKeown, 2000) (Nenkova, 2005), Part of Speech Tagging (POST) ( Habash et al., 2009), word segmentation ( Monroe et al., 2014), sentiment analysis ( Oraby et al., 2013b)( Oraby et al., 2013a), and many more tasks. Each task has a specific goal which can be achieved by utilizing another NLP task. For example, sentiment analysis utilizes stemming algorithms ( Oraby et al., 2013a). Many NLP tasks are a part of more complex tasks.Text plays a central role in NLP and can be found in different forms, such as simple text or extracted rom images (Fathalla et al., 2007), this increases text resources and hence increases the need for more concise text forms.Stemming analysis is essential for many complex tasks. Stemming is a way of reducing a given word into a concise representation while preserving most of its linguistic features (Ryding, 2005). Arabic language is highly supportive for stemming analysis. Arabic language is a derivative language, where words are constructed from basic forms called roots (Ryding, 2005). Stemming for the Arabic language is the process of deriving back the root of a given word. Some stemmers derive multiple roots for a single word, hence, various techniques were used to disambiguate multiple roots. For example, utilizing a words context was used in the technique ContextBased Arabic Stemmer , CBAS, proposed in (El- Defrawy et al., 2015). Arabic stemmers are utilized for many tasks, such as sentiment analysis (Saleh and El-Sonbaty, 2007)( Oraby et al., 2013b)( Oraby et al., 2013a), question answering ( Ezzeldin et al., 2013), and Information Retrieval (IR) Frieder, 2002a)(Larkey et al., 2002)( Taghva et al., 2005).In this paper, a study is conducted to analyze and compare different Arabic stemmers from different perspectives, using a manually annotated dataset. Moreover, the paper presents an enhanced version of root extractors using light stemmers for preprocessing. The paper is organized as follows, section 2 presents a concise introduction of Arabic morphology, which gives the basic intuition of Arabic stemming analysis. Section 3 discusses various techniques and strategies used to develop Arabic stemmers, followed by a detailed comparison and evaluation of existing Arabic stemmers in section 4. Finally, a conclusion is presented in section 5. The rise of Natural Language Processing (NLP) opened new possibilities for various applications that were not applicable before. A morphological-rich language such as Ara-bic introduces a set of features, such as roots, that would assist the progress of NLP. Many tools were developed to capture the process of root extraction (stemming). Stemmers have improved many NLP tasks without explicit knowledge about its stemming accuracy. In this paper, a study is conducted to evaluate various Arabic stemmers. The study is done as a series of comparisons using a manually annotated dataset, which shows the efficiency of Arabic stemmers, and points out potential improvements to existing stemmers. The paper also presents enhanced root extractors by using light stemmers as a preprocessing phase.
Distinguishing between True and False Stories using various Linguistic Features "A lie is a false statement to a person or group made by another person or group who knows it is not the whole truth, intentionally" (Freitas- Magalhães, 2013). Dilmon (2014) defines a lie as "a linguistic message that conveys a falsehood or in which the truth is intentionally manipulated, in order to arouse in the listener a belief which he would not otherwise have held."The efforts to discover linguistic cues to detect lies are based on the assumption that there are differences between the language of an individual when he (or she) is not telling the truth and his (or her) "normal," truthful language. Fraser (1991) claims that these differences are the outcome of a feeling of stress, which is manifest in a decline in capacity for cognitive integration, in precision, in organization, and in ranking things. These difficulties result in a change in the normal elements of the speaker's language.There were a few studies during the last four decades concerning verbal cues that characterize a lie discourse. Dulaney (1982) finds that the response time was shorter, there were fewer special words, a smaller number of verbs in the past tense, and a faster speech rhythm when an individual was lying; there were fewer words in the discourse, as well as a tendency to short messages. Knapp et al. (1974) find that there were more general declarations and fewer factual ones, linguistic ambiguity, repeated declarations, more markers of diminishment (few, a little, hardly) and fewer group markers (we, our, all of us), more markers of the other (they) and fewer personal declarations (I, me). Hollien and Rosenberg (1991) use lexical breakdown to investigate deception (type-token ratio -TTR), and finds less linguistic diversity when a person is practicing deception.The studies of Dilmon (2007;2008;2012) conduct a comprehensive examination of the linguistic criteria that differentiate between the discourse of truth and of deception in the Hebrew language, and attempt to produce a primary test of the cognitive and emotional functions involved in the latter type of discourse. Forty three verbal criteria (Section 2.2) were classified according to the cognitive and emotional functions affecting the speaker, also addressing his level of awareness of these functions. Except one verbal criterion that was automatically computed by a program, the values of all other criteria for each story were computed by hand. This study starts from the end of the studies of Dilmon. Firstly, we implemented and/or applied four feature sets: POS-tag features, quantitative features, repetition features, and special expressions. Secondly, the application of the features is automatically done by a computer program in contrast to Dilmon's features (42 of her 43 features were computed by hand for each story). Thirdly, in contrast to Dilmon's studies that found which are the specific criteria that are statistically significant differentiators, we apply five supervised machine learning (ML) methods and various combinations of feature sets to find the best method for single-document classification, i.e., for each input story identifying whether it is a true or a false story. That will potentially lead to find discoveries concerning distinguishing between truth and false stories.The task of distinguishing between true and false story as well as the interpretation of the obtained results are of practical interest for any language in general and for Hebrew in particular. Such a system can be of great help to the work of organizations, such as workplaces, detective agencies, police, and courts, to identify various types of stories.The rest of this paper is organized as follows: Section 2 presents relevant background on linguistic examination in relevant systems, linguistic examination between discourses of truth and deception, text classification, and text classification of deception and true stories. Section 3 describes the classification model and the chosen feature sets. Section 4 presents the examined corpus, the experimental results and their analysis. Finally, Section 5 summarizes the main findings and suggests future directions. Argamon et al. (2009) describe an automatic process that profiles the author of an anonymous text. Accurate profiling of an unknown author is important for various tasks such as criminal investigations, market research, and national security. The deciphering the profile of someone is performed in the following way: Given a corpus of documents, marked as "male" and "female". Only four features were selected: sex, age, mother tongue, and neurotic level of disturbance behavior. Combination of linguistic features and various ML methods (Support vector machines and Bayesian regression) enable an automated system to effectively determine several such aspects of an anonymous author. Chaski (2005) presents a computational, stylometric method that has obtained 95% accuracy and has been successfully used in investigating and adjudicating several crimes involving digital evidence. Chaski's approach focuses on language features that are easily achievable, e.g., word length, sentence length, word frequency, and the distribution of words according to different lengths. Strous et al. (2009) describe an automatic process that characterizes and identifies schizophrenia in writing. This study investigates and analyzes computer texts written by 36 schizophrenia patients. Each document contains from 300 to 500 words. The system tested differences between these documents to documents written by people who are not sick with this disease. Observations have shown that methods using lexical and syntactic features obtained 83.3% accuracy. 60 features were chosen for the classification process: the 25 most frequent words in the corpus, the 20 most frequent letter tri-grams, and the average number of 15 repetitive words. The main conclusions are: (1) Some of the basic processes in schizophrenia are evident in writing; (2) Automatically identified characteristics of schizophrenic writing are closely related to the clinical description of the disorder; and (3) Automatic classification of samples in writing of schizophrenia is possible. This paper analyzes what linguistic features differentiate true and false stories written in Hebrew. To do so, we have defined four feature sets containing 145 features: POS-tags, quantitative, repetition, and special expressions. The examined corpus contains stories that were composed by 48 native Hebrew speakers who were asked to tell both false and true stories. Classification experiments on all possible combinations of these four feature sets using five supervised machine learning methods have been applied. The Part of Speech (POS) set was superior to all others and has been found as a key component. The best accuracy result (89.6%) has been achieved by a combination of sixteen POS-tags and one quantitative feature.
Selecting Contextual Peripheral Information for Answer Presentation: The Need for Pragmatic Models Answer Presentation is the final step in Question Answering (QA) which focuses on generating an answer which closely resemble with a human provided answer (Perera, 2012b;Perera, 2012a;Perera and Nand, 2014a). There is also a requirement to associate the answer with additional contextual information when presenting the answer. This paper focus of exploring methods to extract additional contextual information to present with the extracted factoid answer. We provide a classification if questions based on the type of the answer required and the number of entities that mentioned in the questions. The question classification is illustrated in Fig. 1. Firstly, question can be categorized based on the information need where questions may require a definition as the answer or a factoid answer which is an information unit (Perera, 2012a;Perera and Nand, 2014a). The definitional questions need definitions which include both direct and related background information and there is no need to further expand the answer with contextual information. So far the way factoid questions presentation involved only the answer itself without contextual information. Recently, Mendes and Coheur (2013) argued that even factoid questions need to present additional information. An advantage of presenting contextual information is that answer is justified by the information provided, so that users can conclude that the answer that is acquired by the system is one that they are searching for.The rest of the paper is structured as follows. Section 2 explores BoW and BoC models to rank contextual information. Section 3 focuses on presenting the experimental framework. Section 5 presents information on related work and we conclude the paper in Section 6. This paper explores the possibility of presenting additional contextual information as a method of answer presentation Question Answering. In particular the paper discusses the result of employing Bag of Words (BoW) and Bag of Concepts (BoC) models to retrieve contextual information from a Linked Data resource, DBpedia. DBpedia provides struc-tured information on wide variety of entities in the form of triples. We utilize the QALD question sets consisting of a 100 instances in the training set and another 100 in the testing set. The questions are categorized into single entity and multiple entity questions based on the number of entities mentioned in the question. The results show that both BoW (syn-tactic models) and BoC (semantic models) are not capable enough to select contextual information for answer presentation. The results further reveals that pragmatic aspects, in particular , pragmatic intent and pragmatic inference play a crucial role in contextual information selection in the answer presentation.
RealText asg : A Model to Present Answers Utilizing the Linguistic Structure of Source Question Question Answering (QA) comprises of four main tasks; question processing, answer search, answer extraction, and answer presentation. The first three tasks focus on extracting the answer while the last aims to present the extracted answer in a humanlike format. With the rise of trend towards building human-competitive QA systems, there been a corresponding demand for the extracted to be presented in a human competitive form rather than the bare answer as a single word or a phrase. A wide range of answer presentation schemes have been reported including user tailored answers (Mendes and Coheur, 2013;Maybury, 2008;Kolomiyets and Moens, 2011;Perera and Nand, 2014a), justification based answers ( Mendes and Coheur, 2013;May- bury, 2008;McGuinness, 2004;Saint-Dizier and Moens, 2011), presentation of paragraph level text summaries with the extracted answer (Mendes and Coheur, 2013;Lin et al., 2003;Perera, 2012b;Per- era, 2012a), presentation of hot links with answers (McGuinness, 2004), and presentation of navigable related answers and contextual information (Saint- Dizier and Moens, 2011;Perera and Nand, 2015a;Perera and Nand, 2014b;Perera and Nand, 2014c). All of the mentioned models aim to build an answer which closely resembles a human generated answer. However, an approach that has not been explored in the mentioned models is to exploit the structure of the question in the formulation of the answer. A human generated answer is based both on the answer structure as well as how the question was formulated (Singer, 2013). For example, given the question "Which river does the Brookyln Bride cross?", the expected answer sentence would be of the form of "The Brookyln Bridge crosses East River".It is essential to understand the types of questions and their linguistic structure in order to suc-cessfully generate a sentence with the answer embedded in it. The questions can be divided in to two main categories based on their interrogative categories; wh-interrogative and polar interrogatives. A wh-interrogative is aimed at getting an answer which represents another entity or a property of a resource mentioned in the question, on the other hand a polar interrogative requests a true/false (yes/no) answer. These two types require two different answer sentence generation schemes; wh-interrogatives require to embed the answer to the modified source question linguistic structure and the polar interrogatives need to transform the same question without further embedding, however it still needs modification based on the answer. Table 1 shows the interrogative types with examples and Part-Of-Speech (POS) tags associated with them and the expected answer sentences.This paper focuses on answer sentence generation based on typed dependency parsing. To the best of our knowledge, no previous study has investigated this method of generating an answer sentence utilizing the source question's linguistic structure. The methodology we introduce here is based on linguistics. The core idea is that the generation of an answer sentence is initiated by identifying the root of the parse tree and then proceed to build the sentence using the nominal subject, a key feature of a Subject-Verb-Object (SVO) style language such as English. In order to identify the grammatical relation that holds the parts of question with the root, we employ typed dependency parse of the complete question. The typed dependency based patterns extracted using a training dataset are used to construct the framework. Answer merging and further realization of the sentence are implemented in order to provide a human-like natural language answer sentence. The complete framework (implemented in Java) and the datasets are available for download from the project website 1 .The remaining part of the paper proceeds as follows. Section 2 introduces the methodology of answer sentence generation. We discuss the process under four main themes; extracting syntactic patterns, applying patterns to new questions, answer merging, and further realization. Section 3 describes the experimental framework including the results. A discussion on related work which investigates different answer presentation methods in natural language is presented in Section 4. Section 5 concludes the paper with an overview of future work. Recent trends in Question Answering (QA) have led to numerous studies focusing on presenting answers in a form which closely resembles a human generated answer. These studies have used a range of techniques which use the structure of knowledge, generic linguistic structures and template based approaches to construct answers as close as possible to a human generate answer, referred to as human competitive answers. This paper reports the results of an empirical study which uses the linguistic structure of the source question as the basis for a human competitive answer. We propose a typed dependency based approach to generate an answer sentence where linguistic structure of the question is transformed and realized into a sentence containing the answer. We employ the factoid questions from QALD-2 training question set to extract typed dependency patterns based on the root of the parse tree. Using identified patterns we generate a rule set which is used to generate a natural language sentence containing the answer extracted from a knowledge source, realized into a linguistically correct sentence. The evaluation of the approach is performed using QALD-2 testing factoid questions sets with a 78.84% accuracy. The top-10 patterns extracted from training dataset were able to cover 69.19% of test questions.
Learning under Covariate Shift for Domain Adaptation for Word Sense Disambiguation Supervised learning methods have been used in many natural language processing tasks. In supervised learning, we create training data for the target task from corpus A and learn a classifier from the training data. This classifier performs well for test data in corpus A; however, it does not perform well for test data in corpus B, which is different from corpus A. This is the problem of domain adaptation 1 . In this paper, we deal with domain adaptation for word sense disambiguation (WSD).WSD identifies the sense c ∈ C of an ambiguous word w in a sentence x. This problem can be solved by the following equation:arg max c∈C P (c|x).The above equation can be solved using supervised learning. However, the domain adaptation problem occurs in a real task. In domain adaptation, P s (c|x) can be derived from source domain S; therefore, we must estimate P t (c|x) in the target domain T using P s (c|x) and other data. Note that the sense c of the word w in sentence x is not changed if sentence x appears in any domain corpus, i.e., P (c|x) does not depend on a domain. As a result, P s (c|x) = P t (c|x). Therefore, it seems that we do not need to estimate P t (c|x) because we have P s (c|x). However, this is wrong because P s (x) = P t (x). The following assumption is referred to as the covariate shift:P s (x) = P t (x), P s (c|x) = P t (c|x).In other words, the domain adaptation for WSD satisfies the assumption of the covariate shift. In this paper, we solve domain adaptation for WSD by learning under covariate shift.Briefly, learning under covariate shift is a learning method through weighted training data. Thus, it has two key points: (1) calculation of the weight of an instance and (2) weighted learning.For the first point, the probability density ratio w(x) = P t (x)/P s (x) is used theoretically as the weight of the instance x. There are two techniques for calculating the probability density ratio. The first is modeling P S (x) and P T (x) and then taking the ratio between them. The second is modeling w(x) directly. Several studies have examined the former method (Jiang andZhai, 2007)(Saiki et al., 2008). However, to the best of our knowledge, the latter approach has not been attempted in NLP research. In this paper, we adopt unconstrained least squares importance fitting (uLSIF) as the second calculation ( Kanamori et al., 2009). Actually, there are many methods to calculate probability density ratio ( Sugiyama and Kawanabe, 2011). In this paper, we use uLSIF because it shows good performance and quick calculation time. uLSIF models w(x) with the sum of N t pieces of basis functions ψ l (x), where N t is the number of target data.Generally, a Gaussian kernel is used as the basis function. However, in this case, the width σ of the Gaussian kernel becomes an additional parameter. Therefore, we suggest using a linear kernel to drop this parameter σ.For the second point, the maximum entropy method (ME) is commonly employed in weighted learning. However, in domain adaptation for WSD, the number of instances is generally small. For this reason, we do not use a weighted ME but a weighted support vector machine (SVM).Furthermore, three rough heaviness values are applied to the weighted SVM for comparison, i.e., a small weight 0.1, a normal weight 1.1, and a large weight 2.1, rather than a detailed weight for each case.In the experiment, we use three domains, i.e., OC (Yahoo! Answer), PB (books) and PN (newspaper) in the Balanced Corpus of Contemporary Written Japanese (BCCWJ (Maekawa, 2007)) and 16 target words that appear frequently in these three domains. There are six types of domain adaptation -OC PB, PB PN, PN OC, OC PN, PN PB, and PB OC giving a total of 96 (= 16 × 6) domain adaptation tasks. Consequently, the effects of the proposed method are confirmed. We show that domain adaptation for word sense disambiguation (WSD) satisfies the assumption of covariate shift, and then solve it by learning under covariate shift. Learning under covariate shift has two key points: (1) calculation of the weight of an instance and (2) weighted learning. For the first point, we employ unconstrained least squares importance fitting (uLSIF), which models the probability density ratio of the source domain against a target domain directly. Additionally, we propose weight only to the particular instance and using a linear kernel rather than a Gaussian kernel in uLSIF. For the second point, we employ a support vector machine (SVM) rather than the maximum entropy method (ME) that is commonly employed in weighted learning. Three corpora in the Balanced Corpus of Contemporary Written Japanese (BCCWJ) and 16 target words were used in our experiment. The experimental results show that the proposed method demonstrates the highest average precision .
Unsupervised Domain Adaptation for Word Sense Disambiguation using Stacked Denoising Autoencoder In this paper, we propose an unsupervised method of domain adaptation for Word Sense Disambiguation (WSD) using Stacked Denoising Autoencoder (SdA).WSD is the task of identifying the sense of a target word in a sentence. In general, supervised learning, such as Support Vector Machine (SVM), can be used for this task because of the fact that this approach is highly accurate. However, if the training and test data come from different domains, the accuracy of this approach is lowered. This problem is called a domain adaptation (Søgaard, 2013). It is considered that this problem occurs due to the difference between the distributions of features in training and test data.SdA is an unsupervised learning method of obtaining the abstract feature of the input data (basic feature) using Neural Network ( Vincent et al., 2010). Recently it has been shown that a higher accuracy in voice and character recognition has been obtained using SdA ( Le et al., 2012). We have applied this method to a domain adaptation for WSD and have shown that the abstract feature obtained through SdA can avoid the problem of domain adaptation.It is well-known from previous works that the most efficient methods for domain adaptation for WSD depend on the combination of training data (from the source domain) and test data (from the target domain) ( Komiya and Okumura, 2011) ( Komiya and Okumura, 2012). Furthermore, in an unsupervised domain adaptation method, even if the accuracy is improved in the combination of the source and target domains, the accuracy rate hardly improve. As a result, the accuracy rate on average of the method decreases, or remains the same. In other words, there are accuracy limitations with each method. In our method, we choose whether or not to apply SdA based on the similarity of features. Our method cannot be applied in the case for pair of do-mains as they are not suitable for SdA.In our experiment, we have used three domains: Yahoo! Answers (OC), Books (PB), and newspaper (PN) from the Balanced Corpus of Contemporary Written Japanese (Maekawa, 2007), along with 16 selected ambiguous words. Domain adaptation has the following six transitions: (1) PB OC, (2) OC PB, (3) OC PN, (4) PN OC, (5) PB PN and (6) PN PB. First, in every domain adaptation, we have compared the accuracy of the basic feature and abstract feature by SdA using SVM. As a result, SdA have been effective in half of the case of domain adaptations. Furthermore, we have explored situations when to apply SdA or not. Consequently, the SdA with similarity of features is effective in all domain adaptations. In this paper, we propose an unsupervised domain adaptation for Word Sense Disambigua-tion (WSD) using Stacked Denoising Autoen-coder (SdA). SdA is an unsupervised learning method of obtaining the abstract feature set of input data using Neural Network. The abstract feature set absorbs the difference of domains, and thus SdA can solve a problem of domain adaptation. However, SdA does not always cope with any problems of domain adaptation. Especially, difficulty of domain adaptation for WSD depends on the combination of a source domain, a target domain and a target word. As a result, any method of domain adaptation for WSD has adverse effect for a part of the problem, Therefore, we defined the similarity between two domains, and judge whether we use SdA or not through this similarity. This approach avoids an adverse effect of SdA. In the experiments, we have used three domains from the Balanced Corpus of Contemporary Written Japanese and 16 target words. In comparison with baseline, our method has got higher average accuracies for all combinations of two domains. Furthermore , we have obtained better results against conventional domain adaptation methods.
An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding Sentiment analysis or opinion mining is the computational study of people's opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes ( Liu and Zhang, 2012). The task of sentiment analysis is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Consumers also need a sounding board rather than thinking alone while making decisions. With the development of Internet, opinionated texts from social media (e.g., reviews, blogs and micro-blogs) are used frequently for decision making, which makes automated sentiment analysis techniques more and more important. Among those tasks of the sentiment analysis, the key one is to classify the polarity of given texts. Many works have been done in recent years to improve English sentiment polarity classification. There are two categories of such works. One is called "machine learning" which is firstly proposed to determine whether a review is positive or negative by using three machine learning methods, including NB, ME and SVM ( Pang et al., 2002). The other category called "semantic orientation" is applied to classify words into various classes by giving a score to each word to evaluate the strength of sentiment. And an overall score is calculated to assign the review to a specific class (Turney, 2002).Recently, researchers have tried to handle tasks of Natural Language Processing (NLP) with the help of deep learning approaches. Among those approaches, a useful one called word2vec has attracted increasing interest. Word2vec translates words to vector representations (called word embeddings) efficiently by using skip-gram algorithm (Mikolov et al., 2013a). It is also proposed that the induced vector representations capture meaningful syntactic and semantic regularities, for example, "King" -"Man" + "Woman" results in a vector very close to "Queen" ( Mikolov et al., 2013b).Besides, with the advancement of information technology, for the first time in Chinese history, a huge volume of Chinese opinionated data recorded in digital form is ready for analysis. Though Chinese language plays an important role in economic globalization, there are few works have been done for Chinese sentiment analysis with huge databases. It inspires us to make an empirical study on Chinese sentiment with bigger databases than usual.The remain of the article is organized as follows: Section 2 briefly describes related work. Section 3 describes details of the methods used in training procedure. Section 4 reports and discusses the results. Finally, we summarize our works in Section 5. In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.
Automatic Classification of Spoken Languages using Diverse Acoustic Features LID is used either as a standalone task or as a preprocessing step, capturing the first seconds (sec) of the recording and processing it in order to transfer the control to the appropriate next stage; e.g. speech recognition systems, multilingual translation systems or call-centers (e.g., emergency calls) routing, where the response time of a native operator might be critical. LID is a process by which a given spoken utterance language is automatically identified (Muthusamy et al., 1994). Most LID systems are based on high level features such as frequency of a single phoneme, phoneme sequences (Zissman and Singer, 1994), syllable, words, and prosody (Thymé-Gobbel and Hutchins, 1996). Such LID systems need a comprehensive corpus, including transcription from trained humans, and long enough intervals to correctly classify, first, these high level features and then the spoken language (Zissman, 1996;Greenberg, 1999). Any error in the higher level feature recognizers is carried over, and probably/possibly amplified in, the following steps. However, providing a comprehensive corpus enables higher level features which ensure better results than using acoustic features alone. LID systems based on higher level features have one principal problem: Tokenizing those features accurately has proven to be the main obstacle thus far in high accuracy of natural LID (Abramson, 2003). Matejka et al. (2005) found that separating gender before processing improved the LID's accuracy.A LID system has two main parts: feature extraction, where a vector of measurements that should characterize the high level features are extracted from the signal; and pattern matching, where these extracted features are processed using statistical (like in this study) or temporal (Rabiner, 1989) methods to recognize speech languages. The approach taken in our study does not resort to the use of phoneme recognizers or any higher level features. Instead, we rely on low-level features alone, rather than using low-level features to predict intermediate features as in previous work. The motivation is "quicker response time and simpler training stages".The rest of this paper is organized as follows: Section 2 presents an overview of previous LID systems. Section 3 describes the different feature sets chosen for this study. Section 4 presents the suggested classification model and the implemented features for LID of seven languages: French (FR), Farsi (FA), Japanese (JA), Korean (KO), Mandarin (MA), Tamil (TA), and Vietnamese (VI). Section 5 describes the examined corpora and experimental results and analyzes them. Section 6 includes a summary and proposes suggestions for future research. Many of the language identification (LID) systems are based on language models using machine learning (ML) techniques that take into account the fluctuation of speech over time, such as Hidden Markov Models (HMM). Considering the fluctuation of speech results LID systems use relatively long recording intervals to obtain reasonable accuracy. This research tries to extract enough features from short recording intervals in order to enable successful classification of the tested spoken languages. The classification process is based on frames of 20 milliseconds (ms) where most of the previous LID systems were based on much longer time frames (from 3 seconds to 2 minutes). We defined and implemented 173 low level features divided into three feature sets: cepstrum, relative spectral (RASTA), and spectrum. The examined corpus, containing speech files in seven languages, is a subset of the Oregon Graduate Institute (OGI) telephone speech corpus. Six machine learning (ML) methods have been applied and compared and the best optimized results have been achieved by Random Forest (RF): 89%, 82%, and 80% for 2, 5, and 7 languages, respectively.
Measuring Popularity of Machine-Generated Sentences Using Term Count, Document Frequency, and Dependency Language Model Natural language generation is widely used in variety of Natural Language Processing (NLP) applications. These include paraphrasing, question answering systems, and Machine Translation (MT). To improve the quality of generated sentences, arranging effective evaluation criteria is critical (Callison- Burch et al., 2007).Numerous previous studies have aimed to evaluate the quality of sentences. The most frequently used evaluation technique is asking judges to score those sentences. Unlike computer algorithms, humans can notice very delicate differences and perceive various characteristics in natural language sentences. Conventional wisdom holds that human judgments represent the gold standard; however, they are prohibitively expensive and timeconsuming to obtain.Because of the high cost of manual evaluation, automatic evaluation techniques are increasingly used. These include very popular techniques that measure meaning adequacy and lexical similarity, such as BLEU ( Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER plus ( Snover et al., 2009). Additionally, a distinctive characteristic of auto evaluation techniques is that they can be applied not only to performance verification, but also to the generation stage of NLP applications. Although these techniques can make experiments easier and accelerate progress in a research area, they employ fewer evaluation criteria than humans.In general, previous research efforts have focused on "technical qualities" such as meaning and grammar. However, customer satisfaction is sometimes determined more by "functional quality" (how the service work was delivered) than by "technical quality" (the quality of the work performed) (Mittal and Lassar, 1998). Especially, Casaló et al. (2008) showed that the customers' loyalty and satisfaction are affected by their past frequent experiences. We focused on this aspect and propose a new criterion, popularity, to consider the functional quality of sentences. We define a popular sentence as one that contains words that are frequently used, appear in many documents, and contain frequent dependencies. Us-ing this definition, we aim to measure the popularity of sentences.In this paper, we investigate the notion of "popularity" for machine-generated sentences. We measured popularity of sentences with an automatic method that can be applied to the generation stage of MT or paraphrasing. Because it is a subjective evaluation, measuring the popularity of sentences is a difficult task. We defined a popular sentence as one that contains words that are frequently used, appear in many documents, and contain frequent dependencies. Subsequently, we began our analysis by calculating Term Frequency (TF). To reflect the characteristics of agglutinative languages, we apply a morpheme analysis during language resources generation. As a result, we obtain a Content Morpheme Count (CMC). To complement areas CMC cannot cover (words that have abnormally high CMC), we apply morpheme-based Document Frequency (DF). Lastly, to consider popularity came from contextual information, we apply a dependency relationship language model. We verify our method by analyzing Pearson correlations between human judgments; human evaluation shows that our method has a high correlation with human judgments. And our method shows the potential for measuring popularity by involving the contextual information.The remainder of this paper is organized as follows. Section 2 presents related works in the field of sentence evaluation. Section 3 explains the approach to measure the popularity of words and sentences. In Section 4, we evaluate the usefulness of our method. In section 5, we analyze the result of experiment Lastly, Section 6 concludes the paper. We investigated the notion of &quot;popularity&quot; for machine-generated sentences. We defined a popular sentence as one that contains words that are frequently used, appear in many documents , and contain frequent dependencies. We measured the popularity of sentences based on three components: content morpheme count, document frequency, and dependency relationships. To consider the characteristics of agglu-tinative language, we used content morpheme frequency instead of term frequency. The key component in our method is that we use the product of content morpheme count and document frequency to measure word popularity , and apply language models based on dependency relationships to consider popularity from the context of words. We verify that our method accurately reflects popularity by using Pearson correlations. Human evaluation shows that our method has a high correlation with human judgments.
Acquiring distributed representations for verb-object pairs by using word2vec Natural language processing (NLP) based on corpora has become more common thanks to the improving performance of computers and development of various corpora. In corpus-based NLP, word representations and language statistics are automatically extracted from large amounts of text in order to learn models for specific NLP tasks. Complex representations of words or phrases can be expected to yield a precise model, but the data sparseness problem makes it difficult to learn good models with them; complex representations tend not to appear or appear only a few times in large corpora. For example, the models of statistical machine translation are learned from various statistical information in monolingual corpora or bilingual corpora. However, low-frequency word representations are not learned well, and consequently, they are processed as unknown words, which causes mistranslations. It is necessary not only to process NLP tasks by matching surface forms but to generalize the language representations into semantic representations.Many approaches represent words with vector space models so that texts can be analyzed using semantic representations for individual words or multi-word expressions. These methods can be classified into two approaches: the word occurrence approach and the word co-occurrence approach. The word occurrence approach includes Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Probabilistic LSA (PLSA) (Hofman, 1999) and Latent Dirichlet Allocation (LDA) ( Blei et al., 2003), which acquire word representations from the distributions of word frequencies in individual documents (a word-document matrix). Recently, many researchers have taken an interest in the word cooccurrence approach, including distributional representations and neural network language models ( Mikolov et al., 2013a;Mikolov et al., 2013b;Mnih and Kavukcuoglu, 2013;Pennington et al., 2014). The word co-occurrence approach uses statistics of the context around a word. For example, the distributional representations for a word are defined as a vector that represents a distribution of words (word frequencies) in a fixed-size window around the word. The neural network language models, including word2vec (Mikolov et al., 2013a;Mikolov et al., 2013b), GloVe ( Pennington et al., 2014) and vector Log-Bilinear Language model (vLBL) (Mnih and Kavukcuoglu, 2013), generate distributed representations, which are dense and low-dimensional vectors representing word meanings, by learning a neural network that solves a pseudo-task of predict-ing a word given its surrounding words. Word2vec is preferred in NLP because it learns distributed representations very efficiently. Neural network language models have semantic compositionality for word-word relations by calculating vector representations; e.g., 'king' -'man' + 'woman' is close to 'queen.' However, they acquire the distributed representations for a word, not phrase structures such as verb and object pairs. It is necessary to obtain representations for phrases or sentences to be used as natural language representations.We devised three methods for acquiring distributed representations for verb-object pairs by using word2vec. We experimentally verified that the distributed representations of different verb and object pairs have the same meaning. We focused on verb-object pairs consisting of verbs whose meaning is vague, such as light-verbs, e.g., the 'do' and 'dishes' pair in "do dishes". The following two sentences are examples that have similar meanings but whose phrase structures are different.1. I wash the dishes.2. I do the dishes.The representations for the verb-object pairs in the first sentence is "wash(dishes)," and those for the second sentence is "do(dishes)" with the light verb 'do'. Despite the difference between the representations of these sentences, they have the same meaning "I wash the dishes." As such, there are various sentences that have the same meaning, but different representations. We examined the performance of each method by measuring the distance between distributed representations for verb-object pairs ('do' and 'dishes' pair) and those for the corresponding basic verb ('wash') or predicated argument structures ("wash(dishes)"). We also experimentally compared the previous methods and ours on the same data set used in (Mitchell and Lapata, 2008). We propose three methods for obtaining distributed representations for verb-object pairs in predicated argument structures by using word2vec. Word2vec is a method for acquiring distributed representations for a word by retrieving a weight matrix in neural networks. First, we analyze a large amount of text with an HPSG parser; then, we obtain distributed representations for the verb-object pairs by learning neural networks from the analyzed text. We evaluated our methods by measuring the MRR score for verb-object pairs and the Spearman&apos;s rank correlation coefficient for verb-object pairs in experiments.
A Light Rule-based Approach to English Subject-Verb Agreement Errors on the Third Person Singular Forms With the increasing number of people all over the world who study English as second language (ESL), grammatical errors in writing often occur due to cultural diversity, language habits, and education background. There has been a substantial and increasing need of using computational techniques to improve the writing ability for second language learners. In addition, such techniques and tools may help find latent writing errors in official documents as well. To meet the urgent need from ESL, a lot of works on natural language processing focus on the task of grammatical error detection and correction. Formally, it is a task of automatically detecting and correcting erroneous word usage and ill-formed grammatical constructions in text ( Dahlmeier et al., 2012).It is not a brand new task in natural language processing. However, it has been a challenging task for several reasons. First, many of these errors are context-sensitive so that errors cannot be detected and then corrected in an isolated way. Second, the relative frequency of errors is quite low: for a given type of mistake, an ESL writer will typically go wrong in only a small proportion of relevant language structures. For example, incorrect determiner usages usually occur in 5% to 10% of noun phrases in various annotated ESL corpora ( Rozovskaya and Roth, 2011). Third, an ESL writer may make multiple mistakes in a single sentence, so that continuous errors are entangled, which let specific error locating and correction become more difficult.In recent decades, existing studies on this task have focused on errors in two typical word categories, article and preposition (Han et al., 2006;Felice and Pulman, 2008;Dahlmeier and Ng, 2011). However verb errors occur as often as article and preposition errors at least, though there are few works on verb related errors. Two reasons are speculated for why it is difficult to process verb mistakes. First, compared with articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech (POS), and in fact many existing processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complicated linguistically. For an English verb, it has five forms of inflections (see Table 1). Different forms imply different types of errors, even, one type of verb form may lead to multiple types of errors. Verb errors are one of the most common grammar errors made by non-native writers of English. This work especially focus on an important type of verb usage errors, subject-verb agreement for the third person singular forms, which has a high proportion in errors made by non-native English learners. Existing work has not given a satisfied solution for this task, in which those using supervised learning method usually fail to output good enough performance, and rule-based methods depend on advanced linguistic resources such as syntactic parsers. In this paper, we propose a rule-based method to detect and correct the concerned errors. The proposed method relies on a series of rules to automatically locate subject and predicate in four types of sentences. The evaluation shows that the proposed method gives state-of-the-art performance with quite limited linguistic resources.
A Machine Learning Method to Distinguish Machine Translation from Human Translation The translation performance of Statistical Machine Translation (SMT) systems has been improved significantly within this decade. However, it is still incomparable to the human translation (Feng et al., 2012;Li et al., 2012). Most translation text generated by SMT systems can be understood in some * Correspondence author. † Thank all the reviewers for valuable comments and suggestions on our paper. This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248 degree but still not good enough. However, a significant proportion of text that exists serious mistakes and even does not make sense, and these text can be easily recognized by human.It is not difficult to understand the reason why SMT systems generate ill-formed or non-sense sentences. SMT systems combine probability models in a log-linear framework (Och and Ney, 2003), where the systems always attempt to find a sentence with the highest probability from the candidates. However, Language Model (LM), such as n-gram LM, and reordering model only have limited capacity to represent context, where sentences with local optimum could often be output. Meanwhile, it can be a very different thing for the entire translation sentence due to complicated semantic and pragmatic issues.Therefore, to improve SMT performance, if poorly translated sentences can be distinguished automatically, it is possible for us to refine these sentences by some extra efforts. In this paper, to order to define the quality of the sentence generated by SMT systems, we borrow the idea from the evaluation of machine translation task, that the more like human translation text, the better the machine translation output is. Considering that the poorly translated sentences show great difference from human text, we compare text generated by SMT systems with human translations. This comparison motivates us to design a predictor to tell whether a sentence is machine generated or human generated. Above all, such a predictor can be treated as a binary classification problem.In this paper, we use Support Vector Machines (SVMs) (Hearst et al., 1998) to solve such a problem. The benefits of SVMs for text categorization have been identified since it learns well with many relevant features (Joachims, 1998). In order to find those poorly SMT-translated sentences, we train an SVM-classifier on a feature space. Most features are linguistically motivated only from the target language side. As only target language is concerned, our model will be facilitated of some direct applications.Among all features, a major part is related to the syntactic parser. The parsing structure of the output sentence is very sensible to the quality of SMT outputs. We therefore especially select these features related to the branching properties of the parse tree. One of the reason is that it had become apparent from failure analysis in (Corston-Oliver et al., 2001) that SMT system output tended to favor rightbranching structures over noun compounding.The remainder of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. This paper introduces a machine learning approach to distinguish machine translation texts from human texts in the sentence level automatically. In stead of traditional methods, we extract some linguistic features only from the target language side to train the prediction model and these features are independent of the source language. Our prediction model presents an indicator to measure how much a sentence generated by a machine translation system looks like a real human translation. Furthermore, the indicator can directly and effectively enhance statistical machine translation systems, which can be proved as BLEU score improvements.
Proceedings of the 30 th Pacific Asia Conference on Language, Information and Computation edited by  
Müller and Ørsnes, In Preparation), 3. Persian (Indo-Iranian, DFG/ANR MU 2822/3-1, Müller  The German Grammar group develops a fully formalized and computer-processable set of grammars that share a set of constraints, that is, they have a common core (see also Müller, 2013 for an overview). Some very general constraints hold for all grammars, some for subgroups of languages. 9. Hindi. The approach to developing the core grammar is bottom-up in that we do not assume a genetically determined Universal Grammar and try to prove its existence in language after language. Rather we treat every language in its own right and try to generalize over sets of languages only later. Some of this knowledge might be part of an UG in the above sense, but we do not make any claims on this issue. We also do not make an explicit core-periphery distinction while working on individual languages. Rather what belongs to the core is determined empirically by comparing languages. If we find a phenomenon in more than one language and think that it is correct to describe the phenomenon by the same means, the respective representations are kept in one file that is used by the respective grammars. This results in a grouping of languages that share the same code, with files containing very general constraints being used by all languages.
Information and Computation (PACLIC 30) Seoul, Republic of Korea  Large amounts of biomedical corpora have emerged from different sources, including scientific literature, lab notes, patents and electronic health records. Most of the efforts in biomedical text mining have focused on the extraction and linkage of specific facts, such as molecular interactions, links between genes and diseases, or patients&apos; symptoms. Such facts are rarely contextualised using the associated scientific or professional methodology (e.g. what methods were used to detect particular interaction, or to diagnose a particular disease). However, methods are the vital, but often neglected, underpinning of science and practice. Given enough data, the ability to extract methodological knowledge would allow us to &quot;infer&quot; common (and possibly best) practice for a given task, and thus indeed learn from vast amount of text. This is obviously a complex task that involves identification, representation and linking of steps in associated methods, requiring a series of NLP methods such as temporal information extraction and discourse analysis. In this talk we will explore finding out what methods are being used to do what experiment from the literature, or to infer what clinical pathways patients have followed, based on the notes in their electronic health records. We will illustrate some of the work in the context of bioinformatics (e.g. recovering a general view of the methods described in the literature) and clinical practice (e.g. reconstruction of patient journeys). We will also discuss how feasible this task is given the known issues with the lack of reported details needed for understanding and reproducibility of associated methods (i.e. how much of a method is indeed present in the literate or clinical records).
Information and Computation (PACLIC 30) Seoul, Republic of Korea  Simultaneous speech translation attempts to produce high quality translations while at the same time minimizing the latency between production of words in the source language and translation into the target language. The variation in syntactic structure between the source and target language can make this task challenging: translating from a language where the verb is at the end increases latency when translating incrementally into a language where the verb appears after the subject. In this talk I focus on a key prediction problem in simultaneous translation: when to start translating the input stream. I will talk about two new algorithms that together provide a solution to this problem. The first algorithm learns to find effective places to break the input stream. In order to balance the often conflicting demands of low latency and high translation quality, the algorithm exploits the notion of Pareto optimality. The second algorithm is a stream decoder that incrementally processes the input stream from left to right and produces output translations for segments of the input. These segments are found by consulting classifiers trained on data created by the first algorithm. We compare our approach with previous work and present translation quality scores (BLEU scores) and the latency of generating translations (number of segments translated per second) on audio lecture data from the TED talks collection.
Information and Computation (PACLIC 30) Seoul, Republic of Korea  Linguists in the Generative Linguistics tradition typically rely on their own intuition as a native speaker for their core data. This practice has continually been challenged to be more rigorous in their data collection methodology, and Experimental Syntax is an effort to address the issue. The purpose of this presentation is to discuss whether some background information of the subjects might affect the acceptability judgments of the subjects, mostly naive native speakers. It is well established, following the sociolinguistic research, that native speakers&apos; use of language might be influenced by some social factors like age, gender, class, and others. The data to be discussed in the presentation are drawn from acceptability judgments on sentences by Korean native speakers, and comprise 68,158 data points, gathered from 302 native speakers with 574 stimulus sentences (around 240 items per subject). The background information that was collected includes factors like sex, age, dialect, and previous exposure to linguistics classes (&apos;familiarity&apos;). We discuss in detail some statistical issues to be dealt with for any proper treatment of the data, focusing on their distributional characteristics. The results show that the &apos;familiarity&apos; factor largely plays a marginally significant role in the distribution of the data.
Application of Language Technology to Language Assessment: Measuring Different Aspects of PACLIC 30 Proceedings 13 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea  Japan is faced with an imminent challenge of cultivating &apos;global human resources&apos;, as the whole society delves into the global information society. The Course of Study defined by the Ministry of Education, Culture, and Sports and Technology (MEXT) of the Japanese government has emphasized communicative competence and / or &apos;communication skills&apos; as a focus of the foreign language subjects (e.g., English) since 1900&apos;s and the Courses of Study for most other subject also mention similar needs. During the academic year of 2014, the Educational Reform Working Group within the leading Liberal Democratic Party proposed the use of TOEFL iBT as an obligatory part of university entrance examination procedures. Furthermore, in 2015, MEXT advised the consideration of utilizing existing 4-skills English language proficiency tests that external test publishers have made available in Japan. Pearson offers various English tests that are automatically scored. Its spoken English test, Versant English Test, uses automated speech recognition and technologies. Versant Writing Test measures reading and writing skills and is scored automatically by using Latent Semantic Analysis. Approximately 60 first-year students at the undergraduate School of Law at Waseda University took Versant English Test and Versant Writing Test four times and the sores are compared to those of Oxford Placement English Test that the same students took three times. Oxford Quick Placement Test is designed to measure vocabulary, collocation, and grammar through reading-based multiple choice tasks. The present study reports results of analyses of these test scores and estimated CEFR levels, and then investigates challenges that Japanese learners and teachers of English are facing.
Endurant vs Perdurant: Ontological Motivation for Language Variations  Modern ontology focuses on the shared structure of knowledge representation and sheds light on underling motivations of human conceptual structure. This paper addresses the issue of whether ontological structures are linguistically represented, and whether such conceptual underpinning of linguistic representation may motivate language variations. Integrating our recent work showing that the most fundamental endurant vs. perdurant ontological dichotomy is grammaticalized in Chinese and on comparable corpus based studies of variations of Chinese, I will explore the possibility that this basic conceptual dichotomy may in fact provide the motivation of changes of perspectives that underlies language variations. I will also discuss possible implication this approach has in accounting for other language changes and variations such as light verb&apos;s argument taking, incorporation, loss of case/agreement, and English-er/-ee asymmetry. In the process, the will resolve three linguistic puzzles and eventually show that the endurant/perdurant dichotomy may in fact be the conceptual basis of the hitherto undefined +N (i.e. nouny) vs. +V (i.e. verby) features prevalent in linguistics. Based on this proposal, the variations involving various types of denominalization and deverbalization can be accounted for.
The syntax of the Chinese excessive resultative construction Back in 1990, Lu (1990) observed that there is a special type of resultative construction in Mandarin Chinese, which is different from other types of Chinese resultatives in both form and meaning. The following illustrative examples are given in Lu (1990). (1)  The photo was enlarged less than expected.They are special in three ways. First, the subject must be the patient of the verb, and the predicate is invariantly in the form of a bare verb plus a bare adjective. Secondly, the sentence final perfective aspect marker le is obligatory. Thirdly, all the examples in (1) have a "more than expected" excessive meaning.We will offer an affectedness-based analysis of the Chinese excessive resultative construction, trying to answer the following questions:(2) a. How does the 'more than expected' reading arise?b. Why do some excessive resultatives also have a normal resultative reading? c. Why is the bi-phrase ('than expected') not able to show up? d. Why is the sentence final le obligatory?2. An affectedness-based analysis of the construction 2.1 Beavers' (2011) theory of affectedness Beavers (2011) proposed that change is an inherently relational concept involving both a theme participant that undergoes the change and a scale participant defining the process of the change over time (following Kennedy and Levin 2008). According to this scalar model of change, all types of change can be defined as a transition of a theme along a scale that defines the change. Beavers (2011) defined an operator result' to capture this notion of affectedness: (3) For all dynamic predicates ø, themes x, events e, states g, and scales s:[[ø (x,s,e) result' (x,s,g,e@&gt;ø (x,s,e) SOURCE (x,bc,e) GOAL (x,g,e)]] (This says for event e described by ø, g is the target state of theme x on scale s iff x transitions to g by the end of e from a contextually determined state bc at the beginning of e. ) (Beavers 2011: 351) Beavers then showed that this scalar model of change can offer a unified analysis of different types of affectedness such as motion, change-of-state, and creation/consumption: The most apparent advantage of this scalar model of change is that it manages to account for the double telicity effect. The following examples are given in Beavers (2011: 349) to show that the theme and the scale participants jointly determine the telicity of the sentence:(5) a. Bill dimmed the lights half dim in/?for five minutes. b. Bill dimmed lights half dim for/??in five minutes. c. Bill dimmed the lights dimmer and dimmer for/??in five minutes.The theme and the scale participants in (5a) are both specific, so the sentence is telic; in (5b) the scale participant is specific, but the theme is not, so the sentence is atelic; in (5c) the theme is specific, but the scale participant is vague, so the sentence is atelic. The sweater was knitted larger than expected.' es [knit'(sweater, s, e) result' (sweater,s,more-than-expected,e)] z knit'(sweater, s, e) says that there is a knitting event of the sweater along a scale of size; z result' (sweater,s,more-than-expected,e) says that the sweater's actual size on the scale exceeds an expected size. This paper offers an affectedness-based analysis of the Chinese excessive resultative construction, which typically describes events of affectedness consisting of two participants, a theme participant and a scale participant measuring the degree of affectedness. In such an event, the theme participant is created or affected according to a beforehand prescribed value (e1) on a scale, while the process of the event results in an actual value (e2) on the same scale. The realized value may or may not be identical to the prescribed value. When the two values do not coincide (e2&gt;e1), the &apos;more than expected&apos; excessive resultative interpretation arises. This analysis crucially hinges upon the assumption that there is a covert comparison between two values on the same scale. If such a comparison cannot be established within a resultative construction, the excessive meaning will not arise.
Halliday  Adverbs have become the ragbag in grammar in which all uncategorized items are relegated. Over the years, there have been several studies (e.g., Biber et al.
A &quot;Maximal Exclusion&quot; Approach to Structural Uncertainty in Dynamic Syntax "Case" and "grammatical function" are central to any syntactic theories; a number of constructions exhibit unique case-marking patterns and linguistic generalisations are often stated with reference to grammatical function ( Keenan and Comrie, 1979). Rigorous accounts of these concepts, however, are pending in "surface-oriented" grammars such as Dynamic Syntax (DS) ( Kempson et al., 2001). The aim of this article is to clarify the relation between case and grammatical function in formal-grammar terms, with examples drawn from Japanese.As will be stated in §2, the case-marking system of Japanese challenges surface-oriented grammars. In particular, DS, which explicates the mechanism whereby a string of words is parsed online and a structure is progressively built up, has not seriously tackled the relation between case and grammatical function (see §3). In this article, we advance the DS formalism from the perspective of "maximal exclusion" so that it models the relation between case and grammatical function in Japanese (see §4). We then apply this account to further data relating to "Major Subject Constructions" (see §5). &quot;Case&quot; and &quot;grammatical function&quot; are central to syntactic theories, but rigorous treatments of these notions in surface-oriented grammars like Dynamic Syntax (DS) are pending. Within DS, it is simply held that a case particle resolves structural uncertainty (i.e., unfixed node) in the course of incremental tree update. We model the relation between &quot;case&quot; and &quot;grammatical function&quot; with special reference to Japanese. In this language, the nominative case particle ga normally marks a &quot;subject&quot; NP, but it may mark an &quot;object&quot; NP. Moreover, ga may occur more than once within a single clause. We will address these issues by proposing the &quot;maximal exclusion&quot; approach to structural uncertainty.
Secondary Predicates in Native and Nonnative Grammars It goes without saying that adult second language (L2) learning differs from child first language (L1) development, owing to various identifiable disparities in cognition and maturation between the two groups. From this truism follows the logical question, for generative second language acquisition (SLA) researchers, how much of the initial state or the biologically determined precursor contributes to the acquisition of a language later in life. Previously, the issue was explored by inquiries into the developmental processes until about two decades ago when researchers started to seriously consider what it is that adults can ultimately know about the target language that are not true of their native languages. Much in line with the developmental research, results gleaned from empirical and longitudinal studies that focus on the final L2 state, suggest, or in many cases conclude, that older learners attain different grammars than native speakers. For those born and raised in the target language setting, language development is, from the onset, controlled by UG (Universal Grammar) principles and parameters. Mature learners are subjected to all kinds of undesirable elements none of which occurs to child learners (for a review, see Whitea, 2003), leading them to the mastery of L2 grammars full of anomalies and aberrations.The present experiment questions whether it is enough to measure L2 grammars against native grammars (for discussion, see Mack, 1997) and shows that native speakers vary in behavior just as much as nonnative speakers, depending on the grammatical features under analysis and the experimental conditions. The typical comparative nativenonnative studies with an eye to pinning down the biological influence bear little fruit if it is true that the final state of the model subjects lacks the supposed uniformity in the knowledge of the target grammar, which, according to Chomsky (1986,1988,1993), is not something in dispute. What is not being investigated is the causes of the native variations (see Shi, 2014;Shia, in progress).A large number of the generative L2 studies conclude that there is something amiss about L2 grammars, for their bearers deviate from the natives whose use of the target language is reliable and consistent. In an influential study involving proficient English speakers of first languages of Korean, Chinese, Indonesian, and Dutch, Schachter (1990) found that these subjects, unlike the native controls, did not always recognize errors in sentences like *What did Susan visit the store that had t in stock?The extent to which an L2 group succeeded was correlated with whether its native grammar instantiated the Subjacency constraint (Chomsky, 1981) as English did. Schachter takes this as support for her Incompleteness Hypothesis; namely "incompleteness will turn out to be an essential property of any adult second language grammar" (pp. 118-119). Johnson et al. (1996) also found from 10 Chinese speakers of English, who had on average lived in the U.S. for 6.45 years, that their abilities to recognize morphosyntactic errors from auditorily presented sentences were lower (54.2% accurate) than the native speakers (98.3% accurate). This indicates, to the researchers, that L2 grammars of the nonnativeborn speakers are "not native in determinacy" (p. 343).In an experiment on English psych verbs (interest, disappoint) and container verbs (e.g., decorate, cover), Juffs (1996) retrieved both production and comprehension data that informed him that the Chinese college students, with no living experience in an English-speaking environment, lagged behind native speakers in consistency, which tended to improve as a function of the increased proficiency level. While low-and intermediate-level learners had trouble producing or processing sentences like "The broken vase disappointed John," those at the advanced level did as well as the English speakers. Chen (2005), in search of the association of verbs consider, find to various complement syntactic frames, uncovered a gradient preference pattern for Mandarin speakers of English: tensed clause &gt; infinitive clause &gt; small clause. The finding was based on a set of within-group statistical analyses conducted to the L2 group. Assuming the lack of preference for the natives, the found preference pattern from the nonnatives suggested, to Chen, that it must be L2 grammars that were faulty, since native speakers, being native, could not go wrong.It is not that generative L2 researchers are oblivious of or blindsided by the fact that native speakers, due to internal as well as circumstantial variables, can falter or fail to comply with the grammatical rules when called upon. For example, in the above study by Schachter (1990), she acutely noted the unusual poor performance by the English-speaking participants on the Wh-movement sentences that "had been piloted on other natives and performance has been much higher" (p. 111). As an explanation, Schachter speculated that the piloted subjects were "graduate students majoring in linguistics" (fn 19).In Johnson et al.'s (1996) error-detection experiment, if we remove the chance responses, based on their formula (p. 343), from the native group data, its accuracy rate would drop to 96.6 percent, from the reported 98.2 percent. In the study of Chen (2005), there was a case where native speakers showed more variations, based on her computations of standard deviations, on the use of consider/find THAT, than nonnative speakers. But spotting such variabilities from native speakers is one thing and taking it into account is another. As has been shown time and again, native variance is largely viewed as inconsequential, reflective of the accidental glitches, and therefore dismissible.This experiment, a mixed design, aims to do the reverse of what has been typically done; that is, to demonstrate L2 grammar is not as flawed as previously thought, provided that the random and experimental errors are carefully identified. The problem, which has been long neglected, is a methodological one -the use of native speakers as the sole yardstick to determine the nature of second language grammars. The null hypothesis tested is that speakers who acquire the target language natively therefore do not vary in linguistic competence; they as natives can always be counted on being up to par when it comes to the measurement of grammatical knowledge.Nativeness may well be correlated with birthplace, but linguistic competence is not. To tap into the components of the faculty of language (Chom- sky, 1972, p. 27;1986, pp. 16-17;1998, p. 115), we ought to rethink the current research procedure. One alternative being explored here is to hold off the input effects as a pernicious confounding variable, so that no subject group is at unwarranted advantage.To that end, the study tested, under contrasting conditions, a set of infrequent yet robust syntactic frames: causatives, resultatives, and depictives. The idea behind the design was that by displacing subjects from their "comfort zone" into a "leading edge" (Rispoli, 2003, p. 819), we are able to take a better look into their inner grammar proper. The independent variables of interest are three: construction (3 levels: causatives, resultatives, depictives), task (4 levels: Guided Production, CombiningClause, Grammaticality Judgment, Interpretation Task), and modality (2 levels: production, comprehension). The construction effects are examined by holding the modality and task effects neutral. To factor in the influence of task and modality, pairs of group data from the comparable tasks or modalities are analyzed. And finally, to see if the key variables (modality u task) interact, tasks from different modalities are compared in pairs. The null hypothesis is rejected just in case the empirical data shows that those who speak English day in and day out fail to deliver the expected outcome on tasks presented under various conditions. They show tendencies to respond to the intricate properties of constructions, tasks, or modalities as opposed to their grammatical knowledge in ways of nonnative speakers. The typical measurement by which the nature of second language grammars is evaluated is the input of native speakers. This paper reports on data from Mandarin speakers of English (n = 19), with an average of 10;3 (year;month) length of residence in the U.S., and native American English speakers (n = 19), and looks at how they dealt with causatives, resultatives, and depictives under four experimental conditions. It was found that native participants did not always behave reliably; they altered, swung, and oscillated just like nonnative counterparts , and there were multiple cases where their fluctuation rates were way higher than those of the latter. Such variances were brought about by the effects of construction, task, or modality. These results cast doubt on the common practice of assessing second language grammars in terms of native intuitions and call on researchers to reconsider the assumption that second language grammars that are legitimate must be native-like.
A Generalized Framework for Hierarchical Word Sequence Language Model Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in various application such as machine translation ( Brown et al., 1990), spelling correction ( Mays et al., 1990), speech recognition (Ra- biner and Juang, 1993), word prediction ( Bickel et al., 2005) and so on.Most research about Probabilistic Language Modeling, such as Katz back-off (Katz, 1987), KneserNey ( Kneser and Ney, 1995), and modified KneserNey ( Chen and Goodman, 1999), only focus on smoothing methods because they all take the n-gram approach (Shannon, 1948) as a default setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modified Kneser-Ney (abbreviated as MKN).An alternative solution is to factor the language model probabilities such that the number of unseen sequences are reduced. It is necessary to extract them in another way, instead of only using the information of left-to-right continuous word order.In (Guthrie et al., 2006), skip-gram ( Huang et al., 1993) 1 is proposed to overcome the data sparseness problem. For each n-gram word sequence, the skip-gram model enumerates all possible word combinations to increase valid sequences. This has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings a greatly increase of processing time and redundant contexts.In ( Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1, ...wm is defined as the set {wi 1 , wi 2 , ...wi n |Σ n j=1 ij − ij−1 &lt; k}. HWS) structure, by which much more valid word sequences can be modeled while remaining the model size as small as that of n-gram. In (Wu and Matsumoto, 2015) ( , instead of only using the information of word frequency, the information of direction and word association are also used to construct higher quality HWS structures. However, they are all specific methods based on certain heuristic assumptions. For the purpose of further improvements, it is also necessary to generalize those models into one unified structure. This paper is organized as follows. In Section 2, we review the HWS language model. Then we present a generalized hierarchical word sequence structure (GHWSS) in Section 3. In Section 4, we present two strategies for rearranging word sequences under the framework of GHWSS. In Sections 5 and 6, we show the effectiveness of our model by both intrinsic experiments and extrinsic experiments. Finally, we summarize our findings in Section 7. Language modeling is a fundamental research problem that has wide application for many NLP tasks. For estimating probabilities of natural language sentences, most research on language modeling use n-gram based approaches to factor sentence probabilities. However, the assumption under n-gram models is not robust enough to cope with the data sparseness problem , which affects the final performance of language models. At the point, Hierarchical Word Sequence (ab-breviated as HWS) language models can be viewed as an effective alternative to normal n-gram method. In this paper, we generalize HWS models into a framework, where different assumptions can be adopted to rearrange word sequences in a totally unsupervised fashion , which greatly increases the expandability of HWS models. For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for n-gram language models.
Processing English Island Sentences by Korean EFL Learners Since Ross's identifications of island constraints in English (Ross, 1967), there have been a lot of debates on the existence of island constraints in other languages. Some languages were believed to contain some island effects, while other languages (e.g. Chinese, Korean, or Japanese) were doubtful about the existence of island effect.The status of island effects of the L1 (the mother tongue) also may influence the acquisition of L2, since it was well-known that the knowledge of L1 might influence the acquisition of L2, which was known as the L1 transfer effects (Selinker, 1969;Odlin, 1989;2003). Korean students learn English as Foreign Language (EFL), since English is not an official language in Korean. There have been some controversies on the existence of island constraints in Korean. Some have argued for the presence of island effects (Lee 1982, Han 1992, Hong 2004), while others have argued against it ( Sohn 1980, Kang 1986, Suh 1987, Hwang 2007. 1 This paper is organized as follows. In Section 2, previous studies are reviewed. Section 3 includes the experimental design, research materials and research method. Section 4 enumerates the analysis Then, the question is whether the island status of Korean may influence the acquisition of the constructions in English. To answer this question is also crucial from the psycholinguistic point of view, since there might be different psycholinguistic or cognitive processes when people produce or understand the island constructions in their native language (L1) and another language (L2).In order to investigate whether the L1 transfer effects also appear in the acquisition of English island constructions, an experiment was designed where the acceptability scores of the Korean EFL learners were measured with the ME method. Then, the collected data were statistically analyzed with R.1 Similar kinds of controversies exist also for Japanese. Nishigauchi (1990) and Watanabe (1992) claimed that there were island constraints in Japanese, but Ishihara (2002) and Sprouse et al. (2011) mentioned that this language had no island constraint.results. Section 5 contains discussions, and Section 6 summarizes this paper. This paper took an experimental approach and investigated how Korean EFL learners process the English island constructions. Since there are some controversies on the existence of the island effects in Korean, the L1 transfer effect may make it difficult for the Korean EFL learners to learn island constructions in English. To examine if the difference between English and Korean affects the acquisition of English island constructions, four different types of target sentences were made for English island phenomena: Complex-NP, whether, subject, and adjunct island. The acceptability scores of Korean EFL learners were measured with Magnitude Estimation (ME). Then, the collected data were statistically analyzed. The analysis results showed that, unlike previous studies, the Korean EFL learners correctly identified all of the English island constructions. This finding showed that the island status of the Korean language did not affect the acquisition of island constructions in English.
Long-distance anaphors and the blocking effect revisited: An East Asian perspective Anaphoric elements are generally claimed to fall into two types: those that obey locality conditions and those that do not. Reflexives in English and their counterparts in East Asian languages, especially Chinese, Japanese, and Korean, display characteristics of one or other type. For example, while the English reflexive himself can only be felicitously used when bound within the same clause, as in (1), the Chinese reflexive ziji in (2) can ambiguously refer to the matrix subject, the intermediate subject, or the lowest subject across the clause boundary, which has been called a longdistance anaphor.(1) John3 thinks Tom5 knows Bill7 likes himself*3/*5 /7. A major claim in the literature is that a distribution of anaphoric elements either obeys or disobeys locality conditions. In addition, it has long been noted that the presence of a first (or second) person pronoun intervening between Chinese ziji and a higher potential antecedent blocks long-distance binding. However, this paper proposes that a third person antecedent can be a blocker in a given discourse, based on Kuno and Kaburaki&apos;s (1977) system. If this is on the right track, the blocking effect in East Asian languages, especially Chinese ziji, Korean caki, and Japanese zibun, can be accounted for with a unified treatment.
Developing an Unsupervised Grammar Checker for Filipino Using Hybrid N-grams as Grammar Rules According to the philosopher and educator Kevin Browne, poor grammar implies two negative sentiments towards the writer: either he is not intelligent or he just does not care about his writing any better. Backing on this problem, there has been many researches and advances in the field of computer-aided grammar checking such as Microsoft Word, Google Docs, Grammarly, LanguageTool, and Ginger. These software solutions can detect syntactical errors such as spelling, punctuation, word forms, and word usages. However, most of these solutions have focused on the English language. There has been very few works in the Filipino language despite being a language of at least 100 million people 1 . Additionally, it is difficult to use an existing grammar checker system of one language and apply it on another since the system would have its specific design and functionalities tackling the unique phenomena of its target language.The Filipino language, just like any other language, has its own unique phenomena which serve as a challenge in developing its own grammar checker system. It has a 'large vocabulary of root, borrowed, and derived words' caused by the arrival and/or colonization of foreign countries including: Spain, USA, and China in the Filipino land 2 . It also has a high degree of inflection and uses variety of affixes to change the part-of-speech of a root word (ex. root: tira 'live [on a house]', tira + han = tirahan 'house') or change the focus and aspect of a verb (tirhan 'live' -neutral aspect/object focus, titira 'will live' -contemplative aspect/ actor focus, tumira 'lived' -perfective aspect/ actor focus. Another linguistic phenomenon in Filipino is its free-word order structure. Filipino sentences, in its natural form, follow the predicate-subject sentence format (ex. Masaya ako -word-perword is translated as 'Happy I') or as subjectpredicate sentence format (ex. Ako ay masayaword-per-word is translated as 'I [none] happy') where the word ay acts as a lexical marker and is usually placed after the subject and before the predicate. In the Filipino language, direct objects, adjectives and adverbs may also be written as phrases and including prepositional phrases, they also follow the free-word order and not being limited to just one position in the sentence (Ramos, 1971). For example, the sentence 'Mark ate an apple.' can be translated to: Si Mark ay kumain ng mansanas., Kumain si Mark ng mansanas., and Kumain ng mansanas si Mark. As seen in the last two translations, the direct object phrase ng mansanas 'apple' can be placed directly after the verb or after the subject yet both produce the exact same meaning.As of this writing, there are still no grammar-checking software systems for Filipino that is publicly available that cover broad-range of grammatical errors.This fact may be associated with the complex structure of the Filipino language which makes it difficult in constructing (error) grammar rules. Among the few existing grammar checkers in Filipino are: Panuring Pampanitikan (PanPam) by Jasa et al. (2007) and Language Tool for Filipino (LTF) by Oco &amp; Borra (2011). PanPam is a syntax and semantics-based grammar checker for Filipino that makes use of error patterns as rules and lexical functional grammar as its parsing algorithm. LTF, on the other hand, uses a rule file containing error patterns in the form of regular expressions and part-of-speech tags and a dictionary file in detecting its errors and providing corresponding suggestions. Although these systems, especially LTF, could distinctly recognize grammatical errors from correct text by using error patterns, the main concern with these systems is that the parser rules, dictionaries, affix-to-root-word mappings, wordto-part-of-speech mappings, error patterns, and other files are manually defined which is a very tedious task to cover the entire language and all possible errors in it especially that the language is ever growing and the number of errors committed by writers are directly proportional to it. This concern is evident on the systems' presented limitations and results where only a small subset of errors was covered.In other languages such as English, there are existing works such as Lexbar (Tsao &amp; Wible, 2009), EdIt (Huang et al., 2011), Google books n-gram corpus as grammar checker (Nazar &amp; Renau, 2012), and Chunk-based grammar checker for translated sentences (Lin et al., 2011) which are unsupervised grammar checker systems that make use of grammatically correct texts, their corresponding part-of-speech (POS) tags, and/or lemmas converted into n-gram sequences and used as grammar rules.The Lexbar application (Tsao &amp; Wible, 2009) generated hybrid n-grams, which are ngrams composed of words, POS tags, and lemmas. These hybrid n-grams are generated from actual tagged word sequences. For example, given phrases such as 'from her point of view' and 'from his point of view', the system will be able to generate the hybrid rule 'from dps] 3 point of view'. This rule can be used to flag the phrase 'from my point of view' as grammatically correct and the phrase 'from him point of view' as incorrect. The Lexbar app was only tested on substitution-correctable errors. The EdIt system (Huang et al., 2011) also made use of hybrid n-grams (called pattern rules) as grammar rules but only generates the rules such as 'play ~ role in [Noun]', 'play ~ role in [V- ing]', and 'look forward to [V-ing]  4 ' from specific lexical collocations such as 'play ~ role' and 'look forward'. These types of rules tackle much more specific error types in English. The key difference of EdIt with Lexbar is that it only limits the number of POS tokens in an n-gram rule to one while Lexbar can have one or more POS tokens such as the rule: 'from [dps] [nn0] 5 ' derived from the phrases like 'from his house' and 'from her balcony'. EdIt applied its rules in detecting errors correctable by substitution, insertion, and deletion. Both Lexbar and EdIt used weighted Levenshtein edit distance algorithm in prioritizing its suggestions.This research aims to build an unsupervised grammar checker system for Filipino using hybrid n-grams as grammar rules following a similar format as Lexbar's grammar rules. These rules will be used to detect grammatical errors in Filipino and provide suggestions such as substitution, insertion, deletion, merging, and unmerging extending the existing suggestions made by both Lexbar and EdIt. This study focuses on using hybrid n-grams as grammar rules for detecting grammatical errors and providing corrections in Filipino. These grammar rules are derived from grammatically-correct and tagged texts which are made up of part-of-speech (POS) tags, lemmas, and surface words sequences. Due to the structure of the rules used by this system, it presents an opportunity to have an unsupervised grammar checker for Filipino when coupled with existing POS taggers and morphological analyzers. The approach is also customized to cover different error types present in the Filipino language. The system achieved 82% accuracy when tested on checking erroneous and error-free texts.
Supervised Word Sense Disambiguation with Sentences Similarities from Context Word Embeddings Conventionally, the meaning of a word has been represented using a high-dimensional sparse Bag-ofWords (BoW) vector. Recently, there has been considerable interest in word embeddings, where words meanings are represented by low-dimensional and dense vectors using deep learning. With word embeddings, the distance between words can be measured more precisely than that provided by a vector based on the BoW model. Therefore, word embeddings has been used effectively for various natural language processing tasks. With regard to word sense disambiguation (WSD) tasks, some studies have considered that the word embeddings comprise embeddings of word senses (Chen et al., 2014)( Neelakantan et al., 2014) (Sakaizawa and Ko- machi, 2015)( Bhingardive et al., 2015);however, these studies only consider unsupervised WSD. To the best of our knowledge, the only study that addresses supervised WSD with word embeddings is by Sugawara(Sugawara et al., 2015). In Sugawara's method, one BoW-based vector and one vector based on context word embeddings (CWE) are merged, and they are used for training a classifier and identification. The method proposed by Sugawara is more effective than the method that only uses a vector based on the BoW model. However, we have found two problems with this method. First, it restricts the position of the word in the context. Second, it includes function words. In this paper, we propose a method that addresses both problems. Specifically, if N example sentences exist in training data, an N-dimensional vector that consists of the similarities between each pair of example sentences is added to a basic feature vector. This new feature vector is used for training a classifier and identification. The similarity between sentences is calculated using CWE. This solves the first problem. In addition, the proposed method only uses content words to calculate similarities between example sentences, which solves the second problem. We used SemEval-2 Japanese task to compare Sugawara's method and the proposed method. We found that the proposed method demonstrated higher precision. Furthermore, we performed experiments with basic features used in SemEval-2 baseline system and determined that the proposed method gave better results.Feature vectors can be created using the words around a target word in a sentence. This method can present a context of the target word with the vector in a binary representation. Therefore, unknown words cannot be handled.To address this problem, superordinate concepts in a thesaurus are used because it provides the similarities between different words.Thus, using a thesaurus is effective for WSD. In this paper, we propose to increase the accuracy of WSD using word embedding as a thesaurus. In this paper, we propose a method that employs sentences similarities from context word embeddings for supervised word sense disam-biguation. In particular, if N example sentences exist in training data, an N-dimensional vector with N similarities between each pair of example sentences is added to a basic feature vector. This new feature vector is used to train a classifier and identification. We evaluated the proposed method using the feature vectors based on Bag-of-Words, SemEval-2 base-line as basic feature vectors and SemEval-2 Japanese task. The experimental results suggest that the method is more effective than the method with only basic vectors.
HSSA Tree Structures for BTG-based Preordering in Machine Translation One of the major common challenges for machine translation (MT) is the different order of the same conceptual units in the source and target languages. In order to get a fluent and adequate translation in the target language, the default phrase-based statistical machine translation (PB-SMT) system implemented in MOSES has a simple distortion model using position ( Koehn et al., 2003) and lexical information (Tillmann, 2004) to allow reordering during decoding. Other solutions exist: e.g., the distortion model in (Al-Onaizan and Papineni, 2006) handles n-gram language model limitations; Setiawan et al. (2007) propose a function word centered syntaxbased (FWS) solution; Zhang et al. (2007) propose a reordering model integrating syntactic knowledge. Also, other models than the phrase-based model have been proposed to address the reordering problem, like hierarchical phrase-based SMT (Chiang, 2007) or syntax-based SMT (Yamada and Knight, 2001).Preordering ( Xia and McCord, 2004;Collins et al., 2005) has been proposed primarily to solve the problems encountered when translating between languages with widely divergent syntax, for instance, from a subject-verb-object (SVO) language (like English and Mandarin Chinese) to a subjectobject-verb (SOV) language (like Japanese and Korean), Preordering is a pre-processing task that aims to rearrange the word order of a source sentence to fit the word order of the target language. It is separated from the core translation task. Recent approaches ( DeNero and Uszkoreit, 2011;Neubig et al., 2012;Nakagawa, 2015) learn a preordering model based on Bracketing Transduction Grammar (BTG) (Wu, 1997) from parallel texts to score permutations by using tree structures as latent variables. They build the needed tree structures and the preordering model (i.e., a BTG) at the same time using word alignments. However it is needed to check whether a given sentence can fit the desired tree structures.It seems of course more difficult to build both the tree structures and the preordering model at the same time than to build only a preordering model if the tree structures are given. In this paper, we rapidly obtain tree structures using word-to-word associations taking advantage of the hierarchical subsentential alignment (HSSA) method (Lardilleux et al., 2012). This method computes a recursive binary segmentation in both languages at the same time, judging whether two spans with the same concepts in both languages are inverted or not. We conduct oracle experiments to show that these tree structures may be beneficial for PB-SMT. We then use these tree structures as the training data to build a preordering model without checking the validity by modifying the top-down BTG parsing method introduced in (Nakagawa, 2015). Oracle experiments show that if we reorder source sentences exactly, translation scores can be improved by around 2.5 BLEU points and 7 RIBES points in English to Japanese) and 5 BLEU points and 10 RIBES points in Japanese to English. Experiments with our tree structures show that better RIBES scores can be easily obtained.The rest of this paper is organized as follows: Section 2 describes related work in preordering and BTG-based preordering. Section 3 shows how to obtain tree structures using word-to-word associations. Section 4 reports oracle preordering experiments. Section 5 gives a method to build a preordering model using tree structures. Section 6 presents the results of our experiments and their analysis. The Hierarchical Sub-Sentential Alignment (HSSA) method is a method to obtain aligned binary tree structures for two aligned sentences in translation correspondence. We propose to use the binary aligned tree structures delivered by this method as training data for preordering prior to machine translation. For that, we learn a Bracketing Transduction Grammar (BTG) from these binary aligned tree structures. In two oracle experiments in English to Japanese and Japanese to English translation, we show that it is theoretically possible to outperform a baseline system with a default distortion limit of 6, by about 2.5 and 5 BLEU points and, 7 and 10 RIBES points respectively, when preordering the source sentences using the learnt preordering model and using a distortion limit of 0. An attempt at learning a preordering model and its results are also reported.
Event Based Emotion Classification for News Articles Emotion classification from text, an extension of sentiment analysis, aims at assigning emotional labels to a given text. It has wide applications such as customer review ( Pang et al., 2002), emotion based recommendation (Cambria et al., 2011), emotional human-computer interaction (Hollinger et al., 2006), eLearning ( Rodriguez et al., 2012), etc. It is also important to understand reader's emotion reactions for reading news articles as they may trigger emotionally charged reactions which may lead to serious social and political consequences. However, news articles are normally used to describe recent events. To maintain objectivity, writers normally avoid using subjectivity and emotion-linked words. Thus, current works on emotion analysis, which use more social media type of text, would not work well for news text.Generally speaking, emotion classification can be done either at document level or at sentence level. In this paper, we focus on document level emotion classification for news articles. Due to the nature of new articles, we need to address two main issues: 1) How to obtain sufficiently high quality labeled news corpus for training and prediction; and 2) How to identify suitable features for this genre of text. To address the first issue, we make use of the crowdsourcing method to obtain labeled data for a set of news articles provided in ACE 2005 ( Walker et al., 2006) and through appropriate filtering, to obtain a reasonably good emotion-labeled corpus. To address the second issue, we first investigate the commonly used features for emotion prediction, including N-gram, Part-Of-Speech (POS), and emotion lexicons ( Lin et al., 2007). However, these features, suited for sentence level classification, seem to be noisy for document level classification. Since news articles mainly describe a specific event and based on psychological studies that event can trigger emotions (Cacioppo and Gardner, 1999), we further explore event related features for emotion classification. Our hypothesis is that for news articles, a specific set of event linked anchor words can trigger emotions of readers and are therefore more important than most of the other words which may not have relations to emotions. Here the anchor word means the keyword of an event, such as "die", "accident", "bomb", etc. Our proposed approach identifies event anchor words and use them as features for emotion classification. The main steps involved in event anchor word extraction involves three steps: First, we make use of the ACE 2005 data as our raw source corpus where event information was already annotated and crowdsourcing is used to obtain emotion linked labels for the news articles. Second, we use the annotated event information to train a CRF model for event anchor words extraction. Last, the extracted event anchor words can then be used as features to train a classifier for emotion prediction. This is different from lexicon based method because lexicon based method relies on externally prepared knowledge. In contrast, anchor words are automatically extracted from training data to be used as features.The main contributions of this work include:1. The construction of an important annotated resource for event based emotion analysis based on ACE 2005 English news articles which can be made available to the research community. 2. The identification of more suitable features for document level emotion classification of news articles without emotion lexicon, and a feasible feature extraction method, which can also be used by other event based applications. The proposed features are more effective than emotion lexicon features and can improve the performance when combined with the bag-ofword features. The rest of the paper is organized as follows: Section 2 discusses related works for emotion classification. Section 3 introduces the construction of the annotated corpus as a training data resource. Section 4 presents our event anchor word extraction and emotion analysis framework. Section 5 gives performance evaluation. The conclusion and future work are summarized in section 6. Reading of news articles can trigger emotional reactions from its readers. But comparing to other genre of text, news articles that are mainly used to report events, lack emotion linked words and other features for emotion classification. In this paper, we propose an event anchor based method for emotion classification for news articles. Firstly, we build an emotion linked news corpus through crowd-sourcing. Then we propose a CRF based event anchor extraction method to identify event related anchor words that can potentially trigger emotions. These anchor words are then used as features to train a classifier for emotion classification. Experiment shows that our proposed anchor word based method achieves comparable performance to bag-of-word based method and it also performs better than emotion lexicon features. Combining anchor words with bag-of-words can increase the performance by 7.0% under weighted F-score. Evaluation on the SemEval 2007 news headlines task shows that our method outper-forms most of other methods.
The interaction of politeness systems in Korean learners of French One has a full understanding of how to be polite in one's own language, however when acquiring another language it is often the case that the first (L1) and second language (L2) express politeness norms differently. Furthermore, as politeness is embedded within a language's grammar system, language learners must have the grammatical competence along with the pragmatic knowhow to select and use politeness expressions appropriately in the target language being acquired.The focus of this article is to investigate the system of politeness surrounding French tu and vous, from a Korean learner of French's perspective. The acquisition of tu/vous is no easy matter; syntactically it may be straightforward to acquire two pronominal forms and use them grammatically, pragmatically however it is very difficult as your grammatical competence will not be much assistance to you in selecting the appropriate form for the situation you might find yourself in (to be discussed). Moreover, this may be compounded by influence from your first language which may not have the politeness concepts the target language has, as illustrated by the below examples (the meanings follow the English): In addressing someone when making a request, the English example (1a) shows there are no tu/vous forms used to indicate politeness, rather politeness here can be framed via choice of structures (e.g., Would you mind telling me… versus the above).French on the other hand (1b), uses tu/vous to express politeness according to whom you are addressing (simplified, tu is 'friendly' while vous is more 'respectful' -to be discussed), while Korean has an alternation between nuh and the null pronoun (the latter for more polite situations) in similar circumstances to French. 1 Thus, it appears that in requests, Korean and French follow similar patterns in that the pronominal is selected according to the context, allowing for a strong possibility of observing Korean L1 influence on Korean learners' L2 French. Formally, the research question pursued in this article is thus: will there be positive transfer from Korean regarding the acquisition of French tu (i.e., they will use it correctly early on given similarities with nuh), and negative transfer regarding the acquisition of vous (i.e., they will use it incorrectly early on given the parallels drawn with the Korean null pronoun)? To the best of our knowledge, studies on the L2 acquisition of tu/vous in French have been largely restricted to Anglophone speakers (to be discussed), and there have yet to be studies involving speakers of Asian languages with complex politeness systems (e.g., Korean) thus filling an obvious gap in the field.Theoretically, this study fits within the research program of interlanguage pragmatics (Bardovi-Harlig, 1999), here specifically focusing on the pragmalinguistic and sociopragmatic knowledge of learners surrounding tu/vous. Following the canonical definitions of Leech (1983) and Thomas (1983), pragmalinguistic knowledge is concerned with the use of linguistic forms to produce speech acts, while sociopragmatic knowledge is concerned with the appropriate use of those speech acts in context. Moreover, the investigation involves interlanguage transfer (in terms of failure; Thomas, 1983) and considers the conditions that might promote them (Takahashi, 2000).In sum, Korean learners of French tu/vous acquisition necessarily covers both pragmalinguistic and sociopragmatic competence, as the learner must successfully assess the situation in order to convey their intended intention in the L2 using the appropriately selected form, while dealing with possible language transfer from their L1 Korean. 1 Korean is more complicated than this, as it is also possible to use a null pronoun in a 'tu' context, as well as a term of address. The key is that nuh cannot be used for polite terms of address to a social superior, while there is no overt Korean counterpart of vous. This will be addressed more with our own data gathered from native Korean speakers. This paper investigates how the French second person pronouns, tu and vous, are acquired by Korean learners of French. This is specifically approached from an interlanguage pragmatics research viewpoint, focusing upon the status of the learners&apos; pragmalinguistic and sociopragmatic knowledge (whether they are explicit or implicit). It is hypothesized that Korean learners of French will face difficulties acquiring vous, but not with tu due to the similarities between French and Korean second person pronoun use in requests, mediated by their implicit/explicit knowledge. Using a discourse completion task and an error correction task, the findings support the hypothesis, showing the interplay between language transfer and their second language developmental status. Moreover, this was detectible by using a combination of tasks which allows pinpointing of knowledge used. The implications for explicit/implicit knowledge status in relation to the use of pragmatic knowledge are discussed against the degree of control learners have over tu and vous.
Integrating Word Embedding Offsets into the Espresso System for Part-Whole Relation Extraction A major information extraction task is relation extraction, whose goal is to predict semantic relations between entities or events expressed in the structured or unstructured text. There are several different kinds of semantic relations that connect two or more concepts. Among those semantic relations, part-whole relation, or meronymy plays an important role in many domains and applications. Extracting part-whole relations in the text is also a crucial step towards applications in several fields, such as Information Extraction, Web/Text Mining and Ontology Building. Such systems often need to recognize part-whole relations for better understanding semantic relationships between concepts. Therefore, in our research, we aim at extraction of part-whole relation. We are interested in relations between entities in the newswire domain.Among approaches to addressing the part-whole relation extraction problem, the Espresso bootstrapping algorithm (Pantel and Pennacchiotti, 2006) has proved to be effective by significantly improving recall while keeping high precision. Espresso is a well-known bootstrapping algorithm that uses a set of seed instances to induce extraction patterns for the target relation and then acquire new instances in an iterative bootstrapping manner. Nevertheless, it has a bias, called semantic drift, to select unrelated instances if a polysemous instance has been extracted as the iteration proceeds.Recently, Mikolov et al. (2013) have introduced the skip-gram text modeling architecture. It has been shown efficiently to learn meaningful distributed representations of terms from unannotated text. The vectors have some of the semantic characteristics in element-wise addition and subtraction. For example, the result of a vector subtraction vec("Madrid") -vec("Spain") is close to vector subtraction vec("Paris") -vec("France"). That is an example of the country to capital city relationship. It indicates that the embedding offsets represent the shared semantic relation between the two word pairs.The example above raises a question whether we can apply those semantic characteristics for partwhole relation? In this paper, we would like to address two important questions:1. Is Word2Vec model appropriate for pairs of part-whole relation? That is, we investigate typical instances of part-whole relation to measure their similarities by cosine distance.2. How to integrate Word2vec model efficiently into the Espresso system?The details of our contribution are as follows:• We apply the Espresso bootstrapping algorithm for part-whole relation, and study the effect of using careful seed sets and fine-grained subtypes on the performance of extracting partwhole relation.• We investigate similarities between two instances of the part-whole relation. Then, we integrate an additional ranker component into the Espresso bootstrapping algorithm to improve the performance when using iterative bootstrapping process and reduce semantic drift phenomenon for extracting part-whole relation. That ranker component uses similarity score between embedding offsets to keep similar instances in each iteration.To the best of our knowledge, ours is the first study to integrate word embedding approach in a bootstrapping algorithm for part-whole relation extraction task. Part-whole relation, or meronymy plays an important role in many domains. Among approaches to addressing the part-whole relation extraction task, the Espresso bootstrapping algorithm has proved to be effective by significantly improving recall while keeping high precision. In this paper, we first investigate the effect of using fine-grained subtypes and careful seed selection step on the performance of extracting part-whole relation. Our multi-task learning and careful seed selection were major factors for achieving higher precision. Then, we improve the Espresso bootstrapping algorithm for part-whole relation extraction task by integrating word embedding approach into its iterations. The key idea of our approach is utilizing an additional ranker component , namely Similarity Ranker in the Instances Extraction phase of the Espresso system. This ranker component uses embedding offset information between instance pairs of part-whole relation. The experiments show that our proposed system achieved a precision of 84.9% for harvesting instances of the part-whole relation, and outperformed the original Espresso system.
An Experimental Study of Subject Properties in Korean Multiple Subject Constructions (MSCs)  Yoon (2008, 2009) claimed that there are two distinct Subjects in Multiple Subject Constructions (MSCs) in Korean. The crux of his argument hangs on reinterpreting the traditionally proposed subject diagnostics as distinguishing between the Grammatical Subject (GS) and the Major Subject (MS) in MSCs. The claimed diagnostics for GS and MS were examined experimentally in MSCs and corresponding Single Subject Constructions (SSCs). We found that: (i) MS diagnostics and GS diagnostics were differentiated even in SSCs and (ii) there was no statistically significant difference between MS and GS diagnostics in MSCs. Implications of these findings are discussed.
Planting Trees in the Desert: Delexicalized Tagging and Parsing Combined Dependency parsing is an important step in language analysis, useful for downstream applications such as machine translation or question answering. Unfortunately, it is not an easy task. Successful parsers rely on dependency treebanks annotated by language experts. While at least small treebanks are becoming available for an increasing number of languages, the world's languages will not be covered any soon. The number of languages for which at least a small treebank is available lies probably somewhere between 50 and 100 (we are aware of treebanks for 56 languages). At the same time, the number of world's languages is usually estimated between 4,000 and 7,000; and 398 languages are reported to have more than 1 million speakers ( Lewis et al., 2016). In order to parse the treebankless languages, several techniques have been developed.( Hwa et al., 2004) projected dependency trees across bilingual word alignments in a parallel corpus. They used a few target-language rules to improve the target trees.( Zeman and Resnik, 2008) proposed delexicalized parsing, a method that trains a parsing model on part-of-speech tags only, ignoring lexical information. The trained model is then used to parse data in a related language for which POS tags are available. It is assumed that POS-tagged data are cheaper and easier to obtain for new languages than treebanks are. Such claim is probably justified, yet it does not provide any immediate solution in the case that no annotated resources are available for the target language.( McDonald et al., 2011) evaluated their multisource delexicalized transfer using POS tags predicted by the projected part-of-speech tagger of ( Das and Petrov, 2011). This tagger relies only on labeled training data for English, and uses a parallel corpus (Europarl) to project the tags across word alignment. Both (Zeman and Resnik, 2008) and (McDonald et al., 2011) notice that varying treebank annotation styles are a major obstacle to meaningful evaluation of any cross-linguistic transfer.Projection across bitexts is the central approach in many published experiments with POS tagging of low-resource languages.( Yarowsky and Ngai, 2001) project POS tags from English to French and Chinese via both automatic and gold alignment, and report substantial improvement of accuracy after using de-noising postprocessing. (Fossum and Abney, 2005) extend this approach by projecting multiple source languages onto a target language.( Das and Petrov, 2011) use graph-based label propagation for cross-lingual knowledge transfer, and estimate emission distributions in the target language using a loglinear model. ( Duong et al., 2013) choose only automatically recognized "good" sentences from the parallel data, and further apply selftraining.(Agi´cAgi´c et al., 2015) learn taggers for 100 languages using aligned Bible verses from The Bible Corpus ( Christodouloupoulos et al., 2010).Besides approaches based on parallel data, there are also experiments showing that reasonable POS tagging accuracy (close to 90 %) can be reached using quick and efficient prototyping techniques, such as ( Cucerzan and Yarowsky, 2002). However, such approaches rely on at least partial understanding of the target language grammar, and on the availability of a dictionary, hence they do not scale well when it comes to tens or hundreds of languages (Cucerzan and Yarowsky experiment with two languages only).In contrast, ( Yu et al., 2016) train a tagging model on language-independent meta-features and transfer it directly to a target language in a fashion similar to the delexicalized parsing; they call their approach delexicalized tagging. They use neither parallel corpora nor any target-language dictionary, rules or other expert knowledge. They compute meta-features on large raw corpora, and they make tagged texts of 107 languages available for download. 1 
Recurrent Neural Network Based Loanwords Identification in Uyghur Most natural language processing (NLP) tools rely on large scale language resources, but many languages in the world are resource-poor. To make these NLP tools widely used, some researchers have focused on techniques that obtain resources of resource-poor languages from resource-rich languages using parallel data for NLP applications such as syntactic parsing, word sense tagging, machine translation, semantic role labeling, and some crosslingual NLP tasks. However, high quality parallel corpora are expensive and difficult to obtain, especially for resource-poor languages like Uyghur.Lexical borrowing is very common between languages. It is a phenomenon of cross-linguistic influence ( Tsvetkov et al., 2015a). If loanwords in resource-poor languages (e.g. Uyghur) can be identified effectively, we can use the bilingual word pairs as an important factor in comparable corpora building. And comparable corpora are vital resources in parallel corpus detection ( Munteanu et al., 2006). Additionally, loanwords can be integrated into bilingual dictionaries directly. Therefore, loanwords are valuable to study in several NLP tasks such as machine translation, information extraction and information retrieval.In this paper, we design a novel model to identify loanwords (Chinese, Russian and Arabic) from Uyghur texts. Our model based on a RNN EncoderDecoder framework ( ). The Encoder processes a variable length input (Uyghur sentence) and builds a fixed-length vector representation. Based on the encoded representation, the decoder generates a variable-length sequence (Labeled sequence). To optimize the output of decoder, we also propose two important features: inverse language model feature and collocation feature. We conduct three groups of experiments; experimental results show that, our model outperforms other approaches.This paper makes the following contributions to this area:• We introduce a novel approach to loanwords identification in Uyghur. This approach increases F1 score by 12% relative to traditional approach on the task of loanwords detection.• We conduct experiments to evaluate the performance of off-the-shelf loanwords detection tools trained on news corpus when applied to loanwords detection. By utilizing in-domain and out-of-domain data.• For integrate these crucial information for better loanwords prediction, we combine two features into the loanwords identification model, so that we can use more important information to select the better loanword candidate.The rest of this paper is organized as follows: Section 2 presents the background of loanwords in Uyghur; Section 3 interprets the framework used in our model; Section 4 introduces our method in detail. Section 5 describes the experimental setup and the analysis of experimental results. Section 6 discusses the related work. Conclusion and future work are presented in Section 7. Comparable corpus is the most important resource in several NLP tasks. However, it is very expensive to collect manually. Lexical borrowing happened in almost all languages. We can use the loanwords to detect useful bilingual knowledge and expand the size of donor-recipient / recipient-donor comparable corpora. In this paper, we propose a recurrent neural network (RNN) based framework to identify loanwords in Uyghur. Additionally , we suggest two features: inverse language model feature and collocation feature to improve the performance of our model. Experimental results show that our approach out-performs several sequence labeling baselines.
6ROYLQJJ(YHQWW4XDQWLÀFDWLRQQDQGG)UHHH9DULDEOHH3UREOHPVVLQQ6HPDQWLFVVIRU 0LQLPDOLVWW*UDPPDUV  
Testing APSyn against Vector Cosine on Similarity Estimation Word similarity is one of the most important and most studied problems in Natural Language Processing (NLP), as it is fundamental for a wide range of tasks, such as Word Sense Disambiguation (WSD), Information Extraction (IE), Paraphrase Generation (PG), as well as the automatic creation of semantic resources. Most of the current approaches to word similarity estimation rely on some version of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Har- ris, 1954;Firth, 1957;Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990;Bulli- naria and Levy, 2012;Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These first-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature ( Bengio et al., 2006;Collobert and Weston, 2008;Turian et al., 2010;Huang et al., 2012;Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems ( Li and Han, 2013;Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect ( Schn-abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one ( Turney and Pan- tel, 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (Finkelstein et al., 2001), MEN ( Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001;Agirre et al., 2009;Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity. In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weight-ing it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation.
Strong Associations Can Be Weak: Some Thoughts on Cross-lingual Word Webs for Translation Many online dictionaries, thesauri and other lexical resources are now capable of providing users with flexible modes of searching and displaying lexical information. In particular, access by meaning is recognised as even more important than access by form. As Zock et al. (2010) remarked, word access in a dictionary is a search problem. The storage of information does not guarantee successful access, and adequate navigational means have to be provided. In other words, while lexical databases tend to contain rich information about words, their usefulness (to humans or to computers) will actually depend on how readily the right information could be retrieved at the right time for the right purpose.The onomasiological approach for organising and retrieving lexical items starts with concepts and leads to forms, which is typically what thesauri are designed for. Word finding in this way often assumes an extensive inter-connection of words, which is largely inspired by psychological models of the mental lexicon (e.g. Aitchison, 2003;De Deyne et al., 2016). Enhancement of word access in electronic dictionaries thus focuses on identifying, capturing and making available a wide range of word associations to enable words to be searched via multiple routes.To this end, empirical evidence from psycholinguistic data, especially word association norms, offers valuable information about the variety of associative relations and their relative significance in the mental word web (e.g. Joyce and Srdanović, 2008;Kwong, 2013). At the same time, computational linguists and lexicographers have attempted to model such relations and even the corresponding associative strengths (e.g. Church and Hanks, 1990;Kilgarriff et al., 2004), not necessarily as ambitious as to reconstruct the human mental lexicon, but often aiming to enhance lexical access with a mechanism taking advantage of the organisation of the mental word repository. For instance, even when a user fails to name the target word, as in the tip-of-the-tongue situation, he or she should be enabled to access the word by means of other closely associated words that can be thought of (e.g. Sinopaknikova and Smrž, 2006;Rapp and Zock, 2014;Zock et al., 2010).A very wide range of associative relations have been revealed from word association norms, but as they are elicited in isolation, their readiness to be computationally modelled and their relevance in specific applications might vary. In this study, we further explore the implications from word association norms especially with respect to bilingual dictionary access. In Section 2, we first compare among several existing word association norms for the distribution of different associative types. In Section 3, we then investigate how thoroughly such associations could be modelled by various means and tools. In Section 4, we discuss the need and relevance of word associations in the context of a specific task, namely translation, and propose that word associations have to be flexibly utilised according to the nature of a task and thus its information demand. The study is concluded with future directions in Section 5.The current investigation focuses on adjectives, which are relatively less addressed than nouns and verbs in related studies. In addition, the polysemy of adjectives bears significant implications on translation, and is worth studying for computeraided translation. This paper discusses the implications of human word association norms on the modelling of word associations from large corpora and the relevance of different types of associations in the process of translation, with a focus on adjectives. It is observed that the proportion of paradigmatic responses found in English norms tends to be higher, whereas a clear preference for syntagmatic associations is exhibited in Chinese norms. Further comparison with corpus-based extracted associations, using various functions in the Sketch Engine, shows that collocational associations might be more effectively extracted, but there is also considerable individual variation for different words. It is suggested that although free associations elicited in isolated context serve to reveal a wide range of potential lexical relations, their usefulness and relevance in real language applications should consider the actual task and its information demand. A purpose-based approach to construct cross-lingual word webs for computer-aided translation is thus proposed.
Dealing with Out-Of-Vocabulary Problem in Sentence Alignment Using Word Similarity Sentence alignment plays an important role in building bilingual corpora for statistical machine translation and many other tasks. Given documents from two languages, the task is to align sentences which are translations of each other. There are three main methods in sentence alignment including lengthbased, word-based, and the combination of the first two methods. Length-based methods were proposed in ( Brown et al., 1991;Gale and Church, 1993).( Wu, 1994) and (Melamed, 1996) introduced methods based on word correspondences. Length-based and word-based methods were also combined to make hybrid methods (Moore, 2002;Varga et al., 2007).Length-based methods which are only based on the number of words or characters in sentence pairs can run very fast but show a low accuracy. Meanwhile, word-based methods which use bilingual lexicon gain high accuracy, but heavily depend on available lexical resources. The length-and-word-based methods which combine length-based and wordbased methods (Moore, 2002;Varga et al., 2007) do not depend on lexical resources and overcome the problem of low accuracy in length-based methods. Nonetheless, a drawback of these length-and-wordbased methods which trained a bilingual dictionary using IBM models is the OOV problem.In this work, we propose an approach to deal with the OOV problem in sentence alignment based on word similarity learned from monolingual corpora. Words that were not contained in the bilingual dictionaries were replaced by their similar words from the monolingual corpora. Experiments conducted on English-Vietnamese sentence alignment showed that using word similarity learned from monolingual corpora can help to reduce the OOV ratio and lead to an improvement in comparison with some other lengthand-word-based methods.We describe phases used in our method in Section 2. Experimental results and discussions are analysed in Section 3. An overview of related researches is discussed in Section 4, and conclusions are drawn in Section 5. Figure 1: Phases in our model; S: the text of source language, T: the text of target language; S 1 , T 1 : sentences aligned by the length-based phase; S 2 , T 2 : sentences aligned by the length-and-word-based phase; S', T': monolingual corpora of the source and target languages, respectively. The components of the length-and-word-based method (Moore, 2002) are bounded by the dashed frame. Sentence alignment plays an essential role in building bilingual corpora which are valuable resources for many applications like statistical machine translation. In various approaches of sentence alignment, length-and-word-based methods which are based on sentence length and word correspondences have been shown to be the most effective. Nevertheless a drawback of using bilingual dictionaries trained by IBM Models in length-and-word-based methods is the problem of out-of-vocabulary (OOV). We propose using word similarity learned from monolingual corpora to overcome the problem. Experimental results showed that our method can reduce the OOV ratio and achieve a better performance than some other length-and-word-based methods. This implies that using word similarity learned from monolin-gual data may help to deal with OOV problem in sentence alignment.
A Pipeline Japanese Entity Linking System with Embedding Features Entity Linking (EL), also known as wikification or named entity disambiguation, is the task of linking mentions in texts to entities in a large-scale knowledge base such as Wikipedia 1 . EL is useful in many 1 https://en.wikipedia.org NLP tasks such as information retrieval (Blanco et al., 2015), question answering ( Khalid et al., 2008), searching digital libraries ( Han et al., 2005), semantic search, 2 coreference resolution ( Durrett and Klein, 2014;Hajishirzi et al., 2013), named entity recognition (Durrett and Klein, 2014) and knowledge base population (Suchanek and Weikum, 2013;Dredze et al., 2010).However, development of Japanese EL has been slow, partly due to the lack of a publicly available Japanese EL corpus. Most previous Japanese EL systems link mentions to English Wikipedia ( Fu- rakawa et al., 2014;Nakamura et al., 2015;Hayashi et al., 2014), which might be less informative because there are about 0.44 million articles in Japanese Wikipedia that do not have correspondence in English. Recently, Jargalsaikhan et al. (2016) released a Japanese EL corpus in which mentions are linked to Japanese Wikipedia entries. In this paper, we investigate several techniques for developing a Japanese EL system, and evaluate on this newly released corpus.An EL system first performs Named Entity Recognition to detect and classify spans of texts which are mentions to certain types of entities. Then, the system links the mentions to entries in Wikipedia. A major challenge here is the mention ambiguity; for example, given the sentence "The I.B.M. is the world's largest organization dedicated to the art of magic.", an EL system should associate "I.B.M" with the organization "International Brotherhood of Magicians", rather than the American technology and consulting company. An or-  thodox approach to address this issue is a pipeline of two components, the candidate generation component which generates a candidate list of possible entities for each mention, and the candidate ranking component which ranks candidates according to multiple features (Figure 1). For candidate generation, another challenge is the variety of mentions. For example, both "Big Blue" and "I.B.M." could refer to "International Business Machines Corporation".We investigate several techniques from each component. For candidate generation, string matching between mentions and entity titles has been the main approach, but we find the recall of string matching not satisfactory; instead, a cross-lingual dictionary turns out to be effective in finding correct candidates (Section 3.1, Section 5.3). For candidate ranking, we explore a set of features used in English EL, and find it effective in Japanese EL as well (Section 3.2, Section 5.4). In addition, we apply several embedding models to encode context information of entities in Wikipedia articles, and show that the embeddings are useful features for disambiguating mentions in texts (Section 4, Section 5.4). This technique would not be directly possible in previous Japanese EL systems which link mentions in Japanese texts to English Wikipedia entries, because the embedding models should be trained on articles written in the same language as texts. As a whole, our system achieves 82.27% accuracy and significantly outperforms previous work (Section 5.5). Entity linking (EL) is the task of connecting mentions in texts to entities in a large-scale knowledge base such as Wikipedia. In this paper , we present a pipeline system for Japanese EL which consists of two standard components , namely candidate generation and candidate ranking. We investigate several techniques for each component, using a recently developed Japanese EL corpus. For candidate generation, we find that a concept dictionary using anchor texts of Wikipedia is more effective than methods based on surface similarity. For candidate ranking, we verify that a set of features used in English EL is effective in Japanese EL as well. In addition, by using a corpus that links Japanese mentions to Japanese Wikipedia entries, we are able to get rich context information from Japanese Wikipedia articles and benefit mention disam-biguation. It was not directly possible with previous EL corpora, which associate mentions to English Wikipedia entities. We take this advantage by exploring several embedding models that encode context information of Wikipedia entities, and show that they improve candidate ranking. As a whole, our system achieves 82.27% accuracy, significantly outperforming previous work.
Toward the automatic extraction of knowledge of usable goods A rich body of information extraction techniques focuses on acquiring knowledge from a huge amount of text data ( Nickel et al. 2016). This allows large-scale knowledge bases to cover a broad range of knowledge. However, an important subfield of knowledge is not fully addressed: knowledge about use of objects such that hand sanitizer is used to kill bacteria and dental floss is used to remove plaque. Every object that humans create has its own purpose and function. We call these pieces of information knowledge of usable goods. Knowledge of usable goods is ubiquitous and in constant demand. People use search engines to find information on effect caused by using a new product, its proper way to use, and so on.Knowledge sources that contain such information would also be beneficial for various kinds of natural language processing tasks, such as question answering systems and textual entailment. However, knowledge of usable goods is not thoroughly covered by current knowledge bases because these resources focus on entities (e.g. person or organization) and their relations (e.g. IsPresidentOf). Section 4.3 shows the gap between kinds of knowledge available in the current knowledge bases and the ones that we aim to acquire.To fill in this gap, this study proposes a set of semantic labels to capture knowledge of usable goods and builds a benchmark corpus, Usable Goods Corpus, to explore the automatic extraction of such knowledge. This work begins with focusing on information of health care and household goods such as air freshener, rice cooker, and nasal strip.We assume that one of the most important aspects of knowledge of usable goods is about effects caused by using/consuming them as in (1 Humans can easily understand what the effects of these goods are: fish-oils reduce inflammation in the body (1a), hand sanitizers kill microorganisms (1b), BB cream tints and moisturizes skin (1c), and dental floss eliminate plaque (1d). However, the automatic extraction of such knowledge is challenging in that these effects can be expressed in various ways such as a verb phrase (1a), gerund (1b), noun phrase (1c), and clause (1d). This poses a problem that superficial linguistic patterns would not help identifying these kinds of expressions. To gauge difficulties of the automatic acquisition of these pieces of information, we conduct human annotation (Section 4) and automatic identification experiments (Section 5).The major contributions of this work are: (i) We define a set of semantic labels to capture knowledge of usable goods, suggesting a new semantic labeling task. (ii) We experimentally build a benchmark corpus (Usable Goods Corpus) to explore the automatic extraction of knowledge of usable goods. The corpus and guidelines will be available when this paper is presented. (iii) We present our initial attempts toward the automatic extraction of such knowledge using a sequence labeling method. The results in this experiment provide measures to estimate the complexity of this task and suggest future directions to build a large-scale corpus. Knowledge of usable goods (e.g., toothbrush is used to clean the teeth and tread-mill is used for exercise) is ubiquitous and in constant demand. This study proposes semantic labels to capture aspects of knowledge of usable goods and builds a benchmark corpus, Usable Goods Corpus , to explore this new semantic labeling task. Our human annotation experiment shows that human annotators can generally identify pieces of information of usable goods in text. Our first attempt toward the automatic identification of such knowledge shows that a model using conditional random fields approaches the human annotation (F score 73.2%). These results together suggest future directions to build a large-scale corpus and improve the automatic identification of knowledge of usable goods.
A Syntactic Approach to the 1 st Person Restriction of Causal Clauses in Korean  The main purpose of this paper is to provide a syntax-based analysis of the differences between the two Korean causal clauses, i.e. ese-clauses and nikka-clauses. Focusing on the various aspects of Mood distinction, we claim that nikka and ese-clauses can be analyzed as indicatives and subjunctives, respectively. Such an analysis enables us to provide syntactic explanations for issues-what we call the 1 st person restriction of ese-clauses and its obvia-tion-which might be considered merely semantic/pragmatic issues.
Towards a QUD-Based Analysis of Gapping Constructions Gapping constructions are characterized by an initial, sentential clause (the source clause) and one or more non-initial gapped clauses in which a verb and, optionally, other material are missing (the gapped clauses). Some examples are given in (1). 1 (1) a. Mary loves apples, and Tom, pears.b. On Saturday, John bought a magazine, and on Sunday, a newspaper. c. Kim played the guitar, Ray, the piano, and Sue, the bass.The missing material in gapped clauses is interpreted as if it were there. In (1a), for example, the gapped clause is interpreted as 'Tom ate pears', receiving the interpretation of the missing material from the source clause. In this paper I provide a novel approach to Gapping constructions that builds on recent QUD-based (Roberts, 1996(Roberts, /2012) accounts of non-sentential utterances in HPSG. In Section 2, I review three previous proposals and discuss their problems. In Section 3, I examine some widely accepted assumptions that have been used to characterize the syntax of Gapping constructions and show that they are not fully justified by empirical data. After discussing the relevance of Gapping constructions to QUD, I present a novel QUD-based analysis in Section 4. Section 5 concludes the paper. In this paper I examine what have often been considered the syntactic properties of Gapping constructions (Ross, 1970) and show that they are in fact discourse-pragmatic in nature.
Retrieval Term Prediction Using Deep Learning Methods Existing Web search engines have very high retrieval performance as long as the proper retrieval terms are input. However, many people, particularly children, seniors, and foreigners, have difficulty deciding on the proper retrieval terms for representing the retrieval objects, 1  related to technical fields. Support systems are in place for search engine users that show suitable retrieval term candidates when clues such as their descriptive texts or relevant/surrounding words are given by the users. For example, when the relevant/surrounding words "computer", "previous state", and "return" are given by users, "system restore" is predicted by the systems as a retrieval term candidate. It is therefore necessary to develop various domain-specific information retrieval support systems that can predict suitable retrieval terms from relevant/surrounding words or descriptive texts in Japanese.In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various fields and have been successfully applied not only in speech recognition ( Li et al., 2013) and image recognition ( Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology &amp; syntax (Billingsley and Curran, 2012;Hermann and Blunsom, 2013;Luong et al., 2013;Socher et al., 2013a), semantics ( Hashimoto et al., 2013;Srivastava et al., 2013;Tsubaki et al., 2013), machine translation ( Auli et al., 2013;Liu et al., 2013;Kalchbrenner and Blunsom, 2013;Zou et al., 2013), text classification ( Glorot et al., 2011), information retrieval (Huang et al., 2013;Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011;Socher et al., 2011;Socher et al., 2013b). Moreover, a unified neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How-ever, there have been no studies on applying deep learning to information retrieval support tasks. It is therefore necessary to confirm whether deep learning is more effective than other conventional machine learning methods in this task.Two objectives were cited above. One was to develop an effective method for predicting suitable retrieval terms and the other was to determine whether deep learning is more effective than other conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), in such NLP tasks. On this basis, Ma et al. (2014) proposed a method to predict retrieval terms in computer-related fields using machine learning methods with deep belief networks (DBN) ( Hinton et al., 2006;Lee et al., 2009;Ben- gio et al., 2007;Bengio, 2009;Bengio et al., 2013). In small-scale experiments they showed that using DBN resulted in higher prediction precision than using either a multi-layer perceptron (MLP) or support vector machines (SVM). To evaluate their proposed method more reliably, the first thing we must do is scale up the experiments. In general, it is not easy to obtain large training data, particularly labeled data for supervised learning. Fortunately, deep learning consists of both unsupervised learning and supervised learning, and unlabeled data can be collected relatively easily. Second, since a number of regularization methods (Srivastava et al., 2014) have been adopted for improving the generalization performance of neural networks, we also need to conduct evaluations when regularization is used.This study is an enhanced version of the previous work of Ma et al. (2014), and the retrieval terms were confined to computer-related fields as before. We implemented deep learning not only with the DBN as done in the previous work of Ma et al. (2014), but also with stacked denoising autoencoders (SdA) ( Bengio et al., 2007;Bengio, 2009;Bengio et al., 2013;Vincent et al., 2008;Vincent et al., 2010). We conducted extensive experiments in which a large amount of unlabeled data was automatically collected from the Web (as a result, the amount of data and the number of labels used in this study were about ten times larger than those used in the previous study (Ma et al., 2014)), and then we compared the performance between DBN and SdA, and between DBN/SdA and conventional machine learning methods, in the respective cases of using or not using regularization methods, i.e., weight decay (L2 regularization), sparsity (L1 regularization), and dropout regularization.Experimental results show that using SdA achieves the highest prediction precision among all the methods and that using both DBN and SdA produces higher prediction precision than that achieved using either MLP or SVM, when regularization methods are not used. On the other hand, when regularization methods are used MLP and DBN performance is improvement in some cases, whereas no performance improvement can be found in SdA. Whether or not regularization methods are used, however, the order of superiority among SdA, DBN, and MLP remains unchanged. The experimental results also show that adding automatically gathered unlabeled data to the labeled data for unsupervised learning is an effective measure for improving the prediction precision. This paper presents methods to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep learning methods, which are implemented with stacked denoising autoencoders (SdA), as well as deep belief networks (DBN). To determine the effectiveness of using DBN and SdA for this task, we compare them with conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM). We also compare their performance in case of using three regularization methods, the weight decay (L2 regularization), sparsity (L1 regularization), and dropout regularization. The experimental results show that (1) adding automatically gathered unlabeled data to the labeled data for unsupervised learning is an effective measure for improving the prediction precision, and (2) using DBN or SdA results in higher prediction precision than using SVM or MLP, whether or not regularization methods are used.
Information and Computation (PACLIC 30) Seoul, Republic of Korea  
Sentence Clustering using PageRank Topic Model Many people buy products through electronic commerce and Internet auction site. Consumers have to use products' detailed information for decision making in purchasing because they cannot see the real products. In particular, reviews from other consumers give them useful information because reviews contain consumers' experience in practical use. Also, reviews are useful for providers of products or services to measure the consumers' satisfaction.In our research, we focus on generating clusters of review sentences on the viewpoints from the products' evaluation. For example, reviews of home electric appliance are usually written based on the following the viewpoints: performance, design, price, etc. If we generate clusters of the review sentences on these viewpoints, the clusters can be applied to various uses. For example, if we extract representative expressions from clusters of sentences, we can summarize reviews briefly. This is useful because some products have thousands of reviews and hard to be read and understood.There are various methods to generate clusters of sentences. Among several methods, we adopt probabilistic generative models for sentence clustering because the summarizations of clusters can be represented as word distributions. Probabilistic generative models are the methods that assume underlying probabilistic distributions generating observed data, and that estimate the probabilistic distributions from the observed data. In language modeling, these are called topic models.Latent Dirichlet Allocation (LDA) ( Blei et al., 2003) is a well-known topic model used in document clustering. LDA represents each document as a mixture of topics. A topic means a multinomial distribution over words in a vocabulary.Unigram Mixture (UM) ( Nigam et al., 2000) as-sumes that each document is generated by a multinomial distribution over words in a vocabulary, φ k = (φ k1 , · · · , φ kV ), where V denotes the size of vocabulary and φ kv denotes the appearance probability of v-th term in the k-th topic. UM estimates a multinomial distribution over topics,where θ k denotes the appearance probability of kth topic. After all, K+1 multinomial distributions, θ and φ = (φ 1 , · · · , φ K ) are estimated from the observed data, where K denotes the number of topics. Using estimated θ and φ, the probability that a document is generated from φ k is calculated. This probability determines the clusters of the sentences.In UM, θ and φ can be estimated by iterative computation. However, since θ and φ are initialized randomly, computation results are not consistent. In addition to this, the number of topics K has to be set as a preset parameter.To estimate the appropriate number of topics, the average cosine distance (AveDis) of each pair of topics can be used (Cao et al., 2009). This measure is based on the assumption that better topic distributions have fewer overlapping words. However, to estimate the appropriate number of topics based on this measure, we need to set several numbers of topics and it takes much time to calculate.In this paper, we introduce PageRank Topic Model (PRTM) to consistently estimate φ and θ using Word Co-occurrence Graphs. PRTM consists of 4 steps as follows:1. Convert corpus W into a Word Co-occurrence Graph G w .2. Divide graph G w into several communities.3. Measure PageRank in each community and estimate multinomial distributions over words in a vocabulary φ.4. Estimate a multinomial distribution over topics θ as a convex quadratic programming problem assuming the linearity of φ.Network structures have been applied to several Natural Language Processing tasks ( Ohsawa et al., 1998) (Bollegala et al., 2008. For example, synonyms can be identified using network community detection method, e.g. the Newman method (Clauset et al., 2004) ( Sakaki et al., 2007). In this research, we also apply the Newman method to detect communities of co-occurrence words in step 2. In step 3, we calculate the appearance probability of nodes using PageRank (Brin and Page, 1998). PageRank is the appearance probability of nodes in a network. In Word Co-occurrence Graph G w , each node represents a word. Therefore, we regard a set of PageRank of nodes as φ. After that, θ is estimated using a convex quadratic programming problem based on the assumption of the linearity of φ in step 4. From these steps, reproducible φ, θ and clustering results can be obtained because the Newman method, PageRank and the convex quadratic programming problem are not depending on random initialization of parameters.There is another advantage to identify communities of co-occurrence words using the Newman method. The Newman method yields an optimized number of communities K in the sense it extracts communities to maximize Modularity Q. Modularity Q is one measure of the strength of division of a network structure into several communities. When modularity Q is maximized, the graph is expected to be divided into an appropriate number of communities.Our main contributions are summarized as follows:• Using PRTM, we estimate consistent multinomial distributions over topics and words. It enables us to get consistent computation results of sentence clustering.• PRTM yields an appropriate number of topics, K, as well as the other parameters. It is more suitable to estimate the number of viewpoints from the products' evaluation than the average cosine distance measurement.In this paper, we first explain our proposed method, PRTM, in section 2. We show the experimental results in section 3 and compare with related works in section 4. At last, we discuss our conclusions in section 5. The clusters of review sentences on the viewpoints from the products&apos; evaluation can be applied to various use. The topic models, for example Unigram Mixture (UM), can be used for this task. However, there are two problems. One problem is that topic models depend on the randomly-initialized parameters and computation results are not consistent. The other is that the number of topics has to be set as a preset parameter. To solve these problems , we introduce PageRank Topic Model (PRTM), that approximately estimates multi-nomial distributions over topics and words in a vocabulary using network structure analysis methods to Word Co-occurrence Graphs. In PRTM, an appropriate number of topics is estimated using the Newman method from a Word Co-occurrence Graph. Also, PRTM achieves consistent results because multino-mial distributions over words in a vocabulary are estimated using PageRank and a multino-mial distribution over topics is estimated as a convex quadratic programming problem. Using two review datasets about hotels and cars, we show that PRTM achieves consistent results in sentence clustering and an appropriate estimation of the number of topics for extracting the viewpoints from the products&apos; evaluation .
The Inner Circle vs. the Outer Circle or As English has spread worldwide, new varieties of English have emerged and they got independent status accordingly. In order to systematically classify them, Kachru (1992) introduced the three concentric circles as way of conceptualizing this pluri-centricity. There should be a distinction between American English (AmE) and British English (BrE) as well.Out of the varieties of English, we chose four different ones and statistically analyzed their properties. To this end, we picked out four components of the International Corpus of English (ICE; Greenbaum, 1996), which are the varieties of British, India, Philippines, and USA. Then, all the sentences with two modal auxiliaries can and may were extracted. Then, a total of twenty linguistic factors were encoded to the extracted ones, and the encoded data were statistically analyzed with R, with the theoretical basis of Competition Model ( MacWhinney, 1982, 1989). In addition, two statistical analysis methods were adopted. One was a logistic regression with which the properties of each component were closely investigated. The other was a Behavior Profile (BP) analysis where the four components were clustered by their similarity.In short, we selected two modal auxiliaries can and may for comparison for the following reasons. As several of the previous studies (Leech, 1969, Coates, 1983Collins, 2009) pointed out, these two modal verbs have similar meanings, and the native speakers interchange them in similar contexts. However, the distributions of these two are systematic, even in native speakers' writings. Then, what happens in non-native speakers' counterparts and how can the phenomena be explained? We are to present one possible type of answer to these questions. In this paper, the use of two modals (can and may) in four varieties of English (British, India, Philippines, and USA) was compared and the characteristics of each variety were statistically analyzed. After all the sample sentences were extracted from each component of the ICE corpus, a total of twenty linguistic factors were encoded. Then, the collected data were statistically analyzed with R. Through the analysis, the following facts were observed: (i) India and Philippine speakers used can more frequently than natives, (ii) Three linguistic factors interacted with CORPUS, and (iii) The distinctions between American and British were more influential than those of the Inner Circle vs. the Outer Circle.
A Correlation Analysis of English Particle Placement of Three East Asian EFL Learners&apos; Writings Ha-Eung Kim Linguistic alternation has been one of the interesting research areas in linguistics. Particle placement is one of such syntactic alternations. It refers to the linguistic phenomenon where a particle is located before or after the direct object (DO) in the phrasal verb constructions. 1 1 Gries (1999) used the term particle movement while Gries (2001) used the term particle placement. The former adopted Chomsky's transformational-generative grammar approach (Chomsky, 1957(Chomsky, , 1965 and thought that particle moved from one position to another. The latter did not presuppose such movement analysis. This For example, let's see the following sentence (Gries, 1999:1). (1) a. John picked up the book. b. John picked the book up.As you can see, the word order in (1a) is 'verb + particle + DO', whereas the order of (1b) is 'verb + DO + particle'. There have been a lot of studies on this topic in traditional grammar and Chomskyan syntax. They have primarily focused on what linguistic factors determine the choice of alternations. Nowadays, as computer technology and statistics develop, there have been a few corpus-based studies to explain these syntactic phenomena with authentic corpus data and statistical analysis. Gries (1999Gries ( , 2001Gries ( , 2003 were such trials, and these studies adopted a multifactorial analysis to investigate the particle placement in the native speakers' writings. These studies also proposed several linguistics factors and the factors were encoded in the corpus data. These studies demonstrated that various linguistic factors and their interactions with the main factors significantly influenced the choice of alternations This paper, however, adopted a monofactorial analysis to examine the particle placement in three Eat Asian EFL learners' writings (Korean, Chinese, and Japanese). The TOEFL11 corpus was used for the EFL learners' writings, and the ICE-GB corpus (the British component of the International Corpus of English; Nelson et al., 2002) was chosen for the native speakers' counterparts. paper adopted Gries' second approach and called the phenomena in (1) particle placement. That is, this paper did not presuppose the movement of particles. Instead, how various linguistic factors influenced the placement of particles was investigated with statistical tools.From these four corpora (Chinese, Japanese, Korean, and ICE-GB), all the relevant sentences were extracted using the tag information. Then, eleven linguistic factors were manually encoded to these sentences. After the process, all the linguistic factors were statistically analyzed with R. Two different types of statistical analyses were adopted in the paper: correlation analysis and a hierarchical clustering. These statistical analyses demonstrated how each linguistic factor played a role in the choice of particle placement, in the four varieties of English.This paper is organized as follows. In Section 2, three groups of previous studies are reviewed with a focus on corpus-based approaches. Section 3 is on the corpus data and research methods. Section 4 contains the analysis results of correlation analyses, and Section 5 the analyses results of a hierarchical clustering. Section 6 is for discussions, and Section 7 summarizes this paper. This paper examines the English particle placements of EFL learners&apos; writings in three East Asian countries (Chinese, Japan, and Korea). Three parts of the TOEFL11 corpus were chosen, and all the sentences with particles were extracted. The ICE-GB was chosen as a native speakers&apos; English. Then, eleven linguistic factors were manually encoded. The collected data were analyzed with R. Correlation tests and a hierarchical clustering analysis was adopted. Through the analysis, the following two facts were observed: (i) each linguistic factor affected differently in four varieties of English and (ii) Japanese English was similar to native speakers&apos; counterparts whereas Korean and Chinese formed another group.
Sentiment Clustering with Topic and Temporal Information from Large Email Dataset The generation of enormous diversified data stream by social networking and communication contributes to the rapid development of text mining and its related area ( Hao et al., 2013). Literature indicates that product reviews, Twitter corpus and news articles are common sources for conducting sentiment analysis (Ravi and Ravi, 2015), whereas Electronic mail (Email), as one of the most adapted means of communication and networking, is a rare option due to its complex structure and natural language characteristics ( Tang et al., 2014). However, the efficiency, compatibility and ease of communication embed great business potential in Email messages ( Tang et al., 2014), which is a promising and meaningful sentiment analysis subject.Sentiment analysis is one of the most appealing areas in text mining among researchers. In the past few decades, sentiment analysis techniques, both machine learning approaches and statistical approaches, have improved significantly and been applied to various industries, such as stock market prediction, customer relationship management, and elearning (Feldman, 2013;Liu, 2015;Ortigosa et al., 2014;Smailovi´cSmailovi´c et al., 2013). Herein, some researchers extend their studies to enriching sentiment analysis by adding additional features. For instance, Mei et al. (2007) propose a novel topic-sentiment mixture model using probabilistic testing for topic and sentiment discovery; Saif et al. (2012) show that adding semantic features results in more accurate sentiment classification. Additionally, Fukuhara et al. (2007) introduce the idea of generating time and sentiment graph using Dice coefficient probabilistic model. However, no qualitative and quantitative experiments have been undertaken for the evaluation of the proposed method.This research paper develops a systematic scheme of approach for discovering sentiment distribution patterns from large Email corpus based on clustering results of topic and temporal information using bag-of-words model as distance matrix and DB-SCAN ( Ester et al., 1996) algorithm for clustering and pattern analysis, addressing the following contributions: a) introducing a systematic scheme of approach composed of bag-of-words term weighting method and DBSCAN clustering algorithm for Email sentiment pattern discovery using topic and temporal information; b) using Email corpus as data source for the ef-fectiveness and feasibility test of the proposed framework;c) discovering sentiment distribution and characteristics discovery in temporal categories and relationship between sentiment variance and topic categories. Sentiment analysis with features addition to opinion words has been an appealing area in recent studies. Some research has been conducted for finding relationship between sentiments , topics and temporal sentiment analysis. Nevertheless, Email sentiment analysis received relatively less attention due to the complexity of its structure and indirect-ness of its language. This paper introduces a systematic framework for sentiment clustering using topic and temporal features for large Email datasets. Interesting Email and sentiment distribution patterns are summarized and discussed with empirical results.
!!&quot;###$%%&amp; &apos; ( ) ( %% *% %% %**%$**% % 2 (!!%( %. ! ( %%$ % % % +7&quot;--( !!,## 7&quot; ( (% %% *%  
Automatic Identifying Entity Type in Linked Data An increasing number of linked datasets is published on the Web. At present, there have been more than 200 datasets in the LOD cloud. Among these datasets, DBpedia (Bizer, C. et al., 2009) and 1 http://zhishi.me/ Yago (Suchanek, F.M. et al., 2007) serve as hubs in LOD cloud. As the first effort of Chinese LOD, Zhishi.me (Niu, X. et al., 2011) extracted RDF triples from three largest Chinese encyclopedia web sites i.e. Chinese Wikipedia, Baidu Baike 2 and Hudong Baike 3 . However, type information is incomplete or missing in these linked datasets. For example, more than 36% of type information is missing in DBpedia (Kenza Kellou-Menouer and Zoubida Kedad, 2012). Zhishi.me only uses the SKOS vocabulary to represent the category system and does not strictly define the "rdf:type" relation between instances and classes. Type information is an important component of linked datasets. Knowing what a certain entity is, e.g., a person, organization, place, etc., is crucial for enabling a number of desirable applications such as query understanding (Tonon, A. et al., 2013), question answering (Kalyanpur, A. et al., 2011;Welty, C. et al., 2012), recommendation (Lee, T. et al., 2006;Hepp, M. 2008), and automatic linking . Since it is often not feasible to manually assign types to all instances in a large linked data, automatic identifying type information is desirable. Furthermore, since open and crowd-sourced encyclopedia often contain noisy data, filtering out the incorrect type information is crucial as well (Heiko Paulheim and Christian Bizer, 2013). Recently, more and more attention has been paid to extracting or mining type information from linked data. However, most of current techniques on obtaining type information are either languagedependent or inferring type information only from internal clues such as textual description of entity. Most existing work was mainly focused on mining entity type from internal clues, and missed out the point that the issue can be boosted by integrating external evidence. Our assumption is that given an entity e1 without type information, if we can find an equivalent entity e2 with type information, we can obtain the type information of e1 directly. In this paper, we investigate whether external evidence from other knowledge base could be helpful to entity type identification, and how to combine internal clues such as abstract, infobox and subject with external evidence. In particular, several learning features are extracted from entity abstract, infobox and subject, and then classifiers are trained to get entity type prediction models. Meanwhile, entity linking tools are utilized to link entities with external knowledge base e.g. DBpedia, where we can get type information. Finally, a voting mechanism is adopted to decide the final entity type. We have implemented our algorithms and present some experimental evaluation results to demonstrate the effectiveness of the approach. The remainder of the paper is organized as follows. In the following section we review the existing literature on entity type identification. Then, we introduce the proposed approach in section 3. We conduct comparative experiments and present the results in section 4. At last, we conclude the paper with a summary of our work and give our future working directions. Type information is an important component of linked data. Unfortunately, many linked datasets lack of type information, which obstructs linked data applications such as question answering and recommendation. In this paper, we study how to automatically identify entity type information from Chinese linked data and present a novel approach by integrating classification and entity linking techniques. In particular, entity type information is inferred from internal clues such as entity&apos;s abstract, infobox and subject using classifiers. Moreover, external evidence is obtained from other knowledge bases using entity linking techniques. To evaluate the effectiveness of the approach, we conduct preliminary experiments on a real-world linked dataset from Zhishi.me 1. Experimental results indicate that our approach is effective in identifying entity types.
SMTPOST: Using Statistical Machine Translation Approach in Filipino Part-of-Speech Tagging Natural Language Processing (NLP) is a field in computer science where it connects human language with technology. In the Philippines, NLP applications and resources have been continually expanding. Specifically, a project conducted by De La Salle University (DLSU), Manila in the span of three years developed numerous NLP products: from language resources such as lexicons, word corpora, tagsets and grammar rules, to tools such as Morphological Analyzers, Part-of-Speech (POS) Taggers, Grammar Checkers and Machine Translators (Chu, 2009). These outputs enabled DLSU to produce research papers and extended applications not only for the Filipino language, but also to English, marking these works as wellestablished at that time.Focusing on POS tagging 1 , Chu (2009) featured taggers from Miguel and Roxas' (2007) comparative study. These POS taggers were implemented on different approaches: PTPOST4.1 (Go, 2006) an extension from past PTPOST researches ( Cortez et al., 2005;Flordeliza et al., 2005), is a probabilistic tagger implementing the Hidden Markov model, Viterbi algorithm, lexical and contextual probabilities; MBPOST ( Raga and Trogo, 2006), a memorybased tagger; Tag-Alog ( Fontanilla and Wu, 2006), a rule-based tagger; TPOST ( Cheng and Rabo, 2004), a template-based tagger; and adding to the list, SVPOST (Reyes et al., 2011), a Support Vector Machines tagger. Despite developments of POS taggers in the country, the Filipino language's evolution requires constant updates on the tools and their resources. Without these updates, the products become outdated in the following factors: data contents, software usability, performance and availability. This paper addresses those issues through experimentation and creation of a new tagger using Statistical Machine Translation (SMT) for the Filipino language. This research is also intended to provide aid in the understanding of Filipino POS, establish a Filipino tagset and support NLP products or processes (i.e. grammar checker, language parsing, speech processing, information retrieval, etc.) in their tasks.In choosing an approach, the use of Hidden Markov Models, Viterbi Algorithm, and Machine Learning (Support Vector Machines, Perceptron, and the likes) has been recurrent to foreign languages. As a challenge and motivation for this research, instead of implementing widely used approaches, it has been set to start up new ventures on a potential tagger -ending up with selecting Statistical Machine Translation. SMT as a tagger is uncommon; as specified in its name, it is mainly used in translating one language to another. However, it is not limited to be used that way. Oda et al.'s (2015) work, used SMT for generating English and Japanese pseudo-codes from a given source code, intended to aid code understanding. Other samples are from the work of Mizumoto et al.'s (2011) Japanese error correction and Nocon et al.'s (2014) Filipino shortcut words normalizer. These examples, provided results that proved using SMT in different areas is feasible by supplying two types of data labeled as source (to be transformed) and target (transformed into).As a data-driven approach, the method for this research leverages SMT by using pairs of word features (source) and POS tag counterparts (target), and translated Filipino Wikipedia data as input for training; while for POS tagging, words or sentences are accepted as input to be automatically transformed into features to match the generated model from training. This paper mainly focuses on elaborating the creation of Statistical Machine Translation Partof-Speech Tagger (SMTPOST). It is outlined in the following order: first is the methodology section in which the construction of SMTPOST is discussed; followed by test results and discussions, including analysis of SMTPOST's performance against other existing taggers; next, conclusion and recommendations; and finally, the list of references used. The field of Natural Language Processing (NLP) in the country has been continually developing. However, the transition between Tagalog to the progressing Filipino language left tools and resources behind. This paper introduces a Statistical Machine Translation Part-of-Speech (POS) Tagger for Filipino (SMTPOST), with the purpose of reviving, updating and widening the scope of technologies in the POS`POS`tagging domain, catering to the changes made by the Filipino language. Resources built are comprised mainly of a tagset (218 tags), parallel corpus (2,668 sentences), affix rules (59 rules) and word-tag dictionary (309 entries). SMTPOST was tested to different tagsets and domains, producing 84.75% as its highest accuracy score, at least 3.75% increase from the available Tagalog POS taggers. Despite SMTPOST&apos;s utilization of Filipino resources and good performance, there are room for improvements and opportunities. Recommendations include a better feature extractor (preferably a morphological analyzer), an increase in scope for all of the resources, implementation of pre-and/or post-processing, and the utilization of SMTPOST research to other NLP applications.
  
On the Possessor Interpretation of Non-Agentive Subjects It is well known that in Japanese, some transitive subjects, in addition to the agentive reading, allow the reading where they do not instigate but rather undergo an event described by the verb phrase, thereby giving rise to an ambiguity, as in (1). 1 1 The following abbreviations are used: ACC  That the ambiguity is real can be shown by the sentence in (2), where the second conjunct serves to ensure the subject is not an agent.(2) Taroo 1 -ga { kare 1 -no/ zibun 1 -no/Ø 1 } T.-NOM he-GEN/ self-GEN/ pro ude-o or-Ø-ta (&gt;ot-ta) kedo, arm-ACC ¥break-CAUS-PST but zibun 1 -de-wa or-Ø-anak-at-ta self-INST-TOP break-CAUS-NEG-DV-PST 'Taroo broke his arm, but he didn't break it himself.' Moreover, direct passivization, which necessarily implies the presence of an agent, renders the nonagentive reading of the subject in (1) unavailable, as shown in (3): 2 (3) * Taroo 1 -niyotte { kare 1 -no/ Ø 1 } ude-ga T.-by he-GEN/ pro arm-NOM or-Ø-are-ta kedo, ¥break-CAUS-PASS-PST but kare.zisin 1 -de-wa or-Ø-anak-at-ta he.self-INST-TOP break-CAUS-NEG-DV-PST 'Taroo's arm was broken by him, but he didn't break it himself.' Thus, these examples clearly demonstrate that the ambiguity is not illusionary and that the subject can have a reading significantly distinct from the agentive reading.Inoue (1976) has shown that there are two conditions to be met in order to obtain the nonagentive-or, in her terms, experiential-reading of the subject: (i) the subject must appear with a verb that alternates in transitivity; (ii) there must be a "proximate" relation, typically that of inalienable possession, between the subject and an object. 3 These are well-established generalizations in the literature, and I do not discuss them in detail. Yet, since this paper focuses on the possessor interpretation of non-agentive subjects, I will illustrate that the possession condition does hold and it affects another dimension of interpretation: distributive and collective readings. Specifically, when plural subjects are non-agentive, only the distributive reading is available because each of the subject referents possesses a referent of the object (i.e., the possession condition). On the other hand, the collective reading is unavailable with non-agentive subjects unless some unusual context is given (e.g., subject referents share an inalienably possessed entity). Thus, under normal contexts, forcing the collective reading renders the nonagentive interpretation unavailable. Consider (4) and (5) below.(4) Huta-ri-no kodomo 1 -ga [ Ø 1 ude]-o 2-CL-GEN child-NOM pro arm-ACC or-Ø-ta (&gt;ot-ta) ¥break-CAUS-PST 'Two children broke their arms.' [distributive: agentive or non-agentive] [collective: agentive] 3 Two terminological notes are in order: One is that Inoue (1976) calls the interpretation under discussion Experiencer, while other researchers call it different names such as Affectee, Possessor, Undergoer, etc. What we are concerned here is the fact that the argument bears the possessor interpretation. Moreover, although they involve lexical causatives and not syntactic causatives, the examples in the text should be regarded as cases of so-called adversity causative. This is because the causative morpheme -(s)ase-in adversity causatives, as in (i), can be regarded as the default realization of a lexical causative morpheme (Miyagawa, 1998). Hence, despite the fact that the non-agentive subject in question has been called different names in the literature, it seems plausible to consider the property of being a possessor as its defining characteristic.However, although it is clear that the nonagentive subject is understood as a possessor, the fact does not guarantee that the subject is linguistically encoded as such. Thus, this paper addresses the question of whether the relation of possession should be directly reflected in syntax when non-agentive subjects are available. Specifically, the paper argues against the view that the possessor interpretation is directly encoded in syntax by showing that approaches encoding the non-agentive subject as a possessor face insuperable difficulties. Instead, I argue that the subject is encoded as an event participant whose manner of participation is underspecified, and that the possessor interpretation results from inference based on linguistic and extralinguistic contexts, along with many interpretations that are possible with the subject in question.The organization of the paper is as follows: in the next section, we will discuss problems with two major approaches under the subject-asencoded-possessor view. In section 3, we will see how the subject-as-underspecified-argument view deals with the possessor interpretation and avoids the problems discussed in section 2. Section 4 concludes the paper. It has been observed that the relation of possession contributes to the formation of so-called adversity causatives, whose subject is understood as a possessor of an object referent. This interpretation is reflected at face value in some studies, and it is assumed there that the subject argument is introduced as a possessor in syntax. This paper addresses the question of whether the observed relation should be directly encoded as such and argues that the subject argument is introduced as merely an event participant whose manner is underspecified. Moreover, it argues that the possessor interpretation arises from inference based on both linguistic and extralinguistic contexts, such as the presence of a possessum argument. This view is implemented as an analysis making use of a kind of applicative head (Pylkkänen, 2008) in conjunction with the post-syntactic inferential strategy (Rivero, 2004).
Resources for Philippine Languages: Collection, Annotation, and Modeling The Philippines is a country in Southeast Asia composed of 7,107 islands and 187 listed individual languages. Among these, 41 are listed as institutional, 73 are developing, 45 are vigorous, 13 are in trouble, 11 are dying, and 4 are already extinct 1 . These numbers highlight that there is a pressing need for a databank on Philippine languages.As highlighted in literature ( Dita et al., 2009;Oco and Roxas, 2012), even those with high number of native speakers have limited available corpora. Towards addressing this scenario, we describe in this paper the collection, annotation, and modeling of various language resources. The paper's structure is as follows: section 2 discusses initiatives in the country and the various language resources we collected; section 3 discusses annotation and documentation efforts; section 4 discusses language modeling; and we conclude our work in section 5. In this paper, we present our collective effort to gather, annotate, and model various language resources for use in different research projects. This includes those that are available online such as tweets, Wikipedia articles, game chat, online radio, and religious text. The different applications, issues and directions are also discussed in the paper. Future works include developing a language web service. A subset of the resources will be made temporarily available online at: http://bit.ly/1MpcFoT.
Generating a Linguistic Model for Requirement Quality Analysis  In this work, we aim at identifying potential problems of ambiguity, completeness, conformity , singularity and readability in system and software requirements specifications. Those problems arise particularly when they are written in Natural Language. We describe them from linguistic point of view but the business impacts of each potential error will be considered in system engineering context where our corpus come from. Several standards give the criteria on writing good requirements to guide requirement authors. These properties are linguistically observable because they appear as lexical, syntactic, semantic and discursive problems in documents. We investigate error patterns heavily used, by analyzing manually the corpus. This analysis is based on the requirements grammar that we developed in this work. We then propose an approach to identify them automatically by applying the rules developed from the error patterns to the POS tagged and parsed corpus. By using error annotated corpus, we can train the error model using CRFs and evaluate it. We obtain overall 79.17% F 1 score for the error label annotation task.
  
Transitivity in Light Verb Variations in Mandarin Chinese --A Comparable Corpus-based Statistical Approach In modern Chinese, there exists a kind of semantically bleached verbs which are called light verbs. They are similar to English light verbs (e.g., take rest, give advice) in the sense that the light verb does not contain any eventive information and the predicative content mainly comes from its taken complement (e.g., Jespersen, 1955;Zhu, 1985) while the light verb itself may only contributes aspectual information. LVC in English has been comprehensively studied in both theoretical (e.g., Butt andGeuder 2001, Cattell, 1984) and computational approaches (e.g., Tu and Dan, 2001), while in Chinese, the identification and differentiation of LVC especially the LVC variations between different language varieties may be more complicated. Due to the semantic versatility, Chinese light verbs usually do not have strong collocation restrictions, e.g.,/// jinxng/jiayi yanjiu 'do research'. However, collocation constraints are sometimes found with these light verbs, e.g., /* jinxing/*jiayi huiyi 'have a conference'; ?/ ?jinxing/jiayikaolv 'give considerarion'. The challenge is even greater when we compare different variants of the Mandarin, i.e., Taiwan and Mainland Mandarin.  have found that even with the very limited collocation constraints, light verb variations still exist: Taiwan light verbs tend to take more types of NPs and even VPs as its complements, for instance, LVC like jinxing/chuli/linshi/ti'an 'to process the supplementary proposal' can only be found in Taiwan corpus. We should also note that light verbs in Chinese can take both verbs, deverbal nouns, and eventive nouns, while the morphological status of these categories are typically unmarked ( Lin et al., 2014), that may make the identification more complicated. For example, we have found several inconsistencies in the POS tagging for the taken complements. In" " (CNcorpus), when either "" zhandou "battle" or "" douzheng "fight" is used individually, it is annotated as a Verb. But if they appear after the light verb, the annotation results are sometimes confusing, as shown below:1. a. /nt /d /p /v /a /u//a /u /v meiri douzai jinxing juliede lianmiande zhandou "Every day they are having fierce and continuous battles."b. /v /u /a /u /n jinxing le jianjue de douzheng "had a resolute fight" In a&amp;b, both "" zhandou "battle" and " " douzheng "fight" are modified by the attribute in De construction, but with different tagging. The inconsistent annotation results may bring a variety of inconveniences for grammatical analyses.Another difficulty for identifying and differentiating Chinese LVC is that in Chinese the difference between light verbs as well as variations are very subtle and complex to be observed, and also this kind of differences are more tend to be frequency or preference difference instead of grammaticality dichotomies which is unlikely to be studied by using the traditional approach. Therefore, in our study, to identify the subtle tendency difference between different light verbs as well as light verb variations, a statistical corpusbased approach based on annotated comparable corpus is adopted, following the research paradigm set up in Lin et al. (2014) and Huang et al. (2014). Our current study further show that the variation differences can further imply the transitivity difference between different speaking communities and this result is also consistent with the generalization that a smaller speaking community away from the main speaking community tends to be more conservative. This paper adopts a comparable corpus-based approach to light verb variations in two varieties of Mandarin Chinese and proposes a transitivity (Hopper and Thompson 1980) based theoretical account. Light verbs are highly grammaticalized and lack strong collocation restrictions; hence it has been a challenge to empirical accounts. It is even more challenging to consider their variations between different varieties (e.g. Taiwan and Mainland Mandarin). This current study follows the research paradigm set up in Lin et al. (2014) for differentiating different light verbs and Huang et al. (2014) for automatic discovery of light verb variations. In our study, a corpus-based statistical approach is adopted to show that both internal variety differences between light verbs and external differences between different variants can be detected effectively. The distributional differences between Mainland and Taiwan can also shed light on the re-classification of syntactic types of the taken complement. We further argue that the variations in selection of arguments of light verb in two Mandarin variants can in fact be accounted for in terms of their different degree of transitivity. Higher degree of transitivity in Taiwan Mandarin in fact show that light verbs are less grammaticalized and hence consistent with the generalization that varieties away from the main speaking community should be more conservative.
The Interaction between SFPs and Adverbs in Mandarin Chinese -A Corpus-Based Approach Research on Sentence Final Particles (SFPs henceforth) has been a long-held issue in the field of Chinese linguistics. A voluminous body of literature has been devoted to the study of structural properties, historical development as well as the semantic or pragmatic properties of SFPs. Zhu (1982: 208) accurately observes the hierarchical structure of SFPs and proposes a three layer classification on them, which include: 1) Tense and Aspectual Information, e.g. Le, Laizhe, Ne 1 ; 2) Sentence Type Marker, e.g. Ma, Ba, Ne 2 ; 3) Speaker's Attitude, e.g. Ou, A, Ne 3 . Three senses of Ne are listed in (1)-(3) and this paper concentrates on the outmost layer of Ne 3. This paper proposes a new methodology in investigating the semantic and pragmatic properties of SFPs in Mandarin Chinese. A case study of the interaction and correlation between SFP-Ne and SpOAs-Shenzhi, Qishi, and Nanguai has been conducted. Two semantic features of [+unexpectedness] and [+intersubjectivity] have been summarized on SFP-Ne.
On the Semantics of Korean modalized question In the standard theories of question (Hamblin 1973;Karttunen 1977;Groenendijk and Stokhof 1984), the meaning of the question denotes a set of propositions (i.e. alternative possible answers to the question). The general purpose of informationseeking questions is to receive a true answer from the addressee by posing such a set of alternatives for consideration. Surprisingly, however, the question marked by nka in Korean concerns speaker's knowledge and issues, thus it reports on the speaker's consideration of a set of alternatives. In (1a), for instance, based on the fact that John had a very subtle smile in the context, the speaker conjectures that 'John is the winner' has a good possibility while acknowledging the negative possibility at the same time. The statement is therefore marked by nka. It contrasts with the factual question marker ni in (1b) without such presumption by the speaker:(1) Context: Mary, a reporter, was waiting for John and Bill who were competing with each other for the win in the finals of the chess competition. After the match, John and Bill came out of the room. John had a very subtle smile and Bill had a poker face. Given their facial expressions, she raises the possibility that John might have won. Mary says:a. Con-i wusungca-i-nka? John-Nom winner-be-NKA 'Could John possibly be the winner?' b. Con-i wusungca-i-ni? John-Nom winner-be-Q 'Is John the winner?' I treat the nka-question in (1a) as a non-factual question (Jang 1999;C. Kim 2010, a.o.): as indicated in the use of 'possibly' in the translation, it is a question about the possibility of the content of the proposition, i.e., the speaker is asking whether it is possible that John won the game, rather than whether he actually won the game. The use of nka indicates the speaker's presumed awareness of asking a weaker question, and specifies the degree of certainty about the proposition in question, just like an epistemic modal. In this sense, I term the nka-question modalized question (MQ, henceforth). A MQ questions about the speaker's belief and knowledge, thus it raises a weaker inquiry than the regular unmodalized question. I argue that the epistemic modality of nka is initiated from its original function of disjunction operator. As shown below, nka coordinates two DPs in (2a) or two TPs in (2b):(2) a. Wusungca-nun Con-inka Pil-i-ta.winner-Top John-or Bill-be-Decl 'The winner is possibly John or Bill. b. Con-i wusungca-i-nka? John-Nom winner-be-NKA 'Could John possibly be the winner?'As indicated in the use of possibly in the translation, nka disjunctions are modalized (Choi 2011, a.o.). I assume that inka-disjunction in (2a) is a disjunction without overt modals in the sense of Zimmermann (2001) and , interpreted as a list of epistemic possibilities. It asserts that the winner might be John or the winner might be Bill in a world w if and only if the proposition contains at least one world that is permitted in w.In fact, MQs are pervasive in diverse languages, not genetically or geographically connected, and some light is shed on the topic from previous studies examining them under various labels. The common semantic denominator of these MQs is that the epistemic uncertainty is produced by the interaction of modal ingredients occurring in questions. To name a few, there are darou-ka 'MOD+Q' in Japanese (self-addressing question; Hara and Davis 2013), as=há=k'a 'SBJN+YNQ+INFER' in St'át'imcets (conjectural question; Littell et al. 2009, Matthewson 2010, and na 'SBJN' occurring in the interrogative in Greek (epistemic subjunctive question; Giannakidou, to appear). Above MQs have doublelayered epistemic modal because they are morphologically decomposed into overt question markers and modal ingredients which contribute to form modalized non-factual questions. In Salish and Japanese the modal component is a modal marker; in Greek it is a subjunctive marker; in English it is possibly, probably, might, etc. Unlike the above MQs, however, the Korean MQ is notable in that the double-layered modal is achieved by a single element, nka. Our discussion on nka crucially hinges on the question of (i) how the semantic categories of MQs can be distinguished within the traditional domain of modality, and how they can be defined, and (ii) how the seemingly distinct notions of disjunction, modal effect, and question are amalgamated in the single element nka.To capture the semantics of double-layered modal, I argue that the nka-disjunction is based on modal-concord structure, positing an implicit possibility modal. The existence of default implicit modal in nka-MQs is evidenced by the fact that, when nka co-occurs with other modal verbs, it withdraws the otherwise strong modality of these verbs. For example, nka combines with biased (i.e. strong) possibility modal verbs such as evidential modal suffix te 'I saw that' (J. Lee 2008, a.o.) and strong possibility modal auxiliary verb kes kath 'look like' (Choi 1995, a.o), but no bias is indicated:wusungca-i-te-nka? John-Nom winner-be-INFER-NKA 'Did I possibly see that John was the winner?' (4) Con-i wusungca-i-n-kes kath-un-nka? John-Nom winner-be-Rel-must-Rel-NKA 'Could John possibly look like the winner?'I take this to argue that the function of nka is to constrain the modal base, just as modal adverbs do. The distinct feature of its restriction, however, lies in the fact that nka partitions the modal base into equal spaces, i.e. p ∨¬p (polarity partition), and nullifies the bias.I thus propose that three seemingly distinct notions of disjunction, question, and possibility modal can be unified under the framework of nonveridical equilibrium (Gianankidou 2013; Giannakidou and Mari (GM) 2016). The epistemic weakening in nka-MQs is obtained by the creation of non-homogenous nonveridical (i.e. modal) states partitioned in equipoised epistemic spaces.The paper proceeds as follows: In section 2, I provide a brief recapitulation of nonveridical equilibrium. Exploring the basic properties of nka in Korean in section 3, I show that its function is akin to the modal-verb modifier restricting modal base. In section 4, I offer the semantic analysis of MQs, showing how a more comprehensive picture of MQs that I provide fits into the framework of nonveridical equilibrium. In section 5, I conclude with theoretical implications. The goal of the current study is to suggest a novel paradigm of epistemic modal operator originated from the disjunction. Our main data is Korean disjunction operator nka which forms a non-factual question. Examining how the modal effect in question is induced by nka, I propose that the prerequisite of nka brings about non-homogenous nonveridical (i.e. modal) spaces partitioned in equipoised epistemic spaces, thus there is no bias between them. I furthermore show how the distinct notions of disjunction, question, and possibility modal can be captured under the theory of nonveridical equilibrium (Giannakidou 2013, Giannakidou and Mari 2016).
L2 Acquisition of Korean locative construction by English L1 speakers  Korean has locative construction as other languages do such as English. Although L2 acquisition of locative construction has been examined in L2 English research, few experimental investigations of Korean L2 acquisition have been conducted. The current study focused on the syntactic alternation among Figure Framed sentence, Ground Framed sentence, Figure only sentence and Ground only sentence. Forced choice task on 72 locative construction have been conducted by 21 Native Korean speakers and 20 advanced L1 English learners of Korean. L2ers showed different acceptability judgment on Korean locative construction which was distinct from their L1 argument structure. The results showed that these asymmetries were driven by L1 effect when the learnability problem arises due to insufficient input.
Towards a Unified Account of Resultative Constructions in Korean This paper discusses what is referred to as predicative resultative constructions in Korean, exemplified in (1a), and argues that they are in fact a kind of clausal resultative construction like (1b) (see the different types of resultatives in Wechsler and Noh, 2001). It is normally understood that in (1a) the resultative predicate ppalkah-key 'redKey' is predicated of the matrix object mwun-ul 'door-Acc' in a controlled structure. In (1b), however, the nominative NP sinpal-i 'shoes-Nom' and the resultative predicate talh-key 'threadbareKey' constitutes a fully saturated result clause (Wechsler and Noh, 2001: 404). Despite some differences (e.g., (in)transitivity of the verb), these two sentences share the notion of resultative: as a result of the event denoted by the main verb, an argument undergoes a change of state denoted by the result predicate.  (1a) in the literature (see Shim and den Dikken, 2007;Shibagaki, 2011 for TP adjunct analysis, Son, 2008 for small clause complement analysis, and Nakazawa, 2008 for adverbial clause adjunct account of Japanese resultatives). While I agree with the general idea that the resultative predicate forms a clause, particularly I propose the following hypotheses in this paper: (i) the resultative predicate, X-key, is morpho-syntactically an adverb rather than an adjective, (ii) X-key forms a fully saturated clause, result clause (sometimes with the predication subject omitted), and (iii) the result clause is a complement of the main verb. A unified analysis of the resultative constructions is then cast in the framework of Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994;Sag et al., 2003). This paper discusses predicative resultative constructions in Korean and argues that they are actually a kind of clausal resultative construction (see the two types of resultatives in Wechsler and Noh, 2001). In particular, I propose the following hypotheses: (i) the resultative predicate, X-key, is morpho-syntactically an adverb rather than an adjective, (ii) X-key forms a fully saturated clause (i.e., result clause) (sometimes with the predication subject omitted), and (iii) the result clause is a complement of the main verb in a resultative sentence. Based on these properties, a unified analysis of the resultative constructions is formalized in Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003).
The use of body part terms in Taiwan and China: Analyzing xue &apos;blood&apos; and gu &apos;bone&apos; in Chinese Gigaword v. 2.0 Embodiment, referring to 'understanding the role of an agent's own body in its everyday, situated cognition' (Gibbs 2006: 1), is a manifestation of the significance of the human body in cognition. Embodiment, of which the tenet is that meaning stems out of 'the organic activities of embodied creatures in interaction with their changing environments' (Johnson 2008: 11), has been drawing scholars' attention for more than three decades.While providing cognitive accounts for meaning generation and functioning as the foundation of conceptual metaphor understanding and interpretation (e.g. Yu 2003Yu , 2007Johnson 1980, 1999;Johnson 2006), embodiment does not address what triggers conceptual metaphors, or the constraints which motivate the selection of a corporeal term to represent another concept.Incorporating the theories of embodiment and of generative lexicon (Pustejovsky 1991(Pustejovsky , 1995, Duann and Huang (2015) proposes a method to uncover what constrains the use of a body part in the stead of a comparatively abstract notion. They focus on the qualia structure of the corporeal terms as the source concept and testify their approach with authentic corpus data. Examining the behavior of four atypical body parts, xue 'blood', rou 'flesh', gu 'bone' and mai 'meridian' in Sinica Corpus ( Chen et al. 1996), they contend that the visibility of these body parts and the telic role from the qualia structure constrain the selection. In this current research, we explore the metaphorical/metonymical uses of xue 'blood' and 'bone', the two corporeal terms with relatively high visibilities compared with rou 'flesh' and mai 'meridian' (Duann and Huang 2015) in the Chinese Gigaword Version 2 (Gigaword2, Huang 2009), and finds out: (1) Regarding the use of xue 'blood', the agentive role predominates in both Taiwan and China, which differs from the argument in Duann and Huang (2015). (2) Concerning the use of gu 'bone', the telic role still predominates. However, China tends to use it in personification much more often than Taiwan. (3) The political ecology of Taiwan motivates the use unique to Taiwan. For the first finding we would like to amend the argument in Duann and Huang (2015): instead of the telic role only, we think both the telic and agentive roles constrain the selection, as these two roles represent eventive dimensions which reveal the interaction between the object/entity and human beings. For the second finding, we claim that China tend to use certain body parts in a more holistic way, which is attributable to the genre of the corpus. For the third finding, we argue certain dimensions of a place lead to the use exclusive to the place. This article, examining the qualia roles retrieved from the metaphorical-ly/metonymically used body part terms in news texts, addresses the similarities and differences of such uses in Taiwan and China. Analyzing the behavior of xue &apos;blood&apos; and &apos;bone&apos;, two corporeal terms with relatively high visibilities compared with rou &apos;flesh&apos; and mai &apos;meridian&apos; (Duann and Huang 2015) in the Chinese Gigaword Version 2 (Huang 2009), this research have the following findings: (1) For the use of xue &apos;blood&apos;, the agentive role predominates in both Taiwan and China, which is not in line with the argument in Duann and Huang (2015). (2) Regarding the use of gu &apos;bone&apos;, the telic role predominates. However, China uses it in per-sonification much more often than Taiwan does. (3) The unique dimension of a place triggers the use exclusive to the place.
A POMDP-based Multimodal Interaction System Using a Humanoid Robot In recent years, with the spread of the household robots, the necessity to enhance the communication capabilities of those robot to people has been increasing. Furthermore, we expect those robots which can observe information from multimodal resources and perform proper actions based on the observed information in interaction with people. In this context, the objective of our study is to achieve effective interaction with a robot using the multimodal information observed by the sensors of the robot. As a concrete system, we have implemented a dialogue system with the framework of partially observable Markov decision process (POMDP) in a humanoid robot called "Pepper" which can observe various multimodal information by its own sensors. Through several experiments, we aim to confirm that our system can assist Pepper to achieve flexible multimodal interaction with people.2 Multimodal dialogue with a robot In recent years, with the spread of the household robots, the necessity to enhance the communication capabilities of those robot to people has been increasing. The objective of this study is to build a framework for a dialogue system dealing with multimodal information that a robot observes. We have applied partially observable Markov Decision Process to modeling multimodal interaction between a human and a robot. Through the experiments, we have confirmed that our proposed framework functions properly and achieves effective multimodal interaction with a robot.
The Cloud of Knowing: Non-factive al-ta &apos;know&apos; (as a Neg-raiser) in Korean  We distinguished the two different uses of factive and NonFactive (NF) in the verb al-ta
Neural Joint Learning for Classifying Wikipedia Articles into Fine-Grained Named Entity Types Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) defined a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research field is to consider a larger set of fine-grained entity types ( Lee et al., 2006;Sekine et al., 2002;Yosef et al., 2012;Corro et al., 2015). Recent studies report that fine-grained NER makes improvements to such applications as entity linking ( Ling et al., 2015) and question answering (Mann, 2002). Given this background, this paper addresses the issue of creating a large gazetteer of NEs with fine-grained entity type information, motivated by the previous observations that a large-coverage gazetteer is a valuable resource for NER ( Kazama and Torisawa, 2008;Carlson et al., 2009). Specifically, we consider building such a gazetteer by automatically classifying the articles of Wikipedia, one of the largest collection of NEs, into a predefined set of fine-grained named entity types.The task of classifying Wikipedia articles into a predefined set of semantic classes has already been addressed by many researchers (Chang et al., 2009;Dakka and Cucerzan, 2008;Higashinaka et al., 2012;Tardif et al., 2009;Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classification is naturally expected to be more difficult than coarsegrained classification. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as "Japan", "Mt. Fuji", and "Tokyo dome", may be classified as Country, Mountain, and Sports_Facility respectively in a fine-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given the same number of labeled training instances, one may obtain far fewer instances for each fine-grained type. Another challenge is in that fine-grained entity types may not be disjoint; for example, "Banana" can be classified as Flora and Food_Other simultaneously.To address these issues, in this paper, we propose Multi-label and multi-task learning with neural networks This paper addresses the task of assigning fine-grained NE type labels to Wikipedia articles. To address the data sparseness problem, which is salient particularly in fine-grained type classification , we introduce a multi-task learning framework where type classifiers are all jointly learned by a neural network with a hidden layer. In addition, we also propose to learn article vectors (i.e. entity embeddings) from Wikipedia&apos;s hypertext structure using a Skip-gram model and incorporate them into the input feature set. To conduct large-scale practical experiments, we created a new dataset containing over 22,000 manually labeled instances. The dataset is available. The results of our experiments show that both ideas gained their own statistically significant improvement separately in classification accuracy.
MINING CALL CENTER CONVERSATIONS EXHIBITING SIMILAR AFFECTIVE STATES Affective content 1 analysis of audio calls is important in recent days with the increasing number of call centers (Pang and Lee, 2008), (Liu, 2012), (Kopparapu, 2015). Perhaps, audio is the best possible modality that can be used to effectively analyze the call center conversations between customer and agent. However, manual analysis of such calls is cumbersome and may not be feasible because large number of recordings take place on daily basis. Therefore only a small fraction of such conversations are carefully heard by the human supervisors and addressed, thus resulting many of those unattended.The difficulty of identifying the affective regions (or emotionally rich) manually in large number of calls is illustrated in Figure 1. The call duration is plotted on the x-axis, while different calls are shown along the y-axis. As represented in Figure  1, the calls are of different durations, and the gray color represents the actual length of the calls. The black color within the call shows the location of a specific affective state (highly correlated to the problematic regions in the calls). It is clear that the locations of such problematic regions are arbitrary, and the durations are of variable length. In spite of such challenges, automatic emotion analysis of call center conversations has attracted the attention of researchers (for example (Petrushin, 1999), (Vidrascu and Devillers, 2007), (Gupta and Rajput, 2007), (Mishra and Dimitriadis, 2013), (Kopparapu, 2015), (Pappas et al., 2015)).Affective content analysis is a technique that extracts emotions from spoken utterance 2 , and thus useful to find the similar emotional utterances in call center calls. In general, affective contents are represented categorically in terms of the different emotion classes (e.g (Petrushin, 1999;Vidrascu and Devillers, 2007;Gupta and Rajput, 2007;Pappas et al., 2015)). Mostly in call center calls, four emotions (namely, anger, happy, neutral, sad) in the categorical space are addressed. Although in ( Nicolaou et al., 2011), authors proposed to capture the time varying emotional information in the dimensional space using audio-visual cues. And in (Mishra and Dimitriadis, 2013), authors proposed an incremental emotion recognition system that updates the recognized emotion with each recognized word in the conversation. They make use of three features from two modalities (i.e. cepstral and intonation form audio and textual features from text), which are obtained at the word level to estimate the emotion with better accuracies. It has been observed that combining linguistic information with the acoustic features improves the performance of the system. As an example, in (Lee and Narayanan, 2005), authors proposed a combination of three information (i.e. acoustic, lexical, and discourse) for emotion recognition in spoken dialogue system and found improvements in recognition performance. Similarly in (Planet and Sanz, 2011), authors described an approach to improve emotion recognition in spontaneous children's speech by combining acoustic and linguistic features.In this paper, we propose a novel framework that automatically extracts the affective content of the call center spoken utterances in arousal and valence dimensions. In addition, context-based knowledge (e.g. time lapse of the utterances in the call, events and affective context derived from linguistic content, and speaker information) associated with the calls are intelligently used to reliably detect the affective content in speech. Unlike (Mishra and Dimitriadis, 2013), we do not fully rely on the use of word recognition to determine the emotion. This makes our system feasible even for resource deficient languages that do not boast of a good automatic speech recognition (ASR) engine. In addition to the linguistic information like in ( Lee and Narayanan, 2005), we also incorporate more knowledge like the time lapse of the utterance in calls, contextual information derived from linguistic content, speaker information etc. For each spoken utterance, the affective content extractor generates probability scores in arousal and valence dimensions, which are then probabilistically combined to label it with any of the predefined affective classes. The framework is motivated by the observation that there is significant disagreement amongst human annotators when they annotate call center speech; the disagreement largely reduces when they are provided with additional knowledge related to the conversation. Unlike (Mishra and Dimitriadis, 2013), the proposed system extracts affective information separately in dimensional space, thus reduces the classification complexity. Moreover in our proposed framework, emotions are extracted at discrete levels of affective classes (i.e. positive, neutral, negative in arousal and valence dimensions), instead of using affective information in continuous scale like in (Nicolaou et al., 2011), thus reducing the complexities related to the difficulties in annotation at continuous level of affective states, resulting less number classes in each dimension. In addition, detection of similar emotional content in large number of audio calls are performed by using the emotion models trained with the freely available acted emotional speech. Therefore the system is able to work even in a scenario if somebody does not have an annotated call center calls. Extensive experimentations on the acted dataset contaminated with 4 The rest of the paper is organized as follows. Section 2 presents the motivation of this work. In Section 3, we propose the framework for affective content extraction, using knowledge for reliable affective state identification and finding similar affective states. Section 4 describes the dataset, experiments, and results. We conclude in Section 5. Automatic detection and identifying emotions in large call center calls are essential to spot conversations that require further action. Most often statistical models generated using annotated emotional speech are used to design an emotion detection system. But annotation requires substantial amount of human intervention and cost; and may not be available for call center calls because of the infrastructure issues. Therefore detection systems use models that are generated form the readily available annotated emotional (clean) speech datasets and produce erroneous output due to mismatch in training-testing datasets. Here we propose a framework to automatically identify the similar affective spoken utterances in large number of call center calls by using the emotion models that are trained with the freely available acted emotional speech. Further, to reliably detect the emotional content , we incorporate the available knowledge associated with the call (time lapse of the utterances in a call, the contextual information derived from the linguistic contents, and speaker information). For each audio utterance , the emotion recognition system generates similarity measures (likelihood scores) in arousal and valence dimension using pre-trained emotional models, and further they are combined with the scores from the contextual knowledge-based systems, which are used to reliably detect the similar affective contents in large number of calls. Experiments demonstrate that there is a significant improvement in detection accuracy when the knowledge-based framework is used.
  
Modeling Answering Strategies for the Polar Questions across Languages Polar questions and responses by answering particles like yes and no are everyday interactions between interlocutors in daily language uses (see, among others, Jones 1999, Holmberg 2016, Krifka 2013, Fretheim 2017 The answering particle yes or no here serves as a proper response to the polar questions, assigning proposition-like meanings as given in the parentheses.In addition to this analytic question of how a single particle induces a sentential interpretation, an ensuing question arises from language differences in the responses to negative questions. Consider the exchanges in (2) and corresponding Korean examples in the following (see Kim 2017): As seen from the contrast between English and Korean, the meaning of yes differs. In English, the response yes confirms the positive proposition of the question while the corresponding yes in Korean confirms the negative proposition denoted by the question. Such a difference distinguishes the polaritybased answering system from the truth-based answering system (Jones 1999). This paper tries to offer a discourse-based approach to account for these two as well as related questions. The paper argues that the propositional meaning of the answering particles is not derived from syntactic operations like movement-anddeletion. It rather has to do with the anaphoric nature of the answering particles ( Ginzburg and Sag 2000, Farkas and Bruce 2010, Krifka 2013, Roelofsen and Farkas 2015. It also shows that the parametric differences between the two different types of answering system, the polarity-based system (e.g., English, Swedish, German) and the truth-based system (e.g., Korean, Chinese, Japanese), are due to tight interactions between the anaphoric nature of answer particles and discourse. The paper then shows how this intuitive idea can be modeled within the grammar of HPSG (Head-driven Phrase Structure Grammar). This paper provides a discourse-based account of polar questions and answering particles. Arguing against syntax-based ellipsis analyses , the paper suggests that polarity particles are anaphoric in nature and their interpretation is determined by the antecedent evoked by the context. It also suggests that the parametric differences between the polarity-based (e.g., English, Swedish, German) and the truth-based answering system (e.g., Korean, Chi-nese, Japanese) have to do with the tight interactions between the anaphoric nature of answering particles and discourse.
International Speech Communication Association Distinguished Lecture: Principles and Design of a System for Academic Information Retrieval based on Human-Machine Dialogue  With the rapid progress of computer technology and worldwide development of information networks, a vast amount of information is now being generated, published, and stored at a number of sites distributed all over the world. Such an affluence of information, however, is useless or may even become harmful unless one has a means for rapidly retrieving the information that it truly necessary and appropriate. Conventional systems for information retrieval, however, are not always easy to use for inexperienced users, and are neither efficient nor accurate. In many cases, it is difficult for the user to identify and express his/her intention precisely, and it is difficult also for the system to infer the user&apos;s intention correctly. These difficulties can be alleviated by introducing spoken dialogue between the user and the system. Furthermore, in conventional systems using keywords, the accuracy of retrieval is reduced by the existence of synonymy, polysemy and homonymy, as well as of unknown words. Still another shortcoming of conventional systems is the lack of ability for properly estimating the degree of relevance of a document to the user&apos;s query, as well as the lack of a proper viewpoint on the cost/performance of retrieval. This talk describes the outcome of a successful Japanese national project conducted under the &quot;Research-for-the-Future&quot; program and led by the speaker as the principal investigator. The system is based on the following three original principles: (a) Dialogue Management based on both User and System Modeling (by introducing a novel type of interacting automaton), (b) Use of &quot;Key Concepts&quot; in information retrieval (including processing of polysemy, homonymy, and unknown words), and (c) Optimization of Retrieval Performance through Relevance Score Estimation (by introducing a measure of relevance of search results based on users&apos; judgments. The advantages of these novel principles have been demonstrated by a pilot system. 10
Assessing Authenticity in Media Englishes and the Englishes of Popular Culture  &apos;Authenticity&apos; has long been a primary concern of sociolinguistic analyses. Early sociolinguistic work insisted that data collected should be &apos;spontaneous and naturally occurring&apos;, a methodological dictum that was, in large part, borrowed from dialectology&apos;s search for &apos;authentic&apos; Englishes that were thought to be endangered by modernization and, later, urbanization. In many ways, authentic Englishes are imagined to represent both literally and imaginatively &apos;authentic identities&apos; of the speakers of those languages. The emphasis on &apos;authentic&apos; Englishes significantly coincides with the development across a number of English-speaking communities of a &apos;Standard Language Ideology&apos;, which promotes myths of &apos;purity&apos; and &apos;timelessness&apos; of the standard language. As standardized Englishes are usually adopted as media languages-and frequently named after the media that use them, such as &apos;BBC English&apos; or &apos;American Broadcast Standard&apos;-these media languages risk losing features that may signal &apos;authentic&apos; language or identities. And the pursuit if authenticity in media Englishes is amplified in Englishes of pop culture, where authenticity must be manufactured as part of the process of creation. This essay will explore the historical basis for the processes that manufacture authenticity in English varieties as normal recurring process of standardization in a pluricentric model of world Englishes. 11
Corpus Linguistic Analysis for Language Planning  The Filipino language, the Philippines&apos; national language, is based on the lingua franca of Manila, the nation&apos;s capital. It has shown rapid and significant changes in its vocabulary, orthography and grammar, thanks to the Philippines&apos; rich colonial history and the conscious efforts in the national and institutional levels to standardize the grammar and orthography of the language. This fact is proof that the socio-political landscape of the times shapes the language of its people. In this talk, I will describe my forays in the analysis and processing of text corpora largely written in the Filipino language. Through corpus linguistic analysis of a historical text collected from various publication domains and genres, and covering approximately one century, I will illustrate some observable long-term and short-term trends in Filipino writers&apos; writing styles, and conduct correlational analysis with some notable Philippine Socio-linguistic trends. Acknowledging that language plays a vital role in the formation of a national identity, and hence, should be cultivated towards intellectualization, I propose that ICT could effectively be used to monitor language usage and provide added insights for language planners, in understanding the interplay between language and socio-political developments. Thus, for this talk, I will also describe various ways by which corpora could automatically be harnessed and analyzed, and used to identify areas of language use that could be highlighted or further improved. Practical insights in large scale text analysis would also be provided; the depictions would all be drawn from my experiences as a corpus linguistic researcher. 12
Modality Markers in Cebuano and Tagalog  Philippine languages, which are predicate-initial, have &quot;defective&quot; verbs (coined by Pigafetta, n.d.) that are formal particles that occur at clause-initial position (like a predicate), lack inflection, and take a complement clause and that generally convey an epistemic stance. At the same time, there are also second-position modal enclitics that attach to the clause-initial predicate expressing epistemic attitude. Both topics, epistemic enclitics and &quot;defective&quot; verbs, have not been properly examined and investigated in Philippine languages primarily due to their peripheral and polysemous nature. Aside from enclitics and verbs, modality may occur in other forms too. In this talk, I shall present the forms and functions of the modal markers in two Philippine languages, Cebuano and Tagalog, as well as how they interact with each other, with pronominal enclitics and other particles, and with negation. In the process of such an investigation, a typology of Philippine languages will be developed in terms of modality features, such as those listed above. Furthermore, I will show current progress in the study of modality in Formosan languages and discuss how these efforts can inform and complement modality studies in Philippine languages. These will hopefully enable us to gain enough understanding of the issues and write up a grammar of modality in Philippine (and, hopefully, Formosan) languages. 13
Doubt, incredulity, and particles in Japanese falling interrogatives  I propose an analysis of the particles no, ka, yo, and ne as speech act modifiers, accounting for the readings of falling interrogatives with and without particles by predicting what they convey about the speaker&apos;s belief revision and formation process. The analysis is set in a CCP-framework formalizing utterance felic-ity in terms of belief and evidence conditions in which speech act felicity is compositionally derived from illocutionary force, sentence final intonation, and modification by particles. 1 Japanese sentence final particles Sentence final particles (SFPs) are a highly productive class of expressives 1 in Japanese. The empirical scope of this paper are the interrogative marker ka and the particles no, yo, and ne. While there is only a consensus to classify yo and ne as SPFs, I analyze all four particles as SFPs in the sense of speech act modifiers occurring in the sentential periphery. SFPs in the Japanese clause Japanese is a strictly left-branching language, hence elements further right in linear order generally scope syntactically higher and enter the semantic derivation later than such further left. Therefore, layered clause models have been proposed in descriptive Japanese grammar. 2 Minami (1974), for instance, locates SFPs in the outermost layer of the clause, which encodes meta-information on the transmission of information 1 In the sense of not contributing truth-conditional meaning. 2 cf. Narrog (2009) for extensive discussion of various lay-ered models, Davis (2011) for discussion pertaining to SFPs. by the utterance. Minami&apos;s next inner layer hosts the interrogative particle ka and the speech act modal 3 daroo, which encode information on the speaker&apos;s judgment of the truth of the prejacent. The position of daroo is immediately preceded by that of no 4 in linear order, which in turn is preceded by tense morphology , as (1) below illustrates. (1) V-T-no-(daroo)-ka-yo-ne SFPs as speech-act modifiers In line with the intuitions and observations motivating layered clause models, I propose that no, daroo and ka modify utterance felicity conditions w.r.t. speaker belief and available evidence (subjective, related to speaker judgment), yo and ne w.r.t. speaker assumptions on addressee belief (intersubjective, related to information sharing/transmission). On my analysis, all thus modify utterance meaning on the speech act level where felicity is computed. As analyzing speech act felicity is independently necessary to account for bare (particle-less) utterances, this is a relatively parsimonious way of accounting for the contribution of Japanese sentence final expressives. 2 Japanese falling interrogatives Falling interrogatives (FIs) occur frequently in Japanese and have uses clearly distinct form canoni-cal, information-seeking questions. In the remainder of this section, I introduce the observations to be accounted for in the analysis. 3 cf. Hara and Davis (2013), Rieser (2017c) for analysis as a speech act modifier operating on a Gricean quality threshold. 4 cf. Rieser (2017a) for discussion on the structural and functional distinction with the homophonous complementizer no. 25
The Phrasal-Prepositional Verbs in Philippine English: A Corpus-based Analysis Multi-word verbs (henceforth MWVs) are word combinations often used by native speakers in conversation because of their colloquial tone (Biber, Johansson, Leech, Conrad, &amp; Finegan, 1999). To distinguish MWVs from other complex verb forms, Quirk, Greenbaum, Leech, and Svartvik (1985) classified MWVs into phrasal verb, prepositional verb, and phrasal-prepositional verbs. A phrasal verb consists of a verb and a particle and is either intransitive or transitive. An intransitive phrasal verb functions like a predication adjunct inseparable from its lexical verb (e.g. The plane has now taken off.) while a transitive phrasal verb requires a direct object (e.g. Victoria will set up the equipment.). Moreover, a prepositional verb has a lexical verb and a preposition with which it is semantically and/or syntactically associated (e.g. Look at the billboards.). And finally, a phrasal-prepositional verb (henceforth PPV) takes a verb, an adverb, and a preposition, with two main structural patterns: verb+ particle + preposition +NP (e.g. Jason had to keep away from salty snacks.) and verb + NP + particle + preposition + NP (e.g. They fobbed her off with a cheap perfume.). According to Quirk et al. (1985), "many of them have idiomatic metaphorical meanings which are difficult or impossible to paraphrase" (p. 1179).Interestingly, corpora-based studies have made it relatively easier to determine the typical behavior of the MWV expressions in both spoken and written discourse (Gardner &amp; Davies, 2007;Ryoo, 2013;Biber et al., 1999). More so, a shift of approach on phrasal verbs, from the traditional to the cognitive, has greatly helped non-native speakers in understanding how productive the particles are (Cubillo, 2002;Kiativutikul and Phoocharoensil, 2014;Garcia-Vega, 2011). For instance, the particle up can have five meaning extensions ranging from literal to figurative ones ( Rosca and Baicchi, 2016). Lindner (1981 in Lu Zhi andSun Juan, 2015) clarified that "all the items of phrasal verbs sharing the same particles, literal or metaphoric, are correlated with one image schema, influencing the whole meaning of phrasal verbs" (p. 3). Understandably, the entire phrase, not its individual units, provides the primary meaning (Sinclair, 2008in Garnier &amp; Schmitt, 2015.Second language and foreign language learners nonetheless find MWVs ambiguous because of their notoriously difficult nature. Most of these MWVs do not have the equivalent expressions in the native language or they are non-existent at all in the mother tongue (Bensal, 2012). Viewed relevant in learning a second or foreign language to attain fluency, most students memorize the list of MWVs in their textbook. But according to Lu Zhi and Sun Juan (2015), learning about the multiple senses of MWVs can actually be systematic.As a major category of MWVs, PPVs have not been extensively studied ( Biber et al., 1999). To date, corpus-based studies on PPVs utilizing ICEPhilippines have yet to be reported. It is therefore the aim of this study to fill this gap and offer significant contributions to the existing literature in Philippine English. This study attempts to determine the most common forms of PPVs in Philippine English and describe their syntactic and semantic features. The study determines the most common forms of phrasal-prepositional verbs (PPVs) in Philippine English using the ICE-PHI and describes their syntactic and semantic features, following Quirk et al.&apos;s (1985) framework. Thirty nine out of the forty-eight words from the list of Quirk et al. (1985) and Biber et al. (1999) were found in the corpus using AntConc 3.4. Results show that come up with, get out of, look forward to, come out with, hold on to, and catch up with are the most frequently used PPVs by Filipinos. These PPVs occur in active voice. They are intransitive verbs and are also inseparable. Findings further reveal that the meanings of the PPVs are the same as the single-word verb meanings provided by the online dictionaries of phrasal verbs, and those single-word verb meanings can replace the PPVs. Hence, they are idiomatic. The study implies that Filipinos use minimal number of PPVs. They appear to be conservative in their choice of PPV structure, but generally show proficiency in using PPVs in their utterances.
A Type-Logical Approach to Japanese Potential Constructions Potential constructions have long attracted much attention in Japanese Linguistics, mainly focusing on their meanings and case-alternation phenomena. I argue in this paper that the past studies have failed to describe their important characteristics in significant ways and propose a completely new analysis from a formal grammar view point. To show what is wrong with the past analyses, let us observe the points Japanese traditional linguistics have assumed, and show why the potential suffix rare must be distinguished from the passive rare, and then propose an analysis which can properly deal with a broad empirical coverage. Observe the standard active and passive pair in Japanese in (1).(1) a. Hitobito-wa sakuban takusan-no People-Top last-night a lot of banana-o tabeta. bananas-Acc ate. 'People ate a lot of bananas last night.' b. Takusan-no banana-ga sakuban A lot of bananas last night hitobito-niyotte taber-are-ta.people by eat-Pass-Past 'A lot of bananas were eaten by people last night.' Sentences in (1) show a typical active-passive correspondence where the passive suffix -rare is used to form the passive complex verb taber-are-ta 'were eaten,' the theme argument banana is subjectivized and the agent argument is demoted to the adjunct marked with oblique case. In Japanese linguistics, it has been assumed that the same suffix -rare is also used to form the potential verbs and that the distinction in interpretation between passives and potentials is dependent on contexts. It is also suggested that complex potential verbs project active or passive potential sentences and the distinctions were made depending on surface case markings of arguments, as exemplified in (2) (see Teramura 1982 for discussion on this dichotomy): 'The child can eat this banana.' (active) b. Kono banana-ga mou taber-are-ru.This banana-Nom already eat-Can-Pres 'This banana can be eaten now.' (passive) Teramura (1982) and his followers call sentences like (2a) 'active potentials' and those like (2b) passive potentials. This dichotomy has led to the analyses dealing with the contrast in (2) in terms of active/passive voice alternation. It seems, however, that this kind of analysis is completely wrong. We will show several pieces of evidence which are clearly inconsistent with the voice-based account of potential constructions.First, let us consider the difference in the subject status of the two constructions. In Japanese linguistics, it has been assumed that the discontinuous honorific form o ... ni-nar triggers agreement with the subjects. In the literature, the behaviors of prefix o and the suffix (light verb) (-ni)-narare sometimes accounted for independently and given separate positions and functions, but I simply take it as a kind of discontinuous morpheme which 'sandwiches' a verb stem and mark its external argument as a person to whom the speaker shows his or her deference.Subject honorification has been assumed to target subjects, referring to people worthy of respect and generative grammarians have suggested the head of honorific form o … ni-nar agree with the subjects which have moved to the spec, TP or Spec, vP position (see Kishimoto 2012, Hasegawa 2006, among others). We argue that the discontinuous morpheme o .. ni-nar does NOT, in fact, trigger honorific agreement with the sentential subjects. Consider (I attach the negative predicate just to make sentences sound natural): Prof-Nom student-BY Hon-blame-PassHon-Past. 'The professor [+honorific] was blamed by students.'It should be noticed here that the derived form sikar-rare comprising the base verb and the passive suffix in (3c) is wrapped by the honorific form O ... ni-nar, whereas the discontinuous honorific form first combines with the base verb, and then is followed by the potential suffix in potential (3a) and (3b). In (3a), the nominative sensei 'teacher' is marked as the person worthy of respect, so the honorific o ....ninar-targets the subject which is the agent of the base verb sikar-, as predicted from the past work. In (3b), the target of honorification is not nominative object, but the dative subject, which should be taken to agree with the honorific form. In passive (3c), though the derived subject is the target of honorification, it is the theme argument of the base verb. We will show that the subject honorification can and must target the external argument (i.e., the agent of base verbs because the potential suffix combines only with action verbs), regardless of their case markings, in potential sentences, whereas only the derived subject (i.e., the theme argument) can be marked as a person to respect in passive sentences. We will also discuss phenomena regarding quantification and anaphora resolution to propose a new, proper analysis of the potential constructions. Potential constructions have long attracted much attention in Japanese Linguistics, mainly focusing on the case alternation of object NPs. I will point out some important characteristics of the constructions they have missed and propose a completely new analysis from a view point of logical grammar. First, we show significant differences between potential and passive sentences which have been assumed to been projected from one and the same suffix-rare &apos;can&apos;. I suggest that these two uses must be distinguished at least in contemporary Japanese. Our type-logical approach to unbounded dependencies has an empirical coverage broader than traditional and generative grammatical approaches and can explain the fact that various arguments including adjuncts can be marked with nominative. We also examine interesting interactions of case alternation with scope alternation.
Standard and Nonstandard Lexicon in Aviation English: A Corpus Linguistic Study English started as the official language of the International Civil Aviation Organization (ICAO) in 1951, and only in 2011 has the ICAO implemented language requirements on aviation personnel including the usage of standard phraseology in all radio communication. In recent years, the majority of aviation disasters have been caused by human errors, and one of the most common forms is miscommunication, which can potentially lead to catastrophic repercussions. One contributing factor to the occurrence of miscommunication is the wrong interpretation of instructions. For instance, the controller may use a certain word with standard definition to command, but the pilot may interpret the word in nonstandard way.Consequently, a single miscommunication may result in a bigger problem due to wrong interpretation.In June 2014, the Transportation Safety Board of Canada (TSB) reported a runway incursion at Ottawa International airport between a Medevac helicopter and A300 cargo plane. The airport controller amended LF 4 Medevac's IFR clearance by stating: "LF 4 Medevac Roger, while we wait amend your Ottawa 3 for a right turn heading 290˚balance 290˚balance unchanged". The tower controller observed that AW139 was taxiing across the hold short line while FDX 152 Heavy (A3OO) was landing on runway 25. According to the findings, Medevac helicopter was given an amendment to its instrument flight rules clearance. The airport controller's first transmission to LF4 Medevac began with non-standard phraseology "while we wait", which can be confused with "line up and wait". As a result, the Medevac pilot expected that a clearance to take off would follow the amendment to the instrument flight rules clearance. Another factor is that the Medevac pilot did not check if runway was clear before taxiing across the hold short line, leading to the runway incursion with FDX 152 Heavy (A300) approaching to land.In March 2013, another case occurred when the non-standard phraseology "actually standby ah" was used in Boeing 727. The freighter was cleared to takeoff on a runway occupied by two snow clearance vehicles. The cancellation of take-off clearance was not received, but a successful high speed rejected takeoff was accomplished on sight of the vehicles before their position was reached. The controller's failure to 'notice' the runway blocked indicator on his display and to his nonstandard use of Radio-Transmission communications, i.e. "actually standby ah" when he cleared B727 for takeoff and saw the vehicles on the runway, added to the occurrence. The right phraseology should be "takeoff clearance cancelled", and any such cancellation issued after the aircraft has started to roll should take the form "abort takeoff". It was found out that the controller had never been required to use either of these phrases since qualifying.In the light of these cases, it is vital to analyze the discourse between pilots and ATCs, who may be native or non-native English speakers, and to recognize the standard phraseology used in nonstandard ways, which may probably lead to ambiguity and thus posing potential errors to communication.The ICAO puts a great emphasis on non-native English speakers in acquiring a certain level of ATC proficiency, whereas native speakers of English are not prompted by ICAO to adhere to the standard phraseology. According to Hyejeong and Elder (2009), the ICAO considers the level of English proficiency of non-native aviation personnel before implementing the ICAO language policies. The article emphasizes that the responsibilities for miscommunication in aviation where English is used as a lingua franca, are distributed across native and non-native English speaking ATCs and pilots. Tewtrakul andFletcher (2010, cited in Swinehart, 2013) conducted a study in Bangkok International Airport with 312 flight recorded citing for common error among three groups: Thai ATC-Thai pilot, Thai ATC-native English speaking pilot, and Thai ATC-foreign pilot who is a non-native English speaker and does not speak Thai. The study revealed that radiotelephony misunderstandings arise most often among nonnative English speakers. Indeed, it is worth noting that the responsibilities shared by pilots and ATCs must adhere to the use of standard phraseology. However, some lexical items (e.g. hold short, priority, etc.) in aviation phraseology could be used in non-standard ways. Mendez-Naya (2006) investigated the evolution of the term right over time. While the word right has a standard use as an adjunct of direction, other definition has also been espoused as "correct" and "exactly". Furthermore, it also functions as a discourse marker, locative or time expressions, adverbs, prepositional phrases, or clauses modifier, making the term more ambiguous. More recently, Swinehart (2013), who expanded Mendez-Naya's study, examined a particular lexical item right and examined its usage in standard and non-standard ways through a corpus of Cockpit Voice Recording (CVR) transcripts from National Transportation Safety Board (NTSB). Surprisingly, only 18.2% of occurrences of "right" in CVR transcripts were used in standard ways. This is a very alarming since almost 80% are generally used in various nonstandard ways. It can be concluded that this is an apparent deviation from the ICAO's efforts to provide "maximum clarity, brevity, and unambiguity" (p. 3-2), creating ambiguity in a field of discourse where clarity of communication is vital. Although Swinehart's (2013) corpus-based study looked into how the lexical item right was used in non-standard ways, the present study broadly investigates other lexical items irrespective of their typologies (Bratanić &amp; Ostroški Anić, 2009). In addition, Swinehart's (2013) study still needs theoretical underpinnings as regards the nonstandard use of such lexical items. This occurrence can be explicated by the emergence of the world Englishes across the globe where pilots and ATCs who may be native or non-native English speakers use English in their own right. The pioneering model of World Englishes formulated by Braj Kachru in early 1980s, also known as the Kachru's Concentric model, allocates the presence of English: the inner circle, where language functions as a native language (ENL); the outer circle, where English functions as a secondary language (ESL); and lastly, the expanding circle where English serves as foreign language (EFL). This model may politically show the nativeness and non-nativeness of English speaking ATCs and pilots in different nations. However, Rosenberger (2009:23) argued that, "while some nations may never have been easy to classify in this tripartite system, the world-wide use of English has produced increasingly overlapping areas of the three circles." Although there is a need to revisit Kachru's three-circle model in this regard, it is still vital to be taken into account since pilots and ATCs either native or nonnative speakers of English coming from different nations speak different varieties of English. Precisely, there is a need to understand the World Englishes paradigm and use it as a theoretical underpinning in describing the lexical items in standard phraseology having non-standard definition. These alarming problems led the researchers to investigate the most common lexicon in standard phraseology with nonstandard definition in aviation discourse that may pose potential problems in communication. Despite the importance of communication for aviation safety, there is a lack of research that would systematically examine the language of pilots and ATCs. This study aims at investigating the lexical items in Aviation Phraseology that has both standard and nonstandard meanings when Pilot and Air Traffic Controller (ATC) use them in radiotelephony.
Ensemble Technique Utilization for Indonesian Dependency Parser Text parsing is one of the major tasks in natural language text processing (NLP). Text parsing is the process of determining the syntactic structure of a sentence. The result of text parsing is a syntactical tree, which is mostly used for higher-level NLP tasks, like sentiment analysis (Di Caro and Grella, 2013) and semantic role labeling (Johansson and Nugues. 2008).There are two kinds of text parsing to date: constituent parsing and dependency parsing. Constituent parsing parses a sentence by determining the constituent phrases of the sentence hierarchically, usually by using a grammar (Aho, 2003). Dependency parsing, on the other hand, parses a sentence by determining a dependency relation for each word in a sentence. In this research, we use dependency parsing, because it is suited for analyzing languages with free word order, such as Indonesian (Nivre, 2007). Figure 1 shows an example of a parsed Indonesian sentence using dependency structure. Figure 1. Example of a parsed Indonesian sentence (TL: That allegation does not miss) with dependency structureUp until now, there have been only a few studies regarding Indonesian dependency parsing (Sulaeman, 2012;Green et.al, 2012). Most of the previous researches focused on rule-based parsing (Purwarianti et.al, 2013), which yielded quite a low accuracy, compared to other languages. Based on these researches, we use ensemble parsing techniques ( Surdeanu and Manning, 2010) in our works. We also built a dependency Treebank corpus used for the model training with 2098 sentences.In the following sections, we describe the relevant studies and some basic concepts about dependency parsing and its models. We then describe the corpus used in this research, our experiment settings, and finally the results and analysis. Two of the main problems in creating an Indonesian parser with high accuracy are the lack of sentence diversity in treebank used for training and suboptimal uses of parsing techniques. To resolve these problems, we build an Indonesian dependency treebank of 2098 sentences (simple and complex sentences) and use ensemble techniques to maximize the usage of available dependency parsers. We compare the combination of seven parsing algorithms provided by MaltParser and MSTParser, which provides both transition-based and graph-based models. From our experiments, we found that the graph-based model performs better than the transition-based model for Indonesian sentences. We also found no significant accuracy difference in models between several simple ensemble models and reparsing algorithms.
Raising to Object in Japanese: An HPSG Analysis In some languages, an argument that belongs semantically to an embedded clause is realized syntactically as an object of a matrix clause, this "raising to object" (RTO) is schematized as follows:(1) [ matrix subject . . . object i [ embedded ∆ i . . . ] . . . ]The term "raising" has its origin in the transformational analysis of such constructions in which the subject of the lower clause is "raised" to become the object of the matrix verb (Postal, 1974;Lasnik and Saito, 1991;among others).In Japanese, it has been noted in the literature on transformational syntax that examples such as (2) share syntactic properties with English counterparts:(2) a. Yamada  Kuno (1976): pp. 23-24, Slightly altered.)As those glosses indicate, (2a) and (2b) show the same case alternation patterns that English exhibits.There are a number of conditions which must be satisfied in order to form a grammatical RTO construction, but in this paper, we focus on the predicational relation between the accusative-marked NP and the complement predicate. More specifically, we argue that RTO involves a non-thematic NP related to the embedded predicate via predication. This paper discusses the so-called raising to object (RTO), which provides interesting problems with respect to the syntactic/seman-tic status of an accusative-marked NP. We argue that two types of matrix verb, control and raising, must be recognized in the construction. The linearization approach can capture the possibility of word order variation, especially , the distribution of accusative-marked NP in the construction. Moreover, we suggest that RTO involves a non-thematic NP related to the embedded predicate via predication.
Using Stanford Part-of-Speech Tagger for the Morphologically-rich Filipino Language A Part-of-Speech (POS) tagger is a software that classifies words into its word classes or lexical categories ( Bird et al., 2009). POS tags and taggers have proven its importance in Natural Language Processing (NLP) when used in advanced NLP researches such as grammar checkers (Go and Borra, 2016), information extraction ( Surdeanu et al., 2011), and word-sense disambiguation (Chen et al., 2009). In a pipeline architecture of an advanced research such as an information extraction system, POS taggers are usually found in the first section producing POS tags or tag sequences. These POS tags may be used as basic features or to produce more advanced features such as syntactic structures using a constituency parser and dependencies between words using a dependency parser ( Surdeanu et al., 2011;Chen and Manning, 2014).Despite being a fundamental NLP tool towards advanced NLP researches, there seems to be few researches made towards the development of a highperforming POS tagger for Filipino, the national language of the Philippines -a Southeast Asian country with a population of 101 million people 1 .The following are the list of POS taggers developed for Tagalog, the dialect from where Filipino was based on: TPOST ( Rabo and Cheng, 2006), MBPOST ( Raga and Trogo, 2006), PTPOST4.1 (Go, 2006), Tag-Alog ( Fontanilla and Wu, 2006), and SVPOST (Reyes et al., 2011); Adding to the list is the recently published POS tagger designed for Filipino named SMTPOST ( Nocon and Borra, 2016). The key difference between Tagalog and Filipino is the presence of accepted English words such as 'cellphone', 'laptop', 'professor', 'polo shirt' as part of the Filipino language leading to nonce borrowings (single word code switching) and even intra-word code switching such as nag-conduct 'conducted' (added prefix nag-) and tinetext 'texting (someone) ' (added infix -in-).Looking into the design of the taggers, TPOST and MBPOST are closely similar because both systems utilize a lexicon list, surrounding words, capitalization, and affix features using a stemmer; where tagging rules are extracted from the training to be used during testing. PTPOST4.1 uses Hidden Markov Model (HMM), Viterbi algorithm, lexicon list, stemmer, and the previous (left) tag before the word. SVPOST makes use of Support Vector Machines (SVM) with predefined features for its training and tagging. SMTPOST presents a novel 1 Based on Philippine Census of Population 2015 approach of using Statistical Machine Translation (SMT) in tagging by 'translating' feature representation of words to POS tags. For example, the verb kumakain 'eating' will be represented as @um$ka highlighting the infix -um-and the partial reduplication ka which is then paralleled to its respective POS tag VBTR VBAF (imperfective actor-focus verb) during training.In terms of evaluation, an independent experiment was conducted to test the performance of the early POS taggers: TPOST, MBPOST, PTPOST4.1 and Tag-Alog using 120,000 words as data, 4% of which were used as testing data (Miguel and Roxas, 2007). The taggers scored 70%, 77%, 78.3%, and 72.5%, respectively, with PTPOST4.1 as the highest among the four. SVPOST on the other hand, conducted its own experiment on 122,318 words producing an 81% accuracy score. SMTPOST, being the most recent development among all Filipino POS taggers, produced 84.75% accuracy in its own 70,312 word dataset. These results however are relatively low compared to the state-of-the-art POS taggers for English (97.64%), French (97.8%), German (96.9%), Arabic (96.26%), and Chinese (93.46%) (Choi, 2016;Denis and Sagot, 2009;Toutanova et al. , 2003).These low POS tagging results also hinders the progress of advanced NLP researches in the Filipino language. For instance, named entity recognition for Filipino is considered to be still in its infancy stage due to the limitation of researchers to either manually or semi-automatically tag their Filipino datasets which still requires a very tedious and time-consuming tagging or cleaning process ( Lim et al., 2007).Analysis show that works for Filipino and the other languages differ in two major factors: features and algorithms used. All of the POS taggers for Filipino uses few features: capitalization, presence of affixes, and partial/full reduplication which is produced during a pre-processing stage by handcrafted rules and a stemmer ( Rabo and Cheng, 2006;Nocon and Borra, 2016). Incorrect stemming by the stemmer also cascaded as tagging errors as seen in TPOST which accounted for 25% of the tagging errors in the mentioned work. Algorithms used for Filipino which mostly relied on sentence template rules, affix features, feature-value(tag) pairs vary significantly than what algorithms the stateof-the-art POS taggers for the other languages are using: Conditional Random Fields, Maximum Entropy Cyclic Dependency Network, Maximum Entropy Markov Model, and others.Due to significant developments in POS tagging, researches show that existing algorithms applied for these high-performing POS taggers are also usable for other languages, up to a certain extent. The Stanford POS Tagger 2 , which uses maximum entropy cyclic dependency network as its core algorithm, has been applied in several languages and achieved decent tagging accuracy results: English (97.28%), Chinese (93.99%), Arabic (96.26%), French (not specified), and German (96.9%) with minimal tweaks such as character length of prefix and suffix to consider, unicode shapes for non-alphabetic languages, distributional similarity, and context window. The Stanford Partof-Speech (POS) tagger has also been packaged in such a way that it is easy to use for training and testing custom models of different languages.This research explores the usage of the Stanford POS tagger for the Filipino language taking into consideration the unique Filipino linguistic phenomena such as free word order structure, and a large vocabulary of root, derived, and borrowed words. This paper is organized as follows: in the next section, we discuss the Stanford POS Tagger, followed by the Filipino linguistic phenomena in Section 3; in Section 4, we describe the experiments conducted in creating a Filipino model for the Stanford POS Tagger; analysis of results are then shown in Section 5, ending the paper with the conclusion and future works in Section 6. This research focuses on the implementation of a Maximum Entropy-based Part-of-Speech (POS) tagger for Filipino. It uses the Stan-ford POS tagger-a trainable POS tagger that has been trained on English, Chinese, Arabic, and other languages and producing one of the highest results in each language. The tagger was trained for Filipino using a 406k token corpus and considering unique Filipino linguistic phenomena such as high morphology and intra-sentential code-switches. The Fil-ipino POS tagger resulted to 96.15% tagging accuracy which currently presents the highest accuracy and with a large lead among existing POS taggers for Filipino.
The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition Named entity recognition (NER) is an essential task in natural language processing that falls under the domain of information extraction. The function of this task is to identify noun phrases and categorize them into a predefined class. NER is a crucial preprocessing step used in some NLP applications such as question answering, automatic translation, speech processing, and biomedical science. In two shared tasks, CoNLL 2002 1 and CoNLL 2003 2 , language independent NER systems were evaluated for English, German, Spanish, and Dutch. These systems focus on four named entity types namely person, organization, location, and remaining miscellaneous entities.Lately, an evaluation campaign that systematically compared NER systems for the Vietnamese language has been launched by the Vietnamese Language and Speech Processing (VLSP) 3 community. They collect data from electronic newspapers on the web and annotate named entities in this corpus. Similar to the CoNLL 2003 share task, there are four named entity types in VLSP dataset: person (PER), organization (ORG), location (LOC), and miscellaneous entity (MISC).In this paper, we present a state-of-the-art NER system for Vietnamese language that uses automatic syntactic features with word embedding in Bi-LSTM. Our system outperforms the leading system of the VLSP campaign utilizing a number of syntactic and hand-crafted features, and an end-to-end system described in (Pham and Le-Hong, 2017) that is a combination of Bi-LSTM, Convolutional Neural Network (CNN), and Conditional Random Field (CRF) about 3%. To sum up, the overall F 1 score of our system is 92.05% as assessed by the standard test set of VLSP. The contributions of this work consist of:• We demonstrate a deep learning model reaching the state-of-the-art performance for Vietnamese NER task. By incorporating automatic syntactic features, our system (Bi-LSTM), although simpler than (Bi-LSTM-CNN-CRF) model described in (Pham and Le-Hong, 2017), achieves a much better result on Vietnamese NER dataset. The simple architecture also contributes to the feasibility of our system in practice because it requires less time for inference stage. Our best system utilizes partof-speech, chunk, and regular expression type features with word embeddings as an input for two-layer Bi-LSTM model, which achieves an F 1 score of 92.05%.• We demonstrate the greater importance of syntactic features in Vietnamese NER compared to their impact in other languages. Those features help improve the F 1 score of about 18%.• We also indicate some network parameters such as network size, dropout are likely to affect the performance of our system.• We conduct a thorough empirical study on applying common deep learning architectures to Vietnamese NER, including Recurrent Neural Network (RNN), unidirectional and bidirectional LSTM. These models are also compared to conventional sequence labelling models such as Maximum Entropy Markov models (MEMM).• We publicize our NER system for research purpose, which is believed to positively contributing to the long-term advancement of Vietnamese NER as well as Vietnamese language processing.The remainder of this paper is structured as follows. Section 2 summarizes related work on NER. Section 3 describes features and model used in our system. Section 4 gives experimental results and discussions. Finally, Section 5 concludes the paper. This paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (Bi-LSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F 1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin .
Multiple Nominative Constructions in Japanese: An Incremental Grammar Perspective Japanese allows Multiple Nominative Construction (MNC), where more than one NP is nominativemarked within a (seemingly) single clause (Kuno, 1973; see also references in §2.3).(1) Ken-ga kami-ga nagai K-NOM hair-NOM long 'Ken's hair is long.'In (1), both Ken and kami 'hair' are marked by the nominative case particle ga. The initial ga-marked element Ken in (1) is often called "major subject" (Kuroda, 1978;1986;. 1 In this article, we provide new data on MNC in connection with rightward displacements ( §2), and argue that these data are adequately handled from the perspective of "incremental grammar," a view where syntactic puzzles are solved as a reflection of the way a sentence is parsed time-linearly ( §3). Our analysis is formalised within Dynamic Syntax ( Cann et al, 2005), with the bonus of predicting the "left-right asymmetries" ( §4). This article defends an &quot;incremental grammar&quot; view, where syntactic puzzles are accounted for in terms of how a sentence is parsed online. To this end, we focus on the Multiple Nominative Construction (MNC) in Japanese, offering new data involving &quot;rightward displacements.&quot; The displacement patterns of nominative NPs are shown to follow from the way an MNC string is parsed left-to-right. Our incremental account is formalised in Dynamic Syntax, with the upshot that only the licit ordering of nominative NPs in MNC leads to a legitimate structure update.
#ActuallyDepressed: Characterization of Depressed Tumblr Users&apos; Online Behavior from Rules Generation Machine Learning Technique According to the World Health Organization (WHO) (2001), depression is a serious mental issue and may be the second leading cause of disease by 2020. Amy Courtney (2014) argues that blogging reduces the symptoms brought about depression. Blogging allows the public to access and comment on such work, allowing additional psychological benefits.Microblogging is an easier way of blogging that allows users to create short contents shared with an audience in real-time. This creates an easier and more convenient way of sharing content and information through the web (Nations, 2015). Contents vary from text, to visual, audio, audiovisual and even the use of links to redirect to other websites. This study aims to characterize the online behavior of depressed individuals using machine learning technique. Further, this research is guided by the following questions: RQ1. What are the characteristics of depressed individuals on Tumblr in terms of their affective, behavioral, cognitive, and linguistic style attributes? RQ2. How do these characteristics compare to offline behavior of depressed individuals in literature? Are they any different?This study focuses on Tumblr because there have not been a lot of studies focusing on adolescents social media postings. Other studies on depression online are focused on an adult age group. Moreover, we find that because Tumblr allows anonymity of users there might be a more genuine response with regards to their posting and the results that we gather since they are not bound to their true identity in person similar to the study of Warner et al (2016).The research covers 17 features of depressed individuals that are explored in other studies as classification problems. With these 17 features, 13 are coming from the posts, while the other 4 are basic information about the user (age, civil status, highest education attained and gender).Despite a number of studies correlating with depression, we extracted features from 4 different processes or style attributes. These are: (1) affective, (2) social, (3) cognitive, and (4) linguistic. In other studies (Reece, 2016;De Choudhury, 2014;Moreno, 2011), only cognitive and linguistic patterns have been extracted. By extending the depression studies to include its social and affective process and allowing a more varied set of attributes, this research aims to create a wholistic characterization of such users. The ubiquitous data provided by social networking sites paved the way for researchers to understand netizens behavior with psychological ailments such as depression. However, most of these researches are aimed at classifying users with depression using blackbox algorithms such as SVM. This does not allow data exploration or further understanding the characteristics of depressed individuals. This research aims to characterize depressed Tumblr users online behavior from rules generation. Characterization is done by collecting affective, social, cognitive and linguistic style markers collected from the respondents posts. Rules are then generated from these features using CN2 algorithm-a rules generating machine learning technique. The rules are analyzed and are compared with observations in prior literature on depressive behavior. We observed that depressed respondents in Tumblr have more photo content in posts rather than just pure textual posts, posts are more negative, and there is an evident use of self-referencing words. These characteristics are also evident in offline behavior of depressed individuals based on prior literature
A Parallel Recurrent Neural Network for Language Modeling with POS Tags Language models (LMs) are crucial parts of many natural language processing applications, such as automatic speech recognition, statistical machine translation, and natural language generation. Language modeling aims to predict the next word given context or to give the probability of a word sequence in textual data. In the past decades, n-gram based modeling techniques were most commonly used in such NLP applications. However, the recurrent neural network based language model (RNNLM) and its extensions ( Mikolov et al., 2010;Mikolov et al., 2011) have received a lot of attention and achieved the new state of the art results since 2010. The most important advantage of RNNLM is that it has the potential to model unlimited size of context, due to its recurrent property. That is to say, the hidden layer has a recurrent connection to itself at previous timestep.Part-of-speech (POS) tags capture the syntactic role of each word, and has been proved to be useful for language modeling (Kneser and Ney, 1993;A. Heeman, 1998;Galescu and Ringger, 1999;Wang and Harper, 2002). Jelinek (1985) pointed out that we can replace the classes with POS tags in language model. Kneser and Ney (1993) incorporated POS tags into n-gram LM and got 37 percents improvement. But they got only 10 percents improvement with classes through clustering. A. Heeman (1998) redefined the objective of automatic speech recognition: to get both the word sequence and the POS sequence. His experiments showed 4.2 percent reduction on perplexity over classes.It is common to build probabilistic graphical models using many different linguistic annotations ( Finkel et al., 2006). However, the problem to combine neural architectures with conventional linguistic annotations seems hard. This is because neural architectures lack flexibility to incorporate achievements from other NLP tasks ( Ji et al., 2016). To address the problem, ( Ji et al., 2016) used a latent variable recurrent neural network (LVRNN) to construct language models with discourse relations. LVRNN was proposed by Chung et al. (2015) to model variables observed in sequential data.Inspired by the POS language models and the LVRNN models above, we use POS features to improve the performance of RNNLM. We assume that if we know the next POS tag, the search range to predict the next word will be shrinked; and the next POS is closely related with the POS sequence that has been seen before. Not the same as Ji et al. (2016), who used a latent variable to model the language annotation, we designed a parallel RNN structure, which consists two RNNs to model the word sequence and POS sequence respectively. And further the state of POS network has an impact on the word network.In summary our main contributions are:• We propose to model words and POS tags simultaneously by using a parallel RNN structure that consists of two recurrent neural networks, word RNN and POS RNN.• We propose that the current state of the word network is conditioned on the current word, the previous hidden state, and also the state of POS network.• We demonstrate the performance of our model by computing lower perplexity. We conducted our experiments on three different corpora, including Penn TreeBank, Switchboard, and BBC corpora.The rest parts of this paper are organized as follows. Section 2 introduces the background techniques, including RNNLM and evaluation for language models. Section 3 elaborates our POS tag language model. Section 4 reports the experimental results. Section 5 reviews related work and Section 6 concludes the paper. Language models have been used in many natural language processing applications. In recent years, the recurrent neural network based language models have defeated the conventional n-gram based techniques. However, it is difficult for neural network architectures to use linguistic annotations. We try to incorporate part-of-speech features in recurrent neural network language model, and use them to predict the next word. Specifically, we proposed a parallel structure which contains two recurrent neural networks, one for word sequence mod-eling and another for part-of-speech sequence modeling. The state of part-of-speech network helped improve the word sequence&apos;s prediction. Experiments show that the proposed method performs better than the traditional recurrent network on perplexity and is better at reranking machine translation outputs. 1
Identifying Deception in Indonesian Transcribed Interviews through Lexical-based Approach Human social behavior has successfully led to the ubiquitous human communication. In this regard, it is also very possible for people to commit lies when communicating with others. Deceit or commonly referred to as lie is any actions of making others believe what we perceived as false, without the receivers know that they are being fooled (Ekman, 1992;Vrij, 2008). A lie can be divided into a variety of classes when viewed from various aspects involved in such actions. For example, when viewed from how bad a lie is, a lie can be classified into a white lie, gray lie, and real lie (Bryant, 2008).Various motivations may underlie a lie. Based on interviews with children and questionnaire survey results from adults by Ekman (1989), according to most of the children and the adults, someone might lie in order to avoid punishment. Referring to this phenomenon, especially if we focus on the realm of interrogation for solving crimes, it is a compelling matter when people are challenged to be able to tell which utterances contain lies. However, for many people, it seems difficult to recognize any deception, considering that the cues to deception can be reflected from diverse aspects ( DePaulo et al., 2003) as well as the need for specific experience in related scientific fields.As in other computational linguistic studies, in order to obtain the best result, sometimes the geographic location of the speakers have to be taken into account when finding the salient features. The location of the speakers can affect their way of thinking, and also their way of speaking. A feature might be very dominant in a particular language yet only considered as an additional feature in other languages. That being said, currently, there is only a small number of deception detection studies using Indonesian language.A lot of studies have been conducted in order to find the best method for distinguishing deception within human communication. Not only in the field of psychology (Ekman et al., 1991) which is the root of this engaging topic, but also in other areas such as text processing (Mihalcea &amp; Strapparava, 2009;Newman et al., 2003) and speech processing ( Benus et al., 2006;Hirschberg et al., 2005;Levitan et al., 2016). In this paper, we present our approach of identifying deception, especially in Indonesian, based on lexical approach. Moreover, we also perform an additional experiment of combining lexical features and acoustic/prosodic features. This paper aims to present a lexical-based approach in order to identify deception in Indonesian transcribed interviews. Using word calculation from the psychological point of view, we classify each subject utterance into two classes, namely lie and truth. We find that the intentions of the people in both telling the truth and hiding the fact can affect the words used in their utterances. We also find that there is an interesting pattern for Indonesian people when they are answering questions with lies. Despite the promising result of lexical-based approach for detecting deception in the Indonesian language, there are also some cases which cannot be handled by only using the lexical features. Hence, we also present an additional experiment of combining the lexical features with acoustic/prosodic features using the recorded sound data. From the experiment, we find that the combination of lexical features with other features such as acoustic/prosodic can be used as the initial step in order to get better results in identifying deception in Indonesian.
Unsupervised Method for Improving Arabic Speech Recognition Systems  One of the big challenges connected to large vocabulary Arabic speech recognition is the limit of vocabulary, which causes high out-of-vocabulary words. Also, the Arabic language characteristics are another challenge. These challenges negatively affect the performance of the created systems. In this work we try to handle these challenges by proposing a new unsupervised graph-base method. Finally, we have obtained a 4.6% relative reduction in the word error rate. Comparing our suggested method with other methods in the literature, it has given better results. Moreover, it has presented a major step towards solving this problem. In addition, it can be easily adaptable to other languages.
Remarks on Epistemically Biased Questions Some varieties of polar interrogatives (polar questions) convey an epistemic bias toward a positive or negative answer. While previous research has revealed much on how different varieties of biased interrogatives contrast with each other in their syntactic and semantic properties, there is a great deal of complexity and subtlety concerning the usage of each type that calls for further investigations.This work takes up three paradigmatic kinds of biased interrogatives, (i) positively-biased negative polar interrogatives (Isn't she home already?), (ii) negatively-biased negative polar interrogatives (Isn't she home yet?), and (iii) rising tag-interrogatives (She is home, isn't she?), and aims to supplement existing descriptions of what they convey besides asking a question. Some varieties of polar interrogatives (polar questions) convey an epistemic bias toward a positive or negative answer. This work takes up three paradigmatic kinds of biased polar interrogatives: (i) positively-biased negative polar interrogatives, (ii) negatively-biased negative polar interrogatives, and (iii) rising tag-interrogatives, and aims to supplement existing descriptions of what they convey besides asking a question. The novel claims are: (i) a positively-biased negative polar interrogative conveys that the speaker assumes that the core proposition is likely to be something that is or should be activated in the hearer&apos;s mind, (ii) the bias induced by a negatively-biased negative polar interrogative makes reference to the speaker&apos;s assumptions about the hearer&apos;s beliefs, and (iii) the biases associated with the three constructions differ in strength, the one of the rising tag-interrogative being the strongest.
The blocking effect and Korean caki When the Chinese reflexive ziji is located far from its antecedents, it is not uncommon to see the blocking effect, since the long-distance binding of ziji is normally blocked by the presence of a first (or second) person pronoun intervening in the reported speech (Y.-H. Huang 1984, Cole et al. 1990, Huang and Tang 1991, Huang and Liu 2001, Pan 2001, Cole et al. 2006, among others), as shown in (1)  The antecedent of Chinese ziji in (1) can be the matrix subject Zhangsan, the intermediate subject Lisi, or the most embedded subject Wangwu. In contrast, ziji in (2) can only be coreferential with the local antecedent Wangwu rather than the matrix subject Zhangsan or the intermediate subject wo of a first person pronoun. This phenomenon of Chinese ziji has long been accounted for in terms of the blocking effect, which occurs when an immediately higher noun phrase differs in the person feature from a lower noun phrase. Therefore, in (2), the intermediate subject wo 'I' serves as a blocker because the person feature of wo 'I' differs from the third person Wangwu. Conversely, it has generally been accepted that Korean caki does not manifest any blocking effects (Yoon 1989, Cole et al. 1990, Sohng 2004, Cole et al. 2006, Han and Storoshenko 2012, Kim 2013, as exemplified in (3). 1 (3) Chelswui-nun nayj-ka cakii/*j-lul Chelswu-Top I-Nom self-Acc cohaha-n-ta-ko sayngkakha-n-ta. like-Prs-Decl-Comp think-Prs-Decl 'Chelswui thinks Ij like himi/myself*j.' 1 Cole et al. (1990), contrary to caki, assume that long-distance casin is subject to the blocking effect, as shown in (i). (i) *Chelswui-nun nayj-ka casini-ul saranha-n-ta-koChelswu-Top I-Nom self-Acc love-Prs-Decl-Comp sayngkakha-n-ta. think-Prs-Decl '*Chelswu thinks I like himself. ' (Cole et al. 1990:18) However, we will not discuss the long-distance binding of casin here.( Cole et al. 1990:19) In (3), caki can only refer to the matrix subject Chelswu while it does not refer to the first person pronoun nay. However, even if the matrix subject Chelswu and the first person pronoun nay in the embedded clause are switched, the coreferential relationship remains unchanged. Here is the relevant example.(4) Nai-nun Chelswuj-ka caki*i/j-lul I-Top Chelswu-Nom self-Acc cohaha-n-ta-ko sayngkakha-n-ta. like-Prs-Decl-Comp think-Prs-Decl 'Ii think Chelswuj likes me*i/himselfj.' Nonetheless, the question then arises as to how we can explain what blocks Korean caki, in a certain context, from referring to the long-distance potential antecedent, as illustrated in (5).(5) Hyengsai-nun nayj-ka caki*i/j pwumo-lul detective-Top I-Nom self parents-Acc salhayha-n phaylyunpem-i-lako kill-Adn reprobate-being-Comp sayngkakha-n-ta. think-Prs-Decl 'The detective thinks that I am a reprobate who killed his (*the detective's/speaker's) parents.' (Park 2016:102) We assume that the first person pronoun nay in (5) functions as a blocker since it is unnatural for caki to refer to the matrix subject Hyengsa in this discourse. 2 Thus, based on the observed fact, this pa-2 Some may claim that (5) is a kind of a special occasion in this context and thus the blocking of caki's referring to hyengsa is attributed just to the lexical property of phaylyunpem 'reprobate', which means to harm one's own lineal ascendant and descendant. Thus, if phaylyunpem is replaced by neutral word pemin 'criminal', caki can also refer to either hyengsa or nay, as shown in (i).(i) Hyengsai-nun nayj-ka cakii/j pwumo-lul salhayha-n detective-Top I-Nom self parents-Acc kill-Adn pemin-i-lako sayngkakha-n-ta. criminal-being-Comp think-Prs-Decl 'The detective thinks that I am a criminal who killed his (the detective's/speaker's) parents.' We agree with the view. If so, however, how should we account for the following sentence? (ii) Salamtuli-un nayj-ka caki*i/j pwumo-lul salhayha-n people-Top I-Nom self parents-Acc kill-And per would like to show that a blocking effect does hold in Korean as well and to suggest the analysis of the blocking effect in Korean caki in terms of a unified account in line with that of Chinese ziji. The organization of the paper is as follows. In the section 2, we discuss what has been said about Korean caki, especially with respect to the properties of caki. Then, in section 3, we review Huang and Liu's (2001) analysis on blocking effects. And in section 4, the blocking effect of Korean caki is considered. Section 5 summarizes our findings and conclusions, with a discussion of some predictions that follow from the current analysis. When the Chinese reflexive ziji is located far from its antecedents, it is not uncommon to see the blocking effect, since the long-distance binding of ziji is normally blocked by the presence of a first (or second) person pronoun intervening in the reported speech. Conversely, it has generally been accepted that Korean caki does not manifest any blocking effects. However, in this paper, we propose that the blocking effect exists in the long-distance binding of Korean caki.
Remarks on Denominal -Ed Adjectives Denominal adjectives derived by the adjectivizing suffix -ed, as in (1) below, are quite common in English and seem to have received the attention they deserve from grammarians and linguists. 1,2 1 Since so many cases of denominal -ed adjectives can be analyzed as verb-based as well (e.g., armed, knobbed, etc.),(1) a. blue-eyed b. bearded c. red-roofed d. black-jacketedThe syntactic and semantic properties of these adjectives are intuitively clear; they are adjectives derived from suffixation of -ed to the nominal base N, either a nominal compound or a noun phrase, and they have the meaning related to possession such as 'possessing N' or 'provided with N', etc.The aim of this paper is to discuss denominaled adjectives in light of recent advances in linguistic theory and make the following claims about their structure, morphology and semantics. Specifically, on the fundamental assumption in the framework of Distributed Morphology (Halle and Marantz, 1993;Marantz, 1997Marantz, , 2001) that there is no component dedicated to word formation, this paper defends the view that the -ed adjectives in question are denominal and argues that bases fored are reduced nominal structures, nP. It is shown that facts pertaining to number marking and interpretation support the nP-based analysis of denominal -ed adjectives. Incidentally, an analysis of the singular and plural forms of foreign nouns in English is offered along the way. much care is taken to present unambiguously denominal ones, i.e., ones which have no verbal counterparts or with prenominal modifiers. 2 See, among many others, Jespersen (1942), Hirtle (1970), Hudson (1975), Ljung (1976), Gram-Andersen (1992), Bauer and Huddleston (2002) and the references cited therein. See also Miller (2006:175ff.) for discussion of the Latinate counterpart -(a)te/-ated.Second, we argue that the adjectivizing suffixed has no contextually determined allomorphs in denominal adjectives. Putative counterexamples are claimed to be stative participles in the sense of Embick (2003Embick ( , 2004, which are deradical, not denominal.Third, we discuss the source of the possession meaning associated with denominal adjectives and argue that it stems from the adjectivizing suffix's denotation which takes a relation as input. This effectively restricts the types of nominals which appear as bases for the suffix: intrinsically relational nouns and relational nouns derived by type shifting. We also argue that the suffix is in sharp contrast with possessive determiners in English: the former imposes type shifting on its non-relational bases, while the latter undergo type shifting to accommodate non-relational possessees.The paper is organized as follows: in section 2, after seeing that the -ed adjectives in question are undeniably denominal, we will argue that their nominal bases are structurally reduced: nPs. In section 3, building on the conclusion reached at in section 2, we will argue that no contextual allomorphy is possible in denominal adjectives and show that putative counterexamples can receive a different analysis. In section 4, we will consider the source of the possession meaning and propose an analysis in which the adjectivizing suffix is required to take a relation as input, which serves to restrict the types of nominal bases appearing in the adjectives. Section 5 will conclude the paper. This paper discusses denominal adjectives derived by affixation of-ed in English in light of recent advances in linguistic theory and makes the following three claims. First, unlike recent proposals arguing against their denominal status, the paper defends the widely held view that these adjectives are derived from nominals and goes on to argue that the nominal bases involved are structurally reduced: nP. Second, the paper argues that the suffixed in denominal adjectives shows no contextual allomorphy, which is a natural consequence that follows from the workings of the mechanism of exponent insertion in Distributed Morphology (Halle and Marantz, 1993). Third, the meaning associated with denominal-ed adjectives stems from the suffix&apos;s denotation requiring a relation, which effectively restricts base nominals to relational nouns, derived or underived. It is also argued that the suffix is crucially different from possessive determiners in English (e.g., &apos;s) in that, while the former imposes type shifting on non-relational nouns, the latter undergo type shifting to accommodate them.
Subjecthood and Grammatical Relations in Korean: An Experimental Study with Honorific Agreement and Plural Copying  The present study investigated the following: i) how NPs bearing differing GRs behave with respect to two proposed subject diagnostics-Honorific Agreement (HA) and Plural Copying on adverbs (PC) and ii) whether scrambling allows non-Subject GRs to control these properties. An experimental investigation using Magnitude Estimation (ME) was conducted. The result revealed that the sentences with Subject NP controller got higher acceptability scores compared to non-Subject NP controllers for both diagnostics and that scrambling did not have an effect on acceptability. While both HA and PC showed a similar pattern of preference for Subject controllers, the contrast between Subject and non-Subject controllers was more pronounced with HA.
A Stylistic Analysis of a Philippine Essay, &quot;The Will of the River&quot; Understanding the depth and craftsmanship of any literary pieces poses challenges. It requires the analysis of the language to provide an objective interpretation and meaning of the literary text. It demands awareness on how the language works, its functions and components. From this point, understanding stylistics is quintessential.The essay, "The Will of the River," by Alfredo Q. Gonzales is the literary text under study. It is a narrative essay about the river, Bacong, whose resolute journey towards the sea is likened to the life of a man. The author's style in writing exemplifies a pattern of structure foregrounded by an unconventional means of opening sentences which are the sentence-initial adjuncts, also called clause-initial adjuncts (Ernst, 2002). Its dominant pattern led to the consideration of the lexicon since adjuncts involve lexical selections as well, its grammatical and semantic functions and other major grammatical components of the text that provided a fertile and challenging ground for stylistic analysis. Significantly, this study hoped to contribute to the academic enrichment of Philippine Literature as a starting point in appreciating local literary writers and literary style of writing.The essay seemed to involve a journey, a personal association with the narrator and a strong allusion to the duties and values of man. These initial observations led to the consideration of the chief gesture of stylistics that is to closely examine the 'linguistic particularities of a text' that leads to the 'understanding of the anatomy and functions of the language' (Toolan, 1998, p. ix). In other words, it is significant to pay attention to the language in the text to gain understanding and meaning of the literary piece because 'literature is made of language' (Watson &amp; Zyngier, 2007, p. xii), and stylisticians uphold this principle for several years. This view is emphasized by Wellek andWarren (1977 in Yeibo, 2011) who posit that "language is the material of literature as stone or bronze is of sculpture, paints of picture, or sounds of music" (p. 137).The paper took an eclectic approach as regards to the theoretical framework. The most important sources are the semantic categories in Biber et al. (1999), the analysis provided in Halli- day and Hasan (1976) and Blake (1990). Blake's conventions of clause elements are: a) subject, it refers to the one that performs the verb; b) predicator, the verb performed by the subject; c) object, the receiver of the action of the verb which could be a person or a thing in the sentence besides the subject; d) complement, refers to the subject; and e) adjunct, refers to anything that does not belong to the first four categories.Furthermore, Halliday's (1994) definition of adjuncts corroborates Blake's when he says that "an adjunct is an element that has not got the potential of being subject. It is typically realized by an adverbial group or a prepositional phrase" (p. 80). Essentially, an adjunct is a grammatical function of adverbial and that adjunct is a realization of adverbial ( Quirk et al., 1985).Biber et al., (1999) classified adverbials by their functions: circumstance adverbials, to add circumstantial information about the proposition in the clause; stance adverbials, to express speaker/writer's stance towards the clause; and linking adverbials, to link the clause to some other unit of discourse.Circumstance adverbials are the most varied class, as well as the most integrated into the clause structure. They add information about the action or state describe in the clause, answering questions such as 'how, when, where, how much, to 'what extent ' and 'why.' (Biber, et al., 1999).The seven major semantic categories of circumstance adverbials in Biber et al. (1999) are place, time, process, contingency, extent/degree, additive/restriction and recipient. Place circumstance adverbials convey distance, direction, or position. Distance adverbials typically answer the question 'How far', and include both general description of distance and specific measurements. Direction adverbials describe the pathway of an action. Position adverbials occur most often with stative verbs. They also occur with communication and activity verbs.Time circumstance adverbials used to convey four-time related meanings: position in time, duration, frequency, and temporal relationship between two events or states. Process circumstance adverbials cover a wide range of semantic roles and are a less unified group than place or time adverbials. The most common subcategory of process adverbials is manner which describes the way in which something is done.Process circumstance adverbials also include the category of means while instrument and agentive adverbials specify the agent of an action and are used with passive construction.Furthermore, like the category of the process, contingency is a more diverse category than time and place. This category covers circumstance adverbials that show how one event or state is contingent upon another, including cause, reason, purpose, concession, condition, and result. While extent/degree circumstance adverbials function as intensifiers or diminishers, additive adverbials show that a current proposition is being added to a previous one. Finally, recipient adverbials typically expressed by for-phrases express the target of an action.The second classification of adverbials is stance Adverbials whose primary function in the clause is to provide comment on the content or style of a clause. Their semantic categories include epistemic stance adverbial, attitude adverbial, and style adverbial. The third classification of adverbials is the linking adverbials whose primary function is to state the speaker/ writer's perception of the relationship between two units of discourse. Because they explicitly signal the connections between passages of text, linking adverbials are important devices for creating textual cohesion, alongside coordinators and subordinators. Their semantic categories include enumeration and addition, summation, apposition, result/inference apposition, contrast/ concession, and transition.Essentially, the syntactic realizations of adverbials are varied ranging from single adverbs and adverb phrases, noun phrases (including single nouns), prepositional phrases, finite clauses, nonfinite clause and its subclasses: ing-clauses, edclauses, to-infinitive clauses, and verbless clauses ( Biber et al., 1999).Among the realizations of adverbials in the essay, predominant is the prepositional phrase and coordinating conjunctions. According to Quirk et al. (1985), prepositional phrases can perform some syntactic functions such as post modifiers in a noun phrase, adverbials of different kinds, verbs and adjective complements, clause subjects, and semi adjectives. Factually, Zihan's (2014) study highlighted two important arguments in comprehending the differences between linking adverbials and conjunctions. First, linking adverbials mark a meaning relationship at discourse level while conjunctions provide a structural link at clause complex level. Second, when a word form which can be used as a conjunction (e.g. and, so) is used as a discourse marker, it no longer belongs to the grammatical class of conjunction. Instead, it is a clause component which functions as a linking adverbial grammatically.Applying these categories and concepts to explore the lexical behavior and grammatical components of the language used in the essay, the analysis would like to answer the following questions:1. What are the occurrences of the initial position adjuncts found in the essay? 2. How do these initial position adjuncts unravel the meaning of the essay?3. What other grammatical features found in the text that help shed the central theme of the essay? The continuous study of stylistics has been regarded as significant in identifying the border between language and literature. Hence the study presented a stylistic analysis of Alfredo Q. Gonzales&apos;s essay &quot;The Will of the River.&quot; The lexis-grammar complementary analysis on the personal narrative of the author focused on the vocabulary of the essay and the grammatical structure of the sentence primarily the use of sentence-initial adjuncts that leads to the unrav-eling of the essay&apos;s general theme of man and nature.
Intrusions of Masbate Lexicon in Local Bilingual Tabloid 'English is the global language' (Crystal, 2003). English has developed a special role that is recognized in every country in the world. It has covered important domains in the global society such as telecommunications, business, commerce, air control, and social media, to name a few. The use of English has privileged some and marginalized others resulting in some sort of 'linguistic deprivation'. In praise of the English language, Simon Jenkins (1995 in J. Jenkins, 2009) stated: 'English has triumphed. Those who do not speak it are at a universal disadvantage against those who do. Those who deny this supremacy merely seek to keep the disadvantaged deprived.' The dominance of the English language in the rapidly globalizing world resulted in linguistic inequality and induced some feeling of anxiety to who cannot speak it (Tsuda, 2005in David &amp; Dumanig, 2008. Today, with over 6,800 languages in the world, English has proven its power and dominance, which has spread in almost two-thirds of the world's population (Crystal, 2003).Considered as the language of prestige and power, English has continuously expanded throughout the world, which gives birth to the emergence of different varieties now commonly called as World Englishes. These Englishes have embraced the world's English and appropriated it to suit their local cultures and contexts based on the needs of their communities. Sik andAnping (2004 in David &amp; Dumanig, 2008) suggests that with its imperialist and globalizing force, English has penetrated many non-English speaking communities results in a linguistic phenomenon called 'code-switching', which also leads to the nativization or indigenization of English throughout the world. This Englishization of nonEnglish contexts, according to B. Kachru (2011), gives birth to 'transplantation' of English to different areas creating various varieties. Saghal (1991in David &amp; Dumanig, 2008) defines nativization as 'a process of transferring a local language to a new cultural environment.' This process of transference is situated and contextualized; it is socially conditioned and determined and takes into the account the various needs and nuances of the cultures. The sociolinguistic realities of the speech communities are considered in order to articulate the people's local, social, cultural and religious identities (Kachru, 1997). Honna (in David &amp; Dumanig, 2008) stresses that 'when English migrates to foreign countries, it diffuses and internationalizes, acculturates and indigenizes, and adapts and diversifies,' which leads to localized or lexical items. These local terminology may or may not have equivalents in the English language. This nativization process of English in local contexts creates localized varieties of English that exist to serve the needs of their local speakers.Much research has already been done exploring the nativization of English in Asia or what linguists refer to as Asianization of English, as cited by David and Dumanig (2008): Malaysia, the Philippines and Thailand (Powell, David &amp; Dumanig, 2008), Brunei (David &amp; McLellan, 2007), Singapore and Pakistan (David, Kuang &amp; Qaisera, 2008), Malaysia (Pillai, 2006), and the Philippines ( Bautista, 1997). These studies have found that, although coming from one original Anglophone source, these English varieties have distinct characteristics and vary from each other graphologically, phonologically, and lexically.These new varieties of English engage in language contact with English and their speakers, in their attempt to express themselves fully in a foreign language, engage in direct lexical borrowings, which initial appear as code-switches. Haugen (1956, p. 26 in David &amp; Dumanig, 2008) describes code-switching as the "alternative use of two languages". Code-switching, according to David (2001) and Kow (2003) functions to build solidarity, to exclude others, to practice power, and to maintain the authenticity of the original source.In the Philippines, the linguistic phenomenon of code-switching is commonly called as Taglish" or Tagalog-English, which appears in both spoken and written discourses. Written discourses include computer-mediate communication (CMC) such as e-mail and chat as well as newspaper prints. Several studies on Taglish have already been conducted ( Bautista, 1997), but it is very rare (at least to the author's knowledge at the moment of writing) to come across studies of nativization of English within a local Philippine English variety. This paper aims to fill in such research gap as it explores the linguistic intrusions of local lexical items in English news articles in a local town within the Philippines. The lexical items in focus are the occurrences of code-switches in the English texts. Philippinization of English has come full circle: It has penetrated not only the center but also the periphery. This paper demonstrates a trend of nativization of English in a rural area as seen in a local daily. Thirty newspaper articles from The Stalwart Journal, a province-wide weekly circulating bilingual journal in the island province of Masbate, Bicol Region in the Philippines, were examined to identify the local lexical intrusions in the English text. The borrowing and assimilation of local lexical terms were analyzed and categorized. These lexical items were found in various categories: people, cultural events, cultural groups, public and private organizations, government programs, program units, government agencies, places, broadcast and social media, transportation, food, animals, human descriptions, public services, and other items. English nativization is shown in the borrowing and switching to local or native lexis in the news articles of the local daily.
Facebook Integration into University Classes: Opportunities and Challenges The influx of Information and Communications Technology (ICT) has revolutionalized the teaching of English to ESL/EFL learners (Cequena, 2013). If students before were used to in-class traditional English language learning delivered within the walls of the classroom, today, the scenario has dramatically changed with the rise of modern technology. 21 st century students now carry portable and handheld electronic and smart gadgets such as laptop, tablet, phablet, netbook, iPad, phone, and other devices and use them every day when doing their school and personal tasks. This '24/7/365 fingertip access' to information allows students to navigate the information superhighway, stay updated and connect interpersonally in virtual spaces with anyone, anytime, and anywhere. This trend extends to the academic world; in fact, in the last decade, research has shown how the World Wide Web or the Internet and other communication technologies have supported meaningful educational experiences (Belz &amp; Kinginger, 2002Garrison &amp; Anderon, 2003;Sykes, 2005;Arnold &amp; Ducate, 2006;O'Bryan &amp; Hegelmeier, 2007;Lord, 2008; among others) to students deemed Digital Natives (Prensky, 2001(Prensky, , 2006).These technological innovations are continually reshaping, redefining, and revolutionalizing the phases and pathways of educational landscapes across the many parts of the globe. Hence, with this technological advancement dominating and permeating globally, it is imperative that the teaching of English, especially among English as second language (ESL) students, must be interactive, responsive, and relevant to make language learning more challenging and meaningful to the learners. The World Wide Web or the Internet's features of interactivity, connectivity and ubiquity make it a good platform for an alternative classroom engagement to trigger some 21 st century skills namely critical thinking and problem solving, collaboration and communication, global awareness, and information literacy (Dohn, 2009). Today, educators can utilize social networking sites (SNS) such as Facebook, Twitter, YouTube, Instagram, Pinterest, Google Hangout, Blogger, and Tumblr as platforms for enhancing students' English language skills.Among these sites, Facebook is the most widely used domain by students for their virtual social activities.Facebook is a SNS that boasts more than 1 billion monthly active users, and it is one of the fastest-growing and best-known sites on the Internet today ("Most famous social network sites," 2016). Established by Mark Zuckerberg in 2004, Facebook is a powerful learning tool that is not only built off of synchronous and asynchronous technologies that has transformed learning but also extended the reach of communicative tools (Blattner &amp; Fiori, 2009). Facebook has a variety of interactive features that students can use. Students can create their own profiles, upload photos and videos, post on their wall posts, share information, join in groups as online communities, among others. Selwyn (2007) stated that Facebook has quickly become the social network site of choice by college students and an integral part of the "behind the scenes" college experience. Thompson (2007) added that the adoption rates of Facebook in universities and colleges are remarkable, i.e., 85% of college students that have a college network within Facebook have adopted it. Furthermore, Pempek (2009) reveals that Facebook enables teachers to provide constructive educational outcomes in a variety of fields. Hew (2011) furthered that Facebook allows teachers to practice a differential pedagogy, in the best interests of the students.Several studies have already explored the pedagogical benefits of integrating Facebook in a language classroom (Selwyn, 2007;Stewart, 2008;Madge et al., 2009;Schroeder &amp; Greenbowe, 2009;Yunus &amp; Salehi, 2012;Shih, 2013;Yu, 2014;Ghani, 2015;Miron &amp; Ravid, 2015;Low &amp; Warawudhi, 2016). These studies have established the pedagogical potentials, benefits and implications of integrating a SNS, particularly Facebook, in the classroom. This study aims to contribute to these ongoing dialogs and explorations, to contextualize the use of Facebook in the Philippine ESL (English as a second language) classroom, and to respond to Prensky's (2006) challenge: "it's time for education leaders to raise their heads above the daily grind and observe the new language that's emerging." Following the tenets of the TPACK Framework (Koehler &amp; Mishra, 2009), which urges the researchers to consider the complex interplay of the three primary forms of knowledge: Content (C), Pedagogy (P), and Technology (T) and their intersections in the language classroom context, the researchers drew implications from these intersections: PCK or Pedagogical Content Knowledge, which refers to the knowledge of pedagogy that is applicable to the teaching of specific content that a teacher intends to teach; TCK or Technological Content Knowledge, which refers to the knowledge of the relationship between technology and content; TPK or Technological Pedagogical Knowledge, which refers to the components and capabilities of various technologies as they used in teaching and learning; and finally the TPACK or Technological Pedagogical Content Knowledge, which is the intersection of the three components characteristic of true technology integration in the classroom. Furthermore, the study is anchored on Horn and Staker's (2014) Blended Learning Framework, employing one of the four models -the Flex Learning Model, which integrates technology into a regular face-to-face or in-class setup. Following the principles of the TPACK Framework (Koehler &amp; Mishra, 2009) and Blended Learning Framework (Horn &amp; Staker, 2014), this study reports findings of integrating Facebook, a Social Networking Site (SNS), in facilitating English language classes at a private university in Manila, Philippines. It aimed to explore students&apos; attitudes towards the use of a &apos;closed&apos; class Facebook group in the English language classroom and to describe how they utilize it as part of their English language learning. Research participants were sophomore students enrolled at an English writing class in the first semester of the academic year 2016-2017. Research data come from surveys, students&apos; wall posts, students&apos; reflections, and individual and focus group interviews suggest that despite some technological limitations, students view and respond positively to the use of Facebook as an alternative platform for English language learning and as an innovative and strategic tool in enhancing lesson delivery, engaging students with the material, and creating a discourse space for self-expression. Pedagogical implications for ESL (English as a second language) and EFL (English as a foreign language) and researchers are offered in the light of these results.
An Empirical Study of Language Relatedness for Transfer Learning in Neural Machine Translation One of the most attractive features of Neural Machine Translation (NMT) ( Bahdanau et al., 2015;Cho et al., 2014;Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, phrase tables and complicated decoding algorithms which are a characteristic of Phrase Based Statistical Machine Translation (PBSMT) systems ( Koehn et al., 2003). It is reported that NMT works better than PBSMT only when there is an abundance of parallel corpora. In the case of low resource languages like Hausa, vanilla NMT is either worse than or comparable to PBSMT (Zoph et al., 2016). However, it is possible to use a previously trained X-Y model (parent model; X-Y being the resource rich language pair where X and Y represent the source and target languages respectively) to initialize the parameters of a Z-Y model (child model; Z-Y being the resource poor language pair) leading to significant improvements ( Zoph et al., 2016) for the latter. This paper is about an empirical study of transfer learning for NMT for low resource languages. Our main focus is on translation to English for the following low resource languages: Hausa, Uzbek, Marathi, Malayalam, Punjabi, Malayalam, Kazakh, Luxembourgish, Javanese and Sundanese. Our main contribution is that we empirically (and exhaustively; within reason) show that using a resource rich language pair in which the source language is linguistically closer to the source language of the resource poor pair is much better than other choices of language pairs. Neural Machine Translation (NMT) is known to outperform Phrase Based Statistical Machine Translation (PBSMT) for resource rich language pairs but not for resource poor ones. Transfer Learning (Zoph et al., 2016) is a simple approach in which we can simply initialize an NMT model (child model) for a resource poor language pair using a previously trained model (parent model) for a resource rich language pair where the target languages are the same. This paper explores how different choices of parent models affect the performance of child models. We empirically show that using a parent model with the source language falling in the same or linguistically similar language family as the source language of the child model is the best.
A Morphosyntactic Analysis of the Pronominal System of Southern Alta The Philippines has over 150 languages (Reid, 2013, pp. 330-331). This large inventory excludes the Sama varieties spoken in the Sulu Archipelago and the South Mindanao languages. Although these varieties are spoken within the Republic of the Philippines; they differ morphosyntactically from other Philippine languages and are generally not included in generalizations about Philippine languages ( Himmelmann, 2005, p. 111). However, Reid (2013) made a distinction between the original settlers and migrants of pre-colonial Philippines. They are the Negritos and non-Negritos, respectively. Although both groups spoke Austronesian languages, the former was nonAustronesian not until the first Austronesian inmigrant the Negritos came in contact 5,000 years ago while the latter groups are the Austronesianspeaking peoples in the Philippines. Reid and Liao (2004, p. 435) conducted a typological study of the syntax of most Philippine languages and claim that Philippine languages are ergative. In support to the previous claim, Dita (2011) conducted a typological study anchored on ergative-absolutive framework by examining the pronominal systems of most of the major languages of the Philippines.Dita (Dita, 2011, p. 1) explains that pronominals are a universal component of human languages and are considered basic vocabulary of any given language. In additon, she explains that personal pronouns are generally closed-class and are unaffected by borrowing or code-switching (Dita, 2011). Pronouns exist together with other closedclass words such as prepositions, articles and conjunctions. Unlike open-class categories (e.g. verbs and nouns), pronouns do not change over time, and they signal grammatical relationships between the verb and the subject or object of a clause. The pronominal system of a language is one of the key components to uncover the morphosyntactic structure and properties of the languages, not to mention other linguistic features that may come along with the analysis. She also explains that this new analysis will provide ample understanding on the mophosyntax of many languages in the Philippines (Dita, 2010).Past studies on the pronominal systems of Negrito and non-Negrito languages focus on reconstructions such as word lists and phonologies (Reid, 1971), Northern Cordilleran subgroup (Tharp, 1974), Arta (Reid, 1989), Alta languages (Reid, 1991), Central Cagayan Agta ( Liao, 2005) and Umiray Dumaget (Lobel, 2013). Other topics include deictics (MacFarland, 2006), reference grammar (Headland &amp; Healey, 1974), supplementary texts (Miller &amp; Miller, 1991). This study aims to contribute in the typological study of the pronominal system of Negrito and Non-negrito languages in the Philippines. Dita (2011) explains that early studies on Tagalog language (Bloomfield, 1917) utilized the nominative-accusative distinction and has then dominated the literature on PL for many years. She further explains other analyses have emerged such as active-stative analysis (Drossard, 1994); the fluid voice analysis (Shibatani, 1999); the hybrid analysis (Machlachlan 1996), and the precategorial symmetrical voice analysis (Foley, 1998). But many of the mophosyntactic analysis of Philippine languages remain unclear. However, ergativeabsolutive analysis that came about in the 1980s with the works of Payne (1982) and Starosa (1986) and, Gerdts (1988) show viable results. Pronouns are one of the universal components of language and they provide information on the morphosyntactic characteristics of any languages such as Philippine languages. Past researches show various analyses on the mor-phosyntax of PLs, a recent typological study claims that Philippine languages (PLs) are er-gative. Another study shows a similar claim; however, this study utilizes the pronominal systems of major Philippine languages and uses an ergative-absolutive framework. This research examines the pronouns of Southern Alta language. It aims to contribute in the ty-pological studies of pronominal systems of Negrito and Non-negrito languages. This study employs an ergative-absolutive framework. The initial result shows that the pro-nominal systems of the Southern Alta language consist of absolutive, ergative, oblique, and genitive pronouns. The ergative-absolutive framework unravels the morpho-syntax of the pronominal system of Southern Alta. The framework helps describe the functions and characteristic of the different sets of pronouns. The study also reveals linguistic phenomena such as inclusivity/exclusivity, first person dual pronouns, homomorphy, clit-icization, hierarchy, person-deixis interface and portmanteau pronouns. In conclusion, the ergative-absolutive framework fits the mor-phosyntactic analysis of the Southern Alta language. This study also suggests to examine the clausal construction including the noun phrases (NPs) of Southern Alta.
Wh-island Effects in Korean Scrambling Constructions It has been widely assumed that gap position cannot occur inside the island structures due to island constraints. In terms of filler-gap dependencies, there exists a gap position, which is the argument of an embedded verb, and an antecedent (or filler), which indicates the sentenceinitial wh-phrase in (1).(1) *What do you wonder [whether John bought __ ]?As one of islands in English, whether-island constraints do not allow any phrases to be out of whether-clause, the sentence in (1) becomes ungrammatical. However, wh-in-situ languages, such as Korean, do not exhibit such island effects as in (2).(2) Mwues-ul ne-nun [John-i ___ sa-ss-nunci] What-Acc you-Top J-Nom buy-Past-Q a-ni? Know-Q Thus, the wh-phrase can be placed in the gap position without degrading grammaticality. However, it is controversial to interpret the sentences with wh-phrase which can undergo the LF-movement, as shown in (3). This study examines the wh-island effects in Korean. Since wh-in-situ languages like Korean allow wh-scrambling, the absence of wh-island constraints is accepted. However, it is controversial whether wh-clauses can take a matrix scope or not. In order to clarify the issue of wh-islands in Korean, the current paper designed an off-line experiment with three factors: island or non-island, scrambling or non-scrambling, and embedded scope or matrix scope. The following acceptability judgment task revealed that wh-PF-island does not exist but wh-LF-island plays a role in Korean. Among results of wh-LF-island, it was observed that a majority of speakers prefer the matrix scope reading.
A Crowdsourcing Approach for Annotating Causal Relation Instances in Wikipedia Commonsense knowledge such as entities and events, and their causal relationships, are indispensable in various natural language processing (NLP) applications, including question answering ( Oh et al., 2013;Oh et al., 2016;Sharp et al., 2016), hypothesis generation ( Radinsky et al., 2012;Hashimoto et al., 2015), stance detection ( Sasaki et al., 2016), and literature curation for systems biology ( Pyysalo et al., 2015;Rinaldi et al., 2016).In many previous researches, corpora for acquiring causal relations were built by annotating two text spans (e.g., entities) and their relations in the text ( Doddington et al., 2004;Hendrickx et al., 2010;Pyysalo et al., 2015;Rinaldi et al., 2016;Dunietz et al., 2017;Rehbein and Ruppenhofer, 2017). However, this approach is extremely work intensive. It involves choosing a target domain, designing an ontology (semantic classes) of entities, building a corpus for named entity recognition, designing an annotation guideline for relations, and annotating the relations between entities. Building such a corpus also requires the annotation efforts of experts. For these reasons, this approach is almost non-scalable to various domains or genres of text although the knowledge of the causal relations is highly target-specific. This paper presents an approach for harnessing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification ( Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics.Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost ( Brew et al., 2010;Finin et al., 2010;Gormley et al., 2010;Jha et al., 2010;Fort et al., 2011;Kawahara et al., 2014;Lawson et al., 2010;Hovy et al., 2014;Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface are often limited to multiple choice questions or free descriptions.This study also explores the potential of crowdsourcing for collecting annotations about causal relation instances. To this end, we tailor a simple micro-task in which crowd workers annotate textual spans with causal relations to the title of a Wikipedia article. We also develop an annotation system that cooperates with a crowdsourcing service. By virtue of the widely used annotation tool brat 1 (Stenetorp et al., 2012), the system is easy to use and extendible to other annotation tasks.We collected 95,008 annotations of causal relation instances for 8,745 summary sentences 2 in 1,494 Wikipedia articles. By analyzing the annotation results, we provide valuable hints for improving the annotation process in terms of the number of crowd workers necessary for an article, the number of agreements necessary for improving the quality of causal relation instances, syntactic profiles of annotated spans (e.g., noun and verb phrases), and common confusions of annotations.The annotation results are also useful for mining expressions inverting polarity of causality (promotion and suppression) and provide supervision data for automatic extraction of causal relation instances from Wikipedia articles. We have released the annotation system, annotated corpus, and the automatic extraction tool on a dedicated website 3 . Although the corpus was built for Japanese Wikipedia articles, we here use English translations for illustrative purposes. This paper presents a crowdsourcing approach for annotating causal relation instances to Wikipedia. Because an annotation task cannot be decomposed into multiple-choice problems , we integrate a crowdsourcing service and brat, a popular on-line annotation tool, to provide an easy-to-use interface and quality control for annotation work. We design simple micro-tasks that involve annotating tex-tual spans with causal relations. We issued the micro-tasks to crowd workers, and collected 95,008 annotations of causal relation instances among 8,745 summary sentences in 1,494 Wikipedia articles. The annotated corpus not only provides supervision data for automatic recognition of causal relation instances , but also reveals valuable facts for improving the annotation process of this task.
Automatic Categorization of Tagalog Documents Using Support Vector Machines Due to the explosive growth of documents in digital form, automatic text categorization has become an important area of research. It is the task of assigning documents, based solely on its contents, to predefined classes or categories.Through time, approaches to this field of study evolved from knowledge engineering to machine learning. In the machine learning approach, the defining characteristics of each document are learned by the model from a set of annotated documents used as "training" data. Such includes Naïve Bayes and Support Vector Machine classifiers.Different standard machine learning techniques treat text categorization as a standard classification problem, and thereby reducing the learning process into two steps -feature selection and classification learning over the feature space (Peng et. al., 2003). Of these two steps, feature selection is more critical since identifying the right features will guarantee any reasonable machine learning technique or classifier to perform well (Scott &amp; Matwin, 1999). However, feature selection is language-dependent.Several preprocessing methods such as stopword removal, lemmatization and root-word extraction require domain knowledge of the language used (Peng et. al., 2003).Methodologies used in researches concerning automatic document categorization are unique from language to language, depending on the structure and morphological rules of the specific language. Although automatic text categorization is becoming a great area of research in most languages aside from English such as Chinese and Arabic, researchers have paid little to no attention in categorizing Tagalog documents. Tagalog exhibits morphological phenomena that makes it a little different than the English language. Thus, this study aims to investigate the factors and explore on different methods that will affect the process of building a Tagalog document classifier. Specifically, this study intends to:· Collect Tagalog news articles and label them according to their category · Represent and extract features from documents using NLP techniques · Build an SVM Classifier · Evaluate classification performance and present results Automatic document classification is now a growing research topic in Natural Language Processing. Several techniques were incorporated to build a classifier that can categorize documents written in specific languages into their designated categories. This study builds an automatic document classifier using machine learning which is suited for Tagalog documents. The documents used were news articles scraped from Tagalog news portals. These documents were manually annotated into different categories and later on, underwent preprocessing techniques such as stemming and removal of stopwords. Different document representations were also used to explore which representation performed best with the classifiers. The SVM classifier using the stemmed dataset which was represented using TF-IDF values yielded an F-score of 91.99% and an overall accuracy of 92%. It outperformed all other combinations of document representations and classifiers.
Extracting Important Tweets for News Writers using Recurrent Neural Network with Attention Mechanism and Multi-task Learning Social media information is now an important source for news writers. People who encounter an incident can post what is happening before his/her eyes using photos and videos. These posts are important primary information, so news writers want to gather them. However, extracting useful information from the vast amount of social media information is laborious. For this reason, services that enable news writers to extract information that can be used as a news source are desired. In fact, some services such as Spectee 1 and FASTALERT 2 have been launched in Japan. These services gather much information from social media and extract information that can be used as news sources.Information that news writers want to extract from social media includes many different topics such as fires, accidents, and other incidents. Therefore, extracting information from social media by filtering with keywords is difficult. Assuming the words "delay" and "train" are included in the keywords, the tweet "xxx line is delayed by accident," which can be used as a news source, can be extracted. However, the tweet "I hope the train is delayed because I haven't studied for today's exam," which cannot be used as a news source, is also extracted. To extract tweets that include important information, filtering by keyword is not enough because the output may include tweets that cannot be used as news sources.For this reason, we have been studying automatic extraction of useful information from social media. Our purposes are to reduce the amount of laborious work and extract information that cannot be extracted by using queries. In this paper, we describe a method to extract tweets that include useful information for news writers. Generally, social media posts are often written in colloquial style and often include abbreviations, slang and emojis. This makes word segmentation difficult. Therefore, our method is character-based approach, not a word-based one. Our method analyzes each character in a tweet by using a Recurrent Neural Network (RNN) and then decides whether the tweet includes important information. We adopted an attention mechanism and multi-task learning in our method and confirmed the effectiveness of our method. Our contribution is to reveal that the combination of attention mechanism and multi-task learning is effective for characterbased approaches. Social media is an important source for news writers. However, extracting useful information for news writers from the vast amount of social media information is laborious. Therefore , services that enable news writers to extract important information are desired. In this paper, we describe a method to extract tweets that include useful information for news writers. Our method uses a Recurrent Neural Network (RNN) with an attention mechanism and multi-task learning that processes each character in the tweet to estimate whether the tweet includes important information. In our experiment, we compared two types of attention mechanism and compared their types with/without multi-task learning. By our proposed method, we obtained an F-measure of 0.627, which is 0.037 higher than that of base-line method.
Discovering Conversation Spaces in the Public Discourse of Gender Violence: a Comparative Between Two Different Contexts In 2010, around 20 people in the United States were being physically abused by a partner every minute (Black, Basile, Breiding, et al, 2011). Gender-based violence is a prevalent problem, even until today: 1 in 3 women have experienced some form of physical or sexual violence worldwide (World Health Or- ganization, 2016). The emphasis on gender points to the context that this violence happens because of unequal power relations between women and men. Gendered expectations and structures of power are passed down and learned through interactions and discussions -discourse datasets are a potential source to analyze for this (Butler, 1988).This study uses principal component analysis, word frequency counts, word associations, and Ngram analysis to compare two different public discourses on gender violence, specifically articles written about the Stanford rape case and the Vizconde massacre. This is done between two sets of discourse that happens in an individualist society(U.S.) and a collectivist society (Philippines). It aims to analyze a conversation space to see what aspect of gender violence discourse appears to be the primary focus -victims, perpetrators, institutions or society as an initial diagnosis of how gender violence is framed in such discourses.People v. Brock Allen Turner(the official name of the legal case of the Stanford rape) began on January 18, 2015 when a college student athlete named Brock Turner was indicted for charges of rape and sexual assault. Turner was convicted on March 30, 2016 for charges of sexual assault. On June 2, 2016, he was sentenced to 6 months of jail. This case raised controversy because of the constant defense of the Turner family, claiming their son's reputation would be ruined, as well as the short amount of time given to Brock Turner for his crime.On the other hand, the Vizconde massacre in June 30, 1991 was a homicide case where one of the victims was raped before being killed. Several men were involved as suspects in the case, including Hubert Webb, Joey Filart, Artemio Ventura, Michael Gatchalian, Hospicio Fernandez and Anto-nio Lejano II. All of them were convicted in regional court as well as the court of appeals. However, the Supreme Court chose to reverse this decision and acquit the men on December 14, 2010. Recent discussion on the memory of the case emerged once more during Lauro Vizconde's death on February 13, 2016.The study is limited to the data of articles about the Stanford rape case starting from when its decision was released on June 2, 2016, until 2 weeks afterward, as well as articles written 6-7 months afterwards. The articles chosen for the Vizconde massacre are the ones written after the announcement of the Supreme Court's reversal and acquittal on December 14, 2010 up to two weeks afterward, as well as articles written 6-7 months afterwards. A huge factor in gender-based violence is perception and stigma, revealed by public discourse. Topic modelling is useful for discourse analysis and reveals prevalent topics and actors. This study aims to find and compare examples of collectivist and individualist conversation spaces of gendered violence by applying Principal Component Analysis, N-Gram analysis and word association in two gender violence cases which occured in the different contexts of the Philippines and the United States. The data from the Philippines consist of 2010-2011 articles on the 1991 Viz-conde Massacre and the data from the United States consist of 2016-2017 articles from the 2015 Stanford Rape Case. Results show that in both cases&apos; conversation space there is a focus on institutions involved in the cases that does not really change over time, and a time-dependent conversation space for victims. Even in two different contexts of gender violence, patterns in conversation space appear similar.
Investigating Phrase-Based and Neural-Based Machine Translation on Low-Resource Settings Recent approaches have shown the promising results in the development of machine translation. During a long period from statistical models ( Brown et al., 1990;Brown et al., 1993) to phrase-based models ( Och et al., 1999;Koehn et al., 2003;Chiang, 2005) to recent neural-based methods (Sutskever et al., 2014;Cho et al., 2014), the phrase-based and neural-based become dominant methods in current machine translation. Statistical machine translation (SMT) systems achieve a high performance in many typologically diverse language pairs ( Bojar et al., 2013). SMT can be applied to any pair of languages with minimal engineering effort (Bisazza and Fed- erico, 2016). Meanwhile, neural machine translation (NMT) has obtained the state-of-the-art performance in machine translation for several languages including Czech-English, German-English, EnglishRomanian ( Sennrich et al., 2016a). NMT has been proposed recently as a promising framework for machine translation, which learns sequenceto-sequence mapping based on two recurrent neural networks (Sutskever et al., 2014;Cho et al., 2014), called encoder-decoder networks. In a basic encoder-decoder network, the dimension of the context vector in the encoder is fixed, which leads to a low performance when translating for long sentences. In order to overcome the problem, (Bah- danau et al., 2015) proposed a method called attention mechanism, in which the model encodes the most relevant information in an input sentence rather than a whole input sentence into the fixed length context vector. NMT models with the attention mechanism have achieved significantly improvement in many language pairs (Jean et al., 2015;Gulcehre et al., 2015;Luong et al., 2015).SMT and NMT models have shown successfully in language pairs in which large bilingual corpora are available such as English-German, English-French, Chinese-English, and EnglishArabic. There are some work that evaluated the phrase-based versus neural-based methods such as the comparison of the two methods on EnglishGerman ( Bentivogli et al., 2016), the comparison on 30 translation directions on the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016). Nevertheless, for low-resource settings like Asian language pairs which contain only small bilingual corpora, there are few work of the comparison of the two methods on such language pairs. Additionally, the problem of unavailable large bilingual corpora causes a bottleneck for machine translation on such languages.In this work, we compared the SMT and NMT methods on several low-resource language pairs. The standard phrase-based SMT was used based on the work of ( Koehn et al., 2007). The NMT model was used based on the state-of-the-art model (Sennrich et al., 2016a) in the WMT 2016, 1 which used encoder-decoder networks with attention mechanism and open-vocabulary translation. Experiments were conducted on Asian language pairs: Japanese-English, Indonesian-Vietnamese, and English-Vietnamese with only small bilingual corpora. Furthermore, in order to overcome the problem of unavailable large bilingual corpora, we extracted a bilingual corpus from Wikipedia to enhance machine translation on both SMT and NMT models. Moreover, we aim to evaluate the effects of enlarging training data to the two different machine translation methods and to the overall performance. Experimental results showed meaningful findings in the comparison of the two machine translation methods on the low-resource settings. This work can be useful as a basis for further development of NMT as well as machine translation in general on the low-resource languages. The scripts, corpora, and trained models used in this research can be found at the repository. 2 Neural-based and phrase-based methods have shown the effectiveness and promising results in the development of current machine translation. The two methods are compared on some European languages, which show the advantages of the neural machine translation. Nevertheless , there are few work of comparing the two methods on low-resource languages, which there are only small bilingual corpora. The problem of unavailable large bilingual corpora causes a bottleneck for machine translation for such language pairs. In this paper, we present a comparison of the phrase-based and neural-based machine translation methods on several Asian language pairs: Japanese-English, Indonesian-Vietnamese, and English-Vietnamese. Additionally, we extracted a bilingual corpus from Wikipedia to enhance machine translation performance. Experimental results showed that when using the extracted corpus to enlarge the training data, neural machine translation models achieved the higher improvement and outperformed the phrase-based models. This work can be useful as a basis for further development of machine translation on the low-resource languages.
Japanese all-words WSD system using the Kyoto Text Analysis ToolKit Word-sense disambiguation (WSD) is a basic procedure of semantic analysis, but it has not been widely used in practice. This is because current WSD systems adopt a supervised learning approach, limiting WSD target words. WSD for all words called "all-words WSD" has been studied for a long time (Navigli, 2009). However, a sense in many allwords WSD systems is defined as a concept, resulting in coarse granularity. Furthermore, the target language is generally English. Japanese all-words WSD has not been achieved, preventing easy access to it. Given this background, we created a Japanese all-words WSD system called KyWSD 1 . KyWSD is 1 KyTea for WSD. useful for several Japanese semantic analysis systems. Using it, we can add sense features when we use a learning method to solve various NLP tasks, thereby improving precision.The substance of KyWSD is a model built using the Kyoto Text Analysis ToolKit (KyTea) 2 , a learning system. By executing KyTea using this model, KyWSD accepts plain Japanese text, segments it into words, and assigns a sense to each segmented word (Neubig et al., 2011). Briefly KyTea is a system learning a morphological analysis model. We build KyWSD using KyTea because all-words WSD can be regarded as a kind of morphological analysis. Therefore, KyTea contains a mechanism for learning a model to adapt to a target domain. The ability to use this mechanism provides KyWSD with high adaptability. For example, adding training data to KyWSD, senses to all words, but a target sense. Thus, KyWSD is an appropriate system for domain adaptation. As seen above, KyWSD provides great value as new use of KyTea.We evaluated KyWSD using a Japanese dictionary task in Senseval-2 (Kiyoaki Shirai, 2001). Adding training data of this task to its original training data enabled KyWSD to perform better than a general supervised support vector machine (SVM) based learning method. This evaluation revealed a peculiar problem of Japanese all-words WSD through which it differs from general WSD. In this paper, we discuss Japanese all-words word sense disambiguation (WSD) and propose a new system KyWSD to achieve it. Ky-WSD uses the Kyoto Text Analysis ToolKit, a learning system building a Japanese morphological analysis model. It accepts plain Japanese text, segments it into words, and assigns a sense to each segmented word. Ky-WSD is open source software that can serve as the baseline system for a Japanese all-words WSD system. Therefore, it can be useful for several Japanese semantic analysis systems and is an advancement in all-words WSD technology. Furthermore, we show that Japanese all-words WSD involves a peculiar problem different from those of general WSD and that KyWSD is adaptable and highly precise .
Language, Information and Computation  
Distances and Trees in Linguistics&apos; This talk is divided in two parts. In Part I, I will discuss the various uses that computers have in linguistics. I will briefly survey how they have been used in the past, and speculate a bit on future applications. In Part II, I will report on some of my own recent research, using computers to study the evolution of languages, especially the languages of China.Part I I will begin with some observations on the term "computational linguistics". In the last decade or two, we see this term more and more in a variety of contexts, in the names of conferences, associations, and even university courses. It is a convenient label, and there is no harm in using it as long as we are clear about what we are labeling.I think it is important to keep in mind that the term "computational linguistics" is not parallel in content with many other terms in linguistics, such as "anthropological linguistics" or "psycholinguistics", or "phonology" or "pragmatics". These terms all refer to content aspects of language; these are, respectively, the role of language in various cultures, the relations between language and psychological mechanisms, how sounds pattern in language, and the use of language in different social contexts. In other words, they all deal with delimited, intrinsic aspects of language on which we are accumulating knowledge.On the other hand, computational linguistics does not deal with any delimited segment of language per se. Rather, computers can be used with profit in all aspects of linguistic scholarship. In fact it is often a measure of the maturity of any aspect of linguistics Professor of Graduate School of the University of California at Berkeley. to see how much computational power it can harness. The more it can harness this power, the more likely it can achieve results which are statistically significant and of cumulative value.We use tape recorders in our fieldwork to preserve the speech sounds of various languages. The linguist who uses this tool has a tremendous advantage over others who rely on pencil and paper alone. For one thing, he has accumulated a database which can be analyzed over and over again, by himself later, as well as by others which wish to verify or build upon his work, and thus make the scholarship cumulative. The support of appropriate tools in all sectors of human endeavor, including, of course, linguistic research. There is a Chinese saying from the Lunyu: i,tiff:Ian14*-ZMMig Roughly translated, this means that to do our work well, we must make sure we have the best tools. And computers are the best tools par excellence for many intellectual tasks.I will dwell on these points at some length partly as a reaction against the excesses of abstract theorizing that have pervaded linguistics these past decades. Linguistics has lost too much of its intellectual resources chasing unicorns such as "ideal speaker-hearers" and "homogeneous speech communities." Anytime we study language in the real world, whether synchronically or diachronically, we find the data inevitably very rich and highly complex. 
Individuals and Modality The modality of natural languages has always been one of the main objects of formal semantics. Because declarative sentences of natural languages are analyzed as a predication to individuals, and normally appear with modality, behaviors of individuals in modal contexts inevitably come into question.In fact, the effort to clarify them has been continuing hundred years since Frege [8,9]. And in the modal logic, researchers succeeded in it to a certain extent, proposed some possible solutions to the problems in the philosophy and ordinary languages. However, such a method is not almighty, suffers from difficulties, e.g. in the interpretation of belief sentences.The reason for the difficulty lies in treating heterogeneous modal phenomena in a homogeneous manner. Therefore, it's necessary to analyze the concept of modality into its parts.They consist of the modality in the narrow sense covered by the traditional modal logic and the modality found in belief sentences. The former is the freedom under the same ontological convention of language, the latter is the freedom under several different conventions. They are called (mere) -modality' and 'multimodality' respectively. The former can be treated by the traditional modal logic or the possible world semantics, but for the treatment of the latter, I introduce the method called 'multi-model'. In this paper, I argue that the modality contained in declarative sentences is heterogeneous by nature, and its indiscernible treatment evoked pardoxical behaviors of individuals in modal contexts. The difficulty is avoided by classifying the modality in two kinds from a normative point of view: 1) modality in a traditinal sense, which is a freedom under a certain convention, and 2) multi-modality which stems from the difference of conventions. The former is treated in the traditional modal logic, but the latter kind of modality which typically appears in belief sentences is captured in another framework called multi-model. This solution deeply concerns the ontology of individuals, therefore the existential presupposition of individuals, too. So, I further propose its treatment by means of a kind of three-valued semantics.
An Analysis of Generic Expressions in Situation Semantics  
Agreement Target Situations * In [1] I put forth a situation-semantic theory of plurality and other phenomena. The purpose of the present paper is to construct a particular application of the above theory to number agreement in English.' After briefly surveying and augmenting [1] (section 2), we propose two theoretical notions, agreement target situations and relevance of situations, with which we analyze number agreement in English. We propose that structural constraints shift agreement target situations in a way governed by the relevance of situations. After introducing these mechanisms (section 3), we discuss examples where the subjects are plural in number but the verbs exhibit singular agreement (section 4). Our analysis will lead us to the conclusion (section 5) that number agreement reflects our knowledge about the structure of the world and the speaker's interests in the discourse. We analyze number agreement in English in the version of Situation Semantics I put forth in [1]. We propose two theoretical notions, agreement target situations and relevance of situations. We propose that structural constraints shift the former in a way governed by the latter. Our analysis shows that number agreement reflects our knowledge about the structure of the world and the speaker&apos;s interests in the discourse.
Comprehending Text: Achieving Coherence through a Connectionist Architecture Language comprehension is mediated by a complex set of processes that operate on the representations at three main different levels: lexical, syntactic and semantic. Early research on aspects of text representation tended to look at general questions about, for example, the availability of representations of surface form. More recently, comprehension is modeled as construction of mental models of text [1]. Several conclusions about mental representations of discourse have been reached. First, such representations are structurally similar to part of the world rather than to any linguistic structures [2]. "Distilled" meanings of the text and their interrelations are represented directly. This representations are built up as the text is read. Information not explicit in a text may be included in the representations of its content, particularly if it is required to establish links between parts of its content [3]. At any point in a text, the representation constructed up to that point is the context for the interpretation of the next sentence. In particular, it restricts the set of possible referents for anaphoric expressions, and allows such expressions to be interpreted correctly in context [4]. Given this agreement as to the theoretical importance of mental models, however, it is surprising that there is little agreement as to exactly what constitutes a mental model, and there is little research demonstrating the construction of a putative mental model while reading. In this article, our specific goal is to delineate a theoretical connectionist system that generates the cognitive representation for narrative comprehension. In particular, attention has focused on the factors of argument overlapping [5] and situational continuity [6]. Researches in discourse processes have shown these two major factors facilitate readers to construct the mental models. Argument overlapping is analyzed in terms of the number of arguments overlapped. Propositions having the greatest degree of overlapping with other propositions are proved to be the most important. In addition, Gernsbacher [7] has advocated that readers construct mental structures while reading a text and they try to map any incoming information onto the evolving structure. Sentences that maintain a previously established time frame are more likely to be mapped onto developing structures, as are sentences that maintain a previously established location and sentences that are logical consequences of a previously mentioned action or event. For instance, it is relatively easy to construct a representation of a situation that involves a chronologically ordered chain of causally related events and action in one location. In such case, incoming situational information can be readily be integrated with the current mental model. Applying connectionism in discourse processes has received a great deal of attention recently [8]. There are many extremely promising aspects of connectionist work that map quite well with the perspective developed within the spreading activation network. Spreading activation network is represented by a set of nodes fully connected to each other. It is a pattern categorization device inspired by neurophysiological considerations. The network has additionally been applied to fairly diverse range both in neural and psychological phenomena [9]. It provides a useful model of human learning and concept formation [10]. In this article, a brief discussion on how a sentence is distilled in a unification process is first presented. Second, a spreading activation network, which nodes are the propositions and the links are derived from the effects of argument overlapping and situational continuity, is formed and trained from the narratives. It is based on the theory that raw linguistic knowledge is said to undergo a reduction to essences and are encoded for storage and recall. The essences are often used to refer the residue of information that remains after a delay. Human inferences are the predisposition of reasoning to operate on traces that are as near as possible to the essences formed from past experience [11]. Comprehension is determined by the content of memory. The verbalized concepts reflect the information that is in the focus of attention. Mental models of text must be formed in order to achieve coherence in language understanding. Constructing a mental model requires continual interaction between the text and the reader&apos;s linguistic, pragmatic, and world knowledge. In this article, a connectionist framework is proposed for the formation of mental models that occurs during comprehension. A unification and spreading activation model are used to simulate the processes. The system is tested using children&apos;s stories. Results attest the validity of the model. Thus, our design contributes to the formation of a durable and functional mental representation of the text information.
Predication of Meaning of Bisyllabic Chinese Compound Words Using Back Propagation Neural Network The processing of the meaning of a word is a difficult job. In all existing text books on semantics [2][3][4], only descriptive treatments are provided for explaning the meaning of a word. There have been some attempts to break meaning into more fundamental units (Chapter 6 of [2]) but without much success. The main obstacle is that we are still unable to quantify meaning. As there is no proper representation of meaning, it is also difficult to process meaning.In some of our earlier papers [5][6][7], we have proposed a scheme to quantify the change in the meaning when a bisyllabic Chinese compound word is built from two Chinese characters. We take the meaning of a word as one of the meaning class given in a dictionary Figure 1 Semantic Distance of meaning classes: riq X it -33 ifa [8]. This dictionary classifies 70,000 Chinese words into Characters Between Words and Ch 12 major, 94 medium and 1428 minor classes. A triangular figure is used to represent the change in meaning during a word formation process(See Figure 1). In Figure 1, W is the word, Ca, Cb are the characters forming the word, Owa, 6wb , Oab are the so-called semantic distances between the word and the characters and between the two characters. The value of 6 is computed according to Table   1. We found that many Chinese bisyllabic words are formed as so-called biased type of words (these are the so-called .1, )p* type of words). Examples are(See Table 2): Traditionally, vA%, %--'e are classified as.1-0411(biased compound word) and A.1, obit are fi*ifiNfl(complement compound word).Our question here is that: Is there any way to predict where the meaning of a word will fall, based on the meaning of the characters that it is made up? In [5], we derive a parameter called semantic strength to do such a prediction. The semantic strengths for the characters, a and b are computed as:As a character will normally combine with a large number of other characters to form words, there are many values of Sa and Sb. We can therefore calculate the overall semantic strength by averaging all the Sa and Sb. This is:n We can consider s one of the basic semantic attribute of a character. We computed s for all characters and use it to predict the meaning of the word. In [5]. It was found that when sa &gt; sb , the word formed ( W= ab) will take a meaning class that is closer the meaning of the first character. This prediction is followed for 73.8% of the total cases. It is therefore obvious that the meanings of the characters, in one way or another, determine the meaning of the words that they form.In this paper, we will like to further investigate into the various other factors that determine the meaning of the compound words. These include: (i) meaning classes of the characters, (ii) entropy values of characters, and (iii) semantic strength of characters. The simulations is performed on a three-layer back propagation neural net (BPNN). BPNN is being selected as it has the ability of producing fairly complicated functional parameters between its input and output neurons.The following sections of this paper cover: In Section 2, we give a brief description on the BPNN that is built for the prediction of meaning of bisyllabic words. In Section 3, we present the results obtained for various sets of input. The training and testing if the BPNN is given in Section 4 and In Section 5, we present our conclusion and some proposals for further research in this area. A three layer back propagation neural net is set up to study the functional dependency between the semantic class of a bisyllabic Chinese word and that of its two constituent Chinese characters. Simulations were performed using a three-layer back-propagation neural net with various combination of inputs. The inputs are (1) semantic classes of the constituent characters, (2) Entropy of the characters and (3) semantic strength[1] of the characters. Our simulations show that we can obtain the meaning class of a bisyllabic word from the meaning classes of its two constituent characters to an accuracy of 81 % by taking the semantic classes and semantic strength of the characters as input. This research establishes the dependency between the meaning class of a Chinese compound word and that of its two constituent characters.
DATIVE SHIFT IN CHINESE AND ENGLISH: A LEXICAL MAPPING ACCOUNT  This paper proposes two revisions to the Lexical Mapping Theory as part of UG and accounts for dative shift and the interaction between dative shift and passive in Chinese and English. The overall strategy is to maximize the universality of the Lexical Mapping Theory by allowing only morpholexical operations to be language-specific. 0. BACKGROUND This paper applies the Lexical Mapping Theory (LMT) in recent developments of Lexical-Functional Grammar (LFG) as part of Universal Grammar (UG) [e.g., 1, 2, 3] to account for the dative alternation (1-2) in Chinese and English.
A CONSTRAINT-BASED LEXICAL APPROACH TO &quot;FLOATING&quot; QUANTIFIERS&apos;  In this paper, I present a new nonderivational analysis of the so-called &quot;floating&quot; quantifiers (FQs, hereafter) in English and Korean within the framework of Head-Driven Phrase Structure Grammar (HPSG). 2 I show that FQs do not move from place to place, arguing that English FQs are adverbs modifying verb phrases whereas Korean FQs are complement NPs. I review a derivational approach to English FQs, classical and conventional, revealing some of their problems in Section 1. Then I proceed to propose my own constraint-based, lexical approach to English FQs in Section 2.1. In Section 2.2, I discuss some consequences of the lexical approach, especially involving certain different behaviors of FQs in Tough, Raising, and EQUI constructions. In Section 3, I treat Korean (and some Japanese) data, and attempt to improve upon [1] and the relevant part of [2] on some particular details, while supporting their basic theoretical merits. After some discussion of parametric differences between English and Korean, Section 4 concludes the paper. 1. The Derivational Approaches In classical transformational grammar, FQs like all, both or each in (1-3) below are originated from subject NPs, and so (1), (2) and (3) are derived by a movement rule from (4a), (5a) and (6a) respectively. (See [3] (291-321) for an earlier formalization of this approach.) (1) The boys all left the girls. (2) The boys both left the girls. (3) The boys each left the girls. (4) a. All of the boys left the girls. b. All the boys left the girls. (5) a. Both of the boys left the girls. b. Both the boys left the girls. (6) a. Each of the boys left the girls. b. *Each the boys left the girls. The movement rule should move the quantifier, which is the head noun of the subject NP, and place it at some place in the VP, and subsequently the preposition of should be erased. This kind of structure-destroying movement rule will not be theoretically permissible in any framework today including even the GB theory. Thus more recently, [4], among others, presented a GB account of the problem. (7) D-Structure: [SPEC [vp all (of) the boys left] S-Structure: The boys [vp all ti left] On this account, it is the subject NP, not the quantifier, that moves and the movement direction is leftward, in contrast to the rightward direction in the classical transformational account. The subject NP (the boys in (4a), for example) originates from 67
Scrambling in German -Extraction into the Mittelfeld There are two basic ideas how to describe scrambling in languages with relatively free constituent order in certain syntactic domains. Firstly, one can assume that a kind of movement takes place, i.e., there is a position in a string where something is missing (a trace) and there is a corresponding position at another location in the string where the missing constituent appears. The alternative is to allow constituents to appear in any order in some particular domain. This domain usually is the domain of the head of a phrase. In HPSG [4], order variation is commonly associated with ordering variations among sister constituents in a flat structure. This concept was extended by Mike Reape [6] to allow for complex domain formation operations which-in his approach-are driven by a feature called UNIONED. In the combination of signs, a functor can specify the UNIONED value of its arguments. The functor is either the head in a head-complement structure or the adjunct in a head-adjunct structure. If one allows adjuncts to domain-union with their heads, the fact that adjuncts can appear at any position between complements in the Mittelfeld can be accounted for.In the following, I will give an account that employs both word order domains and the NONLOCAL-mechanism provided by HPSG. I will not use the UNIONEDfeature suggested by Reape since it can be shown that the clause union phenomena which Reape describes with domain-union can be accounted for with argument attraction along the lines of Hinrichs and Nakazawa [1]. German is a language with a relatively free word order. During the last few years considerable efforts have been made in all syntactic frameworks to explain so-called scrambling phenomena. In the following paper, I deal with some tough cases of German word order which cannot be described by assuming flat sentence structures or word order domains. The phenomena discussed are PP complements of nouns and adjectives , which can appear separated from their heads in the German Mittelfeld, and stranded prepositions. The similarity to fronting of these elements is used to explain these phenomena by a generalized version of the head-filler schema used in the standard HPSG framework.
Crosslinguistic Notions of (In)definiteness * One morning, Alice tells (1) to her husband, John, who doesn't read newspapers. Her use of the president is dependent on the uniqueness of president in the fixed domain she is talking about (i.e. the U.S. politics situation). Let's call it a Russellian domain (R-domain) and such a use of definite description Russellian definiteness (R-definiteness).(1) Hey John, the president resigned!! In the evening John describes a scene he saw in the afternoon with (2). Clearly, the described scene contains at least two dogs, thus the uniqueness of dogs on which his use of the dog is dependent is not w.r.t. the scene as a whole, but rather w.r.t. a domain dynamically constructed by his first sentence. Let's call such a domain a Heimian domain (H-domain) and such a use of definite description Heimian definiteness (H-definiteness).(2) I found a dog on the lawn. The dog was biting another dog An R-domain r is something out there, not necessarily within our comprehension in its totality. An H-domain h is the part of such an r which is known, or already familiar, to the conversation participants, which grows as the conversation proceeds. Then, assuming the lattice-theoretic ontology of [1], we say the denotation of a definite description is max (d, p), where max is a function that maps an R-or H-domain d and a property p to the maximal individual which has p in d; the uniqueness, then, follows from the singular morphology of the NP.The Russellian/Heimian distinction (hereafter, the R/H distinction) is a result of the choice of the domain (cf. [2]). Regarding such domains as resource situations in the situation semantic sense, we argue for the significance of the R/H distinction for English and other languages. We argue that both Russellian and Heimian definites exist in natural languages. Our account captures both the commonality and difference between the two. Further, we suggest that the Russel-lian/Heimian distinction extends to indefinites too.
Construction as a Theoretical Entity: An Argument Based on MandarinExistential Sentences  The role that constructions play in a linguistic theory has changed throughout the evolution of generative theories. Construction specific rules are common when transformations are envisioned as tree to tree operations in classical TG. On the other extreme, constructions, as well as all structural properties, are regarded as derived linguistic properties predictable from various principles in recent GB theories. Since whether a construction is an autonomous linguistic entity or not has great implications for either a formal or a computational linguistic theory, we will examine the status of Mandarin existential construction based on the theory of Construction Grammar [1,2]. We will show that the Mandarin existential construction represents an unique structure-meaning pair that cannot be captured in a grammar unless the pairing is regarded as a theoretical entity in linguistics ([3]). Since constructions are shown to exist in Mandarin Chinese, we support the theoretical claims of Construction Grammar as well as the position that constructions must be taken into account in NLP. I. Existential constructions and its two sub-constructions The existential sentences in Mandarin Chinese, such as (1) and (2), have been traditionally considered to involve movement or lexical rule in a verb-centered paradigm. (1) Zhuo-shang fang LE yi ben shu. table on put ASP a CL book &apos;There is a book on the table&apos; (2) Chuang-shang tang ZHE yi ge ren. bed on lie ASP a CL person &apos;There is a person lying on the bed&apos; The surface structure of these existential sentences can be schematized as follows: (3) [ Locative V Theme ] The apparent structural uniformity represented by (3) gives the verb-centered accounts their strongest motivation. This is because a verb-centered account can easily predict the structure by the cross-the-board nature of the movement rule or the lexical projection rule. However, there are two facts involving the existential sentences that would prove difficult for verb-centered accounts but could be easily predicted by a constructional account. First, the existential meaning is manifested only when the structure schema of (3) is employed. In other words, if the existential sense is attributed to the verbs, the grammar will be overloaded with unnecessary ambiguities in structures other than (3). The verb-centered 91
Clitic Analyses of Korean &quot;Little Words&quot;  In Korean, there are many little words that can best be analyzed as clitics: The copula i-and the adjectival ha-, as well as some nominal and verbal particles. Among others, the clitic analysis of adjectival ha-leads to a unified account of light predicate constructions.
A Cognitive Account of the Lexical Polysemy of Chinese Kai It is very common for a single word to have multiple meanings. Take the Chinese word zou 'walk', for example. It is considerably different for Zhangsan zou banmaxian 'Zhangsan to walk the zebra crossing' and for Zhangsan zou niourou shengyi 'Zhangsan to run a beef business'. This single word with two senses is so related that we would hardly discern the difference. Such case is called polysemy, which is entered once but not separately in the dictionary with its multiple meanings in lexicography. But not all senses of a polysemous word look closely related in a systematical way, for example, Apparently, kai in these examples is in several senses, i.e., the actions described by the verb kai are very diverse. What do they have to do with each other? According to the classical theory of categories, things are in the same category if only if they have certain properties in common. These properties are necessary and sufficient conditions for defining the category. However, multiple senses of a polysemous form do not seem to share objective truthcondition but they constitute a single lexical item, as the above examples show. Its relatedness of meaning cannot be completely made explicable in terms of a componential analysis of senses, because the use of common semantic features fails to specify how many components, or what kind of components, two senses must share in order for them to meet the criterion of relatedness of meaning (for details, see [7]:552-553). Polysemy has always been problematic to traditional truth-conditional semantics ( [4,9]).This study is an attempt to find a solution for the possible groupings of senses observed in a polysemous word kai. The following investigation on kai will illustrate that the existence and properties of polysemy follow directly from the characteristics of human cognition, that is, people tend to group things together not only by analogy, i.e., the properties they have in common, but also by cognitive strategies such as metonymy and metaphor. What we will try to show is that there is a coherent conceptual construction underlying all these expressions, and that much of it is metaphoric and metonymic in nature. Since polysemy has multiple but related senses, finding any coherent system would seem impossible. But its senses are not random. When we look at inferences among them, it becomes clear that there must be a systematic structure of some kind. Based on the prototype theory, which views lexical items as constituting natural categories of senses, the present work aims at proposing solutions to problems resulting from the polysemy of kai. After an in-depth analysis of the polysemy kai, we find that the links between polysemy senses are defined not only by shared properties (i.e. analogy), but also by conceptual connections (e.g. metonymy or metaphor). This study suggests that polysemy reflects human&apos;s categorization of things and can be successfully accounted for by a cognitive approach.
Automatic Sense Disambiguation for Target Word Selection The problem of word sense disambiguation is one which has received increased attention in recent work on natural language processing applications such as machine translation and information retrival. Given an occurrence of a polysemous word in text, it is needed to examine a set of senses in the dictionary and detect the clues for deciding intended sense in the context. Recently, several researches have been experimented with target word selection based on example-based machine translation [1,2,3}. There is no failure of selecting target word because the method is to select most similar one in a thesaurus than exact correspondent. However, the similarity methods based on thesaurus have some problem that it is difficult to make good thesaurus which provides accurate distance measure between words.In order to solve the imperfection problems of disambiguation methods based on similarity, some researches using the exact matching with collocation extracted from bilingual corpus were experimented by [4,5]. But, these methods have a drawback that they can not select a target word when input sentences have nouns which were not included in collocation list because they need exact matching of collocation. And it is not clear how large the bilingual corpus would have to be become authoritative for disambiguation as an application independent collocation.The well-known attempts to utilize information in MRD(Machine Readable Dictionaries) for lexical disambiguation are that of [6,7,8] which select the correct sense of a word by counting the overlaps between a dictionary sense definition and the definition of the nearby in the phrase. The sense of a word with greatest number of overlaps with senses of other words in the sentence is chosen as the correct one. These methods based on a small number of overlap between sense definitions is weak as a clue for disambiguation. As a result, these methods could not have certainty about disambiguation, because the relation is wholly dependent on word usage in the sense definition of a particular dictionary. Another variations of this idea are based on co-occurrence statistics, which is meant the preference two words appear together in the same context [9,10,11] . But, their method has same problems of MRD approaches.Our method is based on the idea that it selects the sense combinations of words as the correct sense with the greatest number of overlaps between input case slots and the predefined clusters which are grouped according to corresponding target word. This paper describes a method of automatic sense disambiguation for target word selection in Korean to English machine translation. At first, we define the concept of cluster for each sense of given verb according to corresponding target word. And then, we propose a method which selects the sense combination of words as the correct sense that has the greatest number of overlaps between input case slots and the predefined clusters for the given verb.
Structual Ambiguity and Conceptural Information Retrieval Many researches found lexical preferences to be critical in resolving attachment ambiguity. Jenson and Binot [JB 87] propose to use dictionary definition for disambiguation. [HR 93] describe how co-occurrence of verbs and nouns with prepositions in corpus can be used as an indication of lexical preference. However, dictionary text especially the definitions are typically uneven in their coverage and inconsistent in the usage of words, while lexical cooccurrence suffers from sparseness of data. Resnik and Hearst (1994) find that using class information did not yield improved performance over lexical association due to multiplicity of classes for a word and lack of disambiguation.Our proposal is to use sense-disambiguated, sizable body of text as the source of lexical preference. Thus, for example, in the sentence 'He woke up to find the house on fire,' one of the word senses of the noun 'fire' occurs frequently in the context of the words which are conceptually similar to 'house.' This is evidence of a conceptual association of the direct object`house object`house' with the prepositional object 'fire.' Unlike in previous proposals [HR 93][RH 93], these co-occurrences of concepts need not in the Verb-Object-Preposition relation. Many researches found lexical preferences to be critical in resolving attachment ambiguity [WFB 90][FBK 82][ MP 80]. Most notably, information from Verb-Obj-Prep-Noun structures (VOPN) has been used to show that LA is very effective in the resolution of PP-attachment ambiguity [HR 93]. We investigated extensions to the lexical association strategy. The extensions include using conceptual association and acquiring the association information from different kind of lexical relations not limited to relations in VOPN structures. We refer to this approach as DeepAttach. Thus, it is possible to take information from all kinds of syntactical structures as long as they are alternations of a common deep structure [PU93] related to that implied by the intended attachment. A collection of sense-disambiguated sentences serves as the source of conceptual relations. No pre-processing is done to find the conceptual relations in these sentences. Instead, information retrieval technique is used to retrieve conceptually most relevant sentences using the words from the ambiguous structure as query. The prepositional phrase is then attached in favor of the constituent that has more conceptual presence in the ranked retrieved sentences. An experiment was implemented to embody the idea. The result shows that 75% of PP&apos;s in 260 VOPN structures can be attached correctly, when simple lexical relevance was considered.
Ambiguity Resolution in Chinese Word Segmentation*  A new method for Chinese word segmentation named Conditional F&amp;BMM (Forward and Backward Maximal Matching) which incorporates both bigram statistics (i.e., mutual information and difference of t-test between Chinese characters) and linguistic rules for ambiguity resolution is proposed in this paper. The key characteristics of this model are the use of: (i) statistics which can be automatically derived from any raw corpus, (ii) a rule base for disambiguation with consistency and controlled size to be built up in a systematic way.
The Postprocessing of Optical Character Recognition based on Statistical Noisy channel and language model Recently, the research of human-machine interface is becoming more and more important. To reduce the workload of user entering text and data into a computer system, there are many human-machine interfaces developed, such as optical character recognition (OCR), speech recognition, etc. However, some errors are usually introduced in the recognition process. Therefore, means for finding and correcting such errors in indispensable.Research in character recognition for Chinese faces more difficulties than for other languages. Firstly, Chinese has a huge character set. Secondly, Chinese characters have more complex structure than alphabetic characters and there are a large number of similar character groups. Therefore, we need some kind of contextual information to detect and correct errors. Language models can be used to provide such contextual information to enhance the correction rate and speed of a man-machine interface system. For example, Tsuyoshi [14] use character frequency and the morphological analysis to improve the handwritten Japanese OCR system. Recently the techniques of Chinese OCR have advanced greatly, the recognition rate has reached 95% and 90% for printed and handwritten text respectively. In view of the limitation of the techniques of OCR, we hope to use natural language models to detect the 5 to 10% errors and suggested their possible correction in a postediting environment. With an effective postprocessing technique and user-friendly posteditor, the usability of OCR technology can be greatly enhanced.2. In this paper, we have adopted a multi-stage postprocessing approach. In the detection stage, the statistical model is used to re-evaluate the confidence of the error counts provided by the image model. The purpose of this stage is to identify the places where the first candidate is wrong. In correction stage, we combine the noisy channel and language model to suggest possible corrections. The techniques of image processing have been used in optical character recognition (OCR) for a long time. The recognition method evolved from early &quot;pattern recognition&quot; to &quot;feature extraction&quot; recently. The recognition rate is raised from 70% to 90%. But the character by character recognition technique has its limitation. Using language models to assist the OCR system in improving recognition rate is the topic of many recent researches. Recently, the related research on Chinese nature language processing has improved rapidly. These improvement include the Chinese word segmentation, syntax analysis, semantic analysis, collocation analysis, statistical language models. In this paper, we will propose a new techniques for Chinese OCR postprocessing and postediting. We combine noisy channel model and the technique of natural language processing to implement an OCR postprocessing system. From the result of experiments, we found noisy channel model very effective for postprocessing. Under the approach, it is possible to recover the correct character, even when it is not in the candidate list produced by the OCR system.
A Quantitative Analysis of Word-Definition in a Machine-Readable Dictionary Many (English) dictionaries (e.g. Collins [1], OALD [2], Longman [3]) have been made available online but they have rarely been used as data in quantitative analysis. Preliminary analysis [4] such as compiling word usage frequency in word definitions, thereby compiling conceptual primitives has been carried out. A notable exception is the mathematical modeling of word length distribution [5]. In this paper, we focus on a quantitative analysis of word definitions. In particular, we examined:1. the ranked-frequency distribution of words in the definitions; 2. the length distribution of word definitions; 3. the frequency distribution of the number of unique tags in word definitions; 4. the coverage statistics of words in dictionary definitions.These quantitative descriptions facilitate a more objective comparison between different dictionaries than merely the number of entries in a dictionary. Ultimately, they can serve as part of a more comprehensive evaluation metrics for dictionaries. For example, word length distribution may indicate whether definitions are given comparatively concisely or brief. Unusual distribution of the number of unique tags might indicate bias or undersampling of the data which might be intended. Poor coverage characteristics might be due to less adherence in using a control vocabulary.Furthermore, these quantitative descriptions can be used to guide the construction of computational lexicon. For example, coverage statistics can be used to determine the size of the control vocabulary in analyzing word definitions as in [6]. Apart from applications, quantitative analysis provides exploratory data for quantitative modeling, giving a more comprehensive description of the phenomenon at hand. The quantitative model of word length distribution [5] is a good example. This paper investigates some of the distributional properties of word definitions in a machine-readable dictionary which • was obtained from the Oxford Text Archive. Three types of distributions were examined: (1) frequency-ranked distribution of words in the definitions, (2) the length distribution of word definitions and (3) the frequency distribution of the number of unique tags of an entry. In addition, the coverage characteristics of headwords over word definitions are also explored. A rough-and-ready comparison of distributional properties between tokens and their morphologically decomposed ones are made. Our result shows that morphological dcomposition does not change the length distribution of the word definitions nor the ranked-frequency distribution of words, significantly. However, it increases the coverage of word definitions dramatically compared with no decompositions. Furthermore, the frequency distribution of the number of unique tags per entry is approximately linear when the data is suitablely scaled (i.e. linear or logarithmic).
Metaphorical paradoxes: A window on the conceptual system Understanding metaphors is a crucial part of understanding language. If one says 'Let me stew over that for a while' or 'We need to let that idea percolate' these sentences could be quite confusing if the hearer does not understand the metaphor in English that IDEAS ARE FOOD.Understanding metaphors is also an important aspect of understanding the conceptual systems of the language speakers. When the TIME IS MONEY metaphor is found in a language (i.e. Don't waste my time; How much is my time worth to you?) it necessarily follows that these speakers have an economic system where an individual is paid for labor according to the amount of time spent on the labor.One aspect of understanding metaphors that has not been extensively explored is the fact that both within and across languages there are many contradictory metaphors. Lakoff [I] demonstrated that inconsistencies in the metaphorical system of the Self exist for both English and Japanese. The fact that these paradoxes exist may inform us about the human conceptual system. For example, if two languages share the same metaphorical paradoxes even though they do not share the same cultural background, 'these metaphors are tapping into some sort of real human experience' [1]. What Lakoff is not able to explain, however, is what the 'real human experience' is that creates these inconsistencies that are inherent in metaphors referring to the Self In this paper I will review the inconsistencies surrounding the English metaphor of the Self and then I will demonstrate that, 1) similar metaphorical paradoxes concerning the Self exist in Chinese and Japanese and 2) these paradoxes occur in English, Chinese and Japanese because the Inner Self (our internal voice) is being compared either with the Social Self (how we interact with others) or the Physical Self (our body). Thus the paradoxes reflect what the psychologist William James referred to in 1892 as the 'spiritual me, material me, and social me' [2]. In addition, I will argue that these paradoxes mirror phenomena found in brain damaged patients. In short, the linguistic and neurological findings point to a strikingly similar portrait of the Self, and argue for studying metaphorical paradoxes in greater detail in order to gain insight into other aspects of our conceptual framework. This paper reviews the inconsistencies surrounding the English metaphor of the Self and demonstrates that 1) similar metaphorical paradoxes concerning the Self exist in Chinese and Japanese, and 2) these paradoxes occur in English, Japanese, and Chinese because the Inner Self (our internal voice) is being compared either with the Social Self (how we interact with others) or the Physical Self (our body). Thus the paradoxes reflect what the psychologist William James referred to in 1892 as the &apos;spiritual me, material me, and social me&apos;. In addition, these paradoxes mirror neurological phenomena found in brain damaged patients. In short, the linguistic and neurological findings point to a strikingly similar portrait of the Self, and argue for studying metaphorical paradoxes in greater detail in order to gain insight into other aspects of our conceptual framework.
PREDICATE-ARGUMENT STRUCTURE OF ENGLISH ADJECTIVES AKIRA IKEYA TOYO GAKUEN UNIVERSITY  This paper will argue the following points. 1) What is a semantic status of prepositional phrases in such sentences as &quot;Tom is good at tennis,&quot; &quot;It is wise of Tom to go there,&quot; &quot;This book is easy for John to read. &quot;? 2) What is the semantic status of to infinitive (henceforth, to VP) in the following sentences : &quot;He is honest to bring back the money,&quot; &quot;He is sure to win.&quot; 3) Is there a semantic difference in such sentences as &quot;It is wise of Peter to go home&quot;, &quot;Peter is wise to go home?&quot; If there is any, what is it? 4) What is the predicate-argument structure of a sentence containing a predicate adjective like &quot;John is easy for Mary to please?&quot; Is it one-place, two place or three-place-structure? 5) By introducing the notion of a predicate modifier we can solve these problems. 6) We conclude that adjectives are simply a one place predicate. 1. The Semantic Structure of Adjectives 1.1 Three Dimensions In English as well as in Japanese there is a group of what is called degree adjectives whose interpretation is heavily dependent on contexts, pragmatic or linguistic. One of such contextual factor is termed THEMATIC DIMENSION by Bartsch [1]. In addition to this dimension it was proposed in Ikeya [4] that it is necessary to recognize two other such dimensions, which are termed COMPARATIVE DIMENSION and DEGREE DIMENSION. It is only after these three vectors are specified, is it possible to determine the truth condition of a sentence which contains a degree adjective. When we say he is good, this sentence has to be specified in what respect he is good, as compared to what he is good, and to what degree he is good For example, in He is very good at basketball for a short Japanese all these dimensions are expressed: at basketball is what we call THEMATIC DIMENSION (TD), for a short Japanese is a so called CD, and very is our DEGREE DIMENSION (DD). 1.2 TD in English Adjectives In English TDs have the following varieties. (1) a. John is good at tennis. b. John is fine in terms of health. c. John is blind of one eye. d. John is quick at words. e. John is cautious with respect to the standard theory. As these examples show, in English TDs are expressed by such expressions like in terms of as regards, or other prepositional phrases headed by of at, about, off in and the like. All these expressions give a semantic specification to adjectives in what respect John is good, fine or quick. It should be noticed that all these expressions grammatically correspond to adverbials. It should be remarked that TD is not obligatory. In such sentence as the business is very slow no TD is expressed. 149
Prototype Theory and Case Assignment  Case assignment is one of the most important issues in theoretical linguistics. The goal of this paper is to describe both syntactic and semantic bases for case assignment in terms of Prototype theory [1, 2] and to give an explanation for the difference in case assignment between English and Japanese by examining the Instrument Subject construction and deverbal nouns. English allows the structural case assignment under the syntactic prototype, while Japanese does not, because it has a strong requirement for the semantic prototype.
HMM Parameter Learning for Japanese Morphological Analyzer Morphological analysis and part-of-speech tagging is an important preprocessing especially for analyses of unrestricted texts. We have been developing a rule-based Japanese morphological analyzer called JUMAN [8]. The rules are represented as costs to lexical entry and cost to pairs of adjacent parts-of-speech (connectivity cost), which are manually assigned. The cost for a lexical entry reflects the probability of the occurrence of the word, and a connectivity cost of a pair of partsof-speech reflects the probability of an adjacent occurrence of the pair. Greater cost means less probability.Since those costs vary according to the domain of texts, it requires much effort to estimate them for texts of a new domain. Some statistical methods have been proposed for part-of-speech tagging of English and other Indo-European languages. Church [4] proposed a method to use trigram probabilities obtained from tagged Brown corpus and achieved over 95% precision in English part-of-speech. tagging. Cutting [5] used Hidden Markov Model to estimate probability parameters for the tagger and achieved 96% precision. This experiment was done on Figure 1: Sample result of Japanese morphological analysis a large scale untagged text. Statistics works well for part-of-speech tagging of a language like English since words are separated by spaces and word order is comparatively more restricted than free word order languages like Japanese and Korean. We have pursued a similar approach based on HMM for Japanese partof-speech tagging, resulting in a poor performance. The reason is that Japanese sentences do not have word separators, thus, word boundaries are not clear, causing spurious ambiguity in word segmentation. Chang and Chen[l] applied HMM to part-of-speech tagging of Chinese. However, they assumed a word-segmented corpus for the training data. We do not assume a large scale tagged corpora. The reasons are the following:1. It is not easy to get a large scale tagged corpus, especially because there is no standard set of parts-of-speech for Japanese language. There is even no consensus on the definition of morphemes. 2. The probabilities of word occurrences and connectivities may vary according to the domain of texts. This necessitates to provide a tagged corpus virtually for each domain.This paper describes how the difficulties in Japanese morphological analysis are overcome by the use of the HMM parameter learning. We put a special emphasis on the effect of the initial probabilities and some domain-independent grammatical constraints. By grammatical constraints we mean pairs of parts-ofspeech or morphemes which never occurs in real texts.Our Japanese morphological analyzer JUMAN and its relationship to HMM are introduced in the next section. Then, the effects of the initial probabilities and grammatical constraints are described by giving some experimental results. This paper presents a method to apply Hidden Markov Model (HMM) to parameter learning for Japanese morphological analyzer. We especially emphasize how the following two information sources affect the results of the parameter learning: 1) The initial value of parameters, i.e., the initial probabilities and 2) some grammatical constraints that hold in Japanese sentences independently of any domain. First and foremost, a simple application of HMM to Japanese corpus does not give a satisfactory results since word boundaries are not clear in Japanese texts because of lack of word sepa-rators. The first results of the experiments show that initial probabilities learned from correct tagged corpus affects greatly to the results and that a small tagged corpus is enough for the initial probabilities. The second result is that the incorporation of simple grammatical constraints works well in the improvements of the results. The final result gives that the total performance of the HMM-based parameter learning achieves almost the same level as the human developed rule-based Japanese morphological analyzer.
Automatic Acquisition of Class-based Rules for Word Alignment Much of the recent interests in bilingual corpora was initiated by Brown et al. (1990). They advocated a new approach to machine translation in which the bilingual corpora are aligned to reveal the mapping between text in one language and its translation in another language. This mapping is formally represented as statistical machine translation model. This model can be understood as a word by word model for generating (language model) of simple sentences S and translation (translation model) S to a sentence T in another language. The i-th word(s) s in S is considered to connect with its translation, the j-th word(s) t in T. Under the model, the probability of the connection (s, t) can be obtained by considering three aspects: lexical translation (t(t I s), the relation of s producing t in translation), fertility O('d*, the relation of change in number of words), and distortion (dis (i I j,1, m), the relation between the position i,j of s, t and the respectively length 1, m of S, 7). Probabilities are associated with these three relations of connection. A bilingual corpus annotated with these connection information can be utilized to estimate the parameters in a statistical translation model. Subsequently, the model can be used together with a bigram or trigram language model in machine translation.We present a rule-based algorithm for word alignment. We refer to this algorithm as SenseAlign. It relies on an automatic procedure for the acquisition of class-based rules. It does not employ word by word translation probabilities to identify alignment; nor does it -use a lengthy iterative EM algorithm for finding such probabilities. The algorithm attempts to handle the problem of undersampling by approximating word-to-word translation probability using hierarchical classifications of words. The translation probabilities are estimated using class-based rules on different levels of specificity. We found that the algorithm can provide translation probability for more source-target word pairs at the cost of slightly lower degree of precision. That allows the algorithm to work in situation where only a small bilingual corpus is available. We have achieved an application rate of 81.8% and precision rate of 93.3%. Since the rules are all based on word sense distinction, word sense ambiguity is also resolved in the process of alignment.The paper is organized as follows. In the next section, we describe SenseAlign and discuss its main components. We give the results of inside and outside tests in Section 3. In Section 4, we compare SenseAlign to several other approaches that have been proposed in computational linguistic literature. Finally, in Section 5, we consider ways in which our present methods might be extended and improved. In this paper, we describe an algorithm for aligning words with their translation in a bilingual corpus. Existing algorithms require enormous bilingual data to train statistical word-to-word translation models. Using word-based approach, frequent words with consistent translation can be aligned at a high precision rate. However, less frequent words or words with diverse translations usually do not have statistically significant evidence for confident alignment. Incomplete or incorrect alignments consequently result. Our algorithm attempts to handle the problem using a hierarchical class-based approximation of translation probabilities. The translation probabilities are estimated using class-based models on 3 levels of specificity. We found that the algorithm can provide translation probability for more word pairs at the cost of slightly lower degree of precision, even when a small corpus was used in training. We have achieved an application rate of 81.8% and precision rate of 93.3%. The algorithm also offer the advantage of producing word-sense disambiguation information.
AUTOMATED ALIGNMENT IN MULTILINGUAL CORPORA  Experiences in computing alignments at the paragraph and sentence level within a project TRANSLEARN in the European Union&apos;s &quot;LRE&quot; programme of research and development in language engineering are reported. About 98% of the sentences in pairs of corpora in different languages have been aligned correctly by a method that uses dynamic programming on numbers of characters per sentence. This parallels the experience of previous researchers for English-French alignment. We have used Portuguese and Greek material in addition to these languages, from a set of 49 European Union official documents. It is argued that the key issue of automated alignment is now the automated improvement of the quality of alignment achieved by methods that rely only on character counts. Cues that are helpful to support such an improvement are identified special words, cognates, syntactic fragments, and a simple measure of semantic weight. A short account of their use in experiments is given.
Using Brackets to Improve Search for Statistical Machine Translation The work we discuss here is embedded within the SILC project at HKUST (Wu 1994;Fung Wu 1994;Wu &amp; Fung 1994;Wu &amp; Xia 1995;Wu 1995a;Wu 1995b;Wu 1995c) which focuses on problems of machine translation learning. We are developing machine learning techniques to bear upon the shortage of adequate knowledge resources for natural language analysis, particularly for Chinese where there is relatively little previous computational linguistics research from which to draw. It is one of our objectives to investigate the suitability for Chinese of the statistical translation model originally proposed by IBM ( Brown et al. 1990;Brown et al. 1993) for Indo-European languages. Henceforth we will therefore use "Chinese" to refer to the source language and "English" to refer to the target language, reflecting the . prototype SILC system.An inherent characteristic of the basic IBM stochastic channel model is the large search space, due to the wide range of distortions that must be allowed in order to successfully transfer sentences of one language to the other. The underlying generative model maps target•language strings into source-language strings (i.e., in the reverse direction from translation). During translation, a maximum likelihood target•language string is sought for the input source-language string, according to Bayes' formula: (1) argmax Pr(elc) = argmax Pr(cle) Pr(e) e eThe distortion operations in the channel model are chosen to permit sufficient flexibility to map English strings into Chinese translations that have greatly different word order. (It is a simplifying assumption of the model that the only sentence translations considered are those where the majority of words can be translated by lexical substitution.) The scheme admits many implausible mappings along with the legitimate translations, but thereby gains robustness. During the recognition process, legitimate translations will be selected so long as the implausible mappings have lower likelihoods. The IBM model employs an A* search strategy on the space of translation hypotheses using incremental hypothesis expansion. The distance-to-goal heuristic is not admissible but reasonable estimates can be made yielding good performance. This approach arguably provides the highest possible accuracy assuming that no additional information is available.In reality, however, additional information can usually be made available. The method we propose here exploits one such type of information, namely, that a preprocessing stage can be used to annotate the input source-language sentence with a syntactic bracketing. We will not dwell on the bracketing method here; numerous approaches for automatic bracketing have been developed, including strategies employing full grammars, local patterns, and information-theoretic metrics. Work on Chinese parsing (Jiang 1985;Zhou &amp; Chang 1986;Lum &amp; Pun 1988;Lee &amp; Hsu 1991;Lee et al. 1992) would be particularly applicable here. We propose a method to improve search time and space complexity in statistical machine translation architectures, by employing linguistic bracketing information on the source language sentence. It is one of the advantages of the probabilistic formulation that competing translations may be compared and ranked by a principled measure, but at the same time, optimizing likelihoods over the translation space dictates heavy search costs. To make statistical architectures practical, heuristics to reduce search computation must be incorporated. An experiment applying our method to a prototype Chinese-English translation system demonstrates substantial improvement.
Reference in Dialogues and Shared Belief Revision Belief sharing in dialogue necessarily involves circular structures, since shared beliefs have circular structures. [1]'s theory of non-well-founded sets or hyper-sets makes it possible to encode such circular structures in first-order fixed point equations, whose solution is guaranteed by his solution lemma. By adopting [1]'s theory, we extend the expressibility of Discourse Representation Structures, called Hyper-DRS(firstly proposed in [14,13]), of [9,8,10,2]'s Discourse Representation Theory, henceforth DRT. (See also [12] for another version of self-referential DRS and its semantics). This extension takes the form of introducing three new types of discourse referents, which we will show reflect our intuitions about dialogue processes. In this paper, we shall show how to treat shared belief revision and reference management in the process of dialogues by distinguishing three types of discourse referents: (standard) discourse referents, shared discourse referents, and the shared belief referent. Shared discourse referents play a role in preserving the topic of dialogues, and shared belief referents are used to convert circular propositions into first-order equational systems, and we can update and revise a circular proposition by hanging propositions on and taking propositions from shared discourse referents appearing in it. Thus, the three types of discourse referents are basic devices for the management of shared beliefs. In the context of shared belief revision, we can also characterize the three types of discourse referents in terms of the notion of accessibility and life span. The shared belief referent and standard discourse referents can not be accessed by both conversants. The conversants can access shared discourse referents, and each conversant can access standard discourse referents which are introduced by herself. Shared discourse referents and the shared belief referents can not be deleted and forgotten when shared beliefs are revised or deleted, i.e., the life span of shared belief referents is the longest in revision processes of discourse representation structures, and the life span of shared discourse referents is longer than that of standard discourse referents in their revision processes. Furthermore, we can explain the connection between shared belief revision, reference management, and dialogue process, and give a mothod of evaluation of dialogue in terms of successfulness in information sharing.As a result, our extended version of DRT with Hyper-DRS's can treat shared beliefs as first-order equational systems, which, being a finite representation of inherently infinite structures, allow us to simulate, and evaluate processes of shared belief revision and reference management in dialogue.In section 2 we consider a probeim of the type of disourse referents connected to dialogue evaluation. In section 3 we propose Hyper-DRS and their construction procedure from dialogue by distinguishing shared belief referernts from the other types of discourse referents. In section 4 we distinguish and characterize shared discourse referents. In section 5 we propose a procedure for shared belief revion based on Hyper-DRS and the distinction of the three types of discourse referents in dialogue. We shall show how to treat shared belief revision and reference management in the process of dialogues by distinguishing three types of discourse referents: (standard) discourse referents, shared discourse referents, and the shared belief referent, using an extended version of Discourse Representation Theory, called Hyper-DRS, which can represent shared beliefs semantically in a notation based on hyper-sets (sets in non-well-founded set theory).
Recursion Problems in Concatenation: A Case of Korean Morphology In Generative Grammar, the infinite magnitude of natural language is accounted for in terms of the notion of recursion. Without it, no syntactic embedding as in relative clause formation or that-complementation in English or even repeated modification as in an adjectival phrase "very, very, very long" can be explained with mathematical elegance.In an ideal agglutinating language there is no problem of morphological recursion because the number of suffix classes is finite and their order of occurrence is fixed:(1) Stem--SX1--Sx2--....-nn Furthermore, each position may be filled only once.However, Korean deviates from the ideal type of an agglutinating language, because certain suffixes may occur in a reversed order:(2) Stem--...SX1.--sx...SXn (3) Stem--...sx--SX1...SXn A particular suffix sx generally appears after SX1, but it may also appear sometimes before SX1. The recursion problem resides in the possibility of readding sx after the string ...sx-SX1 because sx normally occurs after SX1.Using an implementation of Korean morphology in the Malaga system, this paper will present a straightforward solution to the indicated problem of recursion in Korean morphology. With one single rule for adding the particles or endings, recursively applied, both nominal and verbal forms are successfully generated, each consisting of a stem and a sequence of particles or endings. Without properly constraining recursions in generation, no language system can effectively operate. This is especially so with morphological generation, for each well-formed word must be finite in length and is accepted as such only when it actually occurs in text or ordinary usage. This difficulty is compounded when a language system tries to maintain a single rule of concatenation and apply it repeatedly in order to combine a nominal or verbal stem with a sequence of suffixes in a time-linear manner. In an agglutinative language like Korean, however, it can easily be demonstrated how a language system like Malaga, based on time-linear grammars like Hausser&apos;s [1] Left-Associative Grammar, can properly be implemented to constrain undesirable recursive loops in generation. Both nominal and verbal concatenations in Korean are treated in this paper to show how infinite loops can be blocked by imposing appropriate matching conditions on two adjacent input strings to concatenation.
Interpretational Strategies and Semantic Identities One of the most important problems in modeling semantics of Natural Language is undoubtfully the representational locality of the chosen formal features. Formal locality is apparently contradictory to empirical facts; the semantic identity of a linguistic unit-at whatever level it might be-is determined only within global structures. In formal constructions one generally recovers such global structures by more elementary ones, given by locally necessary and sufficient definitional means. Nevertheless, in Natural Language the primacy is rather to the global structures as they are necessary for the definition of the semantic character of the more elementary ones.In interpretational semantics [Rastier, 1987] this last constitutes the major principle. The problem in such an approach is the construction of interpretation strategies. From a structuralistic point of view this turns out to be equivalent to the determination of a specific recurrence of semantic components giving isotopies. An isotopy is a form of logically coherent actualization of a possible co-occurrence of such components. Many different strategies may be operational and thus many interpretational schemata may be possible, corresponding to different senses of the same text. What precisely changes across such strategies is the semantic identity of the relevant linguistic entities insofar as different semic actualizations are selected.Interpretational semantics as a general semantic theory is not yet formalized. The long-term aim of this paper is to provide a plausible approach towards a general formal model. Here we give a glimpse of the formalism by showing how to define a notion of semantic identity and a notion of context so that we shall be able to express semantic identity shifts as effects of interpretational strategies. It is not possible to model semantics of natural language unless one can clearly establish the structure of contextual information and how &quot;context&quot; guides the construction of semantic identities of linguistic units. We claim that this cannot be done unless one can overcome the representational locality imposed by extensional formalisms. In this paper we present a formalism aiming at setting up a non-extensional framework allowing the expression of a notion of context. The very notion of identity is revisited and some forms of non-extensional identity are proposed. In the last section we suggest a representation of metaphor understanding that implements a form of interaction between global contextual information and local semantic identities.
Preferred Clause Structure in Mandarin Spoken and Written Discourse  
A Corpus-Based Study of Adverbial Clauses in Mandarin Chinese Conversations: A Preliminary Analysis The study of the different types of adverbial clauses in the past has been done almost exclusively by logicians, who analyzed the relationship between the adverbial clause and the main clause in terms of truth value, material implication, presupposition, etc. This solely semantic analysis did not allow for notions such as organization and content of the discourse, communicative intent or pragmatic motivations of speaker and hearer. Since these notions did not become the concern of linguistics until recently, we still do not have a clear understanding of the behavior of the different types of adverbial clauses in discourse.One of the most recent discourse analysis of different types of adverbial clauses is Chafe's ( [2]). This study analyzes the adverbial clauses in spoken and written texts and suggests that adverbial clauses vary their functions with respect to two factors. One has to do with their position in relation to the main clause; the other has to do with how tightly the adverbial clause is bound to its main clause. Another work concerning this type of phenomenon is Ford's study on the distribution of adverbial clauses in American English conversations ( [7]). She aims at defining the distribution of adverbial clauses in discourse. Especially her study analyzes the interactional factors that determine whether an adverbial clause will be placed before or after its main clause and concludes that preposed and postposed clauses with different intonation are performing different functions in discourse.The above mentioned two studies offered illuminating insights into the behavior of different types of adverbial clauses. Following their models, this study is an attempt to investigate adverbial clauses in spoken Mandarin conversations on the basis of quantitative analysis. In conversation, adverbial clause subordination is most commonly achieved through temporal, conditional, concessive, and causal conjunctions, such as dang (` when&apos;), ruguoCin, suiran (`although&apos;), yinwei (` because&apos;), etc. in Mandarin Chinese. This study aims at exploring adverbial clauses in spoken Mandarin conversations on the basis of quantitative analysis. There were two-hour conversation database in this research. The adverbial clauses in our database were divided into (a) preposed clauses to their modified material with continuing intonation, (b) postposed clauses to their modified material with continuing intonation, and (c) postposed clauses to their modified material with final intonation (rising question intonation or final falling intonation). After an inspection of our data, we find that the temporal, conditional, and concessive clauses favor to occur before their modified material; and the causal ones, after their associated material. Our data show that causal clauses are fundamentally different from temporal, conditional, and concessive ones in their use.
A Network-Based Writing System for French*  &lt;Systeme Emile&gt; is a French learning software program using network. This paper presents its implemented prototype version, Emile 1.1. It is based on the client/server model connected by a dial-up modem. The data structure, the tutoring tools and the user interface of Emile 1.1 are described in this paper. It mentions what should be added and improved in the next versions and further studies.
Web Access to a Lexical Database using VB/Access CGI Programming  In this paper I report on the development of an application in which HTML forms serve as a front-end to a lexical database. Lexical information and data retrieval strategies are based on the Longman Language Activator. A Visual Basic CGI application connects a front end HTML form with the back-end relational database implemented using Microsoft Access. Three aspects of the applcation are discussed in this paper: (1) the lexical database; (2) the HTML front end; and (3) the Visual Basic CGI programming necessary to connect (1) and (2). The Lexical Database The source for the lexical database implemented for this application is the Longman Language Activator, (LLA), subtitled The World&apos;s First Production Dictionary. The LLA was selected for its unique organization of dictionary information which is intended to make it easier for the user to find the right word or phrase for a particular context. The LLA is organized on one level according to &apos;Key Words&apos; or concepts and on another level by the words and phrases which realize these concepts. The 1052 Key Words in the LLA are said to account for the basic concepts from the core of English. For each Key Word, there is what is called a &apos;Meaning Menu&apos; consisting of numbered sections corresponding to major aspects of the Key Word. The numbered sections follow. Each with another`menuanother`menu&apos; of words and phrases. Further detail about each word/phrase-pronunciation, part-of-speech, definition, examples-is provided below this menu of words and phrases. The same words/phrases presented in the numbered sections for a given Key Word are also listed alphabetically throughout the dictionary. One can easily distinguish entry words and phrases from Key Words by the fact that entry words and phrases are in lowercase , whereas Key Words are in upper-case. • In addition, the LLA includes what it calls &apos;Access Maps&apos;. Access Maps identify Key Words related to an entry word. alphabetical list of entry words and phrases eat EAT 1.2 eat: couldn&apos;t eat another thing FULL 3 eat: have something to eat EAT 2 eat: not get enough to eat HUNGRY/STARVING 1 eat: something to eat F0001
Document Ranking Method for High Precision Rate The general users are more interested in concepts rather than words itself. But many commercial IR systems retrieve relevant documents based on keyword string matching between a query and documents. There are two problems in using the method. The first problem is that words are ambiguous, and this ambiguity is causative of retrieving irrelevant document semantically. Therefore lexical ambiguity has to be resolved. The second problem is that a document is treated as a irrelevant document in spite of a relevant document, for the document does not include the same words as query terms. So an original query has to be expanded to semantically related words.In order to solve the problems, we consider keyfacts as well as keywords. A keyfact is an extended concept of a keyword and can be defined as a verb and an adjective which are every probability that occurs in several times based on threshold value. We collect semantically related keyfacts from an encyclopedia and assigned semantic relationships using in general thesaurus and a special relationship of FT. We use the semantic informations for document ranking and word sense disambiguation. The remainder of this paper is organized as follows. Section 2 gives a definition of the KN and the KN construction method. Section 3 describes word sense disambiguation. The results of performance comparison are presented in section 4. The concluding remarks are described in section 5.2 Keyfact Network 2.1 The Definition of KN In this paper, we collected words and their semantically related words from an encyclopedia(ENCY). Collected informations are used to understand user's request. The KN consists of nodes and edges. Nodes are defined as words rather than word sense and the edges represent binary relationships, such as BT(Broader Term), NT(Narrow Term), RT(Related Term), HP(Has Part), UF(Used For), and FT(keyFact Term). FT is defined as relationship between noun and verb or adjective. Most IR systems have used to the relationships except FT relationship in their thesaurus. FT is effectively used to resolve lexical ambiguity and compute query-document similarity. KN has a semantic and statistic informations like this format 'relationship(freq(x), freq(y), freq(x,y))'.2.2. Construction of keyfact network (ENCY) explains matters systematically. The ENCY has two characteristics. First, it has syntactic characteristic composed of title word and it's explanation part. Second, it has semantic characteristic that most words in the explanation part are semantically related with a title word. ENCY is good to easily collect words and it's semantically related words. We thought that ENCY is a proper text for construction of semantic informations. First, we manually marked keywords like nouns and compound nouns which are semantically related with a title word within each explanation, and we marked keyfacts like verbs and adjectives which are include adjacent subject or object if possible. Second, we assigned semantic relationships using in general thesaurus such as BT, NT, RT, HP, UF and a special relationship of FT. Third, we compute each frequency and co-occur frequency between keyfacts in ENCY. There are 17% ambiguous words of about 22,000 title words. 88% of the ambiguous words are two sense words. KN has 88,010 whole entries, and each entry has an average of 7.3 related words.For example in the query "V .7} 11 7+Tit? (What's the number of letter in the Tripitaka Koreana) ", the word '44' might be one of three meanings in Korean language. The three meanings are the number of letter, self-surrender, and embroidery in Korean language. But KN does not have the semantic 'the number of letter°, because input text of KN is small corpora. Therefore the KN must be changed and expanded on demand a new ENCY. Whenever a new document like an encyclopedia is occurred, we don't have to process first and second step fully manually. Keyfact extractor extracts noun, compound noun, and original form of verb. If the words like homonym and polysemous word exist in a new document, disambiguator will reduce humane intervention. Many information retrieval(IR) systems retrieve relevant documents based on exact matching of keywords between a query and documents. This method degrades precision rate. In order to solve the problem, we collected semantically related words and assigned semantic relationships used in general thesaurus and a special relationship called keyfact term(FT) manually. In addition to the semantic knowledge, we automatically constructed statistic knowledge based on the concept of mutual information. Keyfact is an extended concept of keyword represented by noun and compound noun. Keyfact can be a verb and an adjective including subject or object term. We first retrieved relevant documents with original query using tf * idf weighting formula and then an expanded query including keyfacts is used in both second document ranking and word sense disambiguating. So we made an improvement in precision rate using keyfact network.
Natural Languages Analysis in Machine Translation (MT) based on the STCG (STRING-TREE CORRESPONDENCE GRAMMAR)  and zarin@cs.usm.MY] 0. Abstract The String-Tree Correspondence Grammar (STCG) [1] is a grammar formalism for defining:
An Implementation of a Multilingual Regular Expression Segmentor for Ordinary and Morphologically Rich Lexical Tokens Lexical pattern matching can be used to recognise phrasal patterns of different languages and which reflect morphological variations. For example, it is used to recognise regular English phrasal verbs like ".. give * up .." or reduplicated Chinese words like "GaoXing-BuGaoXing" (literally "happy-not-happy"). Simple phrasal patterns are categorised under the class of Regular Languages (RL) [2]. The degree with which lexical patterns within the language exercise the RL operators varies. Certain classes of lexical patterns, for instance (dead) idioms and Chinese lexicon items, are quite fixed. For such cases, the Concatenation operator in RL is sufficient to generate and recognise the patterns. On the other hand, there are other instances, such as the phrasal verbs and word reduplications (as shown above), which require a combination of Kleene Closure, the Don't-Care symbol and Back-Referencing (a special device by which we can encode a class of word-reduplication as the pattern ( s rrir) where r is the reduplicated word) [4].An interesting theoretical question to ask here is -can we classify lexical patterns based on the degree with which they exercise the aspects of RL operators'? This theoretical question has a further implication to the implementation of such pattern recognition machines.Principle 1: Each class of lexical patterns should be handled by different classes of machinery to take advantage of the computing efficiency.In this paper, we discuss the concepts and component techniques of a Regular Expression Segmentor (REXSEG) which adheres to Principle 1. A Lexical Pattern Command Language (LPCL) was designed and implemented as a grammar to assist computational linguists in matching and processing significant lexical patterns in texts. Finally we detail comparisons with other lexical pattern matchers in use today and the future development of REXSEG. Lexical pattern matching and text extraction is an essential component of many Natural Language Processing applications. Following the language hierarchy first conceived by Chomsky, it is commonly accepted that simple phrasal patterns should he categorised under the class of Regular Language (RL). There are 3 operations in RL-Union, Concatenation and Kleene Closure-which are applied to a finite lexicon. The machinery that recognises RL is the Finite State Machine (FSM). This paper discusses and postulates that the degree with which a class of patterns exercise the aspects of RL operators, is directly proportional to the richness in morphology of lexical tokens.
Sorting by Sound - Arbitrary Lexical Ordering for Transcribed Thai Text Consider the dilemma of the Thai speller: these words are spelled thirteen different ways, but have essentially the same sound (than), and vary only in tone and vowel length: flu, lila, 614 111U, vntd, iiiu, Thu, ru, mu.There are a remarkable number of ways to spell words with this sound. Thai has six different th letters, five ways to show a or aa, and six ways to write the final n. There are also four different tone marks, and a sign (over this letter 91) that means 'ignore me.' Finally, the tone mark does not actually give the tone -rather, it modifies an implicit tone that depends on the word's spelling.Because a simple one phoneme/one grapheme (or one sound/one letter) relationship doesn't exist, words with identical or similar sounds can be widely scattered when lexically ordered. This complicates applications, ranging from 'sound-alike' spell checking to introductory language instruction, that depend on a Thai word's sound, not its spelling.As a result, we find that sorting transcribed Thai is much more convenient that using native Thai orthography. We return to a relatively straightforward relationship between symbols and sounds; one that lets us group words with the same sounds regardless of their original Thai spelling.Sorting transcriptions involves two basic issues: definition and implementation. First, we must define a lexical or dictionary order: if transcribed Thai adds the IPA symbols a, c, a, ti to the English a, e, o, u, what should the combined set look like? Should may come before maay or vice versa? Should two-character symbols like kh or p h be removed from the midst of the k's and p's? What is the proper order of words that vary only by tone?Second, we have to find an easy implementation -one that uses existing sort programs, even with a new character set or lexical order.This paper looks at the issues involved in sorting by sound. Part I states the problem: it presents the terminology and issues of ordering, describes the difficulties of languages like Thai, and looks at questions that persist even with effective transcription systems.Part II outlines the solution. It lists considerations for defining new lexical orders, then gives an algorithm for extracting phonemic signatures as part of a sorting strategy.Finally, Part III deals with the implementation. It shows how to derive phonemic signatures, and uses simple UNIX tools to implement the algorithm for a test alphabet. The method is easily generalized to any consistent transcription scheme. When either Thai or transcribed (Romanized) Thai is sorted alphabetically, words that sound very much alike usually end up far apart. maay and may are thrown to opposite ends of the letter m entries, even though mistaking one for the other causes problems for both foreign students who cannot speak clearly, and Thais who can&apos;t spell. This paper explains how and why the difficulty occurs, and shows why both Thai and transcription are inherently difficult to sort by sound. It introduces a method of preprocessing-deriving phonemic signatures-that lets us define improved lexical or dictionary orders, yet does not require anything but standard sorting code. The method can be applied to other languages-Lao, Khmer, and Burmese-that, like Thai, distinguish words on the basis of vowel length and/or tone. Introduction Consider the dilemma of the Thai speller: these words are spelled thirteen different ways, but have essentially the same sound (than), and vary only in tone and vowel length:
A UNIFIED ACCOUNT OF POLARITY PHENOMENA This paper attemps to explore the essential semantic and pragmatic nature of negative polarity phenomena including free choice cases in Korean, English and other relevant languages. It will be argued that they can be explained in terms of the notion of arbtrary choice (via indefiniteness) and the notion of concession (via inclusion), and these notions are shown to be closely interwoven with a scale set up by a concession marker and/or the context.I present an account of amu 'any' and its corresponding lexical item any in English that unifies both their uses of polarity and free choice. The same account is shown to be applied to wh-indefinites as NPIs. It is noted, however, that there is a slight distinction in markers in Korean between the negative polarity use and the free choice use and that there is a distinction between strong overt negatives and weak covert negatives, with lexical variation in between in negative force within and across languages.In the first part of the paper, I will discuss the notions of arbitrary choice and concession in connection with negative polarity, and then, discuss how a free choice item or generic-like item, licensed by modals, is focused to emphasize arbitrariness in choice. In section 3 it is shown that affective licensors such as conditional, modified generic, 'at most,' and adversative predicates license NPI of clausal origin with existential force. In section 4 I examine NPIs formed by wh-indefinites to see how parallel principles are applied; in section 5 I discuss how disjunction in its open sense can be interpreted conjunctively and how that process is applied to disjunction-marked free choice items in Korean; in section 6 the logical consequences of monotone decreasingness of various licensors are condidered and it is shown that entailment from a disjunctive predicate to a conjunctive S with separate subjects by something like de Morgan's law is not so transparent with weaker licensors. This paper argues, in an attempt at a unified account of negative polarity and free choice phenomena expressed by amu /any or wh-indefinites in Korean, English, Chinese, and Japanese that the notion of concession by arbitrary or d isjunctive choice (based on indefiniteness) is crucial. With this central notion all the apparently diverse polarity-related phenomena can be explained consistently, not just described in terms of distribution. With strong negatives and affective licensors, their negative force is so substantial that concessive force need not be reinforced and the licensed NPIs reveal existential force. With free choice and generic-like items, licensed by modals, weakly negative in their natrue of uncertainty/irrealis, concessive force is reinforced and emphasized and the whole category denoted by the given Noun is reached in the process of concession by arbitrariy choice of its members on quantificational scale, giving the impression of universal force. The logical consequences of monotone decreasingness are transparent with strong negatives but less so with weaker ones.
Contents Foreword VII Classifiers and Semantic Type Coercion: Motivating a New Classification of Classifiers Configuration vs. Information: An Informational Explanation of Command Relations  
Information and Computation(PACLIC 11) There have been two views of categorizing measure words in Mandarin Chinese. The traditional view does not differentiate measure words from classifiers. For example, Chao (1968:584-620) refers to classifiers as individual measures, and subsumes them under the rubric of "measure words". Li and Thompson (1981:106) state that "any measure word can be a classifier." More recently Tai (1990) has pointed out that there is an important distinction between the two notions in that classifiers can only classify over a limited and specific group of nouns, while measure words can be used as a measure for a wide variety of nouns. His definition is as follows: 'A classifier categorizes a class of nouns by picking out some salient perceptual properties, whether physically or functionally based, which are permanently associated with the entities named by the class of nouns; a measure word does not categorize but denotes the quantity of the entity named by a noun.' Underlying the concept that a classifier categorizes a class of nouns based on permanent perceptual properties is the idea that the basic semantic function of nouns is to refer to classic individuals. In what follows we will show that it is inadequate to only allow nouns to refer to classic individuals, and that instead nouns can be coerced by different types of classifiers to refer to kinds and events as well as to individuals. This finding is important not only for its emphasis in understanding the semantics of nouns to be more than just having to do with individuals, but also because it is the first time that the previously abstract semantic distinctions between kinds, individuals and events has been found to be instantiated in a particular system of a natural language grammar, namely, the classifier system. This paper argues that the traditional view that nouns refer only to classic individuals is inadequate. Instead, we argue that nouns are coerced by different types of classifiers to refer to kinds and events as well as to individuals. This finding is important because 1) the semantics of nouns involves more than just individuals, and 2) it is the first time that the previously abstract semantic distinctions between kinds, individuals and events is found to be instantiated in a particular system of a natural language grammar, namely, the classifier system.
Information and Computation(PACLIC 11) The essential difference between the most widely used syntactic frameworks in linguistics (Principles and Parameters, Minimalist Program) on the one hand and those used in computational linguistics (Generalized PhraseStructure Grammar, GPSG, Head-Driven Phrase-Structure Grammar, HPSG, Lexical-Functional, LFG, Categorial Unification Grammar, CUG, etc.) on the other hand is that the former crucially make use of configurational notions, while the latter use informational concepts.In this paper, we show that configurational notions follow from inherent properties of informational concepts.First, we analyze the hidden complexity of the central family of configurational relations (command) from which all the others (government, proper government, etc.) are derived.We then examine the different types of grammatical information, their sources and their propagation mode.Finally we show how simple and general constraints on these might make syntactic dependencies appear to be governed by configurational relations such as command, but just like the Stars might appear to be turning around the Earth. In this paper, we show that configurational notions follow from inherent properties of informational concepts. First, we analyze the hidden complexity of the central family of configurational relations (command) from which all the others (government, proper government, etc.) are derived. We then examine the different types of grammatical information, their sources and their propagation mode. Finally we show how simple and general constraints on these might make syntactic dependencies appear to be governed by configurational relations such as command, but just like the Stars might appear to be turning around the Earth.
Information and Computation(PACLIC 11)  We discuss data showing that, unlike other long-distance anaphors widely documented in the literature (e.g. ziji from Chinese, caki from Korean, zibun, from Japanese, etc.), the Portuguese ele prOprio is not subject-oriented. This supports a reformulation of Principle Z, encompassing subject-oriented and non subject-oriented long-distance anaphora, which shows up as the fourth binding principle. The striking internal congruence of the resulting four principle based Binding Theory cogently makes it apparent that the binding symmetries are far more richer than the problematic single distributional symmetry between anaphors and pronouns that has been continuously assumed to hold by most of the research of the last three decades. We also discuss how the data involving the Portuguese long-distance anaphor add to the growing evidence that the generalization assumed in mainstream GB approaches about the universal correlation between &quot;simplex&quot; anaphors, long-distance anaphora, subject-orientedness and intermediate blocking effects is most likely not to be empirically grounded.
Information and Computation(PACLIC 11)  Like other Indo-European languages, English also employs a particular type of relative clause constructions, the so-called free-relative constructions, exemplified by the phrase like what Kim ate. This paper provides a constraint-based approach to these constructions. The paper begins with surveying on the properties of the construction. We will discuss two types of free relatives, their lexical restrictions, nominal properties, and their behavior with respect to extraposition and piped piping, and finiteness. Following this, we sketch basic theory of the constraint-based grammar, Head-driven Phrase Structure Grammar(HPSG) which is of relevance in this paper. As the main part of this paper, we then present our constraint-based analysis couched upon this framework.
Information and Computation(PACLIC 11) One of the significant problems in Japanese to German machine translation is that information on definiteness and number is in most cases not available on the surface of the Japanese utterance. Japanese has neither number agreement between verbs and nouns nor obligatory plural morphemes. Optional plural morphemes like tachi or domo are only available for nouns that refer to persons.However, for the generation of German utterances the generator needs such information, as in many cases determiners are obligatory. We analyzed the problem based on empirical material collected in the domain of appointment scheduling. In our setting, an interpreter translated 10 dialogues of Japanese and German speakers. Consider the following example from our data: Japanese: The information that Feiertag has to be preceded by the singular indefinite (masculine) determiner ein and that Treffen has to be preceded by the singular definite (neuter) determiner dem does not come out of the surface of the Japanese utterance and therefore cannot be included in the parsing result. It is not an adequate solution to transfer an underspecified representation to the German generation module, because the information that is needed to decide on the definiteness and number of the noun phrase partly comes out of the Japanese surface, partly out of German lexical restrictions and partly out of domain and discourse restrictions. Not all of this information is available in the generation phase. We argue that it is an interlingual problem and therefore must be solved in the transfer module.2 Previous approaches [Murata and Nagao, 1993] describe a solution model that searches for information at the surface of the Japanese utterances to give hints for the choice of determiners in the English counterpart. They state heuristics for the possibility of definiteness and number values concerning words or constructions in the Japanese utterances. Such information can be determiners in Japanese or particle and tense information.But this is only one of the relevant aspects, because it does neither consider restrictions on definiteness and number coming from the target language, nor restrictions based on domain and discourse. Just as little as it is an inherent German problem it is an inherent Japanese problem.[ Bond et al., 1994] already state that the inclusion of information about the target language (English in their case) increases the rate of correct translations.They enrich the heuristics with information on countability of English nouns. But their approach still lacks the integration of knowledge about discourse and domain, which is relevant as we will show.Every approach that is external to the transfer process cannot include source and target language information at the same time and be thus effective. It is essential to find a mechanism that makes it possible to consider discourse and domain knowledge. A significant problem when translating Japanese dialogues into Ger-man is the missing information on number and definiteness in the Japanese analysis output. The integration of the search for such information into the transfer process provides an efficient solution. General transfer rules, preference rules and default rules are combined. The transfer includes conditions to make it possible to consider external knowledge. Thereby, grammatical and lexical knowledge of the source language, knowledge of lexical restrictions on the target language, domain knowledge and discourse knowledge are accessible.
Information and Computation(PACLIC 11) Natural language expressions are inherently ambiguous. Ambiguities can be caused, for example, by the fact that one of the words used does not have a unique meaning, that more than one syntactic structure may be assigned to the expression, that the scopal relations are not clear, etc. Regardless of the cause, ambiguities are problematic for Natural Language Processing, one of the problems being that they decrease processing efficiency: usually all of the possible interpretations have to be assumed to be right until hard facts prove the contrary. Unfortunately, this can oftentimes not be decided on until after a lot of processing already has been done.A way around this dilemma is to have a common representation for all of the possible interpretations of an ambiguous expression, as in Kameyama, 1995). Recent research (Reyle, 1993;Bos, 1995;Pinkal, 1995;Cooper et al., 1996) has used the term underspecification to describe this idea: One does not use representations (we will assume formulae of a certain logic in what follows) that encode one single concrete interpretation but a set of interpretations.One prominent area where underspecification can be used, namely for leaving possible scopal domains undecided on, can be explicated with the Japanese sentence in (1): Here, the scope of the focus particlèdake' (`only') cannot be determined. Thus, all of the *This work was funded by the German Federal Ministry of Education, Science, Research, and Technology (BMBF) under grant 01 IV 101 R. We are grateful to Johan Bos, Karsten Worm, and Manfred Pinkal for comments and to Julia Heine, Daniela Kurz, and Feiyu Xu for their work on the lexicon. translations given in (2) are viable, the expression is ambiguous and an underspecified representation is called for.(1) Yamada kara kari to dake desuYamada from borrow past only cop.+pres.(2) I only borrowed it from Yamada. I only borrowed it from Yamada. I only borrowed it from Yamada.The main reason for introducing the underspecified representations has been processing efficiency. For us, however, using underspecified representations has another motivation. In this paper we will discuss underspecification within the semantic formalism of the machine translation (MT) system Verbmobil, a system where the transfer from source language expressions into target language expressions takes place at the (compositional) semantic level. Since ambiguities on the side of the source language often also appear on the side of the target language, resolution of ambiguity is not always necessary.In the rest of the paper, we will discuss these matters in greater detail. First, Section 2 gives some more examples of semantic underspefication in general and some specific to Japanese in particular. Section 3 describes the Verbmobil project. In Section 4, the semantic formalism which we use is introduced together with a short formal definition. To make things more concrete, Section 5 then discusses how the actual implementation has been made and exemplify this with showing the underspecified representations for several phenomena. Finally, Section 6 sums up the discussion and points to some areas of further research. Semantic representations which are underspecified with respect to, for example, scope, have recently attracted much attention. Most research in this area has focused on treating English language phenomena in a theoretical fashion. Our paper deviates from this twofold: We take Japanese as the language of our investigations and describe how our ideas about underspecified Japanese semantics (e.g., on modality adverbs) have been implemented in a spoken-language machine translation system.
Information and Computation(PACLIC 11)  We report on methods of improving multilingual text alignments that have been produced in a simple dynamic-programming scheme, by automated detection of possible misalignments. Details of methods involving cognates, specially-identified words, and propositional contents of sentences are given, together with notable features of their performance on parallel corpora in a number of different types of European languages.
Information and Computation(PACLIC 11)  According to Li &amp; Thompson (1981), Chinese serial verb constructions consisting of two verb phrases denoting two separate events can be classified into those having alternating, consecutive, purpose or circumstance relations. These classifications may overlap and a serial verb construction may be ambiguous between different interpretations. It has been argued in a recent study (Chan 1996) that there exists an entailment relation between the different interpretations of an ambiguous serial verb construction. In the present study, it is argued that because of the entailment relations between the different interpretations, the truth conditional definition of ambiguity has to be modified if it is to be applied to Chinese serial verb constructions which are ambiguous. In truth-conditional semantics, it is suggested that an ambiguous sentence is true for one interpretation but false for another relative to a certain state of affairs. This definition of ambiguity is not adequate for an ambiguous Chinese serial verb construction because of the entailment relation between the different interpretations. It has to be modified to allow a state of affairs where the sentence is true for both interpretations. According to Li &amp; Thompson (1981), Chinese serial verb constructions consisting of two verb phrases denoting two separate events can be classified into those having alternating, consecutive, purpose or circumstance relations. These classifications may overlap and a serial verb construction can have more than one interpretation. For example, sentence (1) below is ambiguous between the consecutive and purpose interpretations, sentence (2) between the consecutive and alternating interpretations, and sentence (3) between the purpose and circumstance interpretations.
Information and Computation(PACLIC 11) One of the major syntactic differences between non-causative and causative constructions is the increased valency of the latter. A more interesting phenomenon is that the increased argument usually takes either a relation of direct object or a relation of indirect object, which is different from the one assumed in corresponding non-causative constructions. To explicate this phenomenon, clause union analyses have been suggested within the framework of Relational Grammar, supposing that causative constructions are universally biclausal at the initial stratum but monoclausal at the final stratum. Perlmutter &amp; Postal (1974) propose that the intransitive downstairs subject is revalued as an upstairs direct object whereas the transitive downstairs subject is revalued as an upstairs indirect object. Gibson (1980) and Gibson &amp; Raposo (1986), however, claim that the revaluation of downstairs subject is parametrized in accordance with language particular rules. On the other hand, Rosen (1983) contends that Italian allows another type of clause union in which the downstairs subject undergoes no revaluation.Korean syntactic causative constructions below reveal more intriguing phenomena with respect to the revaluation of the downstairs subject:1)(1) a. Chelswu-ka Swuni-ka/-lul/-eykey ttena-key ha-yess-ta.-N leave-Cmp do-Pst thelswu made Swuni leave.' b. Chelswu-ka Swuni-kai-luli-eykey chayk-ul ilk-key ha-yess-ta. -N -N/-A/-D book-A read-Cmp do-Pst`Chelswu Pst`Chelswu made Swuni read a book.'As illustrated, the downstairs subject is marked nominatively, accusatively, or datively, regardless of the transitivity of the downstairs clause. To account for this, Gerdts (1990) and Kim (1990) propose a clause union analysis, partly adopting P &amp; P, G &amp; R, and Rosen (1983).However, this paper claims that the constructions in (1) are finally as well as initially biclausal due to the subcategorizational features of the verb ha, not instances of clause union. This paper argues against Gerdts and Kim's analyses and discusses the theoretical inadequacies of the clause union analyses. On the other hand, this paper demonstrates that the biclausal analysis is empirically valid in that it can provide correct predictions in connection with clause boundary sensitive phenomena. Clause Union Laws require causee nominals to assume the relation of direct object or indirect object depending on the transitivity of corresponding non-causative constructions (Perlmutter &amp; Postal 1974) or in accordance with the language specific rules (Gibson &amp; Raposo 1986) in forming causative constructions. However, the causee nominals in Korean syntactic causatives are nominatively, accusatively or datively marked regardless of the transitivity of corresponding non-causative constructions. This paper argues that this intriguing case marker alternation is due to the interaction of the subcategorization of the verb ha with Subject-to-Object Raising, not the reflection of causative unions as argued in Gerdts (1990) and Kim (1990).
Information and Computation(PACLIC 11) Most previous studies on Chinese causal sentences treat the causal clauses on a par with other types of adverbial clauses. (cf. Chao 1968, Tsao 1979, 88, Li &amp; Thompson 1981 This means that they are supposed to be topics and manifest the adjunct-preceding-main order as shown in (1).(1) vinwei shengbing le, suoyi to liu zai jia-li.because sick ASP so he stay at home-in 'Because he was sick, he stayed at home.' However, as noticed by recent corpus-based studies, while the adverbial clauses commonly precede the main clauses, the causal ones take up both pre-posed and post-posed positions outside of their main clause. Biq (1995), for example; points out that in both spoken and written data, reason before main point is not necessarily the preferred order for expressing the causal relation. Wang (1995, 96) makes the same claim.If the adjunct-preceding-main order is overwhelmingly used in Mandarin discourse, what can be the reason for that causal sequencing is different form conditional and concessive one? We will show that the traditional analysis which treats adverbial clauses as topic in the topic-comment utterance cannot capture the behavior of causal clauses. Instead, our investigation on the interaction between the syntactic and semantic properties of adverbial clauses in written discourse leads to the two following findings that offer a more comprehensive account of the syntax and semantics of the causal sentences:i. The causal clauses cannot be the topic of the sentence.ii. The causal clauses may be the focus of the sentence. This paper argues that the word order of adverbials can be captured only when the causal clauses are analyzed as focus. Previous studies on Chinese causal sentences usually treat the causal clauses on a par with other types of adverbial clauses. This means that they are considered as topics and predicted to precede the main clauses. However, this account does not hold for the causal clauses. According to recent corpus-based studies, they actually take up both pre-posed and post-posed positions with respect to their main clause. If the adjunct-preceding-main order is overwhelmingly used in Mandarin complex sentences, what can be the reason for that causal sequencing is different from the conditional or concessive one? We will show that it is the focal status of the causal clauses that can account for their distribution.
Information and Computation(PACLIC 11)  Previous analyses of (Korean) Light Verb Constructions (LVCs) have failed to provide objective criteria for defining LVCs. The verb ha-is inconsistently regarded as a Light Verb (LV) or as a &quot;heavy verb&quot; depending on its environment. In the face of this problem, I argue, firstly, that all ha-sentences should be analyzed as LVCs when the potential Verbal Noun (VN) has at least one of the verbally case-marked phrases as its argument. Secondly, LVCs (neither LVs nor VNs) are classified into two groups based on their structural differences. Thirdly, LVC sentences in general are ambiguous between the structures of these two groups. Many sentences, however, are disambiguated because the VN in each structure has its own special properties. In this approach, the differences of the behavior of the VN in (LVC) ha-sentences are attributed to their structural differences rather than to &quot;spuriously&quot; multiplied lexical items.
Information and Computation(PACLIC 11) Thai writing does not use spaces to segment text into words. While open text contains many obvious separation points (bigdog vs. big dog), and a smaller group of questionable bind points that are usually permissible either way (toolbox vs. toolbox), there is inevitably a residue of ambiguous partition points (to_pend vs. top_end) for which computer segmentation is essentially random. This causes difficulty for many software applications: line-breaking, spell-checking, machine-assisted translation, text-to-speech, optical character recognition, full-text indexing, corpus-based dictionaries, etc.Yet despite the importance of segmentation, little is known about the characteristics of ambiguous partitions in Thai, or its orthographic cousins Khmer, Lao, and Burmese. Work has been slow and progress poor due to a lack of formal, concrete analysis. We know the problem's gross characteristics, and the general direction of solutions, but there are few theories to guide the way or allow comparison of research results.This paper describes experiments carried out on a 2 megabyte (roughly 400,000 word) Thai corpus. We collected nearly ten thousand distinct alternative segmentations of at least two words in length. Of these, a little more than two-fifths involved genuinely ambiguous partitions. We classify partitioning problems into distinct categories, report on statistical and lexical characteristics of ambiguous partitions, and describe heuristics for disambiguating that do not depend on the availability of a large segmented corpus.Our results make several contributions to understanding Thai text segmentation. First, we categorize breakpoints in a way that distinguishes between choices that are and are not semantically significant, and show how to collect them automatically and consistently. Second, we find that ambiguous instances are fairly rare (roughly 5% of word break opportunities), and have a pronounced Zipfian distribution -a relatively small number of circumstances produce a great deal of ambiguity; and show ways of collecting low-frequency items that exhibit the same behavior. Third, we find that contrary to the canonical examples, resolving ambiguity does not usually depend on knowing or understanding the context it occurs in. Finally, we suggest new methods -stop nodes, go collocates, and analysis of hidden 'swing strings' -to aid in automated disambiguation. Despite the importance of segmentation to a variety of software applications, almost nothing is known about the characteristics or distribution of ambiguous partitions (eg. to Pend vs. top_end) in Thai text. By using special-purpose code to investigate a large (-400K word) text corpus, we were able to extract 36,267 such sequences, involving 9,253 distinct examples. Of these, a little more than two-fifths involved genuinely ambiguous partitions. We classify partitioning problems into distinct categories, report on many of their statistical and lexical characteristics, and describe heuristics for choosing the correct partition that do not depend on the availability of a large segmented corpus.
Information and Computation(PACLIC 11)  Lexical attributes, like syntactic (part-of-speech) and semantic (semantic category) attributes, are in most cases, ambiguous in every languages. Automatic resolution of ambiguity of these attributes can be achieved using different techniques; rule-based, statistical, NN-based and their hybrids. Moreover, one linguistic feature also has influence over the resolution of ambiguity of another feature; eg.. knowledge of syntactical category can assist smooth disambiguation of semantic category and vice versa. Properly disambiguated syntactic and semantic properties of lexicon may significantly help us in word sense disambiguation, text analysis, information retreival, natural language understanding and speech processing etc. In this paper, we have presented our neural network based Classification Tool. We have used this tool in Part-of-Speech tagging and Semantic-Category tagging of Chinese lexicon with the help of thesaurus and large training corpus. Experimental results are analysed and compared.
Information and Computation(PACLIC 11) With the rapid development of the computer technology, there are many efforts in some area of Natural Language Processing to convert normal text or documents into machine readable form. If we convert written data into a canonical form which can be recognized by computer, we can extract needed information and process and utilize it for another purpose.In order to do this, we need to encode documents in a markup system. For the last 10 years, there have been many approaches to develop a mark-up system in many NLP projects. A Mark-up system is a set of instruction, with which a document or text can be stored in a formalized form. In other words, we can build complex data structures with a limited number of tags of this mark-up system.In the development of mark-up systems, standardization problems have been an issue. At the beginning of the 1980s, some researchers tried to develop encoding system and at last SGML (Standardized Generalized Markup Language) and TEI (Text Encoding Initiative) are emerged. With the help of SGML or TEI many documents or texts are structured as DTD (Document Type Definition) of SGML or TEI ( Bryan 1988, Ide et a1.1995, Sperberg-McQueen et al. 1994.In this paper, we present an account of the encoding of printed dictionary. The construction of a lexicon for a limited application domain is both time-consuming and expensive. When we define and implement a lexicon only for some natural language applications, we cannot re-use this lexicon for another applications. To avoid this problem, we can encode the lexical information in a standard format. If we have the lexical information in a standard format, we can transform into the machine readable form or vice versa.If a lexical information in one system is compatible with another application, we can reuse the existing standard lexicon and save our efforts to build a lexicon. Such a reusability of existing lexical information is the main focus of this paper. Calzolari has distinguished two kinds of reusability (Kugler 1995). One is transforming already existing lexical resources into a different format, typically transforming printed dictionaries into a machine readable or machine tractable form (Amsler 1988, Alshawi 1989. The other is exploiting already existing lexical resources for different theories and -typically -for different applications ( Briscoe et al. 1993, Hajicovi &amp; Rosen 1994. Such an idea is realised in this work.In order to maintain the reusablity of lexical information, we have developed the Standard Dictionary Markup Language (SDML). With SDML we can define the logical structure of lexicon and store lexical information independent of specific applications, data structures and theories.With the help of decoding program we can import the lexical information from printed dictionaries, text corpora, and some lexical databases into the standard dictionary. This information from the standard dictionary is then available for various natural language applications. In other countries there have been many such projects during the last 10 years. But we have no representative work on machine readable dictionary or electronic dictionary yet, except for the work of Kang (1996) about encoding Korean dictionaries. Kang adopted the TEI scheme for the encoding of Korean dictionary entries. We have tried an SDML-based encoding, which is adapted to dictionary entries. Because of its simplicity, it is easy to create a lexicon structure that can be applied across various possible dictionaries. During the last 10 years, there have been many efforts in some areas of Natural Language Processing to encode the normal text or documents into machine readable form. If we encode written data using a canonical form which can be recognized by a computer, we can extract needed information and process and utilize it for another purposes. From this point of view, we present an account of the encoding of a printed dictionary. The construction of a lexicon is very time-consuming and expensive work and the application of the lexicon is restricted. In this paper, we describe a logical structure for Korean printed dictionaries as a general lexical representation based on SDML, which can be transformed into another representation for different application requirements.
Information and Computation(PACLIC 11) With information retrieval as a goal, we are very sensitive to the issues of generalization, speed and scalability. Any NLP system that is used for information retrieval must be capable of handling large amounts of general text in a timely manner. Each of the components of such a system, from morphology through semantics, must have similar capabilities. Thematic roles, which provide a very basic "who does what to whom" type of semantics, meet these criteria because they are very general, simple and useable by a wide variety of natural language processing systems. The roles are generally contained in frames that contain the type of each argument for each verb. For example eat would have a frame similar to eat [AGENT, THEME].We have explored the development of these frames by using information found in an on-line version of Longman's Dictionary of Contemporary English (LDOCE 1987). The information in the dictionary includes: definitions; subject field codes; the box codes, that provide information on the type of arguments (ex., human or abstract); a reduced set of grammar codes, that provide information on the transitivity of a verb and the syntactic category of any extra arguments; .and other information much of which is probably extraneous to the roles.We focused on the definitions because we felt that they would provide the best base for a scaleable approach. Not only do the definitions contain information germane to role extraction but they are complete in the sense that every verb has a definition. This is not the case for the other types of information available. The box codes, for instance, are generic (i.e., empty) codes around 17% of the time.Our approach to analyzing the definitions is based on lexical patterns. A lexical pattern is simply a series of consecutive words that is used in more than one definition. Some of these have the appearance of a syntactic pattern (ex., to cause to) while others more directly reflect their lexical nature (ex., longer have because). Lexical patterns are the logical place to start because they are the lowest level of analysis that seems likely to contain sufficient information for role extraction. Obviously, we could have included tagging, syntactic analysis, syntactic patterns, statistics on the box codes or some other information. By focusing on the lowest level of processing, however, we ensure that any information that is extraneous to the process (ex., syntax) is ignored and moreover, that our results will be easily repeatable and scaleable to general text processing.We used a modified Matrix Model (Cook 1989) as a template for the roles. The Matrix Model (Figure 1)   This allows the assignment of role frame by determining the proper row and then the proper column for the verb. Our research goal has been the development of a domain independent natural language processing (NLP) system suitable for information retrieval. As part of that research, we have investigated ways to automatically extend the semantics of a lexicon derived from machine-readable lexical sources. This paper details the extraction of thematic roles derived from lexical patterns in a machine-readable dictionary. Introduction With information retrieval as a goal, we are very sensitive to the issues of generalization, speed and scalability. Any NLP system that is used for information retrieval must be capable of handling large amounts of general text in a timely manner. Each of the components of such a system, from morphology through semantics, must have similar capabilities. Thematic roles, which provide a very basic &quot;who does what to whom&quot; type of semantics, meet these criteria because they are very general, simple and useable by a wide variety of natural language processing systems. The roles are generally contained in frames that contain the type of each argument for each verb. For example eat would have a frame similar to eat[AGENT,
Information and Computation(PACLIC 11) Te-ir resultative is a type of te-ir construction in Japanese, which is composed of a lexical verb followed by an auxiliary verb it '(literally) for an animate entity to exist.' Te-ir sentences can take on such meanings as "progressive," "resultative," "habitual," and "experience," among others. Of particular recent interest is the distinction between the progressive reading and the resultative reading, as illustrated by the following examples:(1) a. Kyoko -ga ima ki -o taosite-iru. NOM now tree ACC fell-IR 'Kyoko is felling the tree.' b . Ki -ga ima taorete-iru. tree NOM now lie-IR 'A tree has been felled (is lying down).' c. Ki -ga ima taos-arete-iru. tree NOM now fell-PAS-IR 'A tree has been felled/is being felled.' d. Kyoko -wa ima kami -o somete-iru. TOP now hair ACC dye-1R 'Kyoko has dyed her hair/is dying her hair.' Thus the sentence (la) takes on a progressive meaning, while (lb) takes on a resultative meaning. 2 (1c) and (1d) are ambiguous. This paper focuses on the long-standing issue of predicting when the te-ir sentence can convey the resultative reading.One complication with the resultative interpretation is the relevance of the verb meaning. Some verbs allow resultative interpretation only in the te-ir form, while others allow resultative interpretation in its "base" form (i.e, without te-ir) : (2) a. Kyoko -wa ima Taroo -no koto -o oboete-iru. TOP now GEN thing ACC remember-1R 'Kyoko remembers (has remembered) Taroo.' b. * Kyoko -wa go-nenkan Taroo -no koto -o oboeta.TOP for 5 years GEN thing ACC remember 'Kyoko remembered Taroo for five years.' (3) a. Kyoko -wa ima hon -o kasite-iru. TOP now book ACC lend-IR 'Kyoko has lent a book.' b . Kyoko -wa san-syuukan hon -o kasita. TOP for 3 weeks book ACC lend 'Kyoko lent a book for three weeks.' The relationship between the resultative interpretation in base form and the one in the te-ir form seems to involve factors too complicated to discuss here. The discussion in this paper, therefore, focuses only on the resultative interpretation in the te-ir construction, ignoring the same interpretation in base forms. The assumption here is that the te-ir construction facilitates the resultative interpretation of the sentences as long as they have the potential of the interpretation. This paper is organized in the following way. The next section (Section 2) analyzes the basic data in order to abstract crucial generalizations, drawing on major studies in the literature. A particular reference will be made to Takezawa's (1991) syntactic analysis andKim's (1993) semantic analysis. Section 3 presents data which both Takezawa (1991) and Kim (1993) fail to accommodate. A new approach will be proposed in Section 4. Section 5 concludes the discussion. This paper discusses semantic factors contributing to the resultative interpretation of predicates in the Japanese te-ir construction. The construction ambiguously takes on either progressive or resultative meanings. This ambiguity is due to the lexical meaning of the verb, and it is the purpose of this paper to single out and characterize the classes of verbs which take on the resultative meaning. l A number of recent studies have focused on the issues of telicity, transitivity, and particularly on unaccusativity and reflexivity, and it has been argued that the resultative interpretation is closely related to the subject&apos;s involvement in the resulting state. While accepting this argument, I will show that notions of unaccusativity and reflexivity alone cannot cover all the data. Similarly, I will argue that Kim&apos;s (1993) account of the analogous construction in Korean, which refers to the concept &quot;possession,&quot; fails to accommodate a certain set of data without making unlikely stipulation. I will demonstrate that there are in fact two separate sets of verbs allowing the resultative meaning. The first set is definable in terms of the subject&apos;s involvement, while the second set makes no reference to the subject&apos;s involvement in the resulting state. I will characterize the second set as verbs of spatial configuration (Levin 1993) and propose the notion &quot;affected locative&quot; to optimally characterize the semantic feature licensing the resultative meaning in the second set.
Information and Computation(PACLIC 11) In Japanese as well as in Korean there are such expressions as follows.(1) watasi wa ringo wo tabetai (Japanese) ITop. apple Obj. eat-want I want to eat apples. (2) na-nun sagwa rul mokkoship ta. (Korean)It should be noticed that -tai desirative, which is so called by Kuno (1973), is mainly used for the first person singular or plural as a subject. In addition to this expression, there are other expressions which are mainly used for the third person as a subject. (3) kare wa rongo wo tabe ta gatteiru (Japanese) he Top. apple Obj. eat want show the sign of He shows the sign of wanting to eat apples. (4) ku nun sagwa rul mokkoshi phohagoitta. (Korean ) In the tradition of Japanese linguistics the main interest has been mainly focused on the following issues. (a) How to derive the type of sentences like (3) from that of (1). (b) What is a relationship between the sentence (1) and (5) below, where nominative case ga is employed instead of wo ? That is, under what condition wo -ga alternation occurs.(5) watasi wa ringo ga tabetai I Top. apple Nom. eat want I want to eat apples. (c) How to derive a sentence like (1) from a sentence like (6) below. (6) watasi wa ringo wo taberu. I Top. apples Obj. eat So far no serious attempt has been made except Sugioka (1986) to treat -tai suffix as a complex adjective forming suffix by combining with an intransitive or a transitive verb and to make an inquiry into the semantic structure of such an adjective. 1. The Theoretical Framework 1.1. The Semantic Structure of Adjectives 1.1. This paper treats the Japanese adjective phrase forming derivational suffix-tai from a new point of view: firstly it tries to approach from a semantic standpoint by applying the proposal made in Ikeya (1991). It will be shown that adjective phrases formed by-tai fits nicely with the semantic structure proposed by Ikeya. Secondly, we attempt to &apos; derive &apos;-tai sentences, by adopting a basic framework of HPSG so that we can &apos; derive &apos; them without having recourse to transformational operations, that is, in a monostratal way. In tackling the problem we have tried to incorporate many ideas proposed so far on this issue.
Information and Computation(PACLIC 11) Corpus-based approaches are fast becoming the most essential and productive technique for theoretical and computational linguistics research (Svartvik 92, Church &amp; Mercer 93). Their impact reaches almost all areas of natural language studies, such as speech processing, information retrieval, lexicography, character recognition etc. Version 2.0 of the Academia Sinica Balanced Corpus (Sinica Corpus) contains 5,345,871 characters, equivalent to 3.5 million words. The Sinica Corpus is the first balanced Chinese corpus with part-of-speech tagging. The following issues have been the major concerns in designing the Sinica Corpus: 1. organization of the corpus, 2. preparation of the corpus, and 3. the use of the corpus. Since a corpus is a sampling of a particular language or sublanguage, which contains an infinite amount of data, it must be representative and balanced if it claims to faithfully represent the facts in that language or sublanguage (Sinclair 87). However there is no reliable criteria for measuring the balance and representation of a corpus. The Brown corpus is the first corpus that claimed to be balanced. It takes the topic domain distribution as the only balancing criterion. In the Sinica Corpus, we explore the possibilities of multidimensional attributes and try to balance the corpus in each dimension. The detailed organization of the Sinica Corpus is discussed in section 2. The Sinica Corpus is a word-based Chinese corpus with part-of-speech tagging. Word segmentation, automatic part-of-speech tagging and quality assurance are major concerns after text selection. They are discussed in section 3. The tools for using a tagged corpus are illustrated in section 4. The Academia Sinica Balanced Corpus (Sinica Corpus) is the first balanced Chinese corpus with part-of-speech tagging. The corpus (Sinica 2.0) is open to the research community through the WWW (http://www.sinica.edu.twiftms-binikiwi.sh). Current size of the corpus is 3.5 million words, and the immediate expansion target is five million words. Each text in the corpus is classified and marked according to five criteria: genre, style, mode, topic, and source. The feature values of these classifications are assigned in a hierarchy. Subcorpora can be defined with a specific set of attributes to serve different research purposes. Texts in the corpus are segmented according to the word segmentation standard proposed by the ROC Computational Linguistic Society. Each segmented word is tagged with its part-of-speech. Linguistic patterns and language structures can be extracted from the tagged corpus via a corpus inspection program which has the functions of KWIC searching, filtering, statistics, printing, and collocation.
Information and Computation(PACLIC 11) Bunrui-goi-hyou (The National Language Research Institute 1994) represents the Japanese thesaurus, and is used in many researches. Needless to say, it is important to expand and refine Bunrui-goi-hyou for the sake of Japanese natural language processing. This paper presents the method to automatically find meanings which must be entered in Bunrui-goi-hyou but not be entered. By this method, we can efficiently fill the deficiency of meanings in Bunrui-goi-hyou.We should notice that our method automatically executes the following:(1) to estimate that a certain noun n has the meaning g which is not entered as a meaning of this noun n in Bunrui-goi-hyou,(2) to show some nouns which have the similar meaning to the meaning however, our method does not trace what the meaning g is. This meaning is manually decided by observing nouns shown in (2). For example, the Japanese noun "J*" has meanings "voice" and "opinion ", but "r . " in Bunrui-goi-hyou has only "voice" and does not have "opinion // . Our method points out that "r4 " in Bunrui-goi-hyou has a deficiency of a meaning, and shows some nouns such as "SR (opinion)" ,"NS (view)" ,"1 (insistence)" etc. which have the similar meaning to "opinion". By observing these nouns, we can manually decide that the lacking meaning of "P" is "opinion".The traditional research to acquire the unknown meaning was done by (Wilensky 1990). In his research, if a sense of a word is unknown, the sense is estimated from similar uses of other words to the use of the word. This approach, which estimates a feature of an unknown word from features of similar words, is taken to cope with the data sparseness problem of a corpus (Dagan 1993). In short, this approach is base on the idea that the unseen part can be estimated from seen similar parts. However, the simple use of this idea alone cannot find an unknown meaning of a word, because even similar words hardly have same polysemy to the word. For example, nouns such as "% (laughter)" ,"' (cheer)", "E PA (scream)", " 7..12 son g)" etc. are similar to the noun "P", but these nouns don't have the ( meaning "opinion".To estimate a lacking meaning, this paper applies the method presented by (Shinnou 1995), which extracts idioms from a corpus. In his research, first, the set of nouns which are co-occurred with a verb v is constructed from a corpus. Second, similar nouns to each other are removed from the above set. In this step, the similarity is measured by Bunrui-goi-hyou. Last, idioms are constructed from left nouns in the set and the verb v.Suppose a noun n have multiple meanings and a meaning in them is not entered as a meaning of the noun n in Bunrui-goi-hyou. In above second step, the noun n tend to be left in the set. That is, we can estimate the noun n with the meaning lacking in Bunrui-goi-hyou by the clue that many idioms with the noun n are extracted through the above steps. Next our method extracts nouns which have a similar meaning to the lacking meaning of the noun n by mutual information. These nouns can be associated with the lacking meaning.Our method can find not only a deficiency of a meaning but also meanings which are not used or are specifically used in the corpus domain.We have experimented by the corpus which consists of Japanese economic newspaper 5 years articles with about 7.85 M sentences. We report the result of this experiment.2 Estimation of nouns with the lacking meaning Shinnou (Shinnou 1995) proposed the method to extract predicative idioms from a corpus by the lexical peculiarity for the noun in an idiom. His method firstly gathers cooccurrence data with the form [noun, wo, verb] from the corpus. For example, from the sentence "IYM I, 4A --btKii) t (I drank whiskey yesterday )" , cooccurrence data P, ttS ( [whiskey, ,obj, drink]) is extracted. Second, the method chooses a verb v, and gathers nouns which can be an object of the verb v from cooccurrence data. Suppose the chosen verb is "ftt (drink)", we can gather " r, 'CA (whiskey)" and " (request)" etc. from [ I, 'CA fttS] and [N*, ',CeKt.1] etc. Next, similar nouns to each other are removed from these gathered nouns. Last, idioms are constructed by left nouns and the chosen verb. Figure 1 shows an example. In this example, "ffik..1* --tS" and ",1 2, --WS" are extracted as idioms. These extractions are correct.Figure.! Suppose a noun n has multiple meanings and one meaning in these meanings is not entered as a meaning of that noun n in Bunrui-goi-hyou. Through above processes, expressions which comprise the noun n are extracted as idioms if the noun n in each expression is used as just the lacking meaning. These extractions are incorrect. For example, Figure 2 shows nouns co-occurred with the verb "W:1'6 (distinguish)". In this example, the expression "L --AL-4-6" is extracted as an idiom, but it is incorrect.  The noun "A" has the meaning "point of view", and "A*" is the similar noun to "SR (opinion)" and "RA (view)". Thus, the expression "AiS --V::4-6" should not be extracted. This mistake occurs for lack of the meaning "point of view" of the noun "5" in Bunrui-goi-hyou.As meanings of the noun "AJM" in Bunrui-goi-hyou, only the meaning "standing point" is entered.This mistake occurs not only in the case of the verb "Az."6" but also in the case that the noun "Atl" in the expression is used as the meaning "point of view". For example, expressions such as "AtiV91,41-6", "A A* '6", "AtitETZ", "AJMNeil-Z" are incorrect extractions. Our method removes correct extractions, that is real idioms, from expressions extracted by the above method. Next, our method classifies left expressions according to the noun in the expression. If the class for a noun is big, we can estimate that the noun has a lacking meaning.To decide whether an expression is an idiom or not, we use the idiom dictionary (Inoue 1994). The expression is decided as an idiom if it is an entry in that dictionary. This paper presents a method to automatically find meanings which should be but are not entered in thesaurus. In this paper, we use Bunrui-goi-hyou as the thesaurus. To find the noun n with the meaning lacking in Bunrui-goi-hyou, we applies our presented method which extracts idioms from a corpus. We use the clue that many idioms with the noun n are extracted through that method. We have experimented with a corpus which consists of 5 years&apos; worth of articles from a Japanese economic newspaper. As a result, we found 177 types of lacking meanings. Furthermore, our method can find not only a deficiency of a meaning but also meanings which are not used or are specifically used in the corpus domain.
Information and Computation(PACLIC 11) For some three decades now, arguments about the character of sentence processing mechanisms have taken the obvious difficulty of multiply centerembedded sentences as a major point of focus. In head-initial languages such as English, doubly-embedded relative clauses (and especially, doubly-embedded object relatives) can be peculiarly problematic. In head-final languages such as Japanese, multiply-embedded sentential complements induce something like the same difficulty. Summarizing the special burdens offered by multiply centerembedded sentences, Eady and Fodor (1981) concluded that the processor has particular trouble with sequences of consecutive NPs, separated in the surface word string from the remainder of their clauses.Sentence processing theories routinely predict that repetitive sequences of this kind lead to comprehension breakdown, though the grounds differ between frameworks; for example, Stabler's (1994) theory of case-assignment relations supposes the processing mechanism to be restricted in its ability to keep track of grammatical relations of the same kind; and Lewis's (1993Lewis's ( , 1996) theory of X'-relations supposes that a sharply limited working memory suffers similaritybased interference when it is overloaded. For Babyonyshev and Gibson (1995) (henceforth, B&amp;G), the claim is that two factors determine level of difficulty for multiply center-embedded sentences: First, the number of structurally-cased NPs (Nom, Acc, and Gen) not yet assigned theta-roles; and second, the number of self-embeddings (SEs). In B&amp;G's theory, sentences become unprocessable when the number of Processing Load Units (PLUs) they accumulate exceeds a critical limit which is set universally at 4 PLUs.Native-speaker intuition suggests, for Japanese, that center-embeddings manifest a source of processing difficulty not yet recognized in B&amp;G's metric, and handled only indirectly by other theories of sentence processing; and that is, beyond the known problems of NP-repetition, a cost of NP-ga repetition arising when more than two consecutive NPs bear the nominative case-marker -ga. In this paper, we present our investigation of NP-ga repetition in multiply centerembedded sentences, and confirm that the ga-effect has a substantial impact on processing difficulty. We also report a follow-up study investigating whether NPs bearing the same case are less discriminable than they otherwise would be, for non-syntactic reasons. Here, we make use of a phonologically conditioned alternation in the nominative case-marker in Korean, and find that identity in a particle's phonological form is not what is at issue. This study investigated the effect of more than two consecutive NPs bearing the nominative case-marker-ga on the processing difficulty associated with Japanese multiply center-embedded sentences. Experiment 1 found that NP-ga sequences are rated substantially less favorably for processability ratings than their matched counterparts; thus, theories that predict processing load must offer an account of this special source of processing difficulty. Experiment 2 used an alternation in Korean nominative particles-i and-ka, phonologically triggered, to narrow the sense in which it might be said that the cost of NP-ga sequences arises because of a problem in discriminating among consecutive NPs. In this case, processability ratings provided no evidence that confusable NP-Nom&apos;s are made more distinct by purely phonological variation in their case-markers.
Information and Computation(PACLIC 11) In this paper I address the ambiguity of donkey sentences, namely multiple proportional readings. Kadmon (1987) notices that a conditional donkey sentence may have three different proportional readings, i.e., an asymmetric reading oriented to either a subject or an object, and a symmetric reading, while a relative-clause donkey sentence is unambiguously asymmetric to a head noun. Recently, it has been observed by Heim (1990) and Chierchia (1992) that the proportional readings of a conditional donkey sentence are disambiguated by the topic structure.As always resonated in the literature of donkey sentences, a solution to the proportion problem hinges on the semantics of indefinite NPs and quantificational adverbs. Two main analyses have been suggested for this problem, i.e., Heim (1990) and Chierchia (1992). Both of the analyses assume that indefinite NPs are existential quantifiers, and quantificational adverbs are quantifiers over situations or events. Based on this assumption, they assign multiple proportional readings to donkey sentences; however, neither of them give a convincing explanation for the role of topic structure, namely disambiguating proportional readings.I start with my previous study, Kwak (1995), about the ambiguity of indefinite NPs and the semantics of quantificational adverbs. For independent reasons, I have proposed that indefinite NPs are ambiguous between quantificational and cardinal readings, which are sensitive to topic structure, and cardinal readings of indefinite NPs are not generalized quantifiers but 'event-dependent' individuals. Quantificational adverbs are defined as quantifiers over events. Based on this study, I consider how proper readings of donkey sentences are derived from the semantics of indefinite NPs and the syntactic representations of topic Section 2 concerns multiple proportional readings of donkey sentences and sensitivity to topic structure. In section 3, previous analyses will be critically reviewed. Finally, in section 4, I will summarize my previous study, and show how this work leads to an appropriate account for the current issue. This paper addresses multiple proportional readings in donkey sentences and deals with a question of why the disambiguation of proportional readings is affected by the topic structure of a sentence. The correlation between proportional readings and topic structure has been observed by Heim (1990) and Chierchia (1992) but has not been given a satisfactory explanation yet. Based on my previous study that indefinite NPs are ambiguous between quantificational and cardinal readings, and proper readings are determined by the topic structure, I will show how appropriate readings of donkey sentences are derived from the semantics of indefinite NPs and the syntactic representation of topic structure.
Information and Computation(PACLIC 11) So-called zero pronominal (i.e., unexpressed pronoun) languages like Chinese,Japanese and Korean are known to be highly context-dependent languages, and pronouns (or any clausal arguments recoverable from the context, for that matter) in these languages are said to be freely dropped (e.g. Sells 1989, Hudson 1994, for Japanese). This seems to suggest the idea that overt pronouns in English, more or less, correspond to zero pronominals in these languages. In the Centering approach to pronoun resolution in discourse, for example, Kameyama (1986), after setting up the rule for pronouns in English, states that the basic rule in Japanese "can be obtained by changing the word pronoun to zero pronominal" in the English rule although she proposes an additional rule for Japanese discourse.Some questions arise, however, at least on the simple, one-toone matching between the anaphoric mechanisms of the two languages. First, if anaphoric pronouns in English always correspond to zero pronominals in the same context in Japanese, what is the function of the so-called pronouns in Japanese (e.g. kare 'he', kanojo 'she')? Similarly, how do those discourse functions expressed by the discourse particles like wa 'topic' in Japanese interact with zero pronominals of the language since such discourse functions take the form of [noun + particle], where the overt (pro)nominal form is required? Also, in terms of methodology, how do we know that the two relevant clausal arguments in English and Japanese are in the same discourse context?These questions are particularly important because most previous analyses of this issue have based their conclusions on constructed discourse fragments and informants' out-of-context interpretations and acceptability judgments of them.The current research attempts to answer these questions, by using the method of text analysis for the collection of natural discourse data. To see the degree of correspondence between pronouns in English and zero pronominals in Japanese, I chose 0. Henry's famous story, The Last Leaf, and its Japanese translation by Yasuo Ohkubo, to see what those pronouns in the English original are translated into in the corresponding Japanese translation.Before starting the discussion, I would like to comment on the reasons for choosing the two texts for our data collection. The last Leaf by 0. Henry was chosen for the English original because the story has been popular among Japanese people and there have been several translations available in Japanese. Also the story is conveniently short enough for us to take the whole story into the scope of our discourse analysis (rather than some excerpt from a longer story, which can hinder us from observing all the discourse factors). Furthermore, the story has multiple characters possible as referents of each pronoun; the characters in the story include "Sue" and "Johnsy" for she, and "the doctor" and "Old Behrman" for he.Among the Japanese translations of The Last Leaf, Ohkubo's translation was chosen because his is most widely accepted; it was first published in 1969, and has been reprinted many times. The book used for the current analysis is in its 57th print and was printed in 1995. In other words, this translation seems to be written with rather natural language of Japanese. Incidentally, this translation has proved to be very close to the original in English in terms of sentence and paragraph divisions, which makes it easier to identify the correspondents between the two texts. This paper presents the results of a comparative text analysis of English anaphoric pronouns &amp; their Japanese counterparts. It first demonstrates that although anaphoric pronouns in English are often compared to zero pronominals in Japanese, not all, but only some, of the English anaphoric pronouns have their counterparts in zero pronominals in the same context in Japanese, and then argues that discourse and perspective factors, which are both structurally prominent in Japanese, play a crucial role in anaphor resolution in the language.
Information and Computation(PACLIC 11) TEI (Text Encoding Initiative) is an international project which aims to provide some guidelines for encoding various kinds of texts in electronic forms. It conforms to the ISO standard of Standard Generalized Markup Language (ISO 1986). The primary result of this project, published in 1994, Guidelines for Electronic Text Encoding and Interchange (TEI P3) edited by Burnard and Sperberg-McQueen, covers many kinds of texts in depth. Dictionary markup is one of them. In this paper, we investigate ways to use and extend TEI encoding scheme for the markup of Korean dictionary entries. To accomplish this objective, it is necessary to consider the logical structure of the Korean dictionary entries.Although TEI suggestions for dictionary encoding (markup) is very comprehensive to cover various kinds of dictionaries, its original commitment is to consider only western language dictionaries Ode and Veronis 1995: 168). We need to cope with problems to be encountered in encoding Korean dictionary entries in conformance with TEI. We try to extend and modify the TEI encoding scheme in the way suggested by TEL In addition, we restrict the content model so that the encoded dictionary might be viewed more as a database than as a simple computerized (originally printed) dictionary.[1] [2] 2. Top Level Elements of a Dictionary Entry Before presenting the model of Korean dictionary entries, let us go over the basic dictionary scheme provided by TEI.  ) TEI P3: Basic Structure of a Dictionary   &lt;text&gt;  &lt;body&gt; &lt;entry&gt; ... &lt;/entry&gt; &lt;entry&gt; ... &lt;/entry&gt; &lt;superEntry&gt; &lt;entry&gt; ... &lt;/entry&gt; &lt;entry&gt; ... &lt;/entry&gt;The text of a dictionary consists of a number of &lt;entry&gt; elements, each of which can consist of a number of &lt;hom&gt;(i.e. homonym) elements.Some dictionary elements, called dictionary top level elements can appear at the level of entry, homonym, and sense, disregarding superentry element for the moment. The following DTD definitions show this:  The definitions of &lt;entry&gt;, &lt;horn&gt;, and &lt;sense&gt; include dictionary top level elements, which are defined as follows: As defined by m.dictionaryTopLevel entity reference, these top level elements are &lt;def&gt;, &lt;eg&gt;, &lt;etym&gt;, &lt;form&gt;, &lt;gramGrp&gt;, &lt;note&gt;, &lt;re&gt;, &lt;trans&gt;, &lt;usg&gt;, and &lt;xr&gt;. For Korean dictionary entries, these 10 elements are all are needed. Overall, some elements can be used as they are provided by TEI, and others need to be modified according to specific needs of a Korean dictionary, in the way to be specified from now on.First of all, the following dictionary top level elements elements are to be used without modifications for the markup of Korean dictionary entries: 1) &lt;def&gt; for definition; 2) &lt;trans&gt; for translation (not used in a monolingual dictionary); 3) &lt;eg&gt; for usage examples; 4) &lt;note&gt; for any kind of notes; 5) &lt;re&gt; for related words. Other dictionary top elements are to be modified as follows.For the &lt;form&gt; element, besides usual &lt;orth&gt; (orthography) and &lt;pron&gt; (pronunciation) elements, we need a sub-element which contains a form showing long vowels and a major morphological constituent break, which is usually marked for entries in Korean dictionaries.We call this element &lt;lenHyph&gt;.In the following example, we provide -the part tagged by &lt;lenHyph&gt; along with parts of &lt;orth&gt; and &lt;pron&gt;.(4) Eg. ,1;?1ZEJC4 'courageous' &lt;form&gt; &lt;orth&gt;gg-Z`. _.t CWorth&gt; &lt;lenHyph&gt;g CE&lt;/lenHyph&gt; &lt;pron&gt;-ECE&lt;/pron&gt; &lt;/f orm&gt; Another top level element is &lt;gramGrp&gt; for grammatical information of an entry. Besides &lt;pos&gt; and &lt;subc&gt; elements provided by TEI P3, we seem to need some elements which specify the kinds of irregular inflection for some verbs and exemplary inflected forms. These are marked by tags of &lt;irreg&gt; and &lt;irrForm&gt;, as shown in the following example.The usage note of an entry (&lt;usg&gt;) can contain academic domains (special fields) in which this item is used, other domains (such as marking for 'old Korean'), and dialect areas, which are prominent in Korean dictionaries. These can be encoded with new tags defined within &lt;usg&gt;. They are &lt;domAca&gt;, &lt;domEtc&gt;, and &lt;dialArea&gt;. They can be used as follows.Eg. ±1.41 7 shower &lt;usg&gt;&lt;dialArea&gt; g ELF&lt;/dialArea&gt;&lt;/usg&gt; Eg. 7 Eg 'river' &lt;usg&gt;&lt;domEtc&gt;9e5V&lt;/domEtc&gt;&lt;/usg&gt;Since the content and format of etymology in a Korean dictionary is constrained in certain ways, some modifications of the DTD definitions of the &lt;etym&gt; element are needed. For this element, we add a new attribute 'hdType' whose value should be one of: 'hj' (for hanja, i.e. of Chinese origin: content being given in Chinese characters), 'foreign' (of any other foreign origin), and 'kor' (of Korean origin proper). For a limited number of kinds of cross reference in a Korean dictionary, we can define various empty elements which mark the kinds of cross reference to be used in the dictionary. Among them are 'synonym', 'antonym', 'long form', 'short form', 'honorific form', etc. One of these elements should be used in the first part of &lt;xr&gt; element. In the first example provided below, a tag specifying antonymy is used.g-'success' &lt;xr &gt; &lt;xrant&gt;&lt;ref &gt; 1lil &lt;/ref &gt;&lt;/xr &gt; Eg. UEXI al &lt;xr&gt;&lt;xrlarge&gt;&lt;ref &gt; XI &lt;/ref&gt;&lt;/xr&gt; &lt;xr&gt;&lt;xrstr&gt;&lt;ref&gt; xi al &lt;/ref &gt; &lt;/xr&gt; Dictionary markup (encoding) is one of the concerns of TEI (Text Encoding Initiative), an international project for text encoding. In this paper, we investigate ways to use and extend TEI encoding scheme for the markup of Korean dictionary entries. Since TEI suggestions for dictionary markup are mainly for western language dictionaries, we need to cope with problems to be encountered in encoding Korean dictionary entries. We try to extend and modify the TEI encoding scheme in the way suggested by TEI. Also, we restrict the content model so that the encoded dictionary might be viewed more as a database than as a simple computerized, originally printed, dictionary.
Information and Computation(PACLIC 11) Korean has many irregular transformation such as contraction. Korean morphological analysis system MoA treats contraction within its system (J.-H. . We propose a method to treat such phenomena by means of a verb conjugation system. Korean verbs generally have many ending forms in conjugation. For example, 7}c}.(go) has 7F as its stem, and takes a variety of conjugated endings such as id L. , 7 1 ad, L and so on. In this way each verb requires a distinct set of ending forms. When we connect the ending eat..1 zF with a verb, 7 FU 1-becomes 11-LI (stem:7F + ending: H LI-CO but 1:c1-(eat) becomes g4 OL1 El-(stem: 124 + ending: trLI 1-). The ending LILF takes the form id LI El-or ;ILI LF according to the verb. The. conjugated form of the ending depends on the verb to which it is connected. When we compile a dictionary, we have to include all possible words with possible endings. However, this method is not practical.We propose a method in which all surface variations are explained by verb conjugation. For example, as for the nonconjugational ending H Li*, the verb 7I-CF conjugates to 71-and the verb C F conjugates to Then, 7F CI-becomes 7 ,:ftirl-(stem:7F + conjugational ending:none + nonconjugational ending: td LIEF), and becomes r F (stem: + conjugational ending: + nonconjugational ending: H LI CF).After proposing a verb conjugation system, we describe a Korean morphological analysis system as a direct application of it.In Korean morphological analysis, methods to reduce ambiguities have been studied. However, most of the systems analyze sentences only within segments (Eojeol, i.e., a sequence of morphemes surrounded by spaces)(J.-H. Kim 1995). We propose a method to reduce some ambiguities by means of using an information over segment boundary. This paper presents a new Korean verb conjugation system, which enables an easy treatment of Korean morphological phenomena such as contraction. This makes the size of the dictionary for ending forms to be small. We also introduce a Korean morphological analysis system. Korean morphological analysis system generally analyzes sentences within the segments(a part between spaces). We propose a system that considers the information beyond segmentation.
Information and Computation(PACLIC 11)  Korean as an agglutinative language shows its proper types of difficulties in morphological disambiguation, since a large number of its ambiguities comes from the stemming while most of ambiguities in French or English are related to the categorization of a morpheme. The current Korean morphological disambiguation systems adopt mainly statistical methods and some of them use rules in the postprocess. In our approach, the morphological analyzer reduces the number of the candidate morpheme strings using adjacency conditions when it analyses a word into morpheme strings. And then the disambiguation depends on rules and statistics successively. As for the rules, the partial parsing using finite state automata decides the compatibility of each pair of words: a negative value is assigned if a word can not co-occur with another word, while a positive value is given if they are compatible. After applying all the rules related to the word, our system chooses only the positively valued strings. When more than two strings still have same value, the priority in the context is decided by the statistics in the next stage. The accuracy of our approach as Korean tagging system is about 97.1% and it may yeild a better result than the Korean morphological disambiguation systems.
Information and Computation(PACLIC 11) Bresnan and Kanerva (1989) proposes Lexical-Mapping Theory (LMT) to correlate the syntactic structures and representations with the predicate argument structure of verbs. In this theory, the syntactic representation is determined by the predicate argument structure of a verb.However, widely accepted LMT account has not been given to the control structure : the widely adopted theory of control is still Bresnan's functional control based on grammatical functions and structural terms instead of the predicate argument structure of verbs. Examining the semantic alternation of the ho construction in Taiwanese, which will be given in Section 2, we find that functional control cannot account for the phenomenon satisfactorily and then we propose argument control both to replace functional control and to try to be in line with the essences of LMT. The Lexical-Mapping Theory (LMT) in Lexical-Functional Grammar (LFG) predicts syntactic representations by mapping from the lexical predicate argument structure of the verbs (Bresnan and Kanerva 1989). Yet the widely followed account of control still is Bresnan&apos;s (1982) functional control based on grammatical functions and structural terms. In this paper, we try to propose a theory of control in line with the LMT theory. The new control mechanism is called Argument Control. Facts involving the HO construction in Taiwanese will be given first to show the inadequacy of Functional Control. Then, based on the observation that the HO construction in Taiwanese manifests a semantic alternation determined by both the property of the matrix subject and of the embedded subject, we propose the theory of argument control. In this theory, the control relation lies between two thematic roles.
Information and Computation(PACLIC 11) It has been pointed out in the linguistic literature that Japanese allows question markers to be optionally omitted in informal speech (Lasnik &amp; Saito, 1992;Inoue, 1996).1 (1) a. John-wa doko-ni ik-imashi-ta (ka)?'Where did John go?' John-Top where-to go-Polite-Past (Q) b. hon-o kat-ta (no)? Did you buy a book?' book-A buy-Past (Q) The question marker drop (QM-drop) phenomenon is commonly observed in informal speech, but it is not the case that it applies freely without any constraint.2 Our major goal is to provide an analysis of the QM-drop phenomenon within the framework of Principles and Parameters (P&amp;P) approach. Adopting a standard assumption that the interrogative force is carried by C° with the LF interpretable feature [+wh] or [+Q],3 we will argue that there are three basic ways to license the interrogative feature of the sentence in Japanese: (i) by overt realization of the [+wh] or [+Q] feature with morphological question markers, (ii) by dynamic agreement of Rizzi (1996), an instance of Spec-head agreement of [+wh] feature, and (iii) by I-to-C head-movement.The paper is organized as follows. In the next section, we provide a brief description of the QM-drop phenomenon. 4 In section 3, we discuss QM-drop in wh-questions, based on Rizzi (1996). Section 4 focuses on QM-drop in yes/noquestions. Section 5 deals with embedded questions, and we discuss some theoretical consequences from our analysis of QM-drop. In section 6, we examine Korean data. The final section is a summary.2. QM-Drop in Japanese QM-drop is always possible with regular lexical verbs as in (1) above while it is sometimes blocked with copulative verbs (Inoue, 1996). Thus, the non-past tense copula presents a grammatical contrast with or without wh-elements as in (2), but the past tense copula does not as in (3). (2) a. sono hito-wa John desu *(ka)?5 'Is that person John?' that person-T John Cop (polite) *(Q) b. sono hito-wa dare desu (ka)? 'Who is that person?' that person-T who Cop (polite) (Q) (3) sono hito-wa John / dare deshi-ta Ocar that person-T John / who Cop-Past (polite) (Q) 'Lit. Was that person John / who?' In embedded questions, QM-drop is completely banned (Lasnik &amp; Saito, 1992). (4) John-wa [Mary-ga gakkoo / doko-ni ik-u *(ka)] Ei-ta. John-T Mary-N school / where-to go-Pres *(Q) ask-Past Lit. John asked Mary was going to school / where. ' (4) shows that a wh-element has no influence on grammatical judgment with regular lexical verbs. The same pattern is observed with copulative verbs as shown in (5). Notice also that the tense specification makes no distinction. (5) a. Mary-wa [sono hito-ga John / dare da *(ka)] Mary-T that person-N John / who Cop *(Q) ask-Past b. Mary-wa [sono hito-ga John / dare dat-ta *(ka)] kii-ta. Mary-T that person-N John / who Cop-Past *(Q) ask-Past Lit. Mary asked if person was John / who.' To summarize, in the matrix clause, QM-drop in Japanese is allowed with regular lexical verbs without any restriction (present or past, and with or without a wh-element). There is some complication with copulative verbs. Regardless of difference in the tense and politeness specifications, QM-drop is possible if the copulative sentence contains a wh-element. With no wh-element in the copulative sentence, only the past tense form can license QM-drop. No QM-drop is permitted in the embedded question. This paper discusses the feature checking mechanism of interrogative sentences in Japanese and Korean. We first focus on a phenomenon of omitting question markers in informal speech in Japanese and attempt to provide an account for it within the framework of Principles and Parameters approach. We argue that question markers can be omitted only if interrogative features of the sentence can be properly checked. In particular we claim that I-to-C head-movement is me of the options for interrogative feature checking in Japanese as well as languages without question markers. A close examination of Korean reveals certain differences between Korean and Japanese. Some theoretical consequences from this analysis are also discussed.
Information and Computation(PACLIC 11) The categorial status of locative and temporal WHs has been of some interest in investigations in the literature. In the next section we will review two theories bearing on this issue. Huang (1982) assumes a phonetically null P in order to explain the argument-like pattern of when and where observed in covert movement, while Murasugi and Saito (1992) propose, instead of this null P, that when and where are actually arguments of the sentence. We point out in Section 2.3 that Murasugi and Saito's analysis is inadequate in handling some data related to overt movement, such as relativization. We thus suggest maintaining Huang's null P hypothesis. Nevertheless, the examples given by Murasugi and Saito indeed pose a problem for Huang's null P theory. The goal of this paper is to look for an analysis so that we can preserve the null P on the one hand, and give a plausible account for the relevant examples in question. The answer, we believe, lies in an LF process, the Null-P incorporation, which we discuss in detail in Section 3.2. On the distribution of temporal and locative WHs 2.1 Huang (1982) One of the curious generalizations reported in Huang (1982) is about the categorial status of locative and temporal WH-phrases. Huang observes that in Chinese when nali 'where' and shemeshihou 'when' undergo LF movement, they behave like arguments rather than adjuncts. The following examples involve a WH-phrase insitu within the Complex NP.(1) a. ni du guo [NP [ The generalization is that argument WHs such as shei 'who' in (la) can occur within an island, whereas adjunct WHs like weisheme 'why' in (1d) cannot. Huang argues that (lb,c) are well-formed because nali 'where' and shemeshihou 'when' appear in the categorial position of [pp P [NP ], where the prepositions can be phonetically null. Thus, nali and shemeshihou are complements of the Ps and are on a par with shei 'who' in (la) in being arguments (although the PPs containing nali 'where' and shemeshihou 'when' are adjuncts).The same analysis extends to English as well. (2a) and (2b) show the familiar argument/ adjunct asymmetry: while the former can stay in-situ, the latter cannot. As the grammaticality of (2c) shows, when and where pattern with argument (i.e. what) in this respect. This follows naturally under Huang's analysis, since, as shown in (3), when or where is a complement and thus an argument of the null P.  The LF representation in (4a) satisfies the ECP which requires that a nonpronominal empty category must be head governed or antecedent governed. The subject trace is antecedent-governed by the COMP (via Comp Indexing Mechanism see Aoun, Hornstein, and Sportiche (1981)) and the object trace is head governed by the verb bought. In contrast, (4b) violates the ECP. Although the subject trace is antecedent governed by the COMP, the adjunct trace is neither head nor antecedent governed. As the LF representation in (5) illustrates, the grammaticality of (2c) is attributed to the presence of the null P: the trace of when/where is headgoverned by this P, satisfying the ECP. Since the subject trace is also governed by the COMP, the ECP is satisfied in this example.  In these representations, where and when are in complement positions as well, which means that the ECP should be satisfied here in the same manner as in (6a) . and (7a). As shown in (9), the LF representations of (6b) and (7b) show that the ECP is satisfied. Thus, Huang's analysis incorrectly predicts that (6b) and (7b) are grammatical. Murasugi and Saito (1992) claim that Huang's null P analysis is untenable for this reason. Instead they argue that the ungrammaticality of (6b) and (7b) can be explained by assuming that temporal and locative phrases can have argument status in sentences but not in NPs. More precisely, they claim that temporal and locative phrases are arguments of INFL or the event predicate associated with V, and that they are pure adjuncts when they occur within NPs. This provides a simple account of the contrast in (6) and (7), assuming that there is no null P associated with where and when . Now let us reconsider (6), repeated as (10), under their analysis. In (10a), the WH-phrase which shelf is in the object position of the P on. Thus the ECP would be satisfied via head-government at LF. In contrast, the ungrammaticality of (10b) is due to the lack of the P, according to Murasugi and Saito (1992). The ECP is violated, since the trace of where is neither head-governed nor antecedent-governed if we assume that antecedent-government is somehow blocked by the presence of the object NP node.' (12) [where' , whod i ti read [the book tj]?As for the example in (2c), repeated below, its grammaticality also follows straightforwardly under Murasugi and Saito's (1992) analysis. Since when and where are arguments of INFL or the event predicate, the trace left by LF WHmovement of when or where is properly governed by INFL (or the event predicate), thereby satisfying the ECP.(13) Who bought that book when/where? This paper is an investigation on the categorial status of locative and temporal WHs. We argue, based on empirical data, for the null P hypothesis proposed by Huang (1982), and against the proposal of Murasugi and Saito (1992) that when and where are sentential arguments. We suggest that the problem raised in Murasugi and Saito (1992) for the null P analysis can be solved by the null P incorporation hypothesis. We further address the issues related to the applications of the P incorporation.
Information and Computation(PACLIC 11)  Under the strict binary foot parsing (Kager 1993), stray elements may occur between bimoraic feet. The stray element may be associated to the preceding foot or following foot at surface level. Stray element adjunction is the mechanism for achieving surface exhaustivity. Each language has its own unique mecbanism of stray element adjunction in order to achieve surface exhaustivity. In Japanese loanwords, the strict binary initial foot parsing creates stray moras. Inaba&apos;s (1996) phonetic experiment shows that the word-medial stray moras associate to preceding feet, and provides evidence for the initial unaccented mora as extrametrical. Since the theoretical points I advance are deeply embedded in other languages, I present a set of possible parameters. Based on the set of parameters, I create a computer program which derives the surface foot structures of input loanwords in Japanese, Fijian, and Ponapean.
Information and Computation(PACLIC 11) A voice organizer uses the voice input and output to process personal data. For example, for recording voice data or querying a voice message from the voice database, a voice organizer must first recognize the input voice and then respond the user with voice output. Hence, a voice organizer includes at least the following voice processing modules. First, a voice recognizer is used to decode the input speech into operating commands. Secondly, the voice output module responds the user with the information that he wants. Thirdly, a voice storage module is used to save the large amount of voice data.On the observation of current available voice organizers, all of them accept only a few voice commands or word-based commands(Huang 94). Using natural spoken language to operate voice organizer is still a difficult problem. In this paper, we proposed a template-matching speech recognizer that allows the user to operate the voice organizer using near natural spoken language. The templates that we define here are those sentence patterns which are frequently used to operate the voice organizer. Among so many sentence patterns, they are classified into several so called "templates". The voice recognizer then match the input voice based on these templates. Hence, the input styles are more flexiable than those of voice-command based voice organizer.In the following Sections, Section 2 describes the system block diagram of our proposed voice organizer. Section 3 go through details of the templated-matching speech recognizer. Section 4 describes the voice response module and voice compression module. The experimental results are given in Section 5. Finally, a conclusion remark is given in Section 6. On the observation of current available voice organizers, all of them accept only voice commands or word-based commands. Using natural spoken language to operate organizer is still a difficult problem. In this paper, a template-based speech recognizer which accepts near(constrained) spoken language is proposed. Since the template-based recognizer is a domain-dependent speech recognition system, representing and matching of sentence templates become the main tasks of the recognizer. We use finate state networks(FSNs) to represent the sentence templates and propose a vowel-based, syllable-scoring method to match a correct template. By replacing the template sets, this method can be easily applied to other domains. Besides, two main functions, voice recording and voice message query, are implemented on our organizer using a fast CELP encoder/decoder to compress/decompress the voice data in realtime. Experimental results shows that the collected 31 sentence templates can greatly improve the voice interface between the user and the voice organizer.
Information and Computation(PACLIC 11) A spontaneously spoken, natural Japanese discourse contains many features which are not part of its written counterpart: disfluencies, interjections, repairs, non-sentential particles, and checked utterances.Aizuchi, or back-channel utterances, and other so-called redundant utterances, such as hai, un, anoo and ee, which are counterparts of the English "uh-huh", "yes", or "ok", are especially abundant in Japanese discourse.These are often taken as the manifestation of the irregularity and nonsystematic nature of spoken language. They are traditionally considered to be spurious or meaningless, not especially contributing to language. From this point of view, these utterances would be nothing but`disfluenciesbut`disfluencies', some kind of 'noise'.Yet we believe these utterances have important functions in discourse, comparable in many respects to what is called discourse markers. In this paper, we show that these utterances are indeed characterizable as discourse markers, and that they comprise a well-defined category, characterizable in a regular manner by their phonologico-prosodic properties. These expressions are important because, among other things, they often provide informatiQn about the structure of discourse they occur in and about the speaker's intention or plan. Sidner 1985 states that discourse markers are necessary for recognizing the relations between the intended acts and the overall plans of the speaker. A number of researchers have noticed the relation between discourse structure and intonational, or prosodic, characteristics. Grosz and Hirschberg 1992, using an independently motivated theory of discourse model, show that there are significant associations between intonational features and discourse structure. Similarly, Nakajima and Allen 1992 discuss the correlation between prosodic information and the topic structure of discourse. One of the first works to discuss the relation between discourse markers and prosody, in the context of discourse structure, is Hirschberg and Litman 1987. Based on a study of "now" in natural recorded discourse, Hirschberg and Litman propose that, in speech, intonational characteristics play a crucial role in distinguishing between cue and non-cue uses, helping to disambiguate the structure of a discourse.We are interested in these Japanese discourse markers for various reasons. First of all, in order to understand and explain the language in actual use, one cannot avoid treating these phenomena. Secondly, they may have particular functions, not required in written language but specifically called for in its spoken counterpart. There are also fair indications that these expressions play crucial roles in determining discourse structures, especially with respect to units of surface discourse as well as of speech acts and planning (Kawamori et al. 1994). Elucidating such roles can not only clarify syntactically relevant features of discourse but may shed light on intended meaning and other issues concerning pragmatics (Takubo 1994).In addition to these theoretical interests, clarifying these phenomena may serve more practical purposes. For example, constructing a truly friendly human-machine interface would most likely require a systematic knowledge of these features. Conversely, the inability to handle these utterances would limit the capacity of an expert system (Pollack et al. 1982), (Whittaker and Stenton 1988). A system without such an ability may fail to allow the user to participate in the reasoning process by not letting her think while the system is giving answers or questions, or to give the exact answer the user wants by not noticing her hesitation or surprise.Attempts at clarifying Japanese discourse markers, however, have not so far been a major enterprise. Even their status as discourse markers itself has not been widely accepted, nor is there a general agreement as to what constitute the category. Moreover, much of what little effort made has been exerted to the acoustic aspects of these expressions, ( Kobayashi et al. 1993), and the qualitative aspects of these Japanese discourse markers have not hitherto received much attention. But phonological study is essential in clarifying the exact phonologico-prosodic nature of these expressions.We investigate this aspect of Japanese discourse markers, looking specifically into their intonational patterns. We analyze the intonational features of discourse markers in naturally occurring utterances of Japanese. The assessment of our analysis and characterization is made using empirical data.The paper is organized as follows. The first section introduces the Japanese tone features represented in terms of a Japanese variant of Tobi system. In the second section, we give a general introduction to Japanese discourse markers. There we discuss the phonological characteristics of responsives and fillers, our main concern in this paper. The third section describes the nature of the experiment we conducted, and the results we obtained. A spontaneously spoken, natural Japanese discourse contains many instances of the so-called redundant interjections and of back-channel utterances. These expressions have not hitherto received much attention and few systematic analyses have been made. We show that these utterances are characterizable as discourse markers , and that they comprise a well-defined category, characterizable in a regular manner by their phonologico-prosodic properties. Our report is based on an experiment involving spontaneously spoken conversations, recorded in a laboratory environment and analyzed using digital devices. Prosodic patterns of discourse markers occurring in the recorded conversations have been analyzed. Several pitch patterns have been found that characterize the most frequently used Japanese discourse markers.
Contrastive Focus and Exempted Anaphor Caki in Korean* It has been proposed that there are two types of constraints on anaphor binding in various languages. These are the syntactic and discourse (or pragmatic) constraints as proposed by Roberts (1987), Reuland (1991, 1993), Iida (1992), Sag (1992, 1994), Baker (1994), and Xue, Pollard and Sag (1994), among others. The dichotomy between syntactic and discourse constraints seems to pertain in Korean too.The proposals of this paper are as follows. First, contrary to the general belief that the discourse constraint only affects long-distance anaphor binding (the case where an anaphor and its antecedent are not coarguments), the coargument binding possibilities of Korean caki (` self) are affected by the discourse constraint when contrastive focus is introduced. Second, the focused caki should be treated as an "exempted" anaphor in terms of Sag (1992, 1994), i.e., the focused caki is exempted from a syntactic constraint, and this exemption allows it to be subject to a discourse constraint not to a syntactic constraint. Third, the syntactic constraint hinges on "syntactic prominence" of an antecedent. The syntactic prominence is determined by two factors concerning the anaphor and its antecedent: obliqueness and linear order. Fourth, discourse constraint hinges on "discourse prominence" of an antecedent. The discourse prominence is partially determined by linear order and a set of presuppositions (the familiarity presupposition in Heim (1982) and the presupposition of contrastive focus). This paper explores the effect of constrastive focus on the binding possibilities of the Korean anaphor caki (`self). Contrastive focus on caki has a special effect in that it improves the acceptability of an atypical binding pattern. To account for this fact, I propose (i) that caki with contrastive focus needs to be treated as an exempted anaphor in terms of Pollard and Sag (1992, 1994), (ii) that the binding possibilities of the exempted caki is determined by a discourse constraint not by a syntactic constraint, and (iii) that the discourse constraint needs to include the familiarity presupposition in Heim (1982) and linear order.
Legitimate termination of nonlocal features in HPSG In earlier versions of the Head-Driven Phrase Structure (HPSG) framework, inheritance of nonlocal features is terminated in accordance with the Nonlocal Feature Principle (NFP). This paper reviews the treatment of wh-question facts offered by Lappin and Johnson 1996, and suggests that their account of certain island phenomena should be adapted by assuming that certain phrase structures license binding of inherited features. In Japanese, Lappin and Johnson&apos;s INHERILQUE feature appears to be dependent on INHERIQUE in order to terminate with a functional C head&apos;s TO-BINDIQUE. For certain languages, C&apos;s TO-BINDILQUE feature must be null if TO-BINDIQUE is null. In the spirit of Sag 1996 and Pollard and Yoo 1996, the facts can be handled by saying that TO-BINDILQUE is licensed on a wh-clause (wh-cl). As a wh-cl requires TO-BINDIQUE, the dependence of the less robust INHERILQUE on INHERIQUE is thus explained.
Information and Computation(PACLIC 11)  This paper addresses the issue of Split Intransitivity (si) and Unaccusative Mis-matches (uMs), proposing a constraint-based approach to si and ums within a recent framework of Head-driven Phrase Structure Grammar. I argue against the widely accepted dichotomous distinction of intransitive verbs, which has been advanced by the Unaccusative Hypothesis [Perlmutter (1978)]. I then propose a quadripartitive distinction of intransitive verbs on the basis of the distribution of subject argument in the semantically motivated argument structure, and show that this quadriparti-tive distinction allows a better understanding of si and ums. The main idea of this proposal will be summarized as the Quadripartitive Split Intransitivity Hypothesis (Qsm).
Information and Computation(PACLIC 11) Procedures of semantic analysis may be one of the most complicated structures people have met with. Some conventional semantic theories presuppose the notion that natural language may be describable by a finite set of rules capable of generating an infinite set of sentences. The difficulty with this approach is that external features depends on being able to clearly determine for each relevant features whether or not an object processes it. Moreover, even those features which have been decided, such as 'heavy", 'short' or 'blue', might still be fuzzy since there are no clear cut boundaries distinguishing heavy from very heavy, short from a little short or blue from purple.A fundamental problem of lexical semantics is the fact that what Ruhl (1989) calls the perceived meaning of a word can vary so greatly from one context to another. Some disadvantages about computational (numerical) semantics are: (i) the danger of overstraining the empirical data to meet the requirement of numerical precision; (ii) the danger of overinterpreting the numerical results of a term. One possible way to diminish the required amount of precision is to use fuzzy statistics. However, Zadeh (1972) and (1983) have proposed certain alternative approaches where the linguistic aspects are mostly emphasized. Since then, many papers have also been published on this topic, for examples, see Joyce (1976), Rieger (1976) and Morgan and Pelletier (1977)  Sanchez et al (1982) etc. For an extensive treatment of the theory of fuzzy sets with applied linguistics the interested reader may refer to see Dubois and Prade (1980) or Manton, Woodbury and Tolley (1994).In this paper, we will apply the fuzzy statistical analysis and computational lexical semantic method to investigate some uncertain and ambiguous problems. Especially we will discuss the degree of object's typicality and similarity, which Especially we will discuss the degree of object's typicality and similarity, which provide a more precise expression in human cognition. . It should be pointed out that the concept of fuzzy statistical analysis applied in this research does not refer to the general notation of constructing certain theories, but to propose some alternative methods in computational lexical semantics. We hope that such analytic technique will be more reliable and significant for the future research. In this paper, we investigated the fuzzy statistics analysis in lexical semantics and apply the fuzzy logic to compute some uncertain and ambiguous problems. The fuzzy propositional computation for the cognitive semantics can account for the degree of typicality and similarity. Which provide a more precise expression in human thought and human cognition. Some essential definitions for fuzzy statistics are proposed to implement these procedures. The empirical results by a sampling survey and fuzzy statistical analysis suggests that the fuzzy statistics and computation are potentially powerful heuristics in analyzing lexical semantics.
Information and Computation(PACLIC 11)  In this paper, I propose a general framework called Epistemic Logic (EL) which captures a full range of epistemic phenomena, and apply it to the three-valued interpretion of natural language sentences. Further, I mention the ability which EL implies, i.e. treatment of indexicals, relationship to Data Semantics etc. 1 Epistemic Logic Epistemic Logic consists of the syntax and semantics of EL with respect to which natural language sentences are interpreted. 1.1 Syntax of Epistemic Logic The syntax of Epistemic Logic consists of a normal version of first-order predicate logic with modal operators`may operators`may&apos; and &apos;must&apos; which is defined as follows: (1) Vocabulary of EL i) individual constants: a, b, c, E Const. ii) individual variables: x, y, z, E Var. iii) j(&gt; 0)-ary predicate symbols: predl E Pred&apos;. (i E /(j) is a segment of the set of natural numbers.) iv) logical connectives: A, V, D, E LC. v) quantification symbols: V, 2 E Q. vi) epistemic modal symbols: must, mayE EM. vii) auxiliary symbols: (,). Term = Const U Var is called the set of terms of EL. Pred = U.Pred3 is called the set of predicate symbols of EL. Voc = Term. U Pred U LC U Q U EM U {(,)} is called the set of the vocabulary of EL. Exp = V oc®1 is called the set of expressions of EL. (2) Formation Rules of EL Formulas: The set Form of EL is the least set which satisfies the following conditions: i) If a1 , • • • , aj E Term, predl E Pred3 , then predi(a i , • • • , (xi) E Form. ii) If 0, x E Form, then (-0), (0 A x), (cb V x), (0 D (0 E Form. iii) If q E Form, then (Vx0), (3x0) E Form. iv) If (/) E Form, then (must 0), (may E Form. VV Exp = (Voc\ {(,)}) U Form is called the set of well-formed expressions of EL. Auxiliary symbols of formulas are eliminated in obvious cases.
Information and Computation(PACLIC 11)  To understand text, we must relate it with specified situations. This paper, on the basis of such an idea, discusses how the things that a text describes and the situation that the text relates to are expressed in a computer and how the topic of a text is extracted. 1.Introduction The topic of a piece of writing is the central idea of it. People not only care for what it expresses, but also care for the most important idea of it. Therefore, if each piece of writing has a marked topic, then retrieval rate and retrieval precision of writings can be increased to a great extent. In computers, a piece of writing is a text. Only when a text relates to certain situations, can its topic be rightly extracted. On the basis of situations matching, the paper presents the method of extracting topics from texts. A situation is something&apos;s world state in a specified moment and environment. So, when a thing&apos;s attributes are described, the thing&apos;s situations must be referred to, and at every moment, reasoning only can be executed in a situation. This is why situations are built. A situation is a function of time and space. For example, things and their behaviors can constitute different situations at the same time and in difference space, or in the same space and at difference time by increasing or decreasing a little information. The situation knowledge of something is the sum of perceptual experience and rational knowledge about a topic. The degree of people understanding a text often relies on their knowledge level of the field to which the text refers. That is, same or similar situation knowledge is the foundation of people&apos;s understanding each other. Situations may be divided into broad sense situations and narrow sense situations. A broad sense situation includes all dynamic state events and static state events in the objective world. It is consecutive tableaus in consecutive time and space. A narrow sense situation is dynamic state or static state events that relates to a specified discourse, that is, it includes the environments in which the discourse happens and the events to which the discourse relates. The task of situation theory is extracting common inner structure from large amounts of real situations in the objective world, probing into the restrained relations among situations, revealing meaning of the language expressions, and presenting a calculable mathematical model for natural language understanding based on situations(Sun Bo 1992). A real situation is a set of specific dynamic or static events. An abstract situation is a set that isconstructed by some elements 357
Information and Computation(PACLIC 11) Natural languages are complex and their syntactic properties seem to differ from one another quite dramatically. Traditional parsing technologies utilize language-particular, rule-based formalisms, which usually result in large and inflexible systems (Marcus 1980). For a recent example of the rule-based approach to Chinese parsing, see ( Lee et al., 1991).In recent years, computational linguistics has seen the development of the Principle-based Parsing ( Berwick et al 1991). A principle-based parser transparently reflects the structure of the contemporary linguistic theory, the Principles and Parameters framework (Chomsky 1981). It is believed that languages are constrained by a small number of universal principles, with linguistic variations largely specified by parametric settings.The merit of principle-based parsing is two-fold. As a tool for linguists, it is directly rooted in grammatical theories. Therefore, linguistic problems, particularly those that involve complex interactions among linguistic principles, can be cast in a computational framework and extensively studied by drawing directly on an already-substantiated linguistic platform. It is designed from the start to accommodate a wide range of languages -not just 'Eurocenteric' Romance or Germanic languages. Japanese, Korean, Hindi and Bangla have all been relatively easily modeled in PAPPI (Berwick and Fong 1991, Berwick forthcoming). As a tool for engineering, it inherits the useful design principle of modularity among at least some of the principles. Differences among languages reduce to distinct dictionaries, required in any case, plus parametric variation in the principles.To show how this project may be concretely executed for Chinese, we first review the basic phrase structures and parameters for Chinese. Our principle source is Jim Huang's seminal dissertation (1982). Then we turn to two particular problems that have generated much interest and productive work in the literature, the BA-construction and the scoping non-ambiguity in Chinese. Both theoretical and computational analyses are presented. The paper condudes with some general observations on principle-based parsing, in relation to linguistics and computation. This paper describes the implementation of Mandarin Chinese in the Pappi system, a principle-based multilingual parser. We show that substantive linguistic coverage for new and linguistically diverse languages such as Chinese can be achieved, conveniently and efficiently, through pa-rameterization and minimal modifications to a core system. In particular, we focus on two problems that have posed hurdles for Chinese linguistic theories. A a novel analysis is proposed for the so-called BA-construction, along with a principled computer implementation. For scoping ambiguity , we developed a simple algorithm based on Jim Huang&apos;s Isomorphic Principle. The implementation can parse fairly sophisticated sentences in a couple of seconds, with minimal addition (less than 100 lines of Prolog code) to the core parser. This study suggests that principle-based parsing systems are useful tools for theoretical and computational analysis of linguistic problems.
Estimating Point-of-View-based Similarity using POV Reinforcement and Similarity Propagation Rapid growth of computer networks has increased the number of machine-readable texts and also made it possible for us to use various search engines to get desired documents. They are, however, keyword-based and strict. Even if some documents are related to a user's interest, he or she cannot obtain them as long as they do not contain the keywords given in advance. Otherwise he or she will be handed too many documents which contain just the keywords but are not necessarily related to his or her interest.To solve these problems, it is necessary to take account of similarity between words or concepts and to make use of the measure in search processing. However, because there are many similar words in a text, employing a similarity measure alone only will expand the range of relatedness and produce more and more results.When considering meanings of words, we, human beings, do not consider the whole meanings at a time, rather some interesting aspects of their concepts just the same as we look at a landscape from some point-of-view. Hence in similarity of words it is required to take account of their POVs. This makes it possible both to expand the range of matching in some situations and to restrict the range in others. Expectation is that employing valid POVs has both the expansion and the restriction be suitable and search processing produces more appropriate results.This paper proposes a similarity measure between words which measure takes account of the effect of POVs. So far many researches on concept (or word) similarity have been performed but none handles POVs in their similarity measures. The proposed method utilizes co-occurrence probability-based similarity as a basis and extends this fundamental measure by weighting the values according to the relevance between input words and POV words. This fundamental measure and its evaluation with some traditional similarity measures are described in 2. The main part of the method, which handles the effect by POVs, consists of two processes, POV reinforcement and similarity propagation. The explanation for these processes and some related issues are presented in 3 and 3.2. Finally 4 shows the result of some experiments, which indicates the effectiveness of this method, and 5 discusses the problems of the method as well as its advantages. This paper. proposes a similarity measure which takes account of point-of-views (abbreviated to POV, hereafter) in the calculation of similarity values. So far many researches on similarity measures have been performed but none takes account of POVs. The similarity measure proposed in this paper is based on co-occurrence probabilities of words and this makes it possible to obtain preferable precision even if POVs are not given. This method consists of two parts of processes, POV reinforcement and similarity propagation. First, the POV reinforcement process, which affects the similarity between words, modifies the weights of links according to the relatedness between the link and the POV word. Second , the similarity propagation process propagates the weights of links and defines a similarity value for word pairs which do not actually co-occur in the corpus. Using those two processes this method becomes capable both to take POVs into consideration and to cope with the sparseness of corpora to some degree. This paper, however, focuses on the POV reinforcement and evaluates the effectiveness of the method..
Information and Computation(PACLIC 11) The availability of large treebanks creates the opportunity to model the structures humans recognize in sentences that appear in everyday natural language. One well known method for modeling such structures is the Inside Outside Algorithm, which was first described by Baker (1979).The statistical induction of context free grammars has the attractiveness that it does not require any presumptions about the grammar that is being created, other than those that limit the size. However, the major problem with this kind of modeling has always been the computational complexity; the algorithm requires 0(n3 1w1 3 ) of training time per sentence w for a grammar with n nonterminals, per iteration.The algorithm has been used in two different ways, both of which reduce the computational complexity. First, there is a line of research (Black, Garside, and Leech, 1993;Hogenhout and Matsumoto, 1996) that concentrates on the reestimation of hand written grammars. This has none or much less problems with time complexity since the structure of the grammar is already decided and usually generates a limited number of parses for a given sentence. However, it completely loses the original attractiveness of modeling without presumptions.A second line of research, which concentrates on inducting a new grammar from scratch, is described in, amongst others, ( Pereira and Schabes, 1992;Schabes, Roth, and Osborne, 1993).In these experiments the algorithm was applied to a treebank. The brackets of the treebank were used to reduce the number of possible structures by disallowing any structure that crosses some treebank bracket. This greatly reduces training time, and they were able to parse short sentences of the Wall Street Journal corpus with 90.2% accuracy using a 15-nonterminal grammar.The extension to the algorithm and the experiments we present aims at improving the efficiency of induction of grammars from scratch. We have been able to strongly reduce the time complexity of the training, and at the same time achieved equivalent results when parsing short sentences of the Wall Street Journal Corpus.Instead of starting with a grammar consisting of all possible rules for a given number of nonterminals, we start with a small number of nonterminals and gradually increase this to the desired number. At the same time we remove rules that have become obsolete so we can work with a much smaller grammar.In this paper we describe the method in detail and report on the results obtained in preliminary experiments. The statistical induction of context free grammars from bracketed corpora with the Inside Outside Algorithm has often inspired researchers, but the computational complexity has made it impossible to generate a large scale grammar. The method we suggest achieves the same results as earlier research, but at a much smaller expense in computer time. We explain the modifications needed to the algorithm, give results of experiments and compare these to results reported in other literature.
Information and Computation(PACLIC 11) There have been two opposing views on the structure of the HIRC in languages like Korean and Japanese, e.g., a in (1) below:(1) a. John-un sakwa-ka cepsi-wiey iss-nun kes]-ul J.-Top apple-Nom dish-on exist-Adn C-Acc pick;up`John up`John picked up an apple which was on a plate, and ...' b. Taroo wa ringo ga sara no ue ni atta no o] totte, T. Top apple Nom dish Gen on at exist NO Acc pick;up`Taro up`Taro picked up an apple which was on a plate and ...' Kuroda (1992:147) Kuroda (1992), . Watanabe (1991Watanabe ( , 1992, Hoshi (1994), and Jhang (1991,1994) analyze a categorially as a nominal projection and functionally as an argument of the matrix predicate, albeit they differ as to the internal structure of a. Murasugi (1994), however, analyzes a as a circumstantial adjunct (Harada 1973) and posits pro as a matrix argument, which is anaphorically related to an NP within a. Their schematic structures are provided below:(2) Argument Analyses of a a. Kuroda (1992) b. Jhang (1991:271) c. Hoshi (1994 This paper, on the one hand, points out several phenomena indicating that Murasugi's analysis is more viable, while on the other hand proposing a more complex structure than Murasugi's to account for other facts as well. The no/kes clause will be analyzed as the complement of a null perception verb whose projection constitutes part of an adjunct clause. There have been two opposing views on the so-called head internal relative construction (HIRC) in Korean/Japanese, i.e., a view that analyzes the HIRC categorially as a nominal projection and functionally as an argument (Kuroda 1992, Watanabe 1992, Hoshi 1994, Jhang 1991/1994, among others) vs. a view that analyzes the HIRC categorially as an adjunct clause and functionally as a non-argument (Murasugi 1994). This paper on the one hand points out several phenomena indicating that Murasugi&apos;s analysis is more viable, while on the other hand proposing a more complex structure than Murasugi&apos;s to account for other facts as well. The no/kes clause in the HIRC will be analyzed as the complement of a null perception verb whose projection constitutes part of an adjunct clause.
Information and Computation(PACLIC 11) This paper deals with some semantic and pragmatic aspects of the so called internally-headed relative clause (IHRC, hereafter) construction in Korean, as is exemplified in (1), although we also touch on problems related to its syntax:(1)a. totwuk-i ton-ul kaci-ko unhayng-ul nao-nun kes-ul cheyphohayssta.thief-nom money-ul carry-and bank-acc leave-mod thing-acc arrested '(Someone) arrested the thief who was carrying money and leaving the bank' b. totwuk-i ton-ul kaci-ko unhayng-ul nao-nun] kes-ul ppayasassta. deprived '(Someone) took away money which the thief carried leaving the bank' This construction has some peculiar properties. First, it has a external syntactic head which is void of semantic contents and there is an so called internal head which determines the semantic value of the whole construction.Second, as pointed out by Park (1994), there is no one-to-one correspondence between externally-and internally-headed constructions, as shown in (2). What makes (2a) grammatical and (2b) otherwise?(2)a. sinsenhan sikumchi-lul salm-un kes fresh spinach-Acc boiled-ref thing qthe outcome) of boiling fresh spinach' b. *sinsenhan salm-un sikumchi fresh boiled spinach 'boiled fresh spinach'Third, there are many cases where the IHRC is not obtained as argued in S.-E. Jhang(1991) and Park:(3) a. *toywuk-i pangeyse nao-n kes-i tomangchessta. thief room-from come-out-rel thing-Nom fled 'The thief that came out of the room fled' b. *Emma-ka ai-eykey wuywu-lul cwu-n kes-ul ttalyessta Mom-Nom baby-dat milk-acc give-rel thing-Nom hit (Someone) hit the baby the Mom gave milk to Jhang claims that the grammaticality of the IHRC construction is sensitive to the case particle.Fourth, there are apparent pragmatic differences between the external and internal constructions in questions as exemplified in (4): (4)a. Kyenchal-un totwukcilha-nun manhun haksayngtul-ul kyothongbep police -Top steal-rel many students-Acc traffic rule wuipan-ulo capassta. violation-for arrested 'The police attrested many students who (habitually) steals things in violation of traffic rules'b. !Kyenchal-un manhun haksayngtul-i totwukcilha-nun kes-ul kyothongbep police -Top many students-Nom steal-rel thing-Acc traffic rule wuipan-ulo capassta. violation-for arrested 'The police arrested many students in the act of stealing things in violation of traffic rules' (4a) is pragmatically sound but (4b) is not. What makes these differences? Can it be attributed to syntax, semantics or pragmatics? This paper will address these issues that are raised in this section. In Section 2, recent papers and articles are reviewed from various points of grammatical aspects. The reviewed papers include Jhang (1991), Park (1994), Jung (1995), and Shin (1995) as well as some foreign authors. In Section 3, I will propose my own analysis of its syntax and semantics assuming HPSG framework. In Section 4, we will provide a pragmatic account of this construction in question. This paper proposes, assuming a HPSG framework, that the meaning of the external head kes is a very underspecified in the sense that it has not much CONTENT value in the Attribute Value Matrix. In this paper kes is analyzed as the external head of the construction that shares features with the internal elements of the complement just as an ordinary head complement structure does. In the same vein, this paper argues that interpretation of kes is largely dependent on the verb following it. As for semantic, it is argued that the entity is seen as either a set of properties or a set of (atomic) situations involving an entity in question. This paper attempts to analyze some grammatical aspects of the so called internally-headed relative clause construction in Korean. This paper proposes that the meaning of the external head kes is underspecified in the sense that its semantic content is filled in by co-indexing it to the internal head under appropriate conditions. This paper also argues that interpretation of kes is determined by the verb following it. Dealing also with the pragmatics of the construction, this paper argues that the crucial characteristics of the construction in question resides in pragmatics rather than in semantics.
Information and Computation(PACLIC 11) Until recently, phonological theories crucially depended on the assumptions of underspecification and feature-filling application of rules in a derivational mode. In particular, Paradis and Prunet (1991) and Cho (1991), among others argue that coronals are underspecified due to their asymmetrical behavior in several phonological phenomena such as place assimilation, deletion, and epenthesis.However, there has been some evidence against coronal underspecification. For example, morpheme structure conditions in both English and Korean need to refer to the unmarked coronals to rule out certain sequences of coronal consonants (McCarthy and Taub 1992). English plural and past tense suffixes should also refer to coronals in order to account for vowel insertion between "two like (or similar) coronals".Then how can we account for the paradoxes involving coronal specification? That is, on the one hand, coronals should be underspecified because of the asymmetrical behavior, but on the other hand, coronals should be specified due to their reference in the description of some other phonological phenomena. In this paper we argue that coronal paradoxes can be resolved in Correspondence Theory which assumes constraint interactions in a parallelistic mode (McCarthy and Prince 1995). In specific, we contend that with the low ranking of Coronal, coronal unmarkedness is naturally derived, without appealing to coronal underspecification. We also argue that Yip's Cluster Condition (1991) which crucially depends on coronal underspecification cannot be maintained. Rather we show that the fact that coronals frequently show up in clusters can receive a unified account within Correspondence Theory.The organization of this paper is as follows: Section 2 examines the analysis of coronal unmarkedness in terms of underspecification theory. Section 3 provides evidence against coronal underspecification from several phonological phenomena, in languages such as English and Korean. Section 4 gives a unified account of coronal unmarkedness and cluster facts within Correspondence Theory, solving the problem of coronal paradoxes. Section 5 summarizes our findings. This paper examines paradoxes involving coronal specification. Although several phonological phenomena such as place assimilation, insertion and deletion argue for coronal underspecification, other facts like morpheme structure conditions demand coronal specification. This paper shows that this problem can be solved with the markedness hierarchy which ranks coronals low under Correspondence Theory. The hierarchy also accounts for the distributional bias of coronals in word clusters without appealing to Yip&apos;s Cluster Condition and coronal underspecification.
Information and Computation(PACLIC 11)  This paper describes an algorithm for Spanish derivational morphology whose output is generalizable to two different lexicon acquisition situations. One is the process of automatic lexicon acquisition via the use of Morpho-Semantic Lexical Rules (MSLRs), (Viegas, Gonzalez, &amp; Longwell 1996) usable in semantically based Natural Language Processing(Nirenburg, et al 1996) in order to considerably reduce acquisition time per entry in a large-scale semi-automatic acquisition environment (Viegas, Onyshkevych, Raskin, &amp; Nirenburg 1966b). The other is its application in the language classroom to fascilitate vocabulary acquisition for second language learning students. It is an ancillary tool to be used in reading or writing tasks. The constructive approach used in the design of this tool addresses the overlap between the morphological and semantic criteria while implicitly addressing morpho-phonological phenomena. The objective is accomplished by means of three major strategies: pre-categorized base stems, inherited stem changes, and eight separately identified patterns of attachment which produce noun, adjective, verb and adverb subsets. These mechanisms produce words depending on whether the word structure is right-headed or left-headed, based on the type of affix producing Morpho-Semantic Lexical Rules (MSLRs) used and on the sequence in which affix attachments succeed. The set of MSLRs, approximately 151, produces words with their respective semantic component for the morpheme or sequence of morphemes, thus defining each derivational paradigm. Their product includes not only simple and complex affixation but also word compounding, allowing also overgeneration of derived forms for broader coverage of written and oral word forms. Unnecessary overgeneration of output paradigms can be automatically checked against the contents of machine readable dictionaries and against electronic texts. The tool includes a Graphical User Interface (GUI) in order to facilitate the vocabulary acquisition process for the L2 learner of Spanish. 1 The GEN_WORD Objective A dual purpose guided the design process while taking advantage of a simple yet sophisticated and productive features of language in general. • To automate the production and acquisition, via Morpho-Semantic Lexical Rules, of a large lexicon to be used by a machine translation system, using a computational semantic approach. • To automatically produce data to be accessed by means of a GUI in order to facilitate the vocabulary aquisition process for Spanish as a Second Language learners. 425
Information and Computation(PACLIC 11) Not all empirical facts are treated equally in science; in theorizing, some are weighed more heavily than others. It is often unavoidable and it should not necessarily be avoided. We will present a case where a semantic theory is influenced more by a seemingly universal fact, but in fact accidental among related languages, than by a few significant exceptions in the language in question, thereby failing to capture a meaningful generalization. In particular, we argue that in-adverbials are not a test for telic predicates, as they are popularly claimed to be; we will show that this claim is triggered by the accidental fact that there are two homomorphic in-adverbials in English and their cognates in other languages. 
Information and Computation(PACLIC 11) In the past twenty years interest in the differences between male and female language has spawned a tremendous literature. The largest body of work on language and gender has revolved around Robin Lakoff's 1975 seminal work in which she identified a set of features that she claimed occurred more frequently in women's speech than in men's. Lakoff's claims along with most of the early assertions about gendered language behavior have been prone to methodological and interpretive criticism. Of late, there has been a renewed interest in refuting this early work, due, in part, to the fact that much of it was largely anecdotal, fairly subjective, culture-specific, and has not been replicable.This study investigated the validity of some of these claims with respect to the domain of children's narrative production. The study compares children's production of and metalinguistic awareness about gendered stereotypes in the language they use to tell a story. Although there is a large corpus of language and gender literature, this literature has focused on gendered aspects of adults' language production in conversation. Little research has been conducted into the presence of gender differences in narrative styles, let alone that of children's. The central goal of this study is to provide a description of how boys and girls produce narratives and whether they produce and/or are aware of gender-based differences in their language.It is claimed that by the time children start school, they have already been exposed to gender-differentiated language and are well on their way to forming their own gendered speech patterns. Before age 6, children "have begun to learn to speak differently as a girl or as a boy, how to speak to other girls and boys, and how to speak about them" (Swann 1992:14). According to Swann 1992, how people speak affects how they are perceived by others. Swann proposes that through spoken language, relationships are negotiated and people reveal things about themselves through speaking. Consequently, a number of stereotypes regarding gendered speech have become solidified in North American culture.Within a culture, people have fairly clear perceptions of how each of the sexes should behave and speak. In a classic study by Kramarae (reported in Kramer 1977), adult subjects were asked to identify speech traits that were typically "male" or "female". The task required that the subjects gradiently classify a list of 51 speech traits on the basis of what extent they were likely to be male or female speech traits. The results of this study show that, at least based on the results of the subjects' judgments, females are perceived (1) to have a higher pitch as well as a wider range in their pitch and speaking rate, (2) to use faster, gentler speech, (3) to be personable and aware of/concerned for the hearer, (4) to be enthusiastic and emotional, (5) to enunciate clearly, (6) to use paralinguistic cues to aid in self expression, (7) to provide lots of detail, (8) etc. The subjects perceived males (1) to be more forceful, (2) to have deeper voices, (3) to speak louder, (4) to be more confident and direct, (5) to be more aggressive, (6) to show their emotions in their speech, (7) to use more non-standard language (slang), (8) etc.Edelsky 1976 attempted to study perception of gender stereotypes in children between the ages of 6 and 14, using adult subjects as a control. She presented both the adult and child subjects with a series of sentences and asked them to identify which sentences would most likely be said by a male or a female and those that were neutral (those which could be said by either or both). She found that the adults' judgments regarding the classification of the stimuli set were consistent across subjects. She also found that the 6 year olds, the youngest subject group in the study, were unable to perform the task with reliable results. Moreover, the 11-year-old subjects' classificatory judgments were becoming similar to those of the adult subjects. These findings suggest that awareness of genderlects does not occur until late childhood or even early adolescence. I found similar results in the study I report on below. However, it seems not only that awareness of genderlects that first shows up in early adolescence, but production of genderlects as well.The aforementioned studies concern the perception of stereotypic patterns in male and female speech. Evidence that these stereotypes are pervasive in our culture comes from the fact that they are tapped into and used every day of .our lives, through jokes, news media, cartoons, television, etc. Could these factors really be exploited through humor if these stereotypes did not actually exist? Is it possible to discredit totally these stereotypes? Stereotypic behavior patterns must have some grounding in truth and be representative of at least a portion of the population or else they would have died out. However, some of these stereotypes are often exaggerated (e.g., pitch, intonational contours, etc.) and that others are definitely unfounded (e.g., verbosity in women) (Prideaux, Hogan, &amp; Stanford, 1993).Variability in language is functional, conveying information to others about the speaker and his or her assessment of the context. Gender is one of the many social classifications that is often associated with, if not manifested by, language use. Gender differences in language use are not bimodal, however, and all males do not always speak in manner X nor do all females speak in manner Y. Gender differences purported to exist in language use usually involve differences of degree rather than differences in kind, at least in the vast majority of languages and cultures. It is probably best to think of the social constructs of "maleness" and "femaleness" as extremes on a scale with much variation occurring between these sex-based poles. No given speech or language trait is exclusive to one sex and there may be as much variation of traits within sex as across.Whether or not males and females are predisposed to, or eventually learn to, speak in different ways, speech is a fairly robust indicator of gender. Pitch is one way in which this gender distinction in language is signaled. Both males and females are able to vary their pitch consistently and it has been proposed that people tend to speak in a manner that accentuates the differences between male and female speech, especially in the presence of the opposite sex. In learning to speak like a boy or a girl, young children may incorporate a plethora of articulatory features that are typically associated with their sex. The question is, do they also incorporate more structurally or semantically based linguistic features as well?Beyond articulatory and structural differences, there have also been studies that investigated gender and knowledge about language and gender. Andersen 1984 was interested in determining what children know about appropriate usage of language in different social roles. She set up specific role playing situations so that children would be able to express the various registers that are in their repertoire. Twenty-four children between the ages of 4 and 7 were involved in her study. She divided them into three groups, equally divided by sex and age. She had each child participate in three play-acting sessions with puppets (a doctor's office scene, parents with a young child scene, and a classroom scene with a teacher and two students). She found that children consistently used a variety of features (including pitch, register differences, morphosyntactic factors, etc.) to distinguish characters and that there was an emergent pattern to' these characteristics. She found that each child in each role-playing situation used prosodic markings to distinguish the roles, specifically "pitch differences, but also intonation, volume, rate and voice quality" (Andersen 1984:132). For example in the family scene, she observed that the "fathers" (i.e., children role-playing as fathers) used deeper voices, spoke louder than the other characters, and "showed a marked tendency to produce shifted vowels." "Mothers" were observed to use higher pitch than the "fathers", to exaggerate their intonation. Moreover, they did not speak as loudly as the "fathers" in the scene.Andersen also found that there was an age difference in the use of these features. She found that the older children were able to maintain these distinctions throughout the role play situation whereas the younger children typically used them at "role-junctures" where a contrast was necessary to avoid ambiguity. After children acquired the ability to use phonological modification, the next step that they seemed to acquire was the context-appropriate use of topic and lexicon. She found that lexical marking of role was used fairly consistently across children, although lexical distinctions of role were not always used appropriately and were less prevalent than phonological markings. While the use of distinguishing morphosyntactic features were not common in these data. She concluded that children are sensitive at a very early age (as young as 4) to the fact that utterances can express a whole range of social information tied to status (age, sex, occupation), as well as to the degree of familiarity of the speaker with the addressee. Also, children learn initially to encode these differences phonologically, then lexically, and only later as morphosyntactic distinctions.Prior to conducting my study, I searched extensively for studies on children's ability to differentiate and imitate the opposite sex through narratir. Apart from Andersen's study on roleplay, I know of no other studies that examine children's awareness of gender differences. The experiment described below is a study designed to examine children's ability to use gendered language. Although very little research has been done with respect to gendered language use and children's narratives, there is a growing body of research on children's narrative formation and developing narrative competence (cf. Berman and Slobin (eds.) 1994). However these studies focus on cross-linguistic and developmental patterns rather than on sex-based ones. This study investigated whether or not gender-based differences are evident in the language (North American English) children use to tell a story. An equal number of boys and girls representing 3 developmental stages (7;2, 10;10, and 13;8) were asked to perform a 3 part guided narrative task, which involved telling 3 stories from 2 wordless picture books. The language of the first and third narratives, representing natural and simulated discourses were analyzed. The linguistic variables of interest included lexical diversity, ratio of foregrounded statements to backgrounded ones, mention of the subjective state of either the story characters or the self, and the total length of the narratives. Results showed that differential performance on only two &quot;gendered&quot; factors, percentage of foregrounded utterance usage and mention of subjective state, were the only findings that showed any sensitivity to gender or age. The results for these variables do not support the presence of either expected gender stereotypes or very robust language differences between the sexes.
Information and Computation(PACLIC 11) A widespread attitude to Korean topic marker, nun, is that it has two functions, one is theme presentation and the other is contrastive topic marking. It has been a lot of controversy on the nature of these two functions associated with the topic marker in Korean or Japanese, whether they are essentially one category or not. Based on Hoji's (1985) observation that they show quite different syntactic behavior from functional categories, assuming that the two functions correspond to two separate functional categories, I will concern contrastive topic only in this paper, postponing the analysis of the theme presentation function for a future research.Most Korean or Japanese linguists acknowledge the notion of contrastive topic, but very few precisely captures the essential semantic and discoursal meaning associated with contrastive topic in a formal framework. In this paper, I attempt to provide a sketch of a formalism for the treatmen of Korean contrastive topic. For this, I will first show that Koran contrastive topic corresponds to a certain kind of prosody in English, which is a fall-rise intonation.Given that English has the same function as Korean Contrastive Topic, recent increasing researches on information structure and prosodic focus theory (Rooth 1985, Partee 1991, Krifka 1992, Vallduvi 1992, Roberts 1996, Steedman 1991, Von Fintel 1994) deepen our understanding of information structure associated with Korean contrastive topic, too. Especially, by exploiting the idea of Robert and Buering, I will provide a formalism for a proper treatment of the notion contrastive topic not only for Korean but also in general. This paper attempts to provide a sketch of formalism for Contrastive Topic in Korean and English. Based on the idea that a discourse structure basically consists of question/answer pairs, the proposed formalism shows 1) Korean contrastive topic corresponds to a certain prosody in English and the information structure for contrastive topic in both languages is essentially same, 2) Contrastive topic always presuppose a bigger question, and 3) the presupposition can be either explicitly satisfied by the preceding question, or implicitly accommodated so that the answer can be felicitous.
Information and Computation(PACLIC 11) Korean verbs' are typically divided into two groups: regular and irregular. Regular verbs are invariant throughout the paradigm. Irregular verbs, on the other hand, show alternations before certain suffixes. In the literature on Korean (e.g., Choy 1959;Huh 1965;Martin 1992), the behavior of "irregular" verbs is usually left unanalyzed as the alternations are considered to be phonologically unmotivated.This paper is organized as follows. After a presentation of surface [p]/[u](or [w]) alternations of /p/-irregular verbs, I argue that the alternation in question should be explained in terms of simplification in coda position, which will be captured through the interaction of structural and faithfulness constraints (section 2). The apparently dual behavior of /h/ will receive a unified account through the interactions among the universal constraints I propose (section 3). The aim of this paper is to show how an optimality-theoretic conception of phonology (McCarthy &amp; Prince 1993, 1995; Prince &amp; Smolensky 1993) overcomes some of the limitations of the traditional ways of treating the so-called irregular verbs&apos; in Korean. Building on the notion of OT, I attempt to shed new light on the properties of some general phonological phenomena of Korean. In the literature on Korean, the behavior of stem-final p&apos; and h&apos; is usually left unanalyzed as the alternations are considered to be phonologically unmotivated. I show in this paper that the irregular alternations are not really irregular but phonologically predictable.
Information and Computation(PACLIC 11) The purpose of this paper is to reconsider nasal assimilation in English within the framework of Optimality Theory (hereafter OT; Prince &amp; Smolensky 1993, McCarthy &amp; Prince 1994. There has been in the literature much discussion on the above phenomena (e.g., Halle &amp; Mohanan 1985, Borowsky 1986 among others). However, none of them provide a satisfactory account. In this paper, I will show that a purely constraint-based approach can account for nasal assimilation in a better way. In so doing, I will argue against an account based on structural interpretations of faithfulness, and propose that identity relation between input and output (i.e., correspondence, McCarthy &amp; Prince (1994)) provides the best account. I will also show that as Lamontagne and Rice (1995) assert, correspondence should be extended to the featural level.The paper proceeds as follows. First, section 2 reviews previous analyses of English nasal assimilation. Second, section 3 presents the general principles of OT and discusses how the constraint-based approach can handle the phenomena under consideration. Finally, section 4 provides a brief summary of the paper. This paper reconsiders nasal assimilation in English within the framework of Optimality Theory and shows that the phenomena can be accounted for in a natural way in terms of some ranked violable constraints. In so doing, I also argue against an account based on structural interpretations of faithfulness, and propose that identity relation between input and output, i.e., correspondence, provides the best account. I also present further evidence that as pointed out by Lamontagne and Rice (1995), it is necessary to extend correspondence to the featural level.
Information and Computation(PACLIC 11) The theory of Underspecification stipulates that either all unmarked (predicted) features (Kiparsky 1982, Archangeli 1988, Avery and Rice 1989, Pulleyblank 1988 among others) or redundant features ( Steriade 1987, Archangeli 1988, Mester and Ito 1989 be not specified in underlying representation. Hence, given that coronal is cross-linguistically the most unmarked consonant, under Radical Underspecification theory, it was considered as underspecified (Kiparsky 1985, Goldsmith 1989, Avery and Rice 1989, Paradis and Prunet 1989 among others) unless the coronal consonant has a dependant feature, which depends on the language. For instance, as Paradis and Prunet (1989) observe, in some languages, vowel or tone spreading occurs through a transparent coronal.In addition to its transparent property in some phonological processes, coronals in many languages are most likely to assimilated to the following labials or velars (Kiparsky 1985, Avery and Rice 1989, Steriade 1995, Crimson 1989. Based on such evidence, it has been argued that coronals do not have either a Place node (Paradis and Prunet 1989) which presumably is not opaque but is transparent to vowels and tones, or a Coronal node (Avery and Rice 1989), which forces it to be assimilated. Although the underspecified thing is different, both hypotheses agree in that coronals should be underspecified in some sense.Nevertheless, it is still believed that underspecification of a feature hinges on the phonological property of a language. Also, there have been a number of phonologists who claim that coronal should be specified despite its unmarkedness (Clements 1987, Steriade 1987, Kim H.S. 1994, Ito and Mester 1989. They contend that all unmarked features should be fully specified and only a redundant feature is underspecified; otherwise all features, although they are unmarked and predictable, are specified.The goal of this paper is to solve problems raised in the traditional analyses regarding underspecification of coronal within the framework Optimality Theory Smolensky 1993, McCarthy and. Hence, in this paper, I will argue based on place assimilation examples that unmarked coronal should be fully specified underlyingly, and that ranking of licensing constraints rather than coronal underspecification explains unmarkedness of coronal and typology of place assimilation.In the following sections, we will review place assimilation of some languages which has been used to establish coronal underspecification. Based on unmarkedness of coronal cross-linguistically, coronal has been considered to be underspecified. Also, place assimilation where coronal tends to assimilate to the adjacent labial or velar by a feature-filling rule has been support the underspecification of coronals. I instead argue that coronal should be specified in Optimality Theory despite its unmarkedness, and place assimilation no longer provides evidence of underspecification of coronal. In addition, I maintain that place assimilation is a way of respecting licensing requirements and that a licensing hierarchy with respect to place feature of a coda empirically resolve current issues regarding coronal underspecification and place assimilation.
Information and Computation(PACLIC 11)  
Programme Committee Conference Secretariat  
Language, Information and Computation  
Ettehkey &apos;how&apos; As a Small Clause Head As discussed by various linguists including Huang (1982), Chomsky (1986), Aoun (1986), Aoun and Li (1993), and Lasnik and Saito (1984), it is a notorious fact that WH-words do not behave uniformly with respect to their syntactic behaviors, especially when they appear in island contexts. What is generally agreed upon is that the so-called adjunct WH-words such as how and why in English and their counterparts in other languages cannot appear in syntactic islands (the ECP effects), whereas the so-called argument WH-words such as who and what in English and their counterparts in other languages are not restricted in such a way. Since Huang (1982) it is equally taken for granted that the same is true of WH-words in WH-in-situ languages such as Chinese and Japanese. Thus, WH-words like WHO and WHAT may reside within an island, but WH-words like HOW and WHY may not.Such a dichotomy between HOW/WHY vs. other WH-words, however, does not fare well, when more extensive data is considered. As T. Chung (1991) observes, Korean WH-word ettehkey 'how,' as opposed to way`why way`why,' does not display island effects, as exemplified in (1) below:'(1) a.[ {ettehkey/*way} yoliha-n] koki-ka masiss-ni? how/why cook-Adn meat-Nom tasty? `Q the meat that you cook how/why is tasty?' I A similar observation in Chinese and Japanese is made by some linguists. See Tang (1990), Tsai (1991Tsai ( , 1994, Lin (1992) for Chinese, and S. Watanabe (1995,1996) for Japanese.b. [koki-lul {ettehkey/*way) yoliha-myen] ssun mas-i epseci-ni? meat-Acc how/why cook-if bitter taste-Nom disappear-Q `Q if you cook meat how, the bitter taste disappears?'Ettehkey appears in a relative clause in (la) and in a conditional clause in (lb). In contrast way is not allowed in such contexts.In this paper, I try to account for why ettehkey and way behave differently with respect to island effects. First, I will briefly review two previous attempts to account for the difference, T. Chung's (1991) VP-adjunct analysis of ettehkey, andD. Chung's (1996) nominal analysis of ettehkey. Second, it will be shown that both approaches fail when more data is considered. Third, I propose to analyze ettehkey as the predicate head of a small clause and provide a theta-theoretic account of the syntactic difference between ettehkey and way. Finally, I make some speculation on the typological difference between ettehkey and its English counterpart how. WH-words are not uniform in their syntactic behaviors. Adjunct WH-words such as HOW and WHY are said to be more restricted in their distribution than argument WH-words such as WHO and WHAT. It is observed, however, that HOW in some East Asian languages behaves more like argument WH-words and does not display ECP effects. In this paper, I try to account for the HOW vs. WHY difference in Korean. First, I briefly review two previous attempts. T. Chung (1991) ascribes the difference to the positional variance: ettehkey &apos;how&apos; is generated VP-internally, whereas way &apos;why&apos; is generated in an IP(AgrP)-adjoined position, i.e., above subject. The trace of island internal ettehkey, but not of way, satisfies the ECP under the assumption that subject (or INFL) is a (special) antecedent governor for adverbs, which he motivates based on the fact that adverbs agree with subject in number and may take plural morpheme tul when subject is plural. Now the trace of ettehkey, but not of way, is antecedent governed by subject due to the hierarchical (c-command) relation. Another attempt was made by D. Chung (1996), who proposes to decompose ettehkey into etteh-ki-ey and attributes the lack of the ECP effects to the nominal feature associated with the nominalizer ki contained in ettehkey. Both approaches fail when more data is considered. Crucially, it will be shown that etteh, the main part of ettehkey, is a predicate, (i.e., it is neither an adjunct nor a nominal element,) but it does not show the ECP effects. I extend the predicate analysis to ettehkey, analyzing it as the predicate head of an adjectival small clause. Now the question is why predicate WH-words do not show the ECP effects like argument WH-words. I provide a theta-theoretic account under the assumption that the theta-identification between a predicate and its arguments is a bilateral relation in the sense that they identify or restrict each other. Thus, ettehkey/etteh, as predicates, are theta-identified and do not show the ECP effects. In contrast, way, as a pure adjunct, is not theta-identified and does show the ECP effects. As for the ECP effects that English how displays, I suppose that it is not a predicate but a pure adjunct, based on the observation that adjectival small clause heads are replaced by what, but not by how, in WH-questions or echo-questions.
Three Kinds of Korean Reflexives: A Corpus Linguistic Investigation on Grammar and Usage  a reflexive has a local antecedent or a long-distance antecedent. The result is that &apos;caki&apos; is almost even in having local and long-distance antecedents, but &apos;casin&apos; has more and &apos;cakicasin&apos; has much more local antecedents. I also examined the thematic roles of the local antecedents of reflexives, which shows that &apos;casin&apos; has relatively more Experiencer antecedents thanicakì has, although in both cases Agent antecedents dominate. The outcome of these frequency analysis suggests that a tendency (probably not grammaticalized yet), or degree of &quot;naturalness&quot; is real and can be captured in the usage data provided that we have a sizable amount of material manageable in an efficient way as provided by the corpus linguistic method of the present day.
Information and Computation (PACLIC12) Intuitively, humans can exhibit their thought, intellect and mind using their mothertongue effortlessly, although human languages are extremely variant and of great complexity. People usually can not be explicit about the rules of their own mothertongue of ambiguous words or phrase, whereas they can use their language appropriately in expression of their thought. Many linguists and psychologists have done numerous researches about this matter; however, it is still can not be made explicit how such knowledge of language is stored in our brain and how we use it (J.B. Gleason, 1993). Therefore, in order to find out some useful rules about languages, it is necessary to be restricted in different subdomains. The purpose of this paper is to give an applicable model in a subdomain of natural language processing (NLP) by computers.In this paper, we are going to submit the fundamental analyses of Chinese temporal Coverbs, postpositions and Coverb-postposition pairs. Many natural language processing between European languages and English with respect to temporals have been researched and implemented. For example, Bree (1992) defined the temporal subordinate conjunctions (SC) and prepositions as temporals. The temporals of European languages such as English, Dutch and German have been studied in detail. Bree, Smit &amp; Werkhoven (1990) have compared English and Dutch temporals. Furthermore, Bree (1992) has discussed the temporals between English, Dutch and German. The completion of examining the temporals of the European languages does not entail the understanding of temporals of all languages over the world. As the result, in order to get a clearer picture of the use of temporals across languages, studies of the Chinese temporal representation is necessary. In this paper, we will study the temporals of Chinese and find some applicable rules in translation from Chinese temporals to logic symbols.In Chinese, a special collection of verbs called Coverb, has the same function as English prepositions. The Coverbs approach was introduced by Francis (1946) who defined the approach as below:Coverbs (CV) are transitive verbs which do not stand alone but precede and are secondary to the main verb of the sentence. Some Coverbs are sometimes used as full verbs; a few are never anything but Coverbs. All can be translated as prepositions in English.Because we only concentrate on temporals, which mark a time clause or a time phrase, not all of Chinese Coverbs will be discussed in this paper, as will be explained in later sections. We are going to proceed in three steps in this paper. The first step is to make a description of an overview of English Temporals and in the way of English temporal logic representations. Secondly, we shall use semantic rules to represent the temporal Coverb, postposition and Coverb-postposition pair in Chinese. The semantics are categorised by means of diagrams and their usage will be illustrated by example. A table indicating Chinese temporals and English equivalents will be created in this stage. Finally, a constraint-based approach for Chinese temporals will be discussed. Some researches concerning the correspondence between the temporal features in English and other European languages have been done by Bree (1994). The subject of this paper is to collect and induce the temporal features of Chinese. The functionality of the Chinese temporal Coverbs, postpositions and Coverb-postposition pairs will be introduced Meanwhile, simple logic representation forms for Chinese temporal sentences will be proposed. The temporal taxonomy of Chinese has been defined and these definitions are also ready to be coded into a program for translation purpose. We develop a temporal representation language into which Chinese sentences involving temporal Coverbs, postpositions and Coverb-postposition pairs can be naturally translated.
Information and Computation (PACLIC12) In the literature, "Focus" -related issues have been studied from different perspectives. Following Culicover andRochemont (1983, 1990) and Horvath (1986), we in this paper assume that this essentially semantic conception of "Focus" can be characterized as a purely formal syntactic feature [+Focus] or [+F], which gets assigned to constituents at a certain • appropriate level of syntactic representation, participating syntactic operations under the general syntactic principles and constraints. In Section 2 we will first review some basic assumptions about the formal characterization of [+F], then moving quickly onto the question of how [+F] is reflected in the formal syntax, especially how it is marked syntactically. Section 3 is devoted to a discussion of the so-called "Focus-Fronting", we in particular will argue that a constituent with feature [+F] will be fronted in exactly the same fashion in English no matter whether it is a Wh-phrase or not, and the so-called "WhMovement" is in fact one type of instanciation of "Focus-Fronting" so that postulation of the former in the grammar is essentially redundant. In Section 4 we will demonstrate that the cluster of properties normally being associated with English "Question-Formation" can be decomposed and simplified. Cleft-sentences and Wh-questions in the language, as well as in many others, have more similarities than differences, and their similarities can be attributed quite naturally to the fact that they both result from the instanciating of a single syntactic rule thus well expected, and their differences can be accounted for independently in a modularized theory of grammar. Our major conclusions are summarized briefly in Section 5. Cross-linguistically, there are three grammatical devices to process the question mark j+Whi: the reduplication of certain elements in the predicate, Subject-Auxiliary Inversion, and the use of question particles. Also cross-linguistically, there are two devices for grammar to process the focus mark (+Focus]: the fronting of focused constituents and the insertion of a. Focus Marker such as the English &apos;be&apos; before focused constituents. In this mode of formulation, a set of language-particular and structure-particular grammatical properties such as those of English interrogative/cleft sentences, and the Archaic/Modern Chinese focus constructions are decomposed, reanalysed and thus significantly simplified.
Information and Computation (PACLIC12) The concept of reflexivization can be represented not only in syntax as shown in (la), but also in lexicon as shown in (lb).( 1 a) Zhangsan sha le ziji (lb) Zhangsan zi-sha le Zhangsan kill ASP self Zhangsan REFL-kill ASP`Zhangsan ASP`Zhangsan killed himself.' `Zhangsan committed suicide.'In Mandarin Chinese, there is a class of reflexive verbs which is prefixed by a reflexive morpheme zi-'self .They can be intransitive such as zi-sha a 'to commit suicide', or transitive such as zi-ren tf, 'to think of oneself as'. The reflexive anaphors, such as mono-morphemic reflexive ziji 'self and poly-morphemic reflexive to-ziji 'himself/herself , have attracted many Chinese linguists' attention ( Sung and Cole 1994, Tang 1989, Chen 1992, Y. Huang 1994, Kao 1993, Xu 1993, Chief and Chen 1995. On the contrary, there are relatively few studies of the reflexive verbs. Two recent studies (Tang 1992, Kao 1993 aim at Mandarin reflexive verbs. However, they do not provide a full set of data to discuss the phenomenon of Mandarin reflexive verbs. Moreover, some of the conclusions drawn on the two studies make incorrect prediction. Therefore, a thorough research of Mandarin reflexive verbs should be conducted. This paper focuses on the presentation and description of the linguistic data of Mandarin intransitive reflexive verbs and provides an alternative analysis.' Section 2 provides a background understanding of the analysis in the present study, including previous analyses and the characteristics of Mandarin reflexive verbs. In section 3, we will present an alternative analysis. By applying the notion of Lexical Mapping Theory, a sub-theory of LFG, a morpholexical operation of lexical binding is proposed to account for the Mandarin intransitive reflexive verbs. Section 4 demonstrates that the present analysis can properly explain the facts that the previous analyses fail to account for. In section 5 we will compare the difference between the present analysis and other analyses. We discuss the implication of the present analysis in section 6. The final section concludes this paper. The present paper focuses on the Mandarin Intransitive keflexive Verbs. These verbs, such as zi-sha &apos;to commit suicide&apos;, and zi fen &apos;to burn oneself, are formed with a reflexive morpheme zi-&apos;self&apos; and a root verb. Take the verb zi-sha &apos;to commit suicide&apos; for example, the reflexive morpheme contributes the reflexive meaning and the root verb sha &apos;to kill&apos; contributes the meaning`to meaning`to kill&apos;; hence, the meaning &apos;to kill oneself. Two previous analyses have different classification of these reflexive verbs. For instance, Tang&apos;s (1992) analysis implies that these verbs are unergative, while Kao (1993) asserts that they are unaccusative. In light of the previous analyses, we present an alternative analysis by giving new data that are largely selected from Sinica Corpus. By scrutinizing the syntactic behavior of reflexive verbs, calculation of thematic properties of the subject, and the event structure of these predicates, we propose a morpholexical operation of reflexivization and conclude that intransitive reflexive verbs are unaccusative. Although our conclusion seems to be as same as Kao&apos;s conclusion, two analyses are strikingly different from each other. We will prove that Kao&apos;s analysis can only account for the partial truth, while the present analysis can fully explain the linguistic phenomenon of Mandarin intransitive reflexive verbs. In addition, the implication of two analyses is also different. It is believed that the present analysis offers additional evidence to support the idea that the distinction between thematic roles is not discrete.
Information and Computation (PACLIC12) This paper describes a method for analyzing embedded noun phrase structures that is derived from the Japanese double-nominal-case construction based on the valency structure used in ALT-J/E. ALT-J/E is a Japanese-toEnglish translation system ( Ikehara et al. 87).An embeddednoun phrase structure is the structure in which a sentence (hereafter, called the embedded sentence) modifies a noun phrase (hereafter, called the modified noun phrase). This structure is equivalent to the English structure in which a noun phrase is modified by a relative clause. Embedded noun phrase structures are frequently used in the Japanese language because an embedded sentence makes the modified noun phrase more expressive. In embedded noun phrase structures, the noun phrase modified by an embedded sentence is usually derived from a certain case of a predicate for the sentence. For this type of embedded noun phrase structure, several analysis methods have beenproposed (Tsujii 83, Yokoo &amp; Hayashi 87). These methods analyze the modified noun phrase as the case extraposed from the embedded sentence.In contrast, there is another type of embedded noun phrase structure in which the noun phrase modifying a certain case, especially the subjective case, of the embedded sentence is extraposed. Because, in this type, the modified noun phrase is not derived from the valency element for the Japanese verb or adjective of the embedded sentence, it is difficult to apply the analysis methods developed to handle the former type. This is the reason why an exceptional process is required to analyze the enbedded noun phrase structure in Japanese syntactic analysis. If the modified noun in the latter type can be connected to any valency element in the embedded sentence, then we can avoid processing the latter type as an exception and analyze both types uniformly. For this purpose, we regard the latter type as the structure derived from the Japanese double-nominal-case construction in which the adverbial particle "wa" is a proxy for the pre-nominal case-marking particle "no". This point of view allows us to treat the modified noun in the latter type as a virtual valency element for the embedded sentence.In the following, chapter 2 defines terms used in this paper, and chapter 3 summarizes the Japanese sentence analysis based on the valency structure in ALT-J/E. Chapter 4 proposes processing the embedded noun phrase structure derived from Japanese double-nominal-case construction, and chapter 5 describes an evaluation that confirms the validity of the method proposed here. Embedded noun phrase structures are frequently used in the Japanese language because embedded sentences make modified noun phrases more expressive. In embedded noun phrase structures , a noun phrase modified by an embedded sentence is usually created by extraposing the case, such as the subjective case, of the sentence. In contrast, there is another type of embedded noun phrase structure. In this type, the noun phrase modifying a certain case of the embedded sentence is extraposed. As conventional methods for analyzing embedded noun phrase structures do not take the existence of the Japanese double-nominal-case construction into consideration , the latter type of embedded noun phrase is handled as an exception. This paper describes a method for analyzing the latter type of embedded noun phrase structure based on the valency structure in the Japanese-to-English translation system called ALT-J/E. We regard the latter type as the structure derived from the Japanese double-nominal-case construction in which the adverbial particle &quot;wa&quot; is a proxy for the pre nominal particle &quot;no&quot;. This point of view leads us to analyze both the former and the latter types of embedded noun phrase structures uniformly based on the valency structure of embedded sentences. Moreover, this paper describes an evaluation that clarifies the validity of the proposed method. The evaluation results show that about 86 % of the latter type of the embedded noun phrase structures are correctly analyzed by the method proposed here. This means the proposed method is valid for analyzing embedded noun phrase structures derived from Japanese double-nominal-case construction.
Information and Computation (PACLIC12) The Japanese language has two kinds of negative word nai meaning 'not' in English: one is the nai occurring after a verb or adjective and the other is the nai which has an independent status as a word. The example of the latter is:(1) Kokoni ringo wa nai here apple Nom not Here is no apple.which is the negation of:(2) Kokoni ringo ga aru here apple Nom is Here is an apple. where omoshiroku-nai is the negation of the adjective omoshiroi. While the traditional Japanese linguistics has generally treated the independent nai as an adjective, it regarded the nai which occurs after a verb as an auxiliary and the nai which occurs after an adjective as an adjective. Hattori (1950) raised a question as to this distinction and proposed to use a category-neutral term. He called the nai after a verb fuzoku-keishiki (dependent form) while he called the one after an adjective fuzoku-go (dependent word).In a strong contrast to traditional Japanese linguistics, almost all the studies in generative grammar have made no distinction between the nai occurring after a verb or an adjective with no independent status, and the nai having an independent status by giving a unified name NEG whose syntactic or semantic status is left unspecified. They have focused their attention on the interaction between a scope of negation and a quantifier or the issue of the behavior of negative polarity items.In this paper we attempt to integrate all the proposals , of each position by making use of both syntactic and semantic features. Moreover, it is our main claim that the independent nai has a distinctly adjectival status in terms of a semantic structure which we propose below.The paper consists of the following arguments. Firstly, the independent nai shows a semantic structure which is characteristic of ordinary adjectives. Secondly, it shares the same feature with the dependent nai in terms of the sensitivity to what is called negative polarity items, that is, NPI. Thus, it can be argued that the independent nai has an overlapping feature of being an adjective and at the same time of being sensitive to NPI. Traditionally, Japanese is said to have two kinds of negative word nai, meaning &apos;not&apos; in English: the nai occurring after a verb or adjective and the nai with an independent status as a word. In contrast to traditional Japanese linguistics, most generative studies of Japanese have made no such distinction but rather focused their attention on such issues as the interaction between a scope of negation and a quantifier and the behavior of negative polarity items. In this paper we attempt to integrate all the proposals of each position by making use of both syntactic and semantic features. We also claim that the independent nai has a distinctly adjectival status with regard to its* semantic structure.
Information and Computation (PACLIC12) This paper discusses the structure of the so-called "Internally-Headed Relative Clause" (IHRC) in Japanese in the constraint-based framework of Head-driven Phrase Structure Grammar (HPSG) and proposes a mutliple inheritance type-hierarchical analysis to account for the dual nature of the IHRC as well as for its restrictive availability.There has been a long-standing debate regarding whether the IHRC is to be analyzed as a relative clause or as a clausal complement (Kim 1996, Kuroda 1992, Ohara 1996, Park 1994. Semantically speaking, the IHRC apparently modifies a noun, strongly analogous to the Externally-Headed Relative Clause (EHRC). Syntactically, however, it is quite distinct from the EHRC; it does not even have a syntactic nominal head which is the target of, modification. Rather, the syntactic structure (on the superficial level at least) of the IHRC is identical with that of a clausal complement. This state of incongruity has led many researchers to assume a quasi-EHRC structure for the IHRC, positing empty categories for the missing syntactic head noun and other positions when necessary. Such relative clause analyses, however, fail to capture some differences between the IHRC and the EHRC. The clausal complement analysis, on the other hand, must explain why and how the IHRC refers to an entity involved in the event denoted by the clause (=entity-reading), while a clausal complement refers to an event as a whole (=event-reading). Park's (1994) analysis of the Korean IHRC leaves this essential question to some type of pragmatics, and Y-B. Kim's (1996) analysis contains crucial technical problems, to be shown in a note below. Thus the adequate analysis of the IHRC in Japanese is yet to be known.The present paper claims that the bipartite debate regarding the identification of the IHRC is misguided. It is an undebatable fact that the IHRC is similar to the EHRC in one respect, and to clausal complement clause in another. Therefore, defining it categorically as either one distorts the reality. I claim that the IHRC in Japanese (and presumably in Korean) is a hybrid construction composed of the clausal complement structure and the relative clause structure, by exploiting the possibility of multiple-inheritance hierarchies. The hybrid structure of IHRC is quite a marked type of multiple inheritance, and I claim that this accounts for the unstability of IHRC as an independent grammatical construction.This paper proceeds in the following way. The Japanese IHRCs will first be compared and contrasted with the EHRCs and clausal complements. Having confirmed the unstability of the IHRC interpretation, I will present the type hierarchical analysis of the IHRCs. Though my discussion is exclusively done on Japanese, I believe the analaysis will find equal applicability in languages like Korean as well.A word is in order before we start so as to avoid confusion of terminology. Precisely speaking, IHRCs and EHRCs refer to the clauses without the nominal head. Following Sag (1996), I will refer to a relative clause with a nominal head as a relative phrase. In what follows, terms IHRC and EHRC are used to refer to relative phrases only when no confusion is likely to occur. Otherwise, IHRC and EHRC are reserved for "clauses" and terms IHRC-phrase and EHRC-phrase are used when a nominal head is included. This paper proposes a mutliple inheritance type-hierarchical analysis of the Japanese &quot;Internally-Headed Relative Clause&quot; (IHRC) in Head-driven Phrase Structure Grammar (HPSG). It has been a long standing issue whether the IHRC shares syntactic properties with the EHRC or it is basically a clausal complement structure. This paper claims that such bipartite debate is misguided. It is undeniable that IHRC is similar to EHRC in one respect, and to clausal complement clause in another. Defining it categorically as either one only distorts the reality. It will be proposed, therefore, that IHRC is a subtype of both a relative clause and a clausal complement. The hybrid structure of IHRC is quite a marked type of multiple inheritance, and I claim that this accounts for the restrictive availability of IHRC and its unstability as an independent grammatical construction, as well as for the IHRC&apos;s intermediacy between the EHRC and the clausal complement structure. This discussion is done on Japanese alone; however, the analysis with minor modifications will accommodate the Korean IHRC as well.
Information and Computation (PACLIC12) Radford (1990) has proposed a theory of language acquisition that makes crucial use of the distinction between lexical and functional categories (e.g. Fukui 1986). The central claim is that there are three distinct stages of development. Table 1 summarizes the three stages and their characterization (taken from O'Grady 1993: 1). Table 1. Three stages of syntactic development Stage Characterization precategorial one-word utterances, no categorial structure 2 lexical (20 mos., ±20%) system of lexical categories and phrases 3 functional (24 mos., ±20%) system of functional categoriesAs indicated in Table 1, Radford's theory implies a developmental gap between the second and the third stages with a specific age difference. Providing data from English acquisition, Radford indicates that the lexical system indeed emerges before the functional system.' He further argues that the timing difference between the acquisition of the lexical system and the functional system is determined by different biologically determined stages of 'maturation' of Universal Grammar (UG). If this timing difference is determined by UG, every language should show the same timing difference in child acquisition data, since this difference in timing is genetically determined.O' Grady (1993) has challenged Radford's maturational theory based on the comparative study of determiner-like words in Korean and English. The rational for this comparison lies in the assumption that whereas demonstratives and other determiner-like words in English (the, a, this, some, etc.) are part of the functional system, their Korean counterparts are simple lexical categories (ibid.: 7). 2 A second key point in his study is that the UG-based maturational theory put forward by Radford was examined side by side with the semantic theory, which states that words which have rather abstract and complex meanings (regardless of their categorial status) are acquired relatively late (cf. Brown &amp; Fraser 1963, Brown 1973. Thus, these two theories make different predictions about the emergence of demonstratives and other determiner-like elements in Korean and English. The semantic theory predicts that these elements must emerge at a later stage in both languages since they denote relatively abstract and complex meanings; whereas the maturational theory makes no such predictions about Korean (O'Grady ibid.: 8-9). Table 2 summarizes these differences. Table 2: Predictions about the development of demonstratives and other determiner-like words in English and  Korean  Language  The semantic theory  English  relatively late  Korean  relatively late O'Grady has looked into speech samples collected from five Korean-speaking children and reported that demonstratives and other determiner-like elements in Korean seemed to develop relatively late since they did not occur in the speech of children under age 3 (ibid.: 11). The data from Korean suggests that determiner-like elements in Korean are acquired no earlier than their English counterparts. According to O'Grady, since the late acquisition of these elements in both Korean and English can be attributed to their relatively abstract semantics, there is no reason to attribute the late acquisition of English determiners to their categorial status (i.e. functional categories) (ibid.: 12).Using the same line of reasoning as in O'Grady's Korean study, the present study examined Radford's maturational theory in the light of data from the acquisition of Japanese. In the attempt to go beyond O'Grady's study, I took two steps: first, I looked into naturalistic data for the development of demonstratives and other determiner-like elements in Japanese, and then conducted a cross-sectional experiment for the comprehension of Japanese demonstratives.The rest of the paper is organized in the following fashion. In Section 2, we summarize Radford's acquisition theory along with the evidence he has provided for the lexical-functional distinction. Section 3 introduces two competing theories which seek to account for the developmental facts. The rational for examining the issue based on the data from Japanese will be included in Section 4. Section 5 presents data from the acquisition of Japanese. Section 6 includes conclusion. While demonstratives and other determiner-like words in English are part of the functional system, their Japanese counterparts are simple lexical categories. The semantic theory (e.g. Brown &amp; Fraser 1963) predicts that these elements must emerge at a later stage in both languages since they denote relatively abstract and complex meanings; whereas Radford&apos;s (1990) maturational theory makes no such predictions about Japanese. In replicating O&apos;Grady&apos;s (1993) Korean study, I collected both production and comprehension data for Japanese. The data indicates that despite their categorial status (i.e. lexical categories), these determiner-like elements emerge quite late. Since the late emergence of these elements in both Japanese and English can be attributed to their relatively abstract semantics, there is no reason to attribute the late acquisition of English determiners to their categorial status (i.e. functional categories). Thus, just like O&apos;Grady&apos;s Korean data, the Japanese data seems to support the semantic theory.
Information and Computation (PACLIC12)  Han (1994) argues that moms are isochronous units, but no inference is drawn for isochrony regarding the length of the syllable in Japanese. As the tempo of speech increases, however, the phonetic reality of moms seems to become less obvious (Beckman 1982 and Larish 1989). This paper provides a new insight on the apparent gap between phonology and phonetics, which comes from the distinction between Initial Foot Parsing (IFP) and Surface Foot Parsing (SFP). Moreover, it emphasizes an important consideration of timing units larger than moms.
Information and Computation (PACLIC12) The purpose of this paper is to consider assimilation of English prefix-final nasals within the framework of Optimality Theory(henceforth, OT; Prince &amp; Smolensky 1993), especially making use of a relation of correspondence (McCarthy &amp; Prince 1994b, 1995, Lamontagne &amp; Rice 1995. There has been in the literature much discussion on the phenomenon (to name a few, Chomsky &amp; Halle 1968, Kiparsky 1982, Borowsky 1986, none of which, however, provides a satisfactory account. In this paper, by employing the framework of OT, I will show that a constraint-based approach can give a better account of the phenomenon. In order to do this, following Kenstowicz(1995), I will posit a constraint of Uniform Exponence which evaluates sets of morphologically related words for segmental and prosodic similarity. In particular, I will assume that the candidates for the allomorph of a prefix are evaluated so as to minimize allomorphic difference in the realization of the morpheme itself. I will show that we can account for the phonology of the nasal-final prefixes in a unified way by appropriately ranking the constraint Uniform Exponence among the other constraints. That is, I will claim that no level ordering (cf. Kiparsky 1982) nor different underlying representations (cf. Borowsky 1986) need to be specified in the grammar, and that what is needed is a set of some ranked and violable constraints.The paper proceeds as follows. Section 2 reviews earlier analyses. Section 3.1 provides a brief introduction to the principles and assumptions of OT, and section 3.2 proposes an alternative analysis of the alternations of the nasal-final prefixes, arguing for a special constraint hierarchy which produces correct outputs. Finally, section 4 summarizes the paper. The purpose of this paper is to reconsider the phonology of English nasal-final prefixes within the framework of Optimality Theory, especially making use of a relation of correspondence. In this paper, I show that compared with previous analyses employing rules and their extrinsic orderings, a constraint-based approach can give a better account of the phenomenon. In order to do this, following Kenstowicz(1995), I posit a constraint of Uniform Exponence. In particular; I assume that the candidates for the allomorph of a prefix are evaluated so as to minimize allomorphic difference in the realization of the morpheme itself. I show that given the assumption above, we can account for the assimilation of the prefix-final nasals in a unified way by appropriately ranking the Uniform Exponence constraint among the other constraints.
Information and Computation (PACLIC12)  Jae-I1 Yeom Ik-Hwan Lee Yonsei University, KOREA The main goal of this paper is to show how to derive a common ground In order to do this, first we need to know what a common ground should be like. In traditional dynamic semantics, a common ground is given as a single information state which is shared by the people in the context. In this paper I am going to show that a common ground must include more than one information state, and that this is necessary to deal with conversational implicatures, which require distinction between speakers and audience. In this sense, the new common ground will be the basis for a conversational model of dynamic semantics. The traditional dynamic semantics is not appropriate for conversations, but rather a model for text processing.
Co-interpretation Network in English Discourse There has been little effort made to substantiate Halliday and Hasan's co-reference framework since their work was published 20-odd years ago, excepted for a slight modification in Martin (1992). Fligelstone (1992), Garside (1993),  and Botley &amp; McEnery (eds. forthcoming) have proposed an annotation scheme for tagging texts canpuationally. However, their theoretical foundation is Halliday &amp; Hasan (1976), which is sketchy and confined to major co-reference relations. In this paper, I will examine thoroughly how English noun phrases are interpreted in discourse. I recognise two roles that a noun phrase plays in discourse. One is that of the introductory expression which may fimction as the antecedent of a co-referential / co-interpretational network. The other is that of the dependent coreferring / co-interpretational expression, which depends on the introductory expression for the interpretation of its co-referent or descriptive content. I will describe the major types of relation between two co-referring / cointerpretational expressions in a discourse in order to prepare for a comprehensive discoursal networking system of English noun phrases. This paper presents a far more detailed account of co-interpretation network among noun phrases in English discourse than the one laid out in Halliday and Hasan (1976). This system will provide a new foundation for the computational tagging of English texts and assist effective textual processing by the computer.
Predictivity vs. Stipulativity in the Lexicon The lexicon is the repository of word-specific information. It includes representations of the syntax and semantics associated with individual words. It also might include generalisations which can be made about word use and idiosyncrasies associated with the behaviour of specific words. It is important to identify regularities where they exist, in order to account for predictivity and systematicity in language use, and to provide a basis for natural language processing systems which efficiently accommodate the flexibility of word use. It is equally important to accurately reflect individual word usage, in order to explain particular interpretations of a word in specific contexts. These two requirements, predictivity and stipulativity, place competing demands on the lexicon, and must be carefully balanced.I will look in this paper at the balance between predictivity and stipulativity in the lexicon, with particular reference to the phenomenon of the resultative construction. I will argue that the lexicon does not exist in isolation of other cognitive processes and that word use is not determined solely at the lexical level. Pragmatic reasoning, based on discourse coherence and world knowledge, will be shown to play an important role in interpretation, and to influence where the line between predictivity and stipulativity must be drawn. Furthermore, linguistic conventionalisation will be shown to affect what information must be explicitly represented in the lexicon and the level of generality of lexical regularities. The consideration of the impact of these various knowledge sources on interpretation will influence the structure of the lexicon, and will lead to a model which provides a fuller account of the resultative data. This paper discusses the balance which must be made in the lexicon between the capture of gen-eralisations about word use and the reflection of idiosyncratic word use, through the example of the resultative construction. The syntax and semantics of that construction are considered in detail, as is the felicity of a range of instantiations of the construction. The conclusions drawn from this analysis emphasise that the balance of information in the lexicon must be carefully considered. This is particularly true in the context of the various knowledge sources which influence the interpretation and use of a word. Pragmatic reasoning and linguistic conventionalisation are argued to influence the structure of information in the lexicon.
Information and Computation (PACLIC12) The goal of computational linguistics is to create computational models of language in enough detail so that we can write computer programs to perform various tasks involving natural language, using the notions of algorithms and data structures from computer science. The ultimate goal is to specify models that are similar to human performance in the linguistics tasks of reading, writing, hearing, and speaking (Allen 1995). To build such a computational model, many researches of computational linguists or computer scientists in the area of natural language processing (NLP) have taken plausible theories of language-related disciplines such as linguistics, psycholinguistics, the philosophy of language, and cognitive science. Especially in the semantic analysis of NLP, the meaning of words has been represented, mainly based on the semantic feature hypothesis, by the result of componential analysis, and the process of semantic analysis was substantially dependent on the information of argument structure and selectional restrictions under the thematic role theory.To make practical working systems in accordance with such theories, we need a lexicon of componential analysis of all words, argument structures that each verb requires, and selectional restrictions which noun phrases should satisfy to meet the required thematic roles. There are various methods of the semantic lexical representation. However, most of them include a taxonomic classification of concepts or words. One of critical bottleneck problems is how to construct a complete and reasonable taxonomy for NLP. Constructing this knowledge manually is in the domain of the linguists' work. On the other hand, computer scientists or computational linguists are responsible for its automatic construction. There have been many approaches to the automatic extraction of linguistic knowledge from large corpora and machine-readable dictionaries in NLP.This study focuses on automatically building a lexicon among the three types of linguistic knowledge and present a new method that is not only more practical and efficient for implementing on computers, but also more reasonable in view of psycholinguistics. First of all, we take a careful look at the problems of existing linguistic theories, and synthesize some of them in a way different from those adopted in the theories of almost all NLP. Then, we extract triples [noun, Case particle, verb] from a large corpus, and we assign Cases to a word by supervised machine learning on the basis of Case particles and the collocational information. Finally, we define the word meaning by a set of Case prototypicalities. In the following we will look at linguistic theories related to the word meaning: namely, lexical field, componential analysis, lexical decomposition, and acquisition of meaning. In addition, we will see that Case particles have a very important role in the acquisition of word meaning in computers. A lexicon is essential to provide a proper semantic analysis of linguistic expressions. Thus, much work has been done to build one automatically. However, the results were not enough to define word meaning, but no more than a kind of thesaurus or taxon-omy. In this paper, we present a new method that is not only more practical and efficient for implementing on computers, but also more reasonable in view of psycholin-guistics. We first note that componential analysis is not real in our cognitive system. We define word meaning by a set of Case prototypicalities. We show that this knowledge can be constructed by supervised machine learning on the basis of Case particles and the collocational information from a large corpus.
Information and Computation (PACLIC12) The goal of computational linguistics is to create computational models of language in enough detail so that we can write computer programs to perform various tasks involving natural language, using the notions of algorithms and data structures from computer science. The ultimate goal is to specify models that are similar to human performance in the linguistics tasks of reading, writing, hearing, and speaking (Allen 1995). To build such a computational model, many researches of computational linguists or computer scientists in the area of natural language processing (NLP) have taken plausible theories of language-related disciplines such as linguistics, psycholinguistics, the philosophy of language, and cognitive science. Especially in the semantic analysis of NLP, the meaning of words has been represented, mainly based on the semantic feature hypothesis, by the result of componential analysis, and the process of semantic analysis was substantially dependent on the information of argument structure and selectional restrictions under the thematic role theory.To make practical working systems in accordance with such theories, we need a lexicon of componential analysis of all words, argument structures that each verb requires, and selectional restrictions which noun phrases should satisfy to meet the required thematic roles. There are various methods of the semantic lexical representation. However, most of them include a taxonomic classification of concepts or words. One of critical bottleneck problems is how to construct a complete and reasonable taxonomy for NLP. Constructing this knowledge manually is in the domain of the linguists' work. On the other hand, computer scientists or computational linguists are responsible for its automatic construction. There have been many approaches to the automatic extraction of linguistic knowledge from large corpora and machine-readable dictionaries in NLP.This study focuses on automatically building a lexicon among the three types of linguistic knowledge and present a new method that is not only more practical and efficient for implementing on computers, but also more reasonable in view of psycholinguistics. First of all, we take a careful look at the problems of existing linguistic theories, and synthesize some of them in a way different from those adopted in the theories of almost all NLP. Then, we extract triples [noun, Case particle, verb] from a large corpus, and we assign Cases to a word by supervised machine learning on the basis of Case particles and the collocational information. Finally, we define the word meaning by a set of Case prototypicalities. In the following we will look at linguistic theories related to the word meaning: namely, lexical field, componential analysis, lexical decomposition, and acquisition of meaning. In addition, we will see that Case particles have a very important role in the acquisition of word meaning in computers. A lexicon is essential to provide a proper semantic analysis of linguistic expressions. Thus, much work has been done to build one automatically. However, the results were not enough to define word meaning, but no more than a kind of thesaurus or taxon-omy. In this paper, we present a new method that is not only more practical and efficient for implementing on computers, but also more reasonable in view of psycholin-guistics. We first note that componential analysis is not real in our cognitive system. We define word meaning by a set of Case prototypicalities. We show that this knowledge can be constructed by supervised machine learning on the basis of Case particles and the collocational information from a large corpus.
Information and Computation (PACLIC12) This paper addresses the problems of subjectivization of embedded arguments bearing various semantic roles in potential constructions in Japanese, as illustrated in (1)  Tazawako. dealt with in the literature of Japanese linguistics.We try to give a unified account of all the case alternations observed in (1) from a semantic point of view, adopting the theory of properties and predication proposed by Chierchia (1984Chierchia ( , 1985 and Chierchia &amp; Turner (1988), among others. This approach also enables us to show that alternations between genitive and other cases under nominalization can be dealt with in the same manner as those between nominative and other cases in matrix sentences. In potential constructions in Japanese, any argument of a base verb, whether it is an argument, an adjunct or even an argument with no thematic relation to the verb, can be marked with nominative case. Addressing the problem of various types of case alternations as well as`gaas`ga-no&apos; conversion, this paper argues for the existence of some semantically motivated mechanism licensing nominative and genitive case, which adequately accounts for the distributions of these cases in these constructions. We also claim that there is a close connection between the morphological realization of case and the overall semantic interpretation of a sentence.
Information and Computation (PACLIC12) One of the research goals of modeltheoretic semantics is to derive the truth condition of (an utterance of) an arbitrary (declarative) sentence compositionally. Surface grammar (morphology and syntax) is sometimes felt not to get along well with compositional semantics, in which case the fine details of surface grammar are often ignored. For example, Kamp and Reyle (1993) treat plural anaphora by ignoring plural morphology of English nouns and verbs. Their treatment has the implication that English plural morphology has no semantic contribution to make and hence redundant (or even an obstacle for a compositional semantics).We have two purposes in this paper. The first is to revise Ishikawa's (1995a) semantics of plurality by modifying its lattice-theoretic structure of situation theoretic structured objects.' The second is to offer an alternative to Kamp and Reyle's treatment that respects English number morphology, by combining the revised version of Ishikawa's semantics of plurality with what we call the Resolution Principle. By doing the latter, we attempt to suggest that natural language surface grammar is, in fact, a better guide to compositional semantics than Kamp and Reyle assume.2 3 2 An Extended Lattice-theoretic Theory of Plurality In order to formulate truth-conditionally-satisfactor* semantics in a composi-tional. manner,-model-iiieoretic semanticists sometimes posit morphology-semantics mismatches. Among them are Kamp and Reyle (1993), who occasionally ignore English plural morphology in constructing their analysis of anaphora. Our goal in this paper is to demonstrate that natural language morphology is a better guide for a compositional semantics than Kamp and Ryle assume. By refining the semantics of plurality put forth in Ishikawa (1995a), we construct an analysis of plural anaphora in a way that respects English plural morphology. Our results suggest that natural language morphology is not as redundant as usually assumed.
Information and Computation (PACLIC12) In natural language there always exist many preferred relationships between words. Lexicographers always use the concepts of collocation, co-occurrence and lexis to describe them. One classic example is "strong" and "powerful". [Halliday66] noticed that these two words were used in different language environments such as "strong tea" and "powerful computer" although they had similar syntax and semantics. Psychologists also have a similar concept: word association. Two highly associated word pairs are "bread/butter" and "doctor/nurse". Psychological experiments in [Meyer+75] indicated that the human' s reaction to a highly associated word pair was stronger and faster than that to a poorly associated word pair.The strength of word association can be measured by mutual information. By computing mutual information between the two words in the word pair, we can get many useful preference information from the corpus, such as the semantic preference between noun and noun(e.g. "doctor/nurse"), the particular preference between adjective and noun(e.g. "strong/tea"), and solid structure(e.g. "the more/the more"). These information are useful for automatic sentence disambiguation.The research in [Hindle+93] showed the role of correlated statistical information in resolving the sentence ambiguity. Consider a simple English sentence:"She (wanted I placed I put) the dress on the rack."For different verbs, the linking directions of the preposition phrase(" on the rack") are different. It can modify the noun(for "wanted") or function as the objective complement of the verbs(for "placed " and "put"). This is the well-known preposition phrase(PP) attachment problem in the analysis of English. Their research showed that a reasonable analysis result can be chosen by comparing the mutual information of the verbpreposition pair("want/on") and • the object-preposition pair("dress/on"). [Magerman+90] used a computational model of mutual information to automatically segment short phrases. [Rosenfeld94] used the concept of trigger pair as the basic information bearing element to extract information from further back in the document's history. Similar research includes [Brent93] and [Kobayashi+94].In Chinese, a word is made up of one or more characters. Hence, there also exists preferred relationships between Chinese characters. [Sproat+90] employed a statistical method to group neighboring Chinese characters in a sentence into two-character words by making use of a measure of character association based on mutual information. Here, we will focus instead on the preferred relationships between words.The preference relationships between words can expand from a short distance to a long distance. While Ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks, they have obvious deficiencies. For instance, N-gram models can only capture the short-distance dependency within an N-word window where currently the largest practical N for natural language is three and many kinds of dependencies in natural language occur beyond a three-word window. While we can use conventional N-gram models to capture the short-distance dependency, the long-distance dependency should also be exploited properly.The purpose of this paper is to study the preferred relationships between words over a short or long distance and propose a new modeling approach to capture such phenomena in the Chinese language.The organization of this chapter is as follows: Section 2 defines the concept of trigger pair. The criteria of how to select a trigger pair are described in Section 3 while Section 4 describes a method to measure the strength of a trigger pair. Section 5 describes the trigger-based language modeling approach. Section 6 gives a example of its applications: PINYIN-to-Character conversion. Finally, a summary of this paper is given in Section 7. This paper proposes a new MI-Trigger-based modeling approach to capture the preferred relationships between words over a short or long distance. It is implemented by the concept of trigger pair, which is selected by average mutual information and measured by mutual information. Both the distance-independent(DI) and distance-dependent(DD) MI-Trigger-based models are constructed within a window of a size from 1 to 10. It is found that the DD MI-Trigger models have better performance than the DI MI-Trigger models for the same window size and it is better to model the preferred relationships in a distance-dependent way. It is also found that the number of the trigger pairs in an MI-Trigger model can be kept to a reasonable size without losing too much of its modeling power. Finally, it is concluded that the preferred relationships between words are useful to language disambiguation and can be modeled efficiently by the MI-Trigger-based modeling approach. Keyword: MI-Trigger modeling approach, preferred relationship, long-distance context dependency, average mutual information, mutual information.
Information and Computation (PACLIC12) In recent years, there has been a growing interest in eliciting linguistic knowledge directly from corpora using statistical methods. Several quantitative measures have been proposed to identify significant lexical relations. These measures, however, are designed to work with large corpora with millions of words. Accordingly, they do not perform well in relatively small texts with a few thousand words. This paper presents a statistical method that is well-suited to extracting recurrent phrases and terms from relatively small texts. The method, a variant of Fung and Church's (1994) K-vec algorithm, is shown to be in line with linguistic generalisations about lexical cohesion in text structures. Most statistical measures for extracting interesting word pairs such as MI and t-score require a large corpus to work well. This paper evaluates some of the most widely used statistical measures and introduces a method that can identify significant bigrams in relatively small texts by adapting Fung and Church&apos;s (1994) K-vec algorithm, which was originally designed to extract word correspondences from unaligned parallel corpora. The proposed method captures the linguistic generalisation abou lexical patterning in texts and can identify recurrent co-occurring word sequences, which might be phrases, terms, or unknown words. In addition, it has the potential of identifying key phrases and terms that reveal topicality in a text.
Information and Computation (PACLIC12) It's well known that Chinese is an ideographic language and there is no word delimiter between words in written Chinese sentences. Therefore word segmentation becomes the very first task when processing Chinese text and in turn the accuracy of word segmentation is essential to the performance of the following procedures. As such, it has been widely studied in recent years and there have been quite a few publications on it [2][3][4][5][6][8][9][10][12][13][14]19,21,25]. As the definition of Chinese word itself is a complicated issue [1,11] and worth a separate paper, here we only focus on word segmentation techniques themselves trying to highlight the achievements so far obtained and the problems to be solved in the future. It's not easy to give a concise summary based on applications since we note that most of the methods are application oriented and driven by their respective needs. In this paper, we attempt to reveal the underlying technologies and give a comparison among them.Of the algorithms or methods reported on Chinese word segmentation [2][3][4][5][6][7][8][9][10], most of them roughly fall into two categories: lexical knowledge based methods and linguistic knowledge based methods. Since one important task for Chinese word segmentation is to reduce unexpected segmentation errors, it' s worthwhile to discuss word segmentation errors first before going to details of the algorithms.There are two major segmentation problems that might affect the accuracy of word segmentation performance, unknown word and word ambiguity. The first problem always happens as long as the text corpus contains new words. As a class of unknown word or new words usually shares similar attributes, new words could be identified by using heuristic rules, called new word detection. The second problem was found to consist of two typical types of ambiguities [2], i.e. words overlapping ambiguity and composition ambiguity.As we know, the new word problem always happens no matter how big a lexicon is in a real application. The two major causes of segmentation errors usually come together leading to unexpected segmentation results. Typical examples of errors are insertion, deletion and misplacement of segmentation boundaries: (a) an insertion between iand fi , which indicates a new word problem; (b) a deletion between and la in 1i , which results in a composition segmentation error; (c) two misplacements in a Val Pi instead of *Ell, which leads to overlapping ambiguity segmentation errors. To discover these ambiguities and disambiguate them is a very difficult but yet very important task [6,10,26].Lexical knowledge based segmenter only makes use of the lexicon knowledge to conduct its segmentation judgment. It is efficient and straightforward in practice. Resorting to more powerful decoding approaches, linguistic knowledge based segmenters combine lexical information with language statistical information to provide better accuracy. The lexical knowledge based approach effectively deals with all the ambiguity problems by ignoring it. Linguistic knowledge based approach alleviates the problems by introducing contextual constraints into the segmenter.In the next section, we will give comprehensive review on various algorithms that use a single lexicon as the sole resource to conduct the segmentation. The section 3 summarizes segmentation methods that apply more linguistic knowledge. Finally, we summarize the discussions. Chinese word segmentation has been a very important research topic not only because it is usually the very first step for Chinese text processing, but also because its high accuracy is a prerequisite for a high performance Chinese text processing such as Chinese input, speech recognition, machine translation and language understanding, etc. This paper gives a review on the development of Chinese word segmentation techniques that have been applied to various applications on Chinese text processing. As the methodology varies in a very wide range according to its applications, in this paper it is viewed in terms of the knowledge resources on which segmentation methods based. We summarize the methods into two categories, that is, lexical knowledge based and linguistic knowledge based methods.
Information and Computation (PACLIC12) Chinese word segmentation is an interesting, but difficult problem. The difficulties include the following: -"word" is not a very well-defined concept in the context of Chinese: linguists do not have generally accepted guidelines, and in experiments native speakers show only about 75 % agreement on the "correct" segmentation.-Even if we have guidelines, the problem does not become trivial. The biggest stumbling-blocks for any automatic segmenter are unseen words which occur neither in the lexicon nor in the training data.Previous work in this area tended to use some combination of stochastic methods and lexical heuristics. Stochastic methods provide robustness and trainability at the expense of compactness and perspicuity in the learnt models, while lexical heuristics tend to be labour intensive and highly corpus-specific But Palmer ( [4]) has recently applied Brill's Transformation-based Error-Driven Learning to the segmentation problem, with promising results. The models which are produced are both more compact and easier to interpret than stochastic models, yet produce a performance which is comparable to that of the best hand-crafted systems. *The first author was supported by a grant from the Studienstiftung des deutschen Volkes. t The second author was partly supported by a grant from ESRC to the Human Communication Research Centre. Correspondence about this paper should be sent to Chris . Brew@ed. ac . uk. Our work also uses Brill's technique. We extend Palmer's work by showing some effects of variation in corpus size and rule complexity. We present experiments which achieve satisfactory performance even with a very simple initial state tagger. A major reason for this success is the large size of the training corpus which was employed.Segmentation-driven retagging Two preliminary studies, reported at the end of this paper, indicate that segmentation performance might actually be improved by simultaneously addressing the task of assigning tags to the text. The benefit of this apparently paradoxical move comes because rules learnt by the system can use the tag layer to describe the context in a richer way than is available in the simpler system. This information can then be exploited by later rules. But while our system assigns part-of-speech tags, it does so only in the service of better segmentation, and the tags which it assigns do not necessarily have any merit beyond their role in assisting segmentation. Palmer ([4]) demonstrated how Brill&apos;s Transformation-based Error-Driven Learning can be applied to word segmentation in various languages. We present experimental results which show that such algorithms can achieve satisfactory performance even with a a very dimple initial state annotator We also present two preliminary studies, which suggest that even higher performance might be achieved if simple morphological information is available to the system, and that segmentation performance might actually be improved by combining segmentation with rudimentary part-of-speech tagging.
Information and Computation (PACLIC12) Bunsetsu, which is comprised of a content word with or without being followed by a string of function words, is a convenient unit for dependency structure analysis of Japanese. There are, however, no spaces indicating bunsetsu boundaries in the orthographic writing of Japanese. Thus a sentence must be segmented into bunsetsu's prior to dependency structure analysis. According to the elementary definition of bunsetsu [Nagao, ed. (1984)], such segmentation might look simple. There are, in reality, many factors that complicate the problem. For example, a prefix and/or a suffix can be attached to a content word, and Chinese characters can be concatenated to form a compound word. Some nouns and verbs have functions different from their original ones. Also, there are many idiomatic usages of morpheme concatenations. All these matters cause difficulties in detecting bunsetsu boundaries. Moreover, there is no system of morpheme categorization in Japanese that has received a general consensus. This situation gives rise to another obstacle to establishing a standard method of bunsetsu segmentation.There have been two major approaches to the bunsetsu segmentation problem: one based on an automaton [Fujio et al. (1997)], or bunsetsu patterns [ Kurohashi (1997)] representing a definition of bunsetsu, and the other based on a set of hand-crafted rules [Suzuki (1996)]. In the former approach, one has to give a definition of bunsetsu manually. The latter involves handiwork in getting knowledge about bunsetsu boundaries. Thus both approaches have problems in keeping consistency, coverage and optimality of manually obtained knowledge. When the task domain and/or the system of morpheme categorization are changed, one has to repeat the whole manual process to get new knowledge, which is rather laborious.This paper proposes a method of bunsetsu segmentation using a classification tree [Breiman et al. (1984)], [Quinlan (1993)]., by which knowledge about bunsetsu boundaries is automatically acquired from a corpus. It enables quick adaptation to a new task domain, and also to a new system of morpheme categorization without the need of changing the algorithm. Effectiveness of the method is shown through experiments on an ATR corpus and an EDR corpus. Bunsetsu, which is comprised of a content word followed by, possibly 0, function words, is a convenient unit for dependency structure analysis of Japanese. There are, however, no spaces indicating bunsetsu boundaries in the orthographic writing of Japanese. Thus a sentence must be segmented into bunsetsu&apos;s by some means prior to dependency structure analysis. Conventionally, such segmentation has been performed by using some kind of hand-crafted rules. This paper describes a novel segmentation method using a classification tree, by which knowledge about bunsetsu boundaries is automatically acquired from a labeled corpus. The method enables quick and easy adaptation to a new task domain, and also to a new system of morpheme categorization without the need of changing the algorithm. Effectiveness of this method is shown through experiments on an ATR corpus and an EDR corpus.
Information and Computation (PACLIC12) The compilation of a large translation lexicon is very costly, laborious, and time-consuming. Automation of all or most of the compilation process is therefore highly desirable. This goal has been made possible by the availability of large parallel corpora, machine-readable bilingual dictionaries, and statistical algorithms that have been proposed in recent years. In this paper, we address the problems encountered by statistics-based and dictionary-based approaches in deriving and augmenting a translation lexicon. We advocate a hybrid approach combining statistical and dictionary information to increase the recall, In addition, we suggest using proximity and global distributions of word pairs to enhance the precision. Fung and Church (1994) propose a simple algorithm to find word correspondences from unaligned parallel texts. The basic idea is that a true word pair should have similar distributions in terms of the position of its occurrence in the text. To estimate the similarity of co-occurrence, the parallel texts are split into the same number of segments (K) and the distributions of each word are represented in a 1...K binary vector. For instance, suppose the Chinese and English texts are divided into ten segments. Suppose further that the Chinese word tiff daxue occurs ten times, with the first 3 occurrences in the fourth segment and the remaining 7 occurrences in the seventh segment and that the English word university appears twelve times, with the first 4 occurrences in the fourth segment and the remaining 8 occurrences in the seventh segment. Using the K binary vectors, the distributions of both the Chinese and English words in question can be represented as &lt;0,0,0,1,0,0,1,0,0,0&gt;. Mutual information (MI) and t-score are then used to estimate the correlation of a proposed word correspondence. The mutual information and t-score of the word pair are defined in (1) and (2). * Chinese Knowledge Information Processing Group, Institute of Information Science, Academia Sinica, Nankang, Taipei 115, Taiwan. E-mail: imgao@hp.iis.sinica.edu.tw where a is the number of pieces of segments in which both the Chinese and the English words occur; b is the number of pieces of segment where only the Chinese word is found; c is the number of pieces of segment where only the English word is found. (2)• t(Vc,Ve)= JP(Vc,Ve) K The t-score is introduced to filter out word pairs with low frequency which happen to co-occur in the same segment. by chance. The threshold value of MI and t-score are set to be 0 and 1.65, respectively. Only word pairs which are higher than the predetermined threshold values and in the frequency range 3-10 are considered to be potential mutual translations. Fung and Church (1994) observe that K, the number of segments in the bilingual texts, has a decisive factor in the performance of the algorithm. If K is too large, there will be many spurious word pairs. On the other hand, if K is too small, very few word pairs can be identified. They thus suggest that K be set to the square root of the length of the bilingual texts. This paper presents a hybrid approach to deriving a translation lexicon from unaligned parallel Chinese-English corpora. Based on the observation that the English translation of a Chinese compound often involves two or more English words, a heuristic is presented that accepts Chinese-English word pairs as correct if two consecutive English words are proposed to correspond to the same Chinese word in ,a statistical or dictionary-based approach. In addition, document-external word distributions are utilised to measure word pair co-occurrences in the whole corpus. Words with low co-occurrence ratios are then filtered out. The two proposed methods can derive a translation lexicon of more than 94% precision.
Information and Computation (PACLIC12) In the field of information retrieval (IR), megabytes or terabytes of text have become commonplace. This is particularly true for commercial systems. Taking LEXIS-NEXIS, one of the leading providers of electronic research services, as an example, the entire services, at the time of writing, consists of more than 11,000 sources: 7,100 news and business sources, and 4,800 legal sources. There are more than 6,900 databases and over 1,100 billion characters on-line. Each week more than 9.5 million documents are added to the more than one billion documents on-line. To efficiently handle such gigantic text sources, traditional information storage and delivery mechanisms are constantly being challenged. While users are happy having at their finger tips the information they need to do their work, they feel frustrated to be overwhelmed by all sorts of documents, journals, reports, and electronic mail messages. There is an increasing demand that requested information be presented in a clear, succinct and systematic way. For example, it would be helpful if the user could be provided with ability of examining the summary or the condensed version of a document first, then be guided to the full document if he or she chooses to. It would be equally helpful if search and retrieval could be carried out on the condensed form of databases so that the retrieved information could be made more focused on. the original need. Undoubtedly, these on-line as well as off-line system capabilities can help alleviating the current information overload.Up until now two major approaches have been attempted to automatically generate summaries or condensed versions of on-line documents. One is text-based and the other is knowledge-based (Spark Jones and Endres-Niggemeyer 1995). The text-based approach is primarily statistical in nature, either relying on simple heuristics (e.g. Brandow et al. 1995 andZechner 1996) or on the combination of various heuristics (e.g. Kupiec et al. 1995 andEdmundson 1964). The goal is to identify key terms in the document, assigning weights to individual sentences, and assembling top-scored sentences as the condensed version of the document. According to some empirical studies (e.g. Morris et al.1992), the optimal length of the condensed version is somewhere between 20% and 30% of the original document length. The knowledge-based approach, on the other hand, involves symbolic analysis to extract significant pieces of information from the document (e.g. McKeown and Radev 1995, Ciravegna 1995and Lin 1995. After a set predefined templates are filled or partially filled, a summary will then be produced via a natural language generation mechanism. Though the text-based approach reveals its own limitations, such as no guarantee for discourse coherence, the general opinion seems to support its capability for real-world applications. The knowledge-based approach can be clean and effective in certain small and limited environments, but since it encompasses both interpretation and generation of natural language, its scalability and generalizability are often questioned. Research performed in this area has been heavily driven by readily available texts, e.g. MUC (Message Understanding Conference) data, and/or restricted to specific topic domains, again those defined by MUC. If needs arise for applications in general 'domains, a large amount of training and modification would be required. At the present time, such an approach can hardly fit in a commercial environment such as the one at LEXIS-NEXIS.In this paper we introduce Surrogater, a simple yet efficient document condensation system that is applicable for commercial purposes. Though it follows the general trend of the text-based strategy, i.e. performing automatic document condensation through sentence scoring and selection, Surrogater distinguishes itself from its predecessors in two ways. First, significant terms, ranging from single word terms to four-word terms, are automatically generated from a small and homogeneous dataset which represents a specific topic. Second, when Surrogater produces document summaries, the identified key terms are utilized exactly as they appear in the naturally running text. No term normalization or regulation is needed. Term clustering or grouping is not pursued. The primary goal of such a simple implementation is to achieve maximum efficiency, generalizability and flexibility that the fast-growing information commerce demands. This paper describes Surrogater, a simple yet efficient document condensation system that is applicable for commercial use. The system consists of two components, a preprocessing component for automatic generation of key terms for a predefined topic, and a condensation component that produces the condensed versions of on-line documents. To evaluate its performance, we compared Surrogater with five other summarization technologies, including Searchable LEAD, a commercial product. Twenty topics across four domains were evaluated, totalling. 30 documents and 1800 summaries. A two-way ANOVA test suggested that Surrogater performed at least equally well, if not better, compared to other commercial or nearly commercial products. More significantly, the empirical comparison did not show any dramatic differences in performance between Surrogater and Searchable LEAD as reported in an earlier study (Brandow et al. 1995)
Information and Computation (PACLIC12) Syntactic ambiguity appears, among others, in coordinate constructions. It is an annoying problem in analyzing structure and meaning of a sentence. A parser, for instance, is to detect the scope of a coordinate structure and identify its inner modification relations. However, the current parsers (e.g., the Link parser) often fail to handle the problem and/or produce a large number of parses.There are a few computational studies that have tried to resolve ambiguities in coordinate constructions (e.g., Paritong, 1992;Cooper, 1991;Bayer, 1996). For example, Kurohashi and Nagao (1994), in analyzing long Japanese sentences, proposed a syntactic analysis method for detecting conjunctive structures by using lexical similarity and structural parallelism. Mela and Fouquere (1996) used a direct process to determine the scope of a coordinate structure based on the concept of functor, argument and subcategorization. Unfortunately, neither of them has sufficiently dealt with .the syntactic structure of a coordination especially when a coordinator (such as and, or and comma) has two or possibly more preceding and succeeding constituents.We in this paper propose a method for determining the structure of a coordinate construction using information on similarities, selectional restrictions, and oilier linguistic cues. In Section 2 we identify the problem and describe the ideas behind our method in Section 3. We give disambiguation algorithms, show an disambiguation experiment, and evaluate its results in Section 4. In Section 5 we suggest an applicability of our method to resolving other syntactic ambiguities. This paper describes a method for determining syntactic structure in coordinate constructions. It is based on the information taken from semantic similarities, selectional restrictions, and some other linguistic cues. We discuss the role the information plays in resolving ambiguities that appear in coordinate constructions, describe the means of acquiring the necessary information automatically from two on-line corpora and a lexical database, and devise two algorithms for disambiguating coordinate constructions. An experiment that follows shows effectiveness of our method and its applicability to resolving ambiguities in some other syntactic structures.
Information and Computation (PACLIC12) In this paper, we discuss a method of removing ambiguities which appear in the process of text understanding based on simulation.We have been studying a text understanding system paying attention to the importance of an imagerial world model. In [Itoh,92, Itoh,95], we showed that the ability to simulate the matters described by sentences on the imagerial world model is one of the basic abilities to understand texts. We also proposed a method of implementing an imagerial world model, a method of simulating on the model and a method of observing the model to extract propositional expressions. We introduced an experimental ,system in order to verify importance of the imagerial world model and validity of each method.In order to clarify the point of issue, we-have done the work under the following constraints.1. We restrict our target texts to those explaining mechanical movements of machines in textbooks for junior high school students or encyclopedias for naive persons. The reasons are that we need to treat a relatively narrow domain in order to implement a world model imagerially in a simple way, and imagerial information seems to be quite important in the domain of mechanical movements.2. We restrict the machines to those which are composed of solid parts and are illustrated with two dimensional figures. It also makes . the implementation of the model easy.3. We deal with only the sentences which explain states or movements of a whole machine or its parts.Though there are some sentences explaining a pressure of gaseous fuel, a flow of fluid, or etc., we neglect them. If we deal with such sentences, we should represent &amp;pressure or a flow of fluid as well as positions and shapes of machine. In other words, the imagerial world model should hold information on multiple attributes. It makes the imagerial model complex.4. We restrict the sentences to those having no . ambiguity. In addition, we also restrict the texts to those which describe movements of a machine one by one along the time axis. The reason is that we want to concentrate on how to process each sentence and simplify how to . control the processes for individual sentences.However, there are many texts which don't satisfy the fourth constraint even ,if they satisfy the first and second constraints. Especially, if we remove the third constraint, it is necessary to deal with multiple attributes changing simultaneously such as a location, pressure, and so on. In addition, ,our world model isn't governed by entire physical law and it should hold some information qualitatively. Therefore, the simulation process comes to have ambiguities.In general, it is usual that ambiguities appear in the process to reproduce a qualitative and continuous world model by simulation based on discrete descriptions such as sentences. Thus, we relax the third constraint and discuss the method of removing ambiguities. In this paper, we aim to propose an elementary method to construct text understanding systems based on simulation.In the next section, we survey ambiguities in text understanding and define the problems we deal with. We also show an outline of the text understanding system proposed in [Itoh,92, Itoh,95], and illustrate how our problems are appeared in the process of text understanding. Then, we show some heuristic characters which are applicable to solve the problems. In section 3, we show the detailed algorithm to remove ambiguities, and in section 4 we introduce our experimental system. This paper discusses how to remove a kind of ambiguity in a text understanding system based on simulation. The system simulates some events mentioned in text on a world model and observes the behavior of the model during the simulation. Through these processes, it can recognize the other events mentioned implicitly. However, in case the system infers plural number of possible world, it can&apos;t decide which one is consistent with the context. We deal with such ambiguities. The ambiguities, in some cases, can be removed by considering contextual information. To remove the ambiguities in the simulation, we define three heuristics based on the characters of the explanatory descriptions and propose an algorithm of &quot;looking ahead&quot;. We implement ,ani experimental system. Accqrding to the ,algorithm, the system finds out the supplementary descriptions in the following sentences and removes the ambiguities by using the contents of the supplementary descriptions.
Information and Computation (PACLIC12) As the rapid progress of computer technology, the need for document computerization is also rised. So, how to enhance the document quality in the computer becomes an important issue. In addition, the document revision technology will be needed as a preprocessor or postprocessor in many application softwares, such as character recognition, speech recognition, machine translation and so on. Therefore, there are many approaches [1] on automatic document revision. The document revision can be further divided into error detection and. error modification. Nevertheless, the document error types is dependent on its processing language. For example, the English spelling checker can not be used to revise Chinese or Japanese document due to the different error types. Thus, the error types e of Chinese document will be described below.The error types of Chinese document can be divided into two parts. The first part is caused by input stage and the other part is caused by editing stage. Moreover, in input stage the different input way, such as phonetic way or radical way, will also have different error types.In the input part:(1) Using phonetic way The errors in Chinese document are mainly caused in two stages-input and editing. There are homonyms or homophones selection error, ambiguous pronunciation error, word segmentation error, similar shape character error, editing operation error and so on. In order to increase the quality of Chinese text, the conventional Chinese document revision system used the similar characters set and language model with some statistical date. Nevertheless, there are the following problems: (1) The perfect similar character set is difficult to make (2) Due to the copyright problem the large and balanced Chinese corpus is very difficult to be obtained (3) The above editing errors can not be solved simultaneously (4) The average success revision rate is not over 75%. In this paper we study the Chinese features and phonetic-input-to-character conversion system for Chinese. It is found that the Chinese phonetic information and the related conversion algorithm are much help to detect and revise the input errors in Chinese document. As to the editing errors, a special code structure of Chinese pronunciation which has only one bit difference among similar pronunciations is proposed In addition, the bits and characters mask technology is also proposed respectively. The experimental result of the proposed system show that the average success revision rate of the proposed system is close to 87%.
Information and Computation (PACLIC12) Japanese word processor cr the computer used in Japan orrinanly employ the Japanese input method through kLyboard strobe combined with Kana (phonetic) to Kanji (ideographic, Chinese) character conversion tedixiogy, because no exttatechi plogy such as the flee hand charader recognition or the speech recogrition is required. The Kana4o-Kanji conversion is perfonned by the morphological analysis an the input Kana string with no space between words. Wad-or phrase-segmentation is carried ort by the analysis to decide the substing of fir input to be converted fromKana to Kanji. Kara-Kanji mixed string, which is the ordinary form ofJapanese written text is obtained as the final result. The major issue ofthis techrology lies in raising the a:curacy ofthe segmentation and the horn:phone prooesing to select the most poperKanji among many hornopluic candidates.The conventional methodology for Foca:sing the homophare has used the function to give de first ptiotity to the wad which was used lastly or to the vvord wlich is used most frequently. In fict, this method is effective in some situations, but axneg imes teals to output the inadequate conversio ' n result due to the lack ofconsideration on the semantic consistency ofthe cotrumence ofwards. While it is difficuk to employ the syntactic or semantic processing in earnest for the wad processor flan the cost vs. perfamance viewpoints, the following trials to imptove the conversion acanacy have been reported: Employing the case frame to check the smartie consistency of combination ofwords [Oshima, Y. et a1,1986], Examining the consistency ofthe concumznce of adjacent wads [Hon m, S. et a1,1986], Employing the neural net ovo&amp; to descrix the consistency * Idle concurrence ofwords [Kobayashi,T. et a1,19921 Making a comurrence dictionary for the specific topic or field, ani giving the priority to the weld which is in tie dictionaty in case the keyvvrid appropriate to the topic is detected in the input [Yamamoto, K. et al, 1992], Emptying the validity of the ancunence of a noun and a verb vvlidi is calculated statistically [Takahasli, M et al, 19961 In any ofthle studies, where the main concern is to examine tbe consistency ofword commence in tie input there are many pv3blems left unsoived Besides these semartic or quasi-semantic *gets, there seems to be the room forte involvement by using word level nesources, min*, by the extensive use ofthe collocation, reasrent combination ofwards wlich co-ocair mote oftim than epected by than:v.. How many collocations should be collected and how muchthey contrioutetothe acancy ofKana-to-Kanji conversion have not been teported yet.In this paper, Ae present same resuks ofcur experiments afKanato-Kanji conversion, foaling on the ugge ofthe latge scale collocation data In chapter 2, descriptions cite collocations used in our system and their classification ate given. In dyer 3, the technological &amp;mem&amp; of our Kana.to-ICanji conversion systems is outlined In chapter 4, the method and the resuks cf The experiments are given along with some disassions. In chapter 5, cotriuring remarks are given Japanese wad prucessa. cr the cvmputer rated in Japaz employs, input method through keyboard vole canbinxIwith Kay Ohmetic) character b Kaiji (ickogrcphi4 Chime) cirraier aynersiattedsvlogy. .71r key fret►. of Karkto-Kanji co► tersion technology is how to rase the wary cfthe cantersicn hough the hamo-phae pvcwsirg we hate so many homcplvnes kits pcpet. , we sprat the mass cf our Karr-taKayi canersicn experiments which embo* dr homcialme processing using catnsite colloartion daft It is shown that ciprzimately 135,000 °goo:0m dai2yields 9.1 %rnise cfie amtersicn axunory ccmparedwith the protoope .Dstan which ha rro collocatiatcbta
Information and Computation (PACLIC12) One of the problems of rule-based translation using only syntactic grammar rules is the difficulty to process the fixed expressions which frequently occur in the context [Bond et. al., 1995] [ Katoh &amp; Aizawa, 1995]  [Lauer &amp; Dras, 1994]  [Schenk, 1986]. "Mary keeps up with her brilliant classmates." and "I prevent him from going there." are -the simple examples for the problem. Word-to-word translation would not process or naturally translate the expressions which cause a lot of complex and ambiguous parse tree structures [Yoon, 1994]  [Yosiyuki et. al., 1994].There are many studies to resolve the above problem [Bond et. al., 1995] [ Lauer &amp; Dras, 1994]  [Li, 1995]  [Yosiyuki et. al., 1994]. A solution of for it considers a fixed pattern on the context as a translation unit. This method reduces the load of syntactic/morphological generation with pre-translated natural equivalents, the number of parse trees, and syntactic ambiguities by scaling down the search apace of syntactic analysis [Lee, 1994a]. A translation unit consists of a simple form with only fixed words and a complex one that includes variable word/phrase/clause in it. However, the studies have interest only on a part of the categories of the units, e.g. compound nouns or phrasal verbs. [Schenk, 1986] tries to get a solution for pattern recognition within a theoretical framework, but he does not consider the syntactic relation between the pattern and its neighbor. However, the relation is the crucial key for machine translation system to naturally translate a phrase/clause/sentence with pattern-based approach, especially in the case of the translation between SVO linguistic structure (e.g. English, German) and SOV one (e.g. Korean, Japanese). One of the global solutions for pattern-based approach is to find and apply all possible bilingual or multilingual fixed expressions as translation units. We define the compound unit (CU) as a combined concept including collocations, idiomatic expressions, and compound nouns [Jung et. al., 1997a] [Jung et. al., 1997b]. The combination of pattern-based approach using CU and rule-based translation makes our translator more tractable and adaptable for the open domains which have various sentence types in World Wide Web (WWW). This pattern-based module finds all compound units and their information between morphological and syntactic analyzer.However, CU recognizer has a problem that it can not use any syntactic information to check syntactic constraints in CUs, but only surface form or its conjugation matching. It causes the degradation of the recognition reliability (precision). We propose the combination CU recognizer with syntactic verifier using partial parsing mechanism. The verification is to increase the reliability of the recognizer by means of pruning wrongly recognized CUs. Partial parser operates on a cyclic trie and uses simple CFG rules for the fast recognition. The experimental results show that it increases the precision of CU recognition up to 99.69%. Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 303-309 2. COMPOUND UNIT RECOGNIZER AS A PATTERN-BASED APPROACH[ Figure 2.1] The system structure of CU recognizer and its syntactic verifier CU recognizer is a plug-in module located between morphological and syntactic analyzer. CU recognition reduces the search space of syntactic analysis and a portion of POS ambiguities.[ Figure 2.1] shows the system structure of CU recognizer and its syntactic verifier. The recognizer finds all the types of CUs in input sentence on its search index using co-occurrence constraint string/POS and syntactic constraint. The index is automatically made from text-based CU dictionary. . The beginning array is used for more rapid access to the constituent tie [Cho, 1992] [ Fredkin, et. al., 1960] [ Knuth, 1973] [ Lee, 1994b]. Each element of the beginning array corresponds to the first two characters in the first constituent of a CU. For example, "go" is the element for all of the CUs that begin "gain." Empirically, in the case of using the first two characters instead of one, the number of the traverse on the index "methods" is reduced to 20 -80%. The constituent trie is a modified trie structure with heterogeneous nodes: fixed (e.g. "gain", "a", "concession") and variable constituent (e.g. "#1 (NP)") nodes [Jung et. al., 1997c]. A part of variable constituents have one or more conditions that consist of syntactic tags (e.g. NP, VP, NP-clause) and co-occurrence constraints (e.g. oneself = {myself, himself, themselves, ...}, one's = {my, his, their, ... }).The element of the information array has the whole information for a CU (e.g. representative POS tag, equivalent(s), and CU key). The array is to get the modularization of the index structure. Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 303-309 The principle of CU search is "most-specific-expression-first" [Yoon, 1994]. The traverse order on the siblings of the trie always keeps this principle. We provide three priorities for the traverse to apply our heterogeneous trie structure [ Table 2.1]. The other search strategies on the structure are same as those of other ordinary trie structures. The syntactic verifier for the variable constituent that has one or more syntactic constraints overcomes the degradation of the precision for CU recognition by the absence of any partial syntactic analysis. For example, The recognizer would find a pseudo CU "take #1 (NP) to" in "But, it doesn't take much to get burned." by the absence. In the case of "Some researchers have charged that administrption is imposing new ideological tests for top scientific posts", it can accidentally find a correct CU "charge #1 (NP-clause) for", but it is an unreliable result without any syntactic verification. We use a partial parser as the implementation of the verifier to raise the reliability of CU recognition. Its operation sequence is as follows.(1) Load CFG rules.(2) Partially parse in the manner of top-down using each of syntactic constraints (tags) in a selected CU hypothesis and a given sequence of POS tags from input sentence. (3) Determine if the two are syntactically matched. In the case of matching failure, continue to match with the other syntactic constraints. (4) Return the matching result to CU recognizer in order to accept the hypothesis.The verifier is also able to partially parse the embedded syntactic structures, e.g. "... that Cray Computer anticipates needing perhaps another $120 million in financing beginning next September", with right recursion from the RHS of each grammar rule. We have a grammar index structure with cyclic trie for the recursion [ Figure 3.1]. There are currently 3 LHS nodes and 38 RHS nodes for 31 CFG rules. We do not have any constraint except the sequence of the rule description because the partial parser is mainly focused on syntactically verifying with a fast and simple way.[ Figure 3.1] The grammar index structure for syntactic verification [ Figure 4.1] The interface between pattern-based approach and rule-based translationLanguage, Information and Computation (PACLIC12), 18-20 Feb, 1998, 303-309 The verifier uses a look-ahead mechanism to match a node on the index with the current input POS tag while avoiding backtracking. The following pseudo codes are the main frame of the mechanism. CU recognition makes a problem of interfacing between pattern-based approach and rule-based translation. Ordinarily, rule-based parsers have a direct input from morphological analyzer without any combination with pattern-based modules. However, CU recognition as a pattern-based approach changes the whole or a part of POS sequence from input sentence by merging and replacing the POSs. To overcome this interface mismatch, We give additional features such as a representative POS, verb/adjective/noun types, a CU key, and a generation code to CU information. There are two cases that the representative POS of a CU is noun. First, simple compound noun such as "portfolio manager" does not need any additional features because the POS sequence of input sentence is not changed. Second, if the CU is a derived compound noun such as "ceiling (NP) on" or phrasal verb such as "talk about", it should have a feature to represent a relation between the verb/noun and its object(s). The feature is defined as the sub-categorization information of CU. There are two additional features; a generation code for the interface with generation module and a CU key which represents the representative meaning of its CU to extract the most similar features from existing equivalent dictionary during EnglishKorean transfer without any additional word information.In the case of variable constituents, although its syntactic verification is operated during CU recognition, whole parsing and transfer/generation is necessary to produce its equivalent. For the case, there are two operations; first, top-down parsing of syntactic analysis to get an equivalent of the constituent and second, insertion into the equivalent of its parent CU. This paper describes the combination compound unit (CU) recognizer with syntactic verifier using partial parsing mechanism. The recognizer finds all the CUs, combined concept including collocations, idioms, and compound nouns, in input sentence. CU information reduces the search space of syntactic analysis and a portion of Part-Of-Speech (POS) ambiguities. Syntactic verification is to obtain precise CU recognition results by means of pruning wrongly recognized units that are caused by improper variable hypotheses. The experimental results show the precision of CU recognition is increased to 99.69% with 31 CFG rules on cyclic trie structure for 1,268 WSJ articles in the Penn Treebank. They also show CU recognition increases the understandability of translation for Web documents.
Information and Computation (PACLIC12) One vital theme in natural language processing is how to produce a good machine-readable dictionary. For this, how we collect, sort and allocate information to the dictionary headwords is critical, and this is what I shall be discussing in this paper. Selecting headwords is an important problem in developing a dictionary. We have analyzed several machine-readable dictionaries and large volumes of corpora in our aim of developing an extensive machine-readable dictionary with one million headwords. This paper shows analysis and comparison of results and discusses how we collect, sort and allocate information to the dictionary headwords.
Information and Computation (PACLIC12) In modeling human sentence processing performance, many models appeal to limitation in the memory architecture of the Human Sentence Processor (HSP) to explain the HSP's performance in processing nongarden path and garden path sentences (Frazier and Fodor, 1978;Church, 1982;Gorrell, 1987;Gibson, 1991;Jurafsky, 1992). However, upon closer examination, the memory architectures proposed by many of the parallel models (e.g. Gibson, 1991;Jurafsky, 1992) do not inherently explain nor motivate (separate from their parsing algorithms or evaluation measures) the differences between local syntactic ambiguities which may cause the HSP to garden path, and those that never do (Lee, 1996). In addition, the HSP seems to require linear time for processing non-garden path sentences, and non-linear time for garden path sentences (MacDonald, 1992).1 Unrestricted parallel memory architectures do not inherently restrict a parser to these time properties.The dilemma faced when modeling the HSP is that the parser must at times be able to easily pursue multiple analyses in parallel, but at other times be unable to do so or to have difficulty in doing so. In order to model the HSP's ability to pursue multiple analyses, some models have very powerful memory architectures. Rather than have the memory architecture limit the parser' s ability to pursue multiple analyses in parallel when garden pathing is required, they use the parsing algorithm and an evaluation measure (Gorell, 1987;Gibson, 1991;Jurafsky, 1992). However, the memory architecture of these parsers alone does not explain why the parser should have to choose between multiple analyses at all, especially when there are only two possible analyses. That is, at times, the parser is able to easily maintain two alternate structures in memory (in order to avoid spurious garden pathing), and so it is unclear from the memory architecture alone why at other times (when garden pathing is required) the parser is suddenly unable to do so. In addition, when the memory architecture is considered alone, the inherent time and space properties of a parser with these types of powerful memory architectures are non-linear on even non-garden path sentences.So a desirable memory architecture should inherently allow the parser to easily pursue only the kind of structural ambiguities that the HSP finds easy to pursue, and should inherently constrain the parser to have linear time (and space) properties on non-garden path sentences. At the same time, it should also inherently explain why the parser should have to choose between two analyses at all (i.e. why limited memory should cause the parser to garden path at all). A parser which uses 3D-trees, a very restricted parallel memory structure, and limits the number and accessibility of 3D-trees by its memory architecture, can adequately model the psycholinguistic data that originally motivated these parallel models, and has the desirable properties of linear * Department of Foreign Languages and Literatures, National Chiao Tung University, 1001 Ta Hsueh Road, Hsinchu, Taiwan R.O.C. 30050, Email: clee@cc.nctu.edu.tw 1 It is unclear what the space requirements might be for non-garden path and garden path sentences, though it seems that if memory is limited, then linear requirements would seem very reasonable, at least for non-garden path sentences, and maybe even for the processing of garden path sentences. A 3D-Tree is an efficient representation of a set of regular syntactic trees (2D-trees) that have the same tree skeleton but which may have different labels for the nodes. A node of a 3D-tree may have one or more sets of children as well as one or more parent nodes as long as the 2D-trees represented have the same tree skeleton. A 3D-tree is therefore able to efficiently represent the type of ambiguous structures which never seem to cause the Human Sentence Processor (HSP) to garden path. The advantages of 3D-trees over other parallel structures and the computational properties of building, manipulating, and maintaining, 3D-trees are presented in this paper. In addition, when the number and accessibility of 3D-trees are limited by the memory architecture of a parser in order to model certain psycholinguistic phenomena, the parser has the desirable computational properties of linear space requirements for parsing any sentence, and linear time requirements for parsing non-garden path sentences.
Information and Computation (PACLIC12)  This paper presents the &apos;techniques of correcting for spelling errors, orthographical errors, and grammatical errors in computer-based text. And this paper addresses an extension that goes beyond normal checking of isolated single word by taking multi-words as well as a sentence. The candidate words for spelling errors are created by applying function of rules and correction rule table that contains heuristic information of collocation. To prevent excessive creation of candidate words and improve accuracy, we use the high frequency word dictionary that contains 300,000 words derived from corpus. For constituent errors, by applying grammar based partial parsing rules, collocation words errors between the words can be found. We make an experiment with correction techniques on corpora that are the final result of SERI&apos;s research, texts, newspaper materials, and public materials. The system has 98% accuracy rate when the 8.5% errors caused by unregistered words were excluded. The average number of prospective candidates suggested by the system is 1.12.
Information and Computation (PACLIC12) LFG [2] has been regarded as a suitable linguistic formalism for transfer-based MT systems. Traditional LFG framework is syntax-based which, as illustrated in Figure 1, represents the syntax structure of a sentence in a C-structure F-structure Figure 1: The c-and f-structures for the sentence "John told a story."hierarchical, tree-like manner (i.e. c-structure) and the higher syntactic and functional information in a relatively order-free functional structure (f-structure). F-structures display linguistic information as relatively order-free attribute-value bundles. This allows linguistic information to be retrieved from or inserted into an f-structure easily for aiding lexical selection during the source-to-target language transfer'.Although f-structure provides a suitable medium for transfer, the linguistic information captured in it is syntaxbased. Thus, on its own, it is incapable of providing adequate information for word sense disambiguation during the lexical selection. A higher level of linguistic information, which is more language-independent (e.g. semantic information), is required to disambiguate the source language words. However, as traditional f-structures deal with syntactic information only, in the early LFG formalism, there were no guidelines to govern the incorporation of any higher level linguistic information. This makes the use of the LFG formalism for MT less desirable.With the aim of improving the ability of the LFG formalism to act as a Universal Grammar for language comparison, recent research on LFG has moved to the extension of the existing structural representation of syntactic and functional information to include some level of semantic information. Recent work on LFG shown that argument structure (a-structure), which represents thematic information of sentences, is capable of capturing more language-independent linguistic information for generalising the similarities across languages [4,3,5]. Thematic information represented in a-structures can be incorporated in traditional f-structures according to the lexical mapping theory [5,6,12] for enriching the information expressive power of f-structures. This seems to provide a solution for improving the ability of the LFG formalism for MT. The rest of this paper shows how a-structure improves the lexical selection process and how it can solve the problem in transferring some English passive sentences to Chinese. Lexical Functional Grammar (LFG) has been quite widely used as the linguistic backbone for recent Machine Translation (MT) systems. The relative order-free functional structure (f-structure) in LFG is believed to provide a suitable medium for performing source-to-target language transfer in a transfer-based MT system. However, the linguistic information captured by traditional f-structure is syntax-based, which makes it relatively language-dependent and thus inadequate to handle the mapping between different languages. Problems are found in the lexical selection and in the transfer from some English passive sentences to Chinese. The recent development of the relatively language-independent argument structure (a-structure) and the lexical mapping theory in the LFG formalism seems to provide a solution to these problems. This paper shows how this can be done and evaluates the effectiveness of the use of a-structures for MT
Information and Computation (PACLIC12) In parsing Chinese, the practice of deriving the semantic structure of a sentence from word-class information faces two main challenges: (i) it is difficult to determine word classes due to the lack of inflections in Chinese;(ii) there is a lot of ambiguities in the mapping of word classes to the grammatical roles and to the semantic structure. Table 1 summarizes the types of dependency relation that could exist in word bigrams composed from nouns, verbs, and adjectives. traditional approach on the task of identifying the attributive and the unmarked coordinative relations in word bigrams made up from nouns, verbs, and adjectives. This paper describes a semantic classification of Chinese and compares its performance with CKIP&apos;s word-class classification on the task of identifying the attributive dependency relation and the unmarked coordinative dependency relation of word bigrams made up from nouns, verbs, and adjectives.
  
The Underlying Representation of the Tough Construction in English In the past thirty. years have been published a substantial number of discussions on the tough construction in English. The tough construction is characterized by the semantic relation which is observed between the subject and a missing non-subject element inside the infinitival clause found therein. Within traditional transformational grammar (TG), the tough construction was derived via a transformational rule such as Tough Movement which was formulated to relate, for example, the underlying structure It is easy for Mary to read this book to the tough construction This book is easy for Mary to read. Moreover, as a trigger of such transformation, not only a class of adjectives (tough adjectives, such as easy in the above examples) but also a small number of nouns (tough nouns) have been identified. Tough nouns include problem, breeze, bitch, etc. Thus, the tough sentence This problem was a breeze to solve, for example, was transformationally derived from the underlying structure It was a breeze to solve this problem'.Yamamoto' presents an analysis of the tough construction in English within the framework of Lexical-Functional Grammar (LFG) 4i. This analysis posits the lexical form of the copular verb be in (1) and that of a tough adjective (difficult) in (2) . So the f-structure of the tough sentence This book is difficult for children to read in this analysis is illustrated as in (3) . Note that Yamamoto's lexical forms in (1) and (2) differ from the ones assumed in Kaplan and Bresnan's 6 description of the tough construction, where the SUBJs are analyzed as non-thematic. While referring to Kaneko's' convincing arguments for the thematicity of the tough subject, Yamamoto takes the position that both be and a tough adjective within the tough sentence subcategorize for a thematic SUBJ.   Information and Computation (PACLiC12), 18-20 Feb, 1998, 357 Language, • The two curved lines and Q in (3) represent two instances of functional control' . Line (:), shows the relation between the SUBJ (ECT) of the tough sentence this book and the SUBJ of the open complement (XCOMP) subcategorized by the copula be. This relation is ensured by the functional equation on the second line of (1) . Line ® represents a long-distance functional control relation 9 holding between the SUBJ of the XCOMP and the OBJ (ECT) of the closed complement (COMP) which is subcategorized by the tough adjective. Yamamoto captures this control relation in terms of the control equation shown on the third line of (2) . Note that the above two control relations inside the f-structure in account for the relation between the subject and a non-subject element inside the infinitival clause within a tough sentence; researchers in TG have attempted to explain this in terms of a transformational rule.The f-stmcture in (3) contains another control relation which holds between children and the missing subject of the infinitival complement to read. Based on the optionality of the designated controller children within the sentence, Yamamoto identifies the PP for children to be an ADJ (UNCT) . This control relation cannot be an instance of functional control in LFG, since' the controller of functional control is defined as one of the three semantically unrestricted syntactic functions: SUBJ, OBJ, or 0BJ2 8. Hence, Yamamoto concludes that the control relation mentioned above is an instance of anaphoric control'. This control relation is ensured in Yamamoto's analysis by means of the constraining equation shown on the second line of (2) . This equation constrains the value of the SUBJ within the COMP, subcategorized by the tough adjective, to be a morphologically unexpressed pronominal ('PRO') . This pronominal anaphorically refers to children in (3) , and thus the semantic relation between children and the missing subject of the infinitival clause to read is explained There seems to be two difficulties in Yamamoto's approach to the tough construction in English.Firstly, Rappaport'° claims that nouns • do not subcategorize for direct syntactic functions SUBJ, OBJ, and OBJ2. It follows then that the lexical forms of tough nouns, such as breeze in This problem is a breeze to solve are without specification of SUBJ. Moreover, the functional coherence principle, which is one of the well-formedness conditions on f-structures in LFG, states that all the grammatical functions within an f-structure must be governed by a local predicate'. Now suppose we extend Yamamoto's analysis of the tough construction to the kind of tough sentences which include tough nouns. In this case, a tough noun should provide information as the PRED of the XCOMP subcategorized by the copula be. However, the lack of SUBJ within the lexical form of a tough noun renders the f-structure of the XCOMP ill-formed, for the SUBJ of the XCOMP cannot be locally governed. Hence, it is impossible to maintain Yamamoto's analysis of the tough construction in explaining tough sentences which include tough nouns.Secondly, Quirk et al. 11 classify be as a stative copula and state that the clause where be occurs as its verb codes a state. They also discuss that the copular verb be differs from ordinary verbs in that its semantic content is almost empty. Except for the information on tense, aspect and number, the semantic information copular sentences carry is mostly supplied by the complement which follows be. The complement of be can be an adjectival as in Mary is friendly, a nominal as in Judy is a friend of mine, or an adverbial as in The students are in the language lab. In Yamamoto's analysis of the tough construction, be is obviously the predicator of the tough subject both syntactically and semantically. Here Yamamoto faces a dilemma. That is, given that be does not have semantic content except for information on tense, aspect and number, this copula should not assign any thematic role to the tough subject by itself. However, it is assumed in Yamamoto's analysis that the tough subject, whose predicator is be, is thematic, thus participating in the semantics of be. At the same time, if the tough subject is thematic and characterizable with a thematic role, then be, which Yamamoto analyzes as its predicator, should have semantic content where the referent of the subject is a participant. However, this is not the case as Quirk et al. point out. In this paper, we adopt the theoretical framework of Lexical-Functional Grammar (LFG) (Bresnan 1982a, Dalrymple et al. 1995) and propose an analysis of the tough construction in English. Our analysis is first presented as one which accounts for two kinds of tough sentences in English, i.e. those with a tough adjective complementing the copula be, and those with a tough noun complementing the copula be. For this purpose, we briefly review Yamamoto (1996) and point out two problems. One concerns the explanation of instances of tough sentences involving tough nouns, and the other, the syntax and semantics of the copular verb be and its complements. Then we go on to present our analysis of the tough construction in English which overcomes difficulties in Yamamoto&apos;s analysis. Next, we discuss yet another kind of tough sentence, as noted in Suzuki (1976), where a PP appears as the complement of the copula be. We point out that there are shared properties underlying the surface variations in the three kinds of tough sentences mentioned above.
Information and Computation (PACLIC12)  This paper gives attention to the so-called Sequence of Tenses phenomenon in English and explores what sorts of factors influence tense choice. Our analysis proves that what plays the crucial role in tense choice is the reporting speaker&apos;s attitude to the message, and that he or she chooses the tense taking three simplified factors into consideration: the semantic relation between the reporting verb and the reported message, the syntactic relation between the reporting clause and the reported clause, and extra-sentential, pragmatic factors. It is shown that although the reporting speaker is the final authority in choosing tense, he or she has to resort to these factors and judge how much responsibility he or she has for the truth value of the message. 0. INTRODUCTION When the reporting verb is in the past, the reported clause usually takes a past tense. This has often been referred to as the Sequence of Tenses. The present tense is also possible, however, as an alternative and marked choice as in: (1) John said he was/is happy. The purpose of this paper is to explore what sorts of factors influence tense choice with special reference to the past reporting. We will propose that what is crucial to tense choice is the (reporting) speaker&apos;s attitude, and we will analyze the phenomenon by way of three factors: the semantic relation between the reporting verb and the reported message, the syntactic relation between the reporting clause and the reported clause, and extra-sentential, pragmatic factors. We will show that the speaker dynamically employs these factors to choose a proper tense and that these simplified factors can be unified into one simple principle. Our research will cover reported thought and perception as well as reported speech, which will make our theory more comprehensive. First we will examine the inadequacies of the previous studies and seek for the implications they can have on the present study. The second section will clarify the reporting and argue for the significance of the reporting speaker. The factors relevant to tense choice will be discussed in sections 3 and 4: the former deals with two fundamental factors, i.e. semantics and syntax, and the latter analyses pragmatic factors in great detail. The final section is a conclusion.
Org anizin g Organizing Members  
  In the early days of engineering models of language, a popular intuition was that the easiest way to improve performance was to remove linguists. This was not an unreasonable, or even necessarily wrong intuition, and reflected the fixation by linguists of the day on linguistic structures to the exclusion of consideration of statistical dependencies between lexical or phrasal units or the non-categorical nature of real language use. But while the linguistics-blind approach showed promising initial beginnings, and the sheer brute-force nature of the approach led to the development and popularization of important statistical methods for dealing with sparse data, nevertheless, looking to the future, I&apos;ll argue that now the right way to improve engineering models is through the incorporation of more linguistics. But what kind of linguistics? I want to sketch how formal approaches to linguistics should reshape themselves from this brush with engineering: in a quantitative direction which is actually rather more compatible with much of the &apos;fuzzier&apos; linguistic work in such topics as grammaticalization and sociolinguistics, though realized with more mathematical precision.
EXTENSIBILITY IN JAPANESE NOUN MODIFICATION  This paper discusses the relationship between the modifying clause and the modified noun in Japanese. In some cases the closeness between them is weaker than in the prototypical case, whereas in others the modification is actually impossible. Within the framework of Matsumoto [I], who applied Frame Semantics to Japanese clausal noun modification, we analyze what conditions make the modification possible. We propose that the degree of linkage between modifier and modified extends from impossibly distant to acceptably close, where the Relational Frame is evoked. This type of phenomenon can be found in various dimensions: space, time, and the cause-and-effect relation. The degree of extensibility is determined by the world-view of the speaker, the tense/aspect choice in the clause, the aspectual character of the noun, and so on.
On the Meaning of Shenme &apos;what&apos; in Chinese Bare Conditionals and its Implications for Carlson&apos;s Semantics of Bare Plurals In this paper, I am going to examine the differences between shei 'who' an shenme 'what' in what Cheng and Huang (1996) refer to as "bare conditionals". In such constructions, the conditional and consequent clauses each contain a wh-phrase of the same form and the choice of value for the second wh-phrase varies with the choice of value for the first wh-phrase. This construction is illustrated in (1).(1) Shei xian lai, shei xian chi who first come who first eat`If eat`If x comes first, x eats first./Whoever comes first eats first.'I will show that the semantics of shenme 'what' and shei 'who' in such constructions differ with respect to how they refer. When the two wh-phrases in pair in bare conditionals involve sheìsheìwho', they must refer to the same person as in (1); but when the wh-phrases in pair involve shenme 'what, they may refer to the same object as in (2) or they may refer to a different object but of the same kind as in (3).(2) Wo zheli de dongxi, ni yao shenme, jiu na shenme I here Gen thing you want what then take what`As what`As for my things here, if you want x, then you can take x.' (3) a. Ni dapo shenme, jiu de  (2) does not need comments; (3) is worth some more remarks. Imagine a situation where a person breaks a bowl and is asked to compensate for it. In this case, if (3a) is to be true, the person who broke the bowl has to buy a new bowl which is different from the original one for compensation. The meaning of shenme 'what' in (3a) can be compared with the meaning of it in (4).(4) If you break a bowl, then you have to buy it for compensation.Unlike (3a), for (4) to be true, what you have to buy for compensation is the original bowl, which is broken. The contrast between (3a) and (4) is similar to the contrast between (5) and (4), that is, between it and one.(5) If you break a glass, then you have to buy (another) one for compensation.Similar remarks apply to (3b). (3b) can be true in a situation where someone buys a new computer and his younger brother wants to have a computer too. In this situation, the two computers need not be the same one though they could be of the same brand. It thus seems that the shenmeanaphora seen in (3) is more like one-anaphora than it-anaphora.In fact, a large number of bare conditionals involving shenme 'what' are ambiguous between the object-identity reading and the non-object-identity reading. One such example is (6).(6) Ni xiang chi shenme, mama jiu zhu shenme gei ni chi you want eat what mother then cook what for you eat a. 'If you like to eat x, then mother cooks x for you to eat.' b. `If you like to eat something of kind x, then mother will cook something of that kind for you to eat.'The purpose of this paper is to discuss what makes the interpretation of the two wh-phrases different and how this difference can be accounted for. I will show that shei 'who' and shenmèshenmèwhat' actually have different kinds of denotations. While shei 'who' denotes the same type of things as proper names and definite descriptions, shenme 'what' may also denote the same type of things as bare NPs, in additional to proper names and definite descriptions. On the basis of this, I will suggest that in addition to 'thing x', shenme 'what' should also be optionally analyzed as`kind as`kind x', parallel to the analysis of bare nouns. This, together with Carlson's (1977) semantics of bare plurals and verb meanings, will account for why shenme-anaphora behaves like English oneanaphora. Apart from the above issue, I will also show that unlike Carlson's approach to bare plurals, Wilkinson's (1996) approach cannot be carried over to Chinese bare conditionals. Finally, I will examine Heim's (1987) proposal of treating what as a disguised expression of 'something of kind x', showing that it cannot be extended to Chinese bare conditionals, either.Consider the contrast between (7) and (8).(7) Q: Ni xihuan shei you like whòwhòWho do you like?' A: Wo xihuan Zhangsan/nei-ge nithai I like Zhangsan/that-Cl girìgirìI like Zhangsan/that girl.' A': *Wo xihuan niihai/jinfa-nillang I like girl/golden-hair-girìgirìI like girls/blondes. (8) Q: Ni xihuan shenme you like what`What what`What do you like?' A: Wo xihuan zhe-ben shu I like this-C1 book`I book`I like this book.' A': Wo xihuan shu I like book`I book`I like books.'The examples in (7) and (8) show that to answer a shei-question, either a proper name or a definite description is an appropriate answer, but a bare NP is not. On the other hand, to answer a shenme-question, one can respond with either a definite description or a bare NP. Shei 'who' and shenme 'what' thus allow different ranges of possible answers. I take this fact to be an indication that the two wh-phrases permit different types of denotations. Shei 'who' may denote something of the same type as proper names or definite descriptions, whereas shenme 'what' may denote something of the same type as proper names, definite descriptions or bare NPs. It should be noticed here that Chinese does not have a plural marker. Therefore bare plurals take the form of bare nouns. I will show that the above distinction between shei 'who' and shenme 'what', coupled with Carlson's semantics of bare plurals to be discussed in the next section, will account for the non-object-identity reading of shenme 'what' in bare-conditionals. This paper shows that the semantics of shenme &apos;what&apos; exhibits a double quantification phenomenon in Chinese bare conditionals. I show that such double quantification can be nicely accounted for if one adopts Carlosn&apos;s semantics of bare plurals and verb meanings as well as the assumption that shenme &apos;what&apos; may denote kinds of things as bare plurals do.
LEXICAL INFORMATION AND BEYOND: CONSTRUCTIONAL INFERENCES IN SEMANTIC REPRESENTATION Knowledge representation of the verbal system has always been one of the core issues in both theoretical and computational linguistics. In the early generative paradigm, verbs are considered to be the structural head of the sentence since it subcategorizes its arguments and hence dictates the form of the sentence. Verb meanings are treated as general tendencies in selectional preferences, and the semantic details of individual verbs are largely neglected. Recent development in lexical research has shifted the focus to investigating the interdependency between verb meanings and syntactic behavior. Most lexical semanticists share a common assumption that the syntactic behavior of a verb, especially its argument expression, is determined by the meaning of the verb (cf. Levin, Song and Atkins 1996, Pustejovsky 1995, Levin 1993, Atkins and Levin 1991, Atkins et al. 1988. Levin (1993) tries to categorize English verbs into semantically distinct classes on the basis of their argument alternation patterns. Pustejovsky (1995)'s program on generative lexicon moves one step further in spelling out a multi-layered representational scheme for lexical information that includes Argument Structure, Event Structure, Qualia Structure, and Inheritance Structure. His goal is to fully represent the interaction of word meaning and its composition.Another commonly shared belief among lexical semanticists is that the traditional 'sense enumerative lexicon', as Pustejovsky puts it, is inadequate to capture the real picture of natural language semantics. Specifically, such an approach cannot account for three lexically relevant phenomena: the creative use of words, the permeability of word senses, and the expression of multiple syntactic forms with a single word (Pustejovsky 1995: 39). It is argued that a more powerful compositional mechanism with predicative strength is needed to allow coercion of lexical meanings into more complicated or innovative senses.In view of the fact that lexical knowledge involves more than just the listing of different senses or simple combination of word meanings, we would like to present in this paper an interesting case in Mandarin where a single verb gives rise to various contextualized meanings when combined with a complement NP. The issue under study is the transitive use of GAN () `rush' in the pattern of [GAN + NP], such as GAN gongche 'the bus' or GAN baogao 'a paper'. The appropriate interpretation of the pattern requires incorporation of semantic components beyond the purely lexically-specified information. It is proposed that to fully represent the semantics of the pattern, constructional inferences should be included in the knowledge representation of lexical information. This paper aims to show that semantic representation of verbs may require the inclusion of constructional inferences in addition to lexical specifications. By examining the transitive pattern of the Mandarin verb GAN (,y) &apos;rush&apos; , we found that verbal semantics can only be adequately represented if constructionally coerced information is taken into consideration. The construction [GAN + NP] renders specific interpretations that are not directly derived from the lexical meaning of either the verb or the object NP. The construction itself carries salient information for the appropriate interpretation. Besides outlining a compositional approach with Qualia Structure (Pustejovsky 1995) , this paper attempts to account for the construction-triggered meanings from the perspective of Construction Grammar and to explain their interrelationship with cognitive mechanisms for sense extension.
Alternation Across Semantic Fields: A Study of Mandarin Verbs of Emotion Many recent linguistic studies explored how lexical meaning predicts syntactic regularities (Levin 1993, Pustejovsky 1995. One important approach is to study the contrasts in near synonym pairs to identify the minimal semantic attributes that motivate the contrasts (Tsai et al 1998, Liu et al 1997Liu et al &amp; 1998). In this current study, we extend the range of the study to a semantic field, which contains more than one synonym pairs. Thus we can attest to the primary status of the proposed semantic attributes by showing that the generalization can be extended to the other synonym pairs in the same semantic field. Tsai et al (1998) discussed the contrast between the synonym pair KUAILE and GAOXING AR, and based on their findings we re-examine the contrast in a broader range, i.e. the verbs of emotion. We have four results from this study: 1) we find that the contrast is not specific to KUAILE and GAOXING, but to the whole semantic field of verbs of emotion;2) we define the contrast more precisely; 3) we can trace the cause of the contrast; and 4) we can identify the influence of the compound structure.In this paper we will examine seven types of emotion verbs, i.e. happy, depressed, sad, regret, angry, afraid and worried. All the observations and statistics in this paper are based million words (CKIP 1995). We consider only the verbs with a frequency of over 40 in Sinica Corpus. The verbs under examination in the paper are listed in Table 1 In this paper we will explore the consistent contrast between VV-compounds and non-W-compounds across seven subgroups of verbs of emotion. The distinctive syntactic features for the contrast include the distribution of the grammatical functions, the cooccrrence restrictions with head nouns and head verbs, the compatibilities with the imperative and evaluative constructions, the aspect, and the transitivity. We conclude that the contrast is motivated by event structure properties. To describe a state-type event, the speaker could choose to focus on the inchoative stage or the homogeneous stage of the event. In addition, since VV compounding has the function of type-lifting an event to a referential term, or to refer to its generic properties, it is natural to predict that VV compounding is a predominant source for the verbs of indicating homogeneity.
ENGLISH-STYLE AND CHINESE-STYLE TOPIC: A UNIFORM SEMANTIC ANALYSIS* Topic-comment structures (TCS) have attracted much research in a wide range of languages. By studying Chinese data, the difference between the topic "English-style", which is co-referential with some (overt or covert) element in the comment, and the topic "Chinese-style", which is related to the comment by a relation of aboutness, is quite striking.' Both types of topic are exemplified in (1) and (2)  that tree leaves big (That tree, the leaves are big.)In (1) there is an empty trace, interpreted as a zero anaphor that is bound by the topic. By contrast, there is no such syntactic position inside the comment associated with the topic in (2). Instead, topic and comment are related with each other pragmatically. Therefore, the topic in (2) is considered by some scholars to be "base-generated" in front of the sentence, and to have an "aboutness"-relation to the remaining sentence (e.g. [5]). However, a definition of "aboutness" is still not given. A pure syntactic definition of topic as an extraposition fails in the face of examples such as (2) while the relational approach advocated in [10] leaves open how to characterize the relation between topic and comment in (2). Furthermore, the semantics of the Chinese-style topic construction remains unexplored.In this paper, we propose a uniform semantic framework for interpreting the TCS in Chinese. We assume that the topic constituents bind anaphors inside the comments, such as pro-forms and implicit anaphora. Because the comments contain unresolved anaphors, they are semantically incomplete or underspecified. The semantic interpretation of TCS is given by anaphora resolution. In the absence of a direct anaphoric relation as in (2), a bridging inference is triggered. The proposed interpretation will be modelled in a discourse semantic framework by extending Discourse Representation Theory (DRT) with topic-comment articulations. We will show that this approach can also capture multiple-topic constructions and topic chains.The paper is structured as follows. Section 2 gives a brief review of related approaches. It shows that to capture the topic-comment relations we have to take a closer look at the semantic properties of the TCS. Section 3 discusses how bridging works to account for the semantical relations in the TCS. Section 4 presents a DRT-based framework of topic-comment structure, which is applied to some examples of linking and bridging as well as to some dangling topic constructions involving ellipsis in section 5. We extend our framework to topic chains in section 6 and to multiple-topic constructions in section 7. Section 8 concludes with pointers for future research. This paper provides a uniform semantic framework for Topic-Comment Structures in Chinese. It is assumed that the topic element relates to an anaphor occuring in the comment. The comment which contains an unresolved anaphor is semantically underspecified. The semantic interpretation of topic-comment constructions results from the resolution of the anaphoric relation between topic and comment. In the case where there is no explicit anaphor in the comment, a bridging inference takes place. The proposed analysis is modelled in a DRT-based framework. The introduction of a DRS segmented for topic-comment articulation, called TC-DRS, serves to compute the topic-comment relation within the discourse semantic theory DRT compositionally.
MATAPHORICAL EXTENSION AND LEXICAL MEANING The term "metaphor" has been used with a variety of senses, which accounts for many of the controversies and misunderstandings surrounding this term. For example, "metaphor" is employed on the one hand as a genetic term for any figure of speech which includes figures such as metonymy, synecdoche, hyperbole, etc. On the other hand, there are more narrow definitions, according to which metaphor contrasts with alternative figures such as metonymy.One of the most influential linguistic treatments of metaphor and metonymy as distinct, mutually exclusive types of expression is that of Jakobson and Halle [2], who describe the dichotomy between these two kinds of tropes as reflecting a "bipolar structure of language" that appears to be of "primal significance and consequence for all verbal behavior and for human behavior in general". They claim that, according to the metaphorical way, one topic leads to another through their similarity, whereas according to the metonymic way, discourse is developed along the lines of topic contiguity.The focus of this paper will be on a different perspective of the relation between metaphor and metonymy. We maintain that the more common paradigm to be observed in human language appears to be one where the two are not mutually exclusive but rather complement one another. As observed in Goossens [3], two main patterns are said to be associated with the ways in which metaphor and metonymy interact: 1) Metonymy functions within a metaphor; and 2) Metaphor and metonymy coexist in some uses of a figurative expression. We agree with Goossens in that although metonymy and metaphor are "clearly distinct in principle, they are not always separable in practice." This is especially true with the conceptualization of grammatical structures.We will adhere to Goossens' view and apply it to the processes known as grammaticalization, which is believed to be metaphorically structured [4]. Grammaticalization in our study is considered as a subtype of metaphor, which can be defined as "a metaphorical shift toward the abstract" [5]. We also accept Bybee and Pagliuca's idea [6] that grammatical meaning develops from lexical meaning by a process of "generalization or weakening of semantic content," which naturally leads to the claim, made by Claudi &amp; Heine [4], that metaphorical extension is one important mechanism in the grammaticalization process:A concrete lexical item is recruited to express a more abstract concept ... this emptying of lexical content is a prerequisite to grammaticalization because grammatical functions in themselves are necessarily abstract.The purpose of this paper is therefore to ascertain into the nature of metaphor extension and the creation of lexical meaning by examining closely two Chinese lexical items: 4 zai "to be (here)" and shang "up". Specifically, we will address the following questions:1. What is the driving force for polysemy in Chinese lexicon? That is, are the meaning chains of polysemic words motivated by metonymies and metaphors in Chinese? And if so, how? 2. What metaphorical or metonymic device can, if any, be said to be responsible for the conversion between different parts of speech (noun to verb, noun to adjective, etc.) in Chinese? 3. Is metaphorical transfer the driving force for prototypical categories? Is grammaticalization more suggestive of a metonymic structure regarding its continuum nature? Metonymy and metaphor reflect an important part of the way people ordinarily conceptualize of themselves, events, and everyday world [1]. We will argue for this position via the lexicalization process of two linguistic items in Chinese: shang and zai, and demonstrate that grammatical meaning develops from lexical meaning by a process of &quot;generalization or weakening of semantic content,&quot; which is in fact metaphorical in nature. The purpose of this paper is to ascertain into the nature of metaphorical extension (via metaphor and metonymy) and the creation of lexical meaning as they are seen in the two lexical items mentioned. Though data gathered from corpus, dictionaries and native speaker intuition, we wish to examine the relationship between conversion, metaphor and metonymy, and understand better 1) the driving force for polysemy in Chinese lexicon; 2) the different driving forces, concerning metaphor and metonymy, for prototypical categories and grammaticalization.
LEXICAL INFORMATION AND PRAGMATIC INFORMATION: REFLEXIVITY OF AN EVENT AND RESULTATIVE CONSTRUCTIONS IN JAPANESE  This paper examines the interaction of semantic factors of reflexivity in the availability of result type te-ir and tear constructions in Japanese. The semantic factors of reflexivity have been examined in a number of studies, and they are known to be of relevance to the te-ir construction of result interpretation as well as to the availability of the tear construction. It has not been made explicit, however, whether the reflexivity is a lexical property of a predicate or it is part of the pragmatic information or the information provided in the actual event in context. I will demonstrate that reflexivity relevant in the two constructions derives from two separate sources; the reflexivity as lexically-encoded information and the reflexivity as contextually-supported. The interaction of lexicon and pragmatics has been studied from various points of view. The definition of lexical information and that of pragmatic information varies across theories. The present study suggests one way of discriminating the two. 1. LEXICAL INFORMATION AND PRAGMATIC INFORMATION It has been of much concern these days how lexicon is to be structured. Lexicon was once thought of as a simple list of words with a limited set of information necessary for syntactic mapping. In the trend of lexlicalism which dates back to early 1980s, more emphasis came to be placed on lexicon. At the same time, syntax was simplified and generalized. The more simplified the syntax was, the more complicated the lexicon had to be, since the overall complexity of linguistic structures was never reduced. For instance, one of the essential parts of lexical information is argument structure. The argument structure used to be more closely tied to subcategorization, which is relevant to syntactic mapping. As argument structure was elaborated in a greater detail, it came to be more associated with event structure of verbs, examining more closely at the semantic side of information than the syntactic side. The event structure is closely tied to the way the event denoted by the verb canonically evolves in the real world. This means that lexical information necessarily draws on extra-linguistic information. It is well-known that the interpretation of a sentence is greatly affected by pragmatic, contextual factors of various kind. However, it is not true that pragmatic factors can do anything. Many sentences are never grammatical in whatever kind of context they are used. In fact, the extent to which pragmatics can override syntactic restrictions is very limited. Pragmatic effect is usually of more modest kind: it, for instance, disambiguates the referent of a pronoun, or it saves a sentence which is grammatically correct but is uninterpretable due to semantic anomaly. There are still cases where pragmatic factors interact more closely with syntactic restrictions. Then the question is how pragmatic information interacts with syntactic and lexical information of constituent words.1
THE DATIVE IN MODERN ICELANDIC: EXPLORATION OF THE SEMANTIC GENOTYPE OF THE DATIVE IN NATURAL LANGUAGES* Studies in many languages have revealed that nominals in the overt dative case are carriers of tremendously differing kinds of semantics [1][2][3][4][5][6][7][8][9]. Languages also vary according to the kinds of semantic information their dative nominals encode [8][9]. Why is the dative semantically so variable? What is the factor that determines the semantics underlying this overt case? We believe the answers to these questions can be found through exploration of the semantic genotype of the dative, i.e., the semantic origin of the dative case, a term borrowed from genetics. We also believe that the nature of the dative can be seen through investigation of the semantics of the dative in many individual languages. In section 2 of this paper, we assume the set of semantic roles and hypothesize that the dative in Modem Icelandic (henceforth Icelandic) cannot project the locative role within the argument structure of the verb. In section 3, we discuss the distribution of the possessive datives in Icelandic, i.e. the dative nouns which occur as possessors attributively modifying nouns. Yip et al. [10] mention the following two conditions on the occurrence of possessive datives in Icelandic: (i) Icelandic possessive datives occur as the possessors of the objects of prepositions within PPs, and (ii) they occur as possessors of the inalienable possessor-possessed relation. Section 3 of this paper proposes modification to the first condition above. Finally, in section 4, we claim that the modified condition follows from our hypothesis from section 2 that the sentential dative NPs in Icelandic do not encode the locative role. This claim is further supported in the same section, in reference to the data from Latin. This paper discusses distribution of the dative in Modern Icelandic. Similar to Latin, Modern Ice-landic exhibits occurrences of the possessive dative, i.e., the dative which marks the possessor of a noun. However, although Latin shows free occurrences of possessive datives, there is a syntactic restriction imposed on the distribution of possessive datives in Modern Icelandic. The possessive dative in Modem Icelandic is limited within a PP which denotes the static position of an entity participating the semantics of the sentence. This difference between Latin and Modern Icelandic follows from the fact that the dative in Latin inherently possesses the locative semantics, while that in Modem Icelandic does not.
Mapping Image-Schemes and Translating Metaphors Current understanding of metaphors has centered on identifying conceptual metaphors in a particular language, such as the following metaphors in English: TIME IS UNDERSTOOD AS) MONEY, LIFE IS A JOURNEY, GOOD IS UP, BAD IS DOWN, etc. [2]. These are conceptual metaphors, that is, they map one conceptual domain (i.e. money) onto another (i.e. time), as opposed to image metaphors, which map only one visual image onto another visual image. An example of a image metaphors is: 'Her waist is an hour-glass.' The visual image of the shape of an hour-glass is mapped onto the waist.Image metaphors are 'one-shot' deals, unlike conceptual metaphors which allow many concepts in the source domain to be mapped onto corresponding concepts in the target domain. These concepts are called image-schemas. For example, in the metaphor LIFE IS A JOURNEY we find the following metaphorical usages, as in (1). All these examples have to do with the image schema of speed. Speed in the source domain of JOURNEY relates to the speed at which the journey (usually in a car) takes place. This image-schema maps onto the speed at which LIFE takes place.Lakoff [1] proposes the Invariance Principle to guarantee that the mapping is consistent in the both the source and target domain. The Invariance Principle states:Metaphorical mappings preserve the cognitive topology (that is, the image-schema structure) of the source domain -, in a way that is consistent with the inherent structure of the target domain. (P. 215) Thus, given the Invariance Hypothesis, speed could not map onto the direction that one is going in the LIFE IS A JOURNEY metaphor. Direction is a different image-schema that exists in a JOUNEY and may or may not map onto LIFE in the LIFE IS A JOURNEY metaphor.In addition, image schemas that map cross-linguistically for the same conceptual metaphor may be a) central to the metaphor and b) a part of all human's conceptual system. Looking at the mapping problem from a cross-linguistic point of view also gives us the additional benefit of being able to formulate principles for translation of conceptual metaphors, which is a difficult problem in NLP.In what follows, we explain the image-schemas that are mapped in the metaphor in ANIMALS ARE HUMANS for English (Section 2) and Mandarin Chinese (Section 3). (In this paper, 'Chinese' or`Mandarin or`Mandarin Chinese' refers to the Mandarin Chinese spoken in Taiwan and English refers to American  English.) In Section 4, we lay out the principles for conceptual metaphor translation and give examples to support our principles. In Section 5, we summarize our findings and point to future areas of research. In this paper we demonstrate that identifying the mappings between the source and target domains for a conceptual metaphor allows for both a greater understanding of the conceptual basis of metaphors, and more effective language translation. We first introduce and explain the Animal Metaphor to support our idea. We show that the Animal Metaphor exists in both English and Chinese, but that it maps different information from the source to the target domain. We then propose three principled steps to aid in the translation of metaphors from one language to another, using the animal metaphor as an example. Lastly, we summarize our findings and discuss future areas of research.
UTTERANCE UNITS AND EXCHANGES IN SPONTANEOUS JAPANESE DIALOGUE It is often said that ordinary conversations are full of irregularities. Some say that there is nònògrammar' in every day spoken language. In theoretical linguistics, such irregularities in every day conversation has been considered due to various limitations on the performance language use. These phenomena have been regarded as rather peripheral and not a central concern of theoretical linguistics, at least that of a grammatical theory. Such an attitude, however, has made it rather difficult to apply the scientific results of theoretical linguistics to the language in actual use. Grammatical theories are particularly vulnerable to. the vulgarity of conversational language because many people speak in incomplete fragmentary sentences.But the fact still remains that people do understand everyday language and that they usually do not find their conversational language anyway more difficult to understand than its written counterpart; on the contrary, it is usually the case that people find the written language spoken to them harder to interpret than spontaneously spoken casual language. So there seems to be a sort of paradox; everyday conversational language seems to lack proper 'grammar' and regularity enjoyed by its written counterpart but is not particularly difficult to understand, while its written counterpart with a proper grammar and regular structure is usually harder, when spoken, to understand.We believe that the paradox is only superficial. The paradox ensues if one assumes that since conversational language does not always follow the rules of its written counterpart, it lacks rules and grammar. If one accepts that conversational language has its own grammar and rules, somewhat different from those of its written counterpart, then there will be no paradox involved. In other words, what we need is a theory of performance, a grammar of language in actual use.We will in this paper explore this aspect of spoken language looking specifically into what consititutes its basic unit.[2]: For example, the following would be a typical Japanese sentence in a Japanese grammar book (1) Sensei-ga hon-o kai-ta Teacher-SUBJ book-OBJ write-PAST`The PAST`The theacher wrote a book.'Since most of those sentences appearing in a grammar book are of this form, in which verbal forms come at the end, Japanese is typically considered a SOV language. On the other hand, such features as dropping case and conjunctive particles, frequent use of end particles and interjections, and abnormal ordering of words have long been considered signs ofàgrammaticism151. These facts seem to corroborate the view that in Japanese grammatical sentence units are well established and rather easy to recognize.When one observes Japanese in actual use, however, one can see that such a view is too simple, because a spontaneously spoken Japanese discourse is full of features which are totally beyond the grasp of grammar books: phrases ending with conjunctions, interjections with verbs attached, unfinished utterances and other disfluencies.These characteristics are often taken as the manifestation of the irregularity and non-systematic nature of spoken language. Although one may decide to ignore these as unimportant performance issues outside the domain of linguistics proper, if one wants sincerely to analyze language in its totality, one cannot avoid meeting these issues.Especially when one wants to recognize 'sentences' in discourse, these characteristics inevitably stand in one's way, destroying the sentential structures expected by grammar. Because of these, from the stand point of the conventional grammar, clarifying such matters as which utterances constitute one sentence and which utterances belong to two different sentence units becomes extremely difficult. If the task of discerning sentence units, the building blocks of discourse, is difficult, the difficulty of discerning discourse structure is enormous. In dialogue, speakers are engaged in various types of speech acts other than just those frequently encountered such as question, command, promise. The speaker often tries to make sure that his interlocutor has received the information that he is conveying, that his intentions are understood, and that his attitudes toward his interlocutor and the general surrounding environment are clear. Especially in spontaneously spoken discourse or dialogue, these various speech acts are not necessarily expressed in full-fledged well-formed sentences; they are often couched in rather fragmentary, truncated semi-sentences or phrases. In an analoguous manner to Parsons&apos; subatomic event structures, we call these speech acts corresponding to non-full-fledged sentences SUBATOMIC SPEECH ACTS. We show in this paper how these subatomic speech acts are used to form mutual beliefs and to coordinate discourse. We use examples from actual Japanese spoken dialogue to demonstrate that our theory accounts for some of the characteristic phenomena in discourse and assigns correct interpretations and explanations to these phenomena.
COHERENCE VIA COLLABORATION: A STUDY OF CHINESE CAUSAL CONNECTIVES  This paper discusses the discourse behavior of Chinese causal connectives yinwei &quot;because-and suoyi &quot;so&quot; from the perspective of grammaticalization, and claims that the underlying motivation for such is coherence, which involves pragmatic and cognitive interpretation. We claim that the majority of yinwei and suoyi are used pragmatically, and they are grammaticalized. This grammaticalization of discourse connectives is motivated by &quot;collaboration.&quot; This paper explicitly identifies the semantic and pragmatic functions of yinwei and suoyi. Our findings support that conversation is a collaborative process, and shov1/4 that interlocutors collaborate not only on phrasal or sentential level, but on discourse-level as well. 1. Introduction Pragmatic approaches to connectives are prevalent in the linguistic literature [2][13]115]. We believe that Chinese causal connectives yinwei &quot;because&quot; and suoyi &quot;so&quot; perform pragmatic functions in discourse. and are involved in the maintenance of coherence. The usage of causal connectives deserves investigation. and those pragmatic functions of discourse connectives also need further classification. Our first hypothesis is that yinwei and suoyi are grammaticalized and used pragmatically. We suggest that connectives are often used to achieve discourse coherence. Semantically, ..yinwei and suoyi convex causal relation, but besides semantic function, they also perform pragmatic functions of coherence_ That is. one can use yinwei and suoyi to link two clauses or discourse units that have no causal relationship. This paper will also demonstrate that discourse connectives play an important role in the turn-taking behavior. Interlocutors use connectives to take, yield or hold one&apos;s turn. Our second hypothesis is that this grammaticalization of discourse connectives is motivated by &quot;collaboration.&quot; This paper investigates the collaboration on discourse coherence via the use of connectives, and aims to claim that collaboration occurs on discourse-level. It is conversants&apos; intention to collaborate that results in those pragmatic functions of connectives, and then the grammaticalization. We examine Chinese causal connectives. and discuss their collaborative usage in terms of form and type of discourse.
RECONSTRUCTION, BECK EFFECTS AND WH-PHRASES IN SITU* This paper discusses the interpretation of a wh-phrase that remains in situ. Huang [9] proposes that wh-phrases in situ move at LF (Logical Form) just like overtly moved wh-phrases. Under his analysis, we expect that wh-phrases left in situ are interpreted in the same way as those moved to the Spec of CP already at S-structure. However, there is some difference in the interpretation of these two types of wh-phrases. Examining relevant sets of data from English and Japanese, I argue for the position that Reinhart [16], [17] take; in situ wh-phrases do not move to the Spec of CP at LF.Throughout the paper, I assume a modified version of Karttunen [10]'s semantics of Question. A question meaning is associated with a set of possible answers as proposed in Hamblin [6], but not with a set of true answers as seen in Karttunen. The meaning of (la) is represented as in (lb). It is roughly equated with a set of propositions of the form 'x came' where x is a student. When John, Bill and Mary are the students in the world, (1c) is the relevant set for the question in (la). A possible answer is a subset of the set of propositions that the question denotes when each member of the subset is true in the world.(1) a. Which student came? b.Ap3x [student (x) A p = A came (x)] c.{John came, Bill came, Mary came} where John, Bill, Mary are the students This paper argues that wh-in situ is interpreted in its original position. Focusing on the ambiguity of how many sentences, it shows that the semantic behavior of wh in-situ can be explained without postulating LF (phrasal) movement. It is then pointed out that the fact that wh-phrases in situ lack anti-reconstruction effects follows directly from this non movement approach. The paper also shows that certain intervention effects discussed by Beck do not necessarily present a problem to this approach. It is suggested that the intervention effects are due to a constraint on movement of`whof`wh-features&apos;, but not of`whof`wh-phrases&apos;.
A NONCONFIGURATIONAL APPROACH TO THE WEAK CROSSOVER EFFECT IN KOREAN Since at least [30], the weak crossover (WCO) effect has been used as a diagnostic for movement. In this paper, we assume that scrambling in Korean is licensed by a flat clausal structure and relative freedom of the linear precedence constraints among the non-head constituents, which entails that scrambling does not involve movement and thus leaves no trace. (See [2], [9], and [10] among others.) This predicts scrambling does not induce WCO effects at all. Then the putative WCO effect reported in the literature ( [5], [6], [16], [25], and [26]) among others) must be accounted for in different ways.In this paper, first, we will review [26]'s claim that the WCO effect occurs in the psych-verb construction with backward reflexivization in Japanese, showing that the construction actually cannot be considered to have the WCO effect in some respects and thus cannot be evidence that scrambling is an instance of movement. Then we will consider some canonical cases of WCO and some other relevant data, trying to show that the so-called WCO condition can be accounted for in nonconfigurational terms such as relative obliqueness and linear precedence between a pronominal and its binder, rather than in terms of c-command. To this end, we will explore binding conditions of non-locally bound anaphor caki 'self, the overt pronoun ku 'he', and the empty pronoun pro. Configurational approaches to weak crossover effects in Japanese and Korean have been proposed based on the notions such as c-command and reconstruction (e.g., [5], [25], and [26] among others). However, empirical problems arise from the approaches simply because they cannot fully account for the relevant Korean data. This paper proposes nonconfigurational operator binding conditions formulated in terms of obliqueness and linear precedence within the framework of Head-Driven Phrase Structure Grammar. The proposed syntactic operator binding conditions are formulated in only a necessary condition, which suggests that the pronominal binding possibilities are also determined by other factors such as discourse and/or processing factors.
RELATIVE CLAUSE CONSTRUCTIONS WITH POSSESSIVE SPECIFIER GAPS: A CONSTRAINT-BASED APPROACH * In this paper, I discuss a class of relative clauses that are distinct from ordinary relative clauses (RCs) in that no complement or adjunct gaps occur in the relative clause. A typical example is a phrase like (1).(1) [nwun-i yeypp-un] ai eye-nom pretty-rel child 'child whose eyes are pretty'I will argue that the subject NP of the RC nwun 'eye' has a specifier gap inside it and the gap is coindexed with the head noun ai 'child'. Let me propose to call RCs like (1) possessive RC constructions, partly borrowing from the English possessive relative pronoun whose.Korean, Japanese and Chinese linguists have long been interested in such possessive RC constructions because they appear to violate Ross's Complex NP Constraint (CNPC) when the subject is modified with another relative clause, as shown in (2):(2) [fie, ip-un], os-i terewu-n], sinsa, wear-rel clothes-nom dirty-rel gentleman 'gentleman whose clothes that he wears are dirty'It has been claimed that the expression like (2) constitutes a CNPC violation because the subject of the embedded relative clause ip-un has been extracted to the head noun crossing two S boundaries, as indicated by the gap in the inner RC and the two occurrences of its index in (2). ( Kuno (1976) and Yang (1986)) Recently, Na and Huck (1993) proposed essentially a semantic-pragmatic approach to possessive RCs, and their explanation is based on a certain pragmatic relation between a noun in the relative clause and its head noun, for example, os 'clothes' and sinsa 'gentleman' in (2), what they call a "thematic subordination" relation.' In the case of the two nouns 'clothes' and 'gentleman,' according to their This study is supported by Kyung Hee University Research Fund in the academic year 1998. Parts of this paper were presented at the Monthly Meeting of the Korean Society for Language and Information in September 1998, at the International definition of thematic subordination, 'clothes' are subordinately related to the 'gentleman' in our real world because the former may be part of the latter's personal property. Similarly, the noun nwun 'eye' is thematically subordinated to the noun ai 'child' because of the inalienable relationship between the two. Both (1) and (2) are well-formed RC constructions since a noun in the RC is thematically subordinate to the head noun, as their explanation goes. If such a relation does not hold, unacceptable RCs are produced. For example, a gentleman is not in the relationship of thematic subordination with clothes, because it is inconceivable to imagine that the gentleman is part of the clothes, and so (3a) is unacceptable. For a similar reason, (3b) is unacceptable: sunset hardly has anything to do with the property of a pencil and so the former is not thematically subordinated to the latter.(3) a. ??[sinsa-ka terewu-n] os gentleman-nom dirty-rel clothes '??Clothes whose gentleman is dirty' b. ??[nol-i yeppu-n] yenphil sunset-nom pretty-rel penciìpenciì??pencil whose sunset is pretty' So in order for such a RC as (1) or (2) to be well formed, the denotation of a noun in a relative clause must be thematically subordinated to the denotation of the head noun of the relative clause.Although their pragmatic description is interesting on its own, revealing certain conventional world views and attitudes of the speakers, such a line of investigation is short of a linguistic explanation of the phenomenon under consideration unless it is shown how it is connected with and supported by some solid syntactic grounds. For example, it would be linguistically uninteresting to merely point out that expressions such as "Colorless green ideas sleep furiously" are pragmatically odd; what we are really interested in is to show how the pragmatic oddness comes about from the syntax of the sentence. Since we want to understand why noun phrases like (1) or (2) are well formed while (3) ill-formed, we need to look for syntactic explanation before we are complacent of pragmatic speculation and then to show how the two components interact. This paper is an attempt to formulate such a syntactic explanation and furthermore the syntax-semantics-pragmatics interface. This paper is concerned with a particular kind of Korean relative clause constructions that contain possessive specifier gaps, as opposed to complement or adjunct gaps. Those relative clauses have been known to violate Ross&apos;s Complex NP Constraint, and various attempts have been made to account for the apparent island violations. This paper argues that those relative clauses have possessive specifier gaps in the main relative clause, rather than in the embedded relative clause, and so there are no island violations involved in such relative clauses. It is argued that the island violations in question can be viewed as violations of selection restrictions of very general kind holding between nouns and their specifers. It is not necessary to impose any special constraints to account for the apparent violations observed in the particular kind of relative clauses discussed in this paper.
JAPANESE CATEGORIAL GRAMMAR BASED ON TERM AND SENTENCE The phrase structure grammar which stems from the structural linguistics and was adopted by the generative transformational grammar has been the main grammatical theory of parsing of English. It is further applied -often without critical consideration -to other languages, also to Japanese. It certainly provides an effective tool for the syntactic parsing, but when it comes to the semantic parsing of Japanese, it seems preferable to adopt the categorial grammatical approach which reflects the semantics of Japanese in a direct form, all the more because the word order of Japanese is based on the inverse Polish notation, in contrast to English syntax based on the syntagmatic distribution of formatives.There has also been a categorial grammatical approach to the parsing which originates in the exploring work of Bar-Hillel(1953) followed by Urzkoreit(1986), Moortgat(1988), Wood(1993), Carpenter(1997) to mention a few. But the strict categorial combinatorics contradicts the free word order of Japanese and the frequent elimination of sentence elements, to make it difficult to apply the original categorial grammatical method to Japanese. So, as to the essential part of Japanese, I abandon the simple categorial grammatical technique such as application, composition, division etc., and introduce the HPSG-like compositional principle based on term and sentence.1One of the basic ideas for this stems from Sakai(1979) who advocates the view that japanese verbs and adjectives are all sentences as such, and missing arguments are implicitely complemented by the context. According to this view, the following sentence (1) Taro-ga ringo-wo tabe-ta (Taro-NomCM apple-AccCM eat-Past) (Taro ate an apple.) (NomCM: nominative case marker, AccCM: accusative case marker, Past: past tense marker) is analyzed as follows:(t: category of terms, s: category of sentences) I.e., the term`ringoterm`ringo-wo is applied to the sentencètabe-ta' to form the sentence 'ringo-wo tabe-ta', further the term`Taroterm`Taro-ga' is applied to the sentencèringo-wo tabe-ta' to form the sentence (1). Here, the expressions (3) i) tabe-ta ii) ringo-wo tabe-ta iii) Taro-ga ringo-wo tabe-ta are all grammatical sentences. And the sentence (the transitive verb in its usual terminology) `tabe-ta' functions as the head of (3i,ii,iii). I call it their respective verbal head. Conversely, there is a construction in Japanese where a sentence is applied to a term to form a new term. E.g., (4) Taro-ga tabe-ta ringo (Taro-NomCM eat-Past apple) (the apple which Taro ate) (4) is analyzed as follows:Here, `ringo' functions as the head of (5) as a whole. I call it the nominal head of (5). (The modifying sentence in (5) corresponds to a relative clause in English. But the modifier can also be a simple adjective, because an adjective as such is already a sentence in JCG.)Such a duality between term and sentence is not too surprising a phenomenon with respect to the natural languages in the world. Some American Indian language reportedly expresses a sentence and its nominal correspondence in the same expression.2 I call the above-mentioned grammatical framework the Japanese Categorial Grammar (JCG), whose detail is stated below. In this paper, I propose a japanese categorial grammar JCG which gives a foundation of semantic parsing of Japanese. JCG is not an orthodox categorial grammar with functional application, composition etc., but a hybrid system with HPSG based on term and sentence which correspond to the nominal head and verbal head in HPSG respectively. Construction rules of JCG are designed so as to be used as parsing rules of Japanese, but at the same time, they give the semantic correspondence of the japanese expressions via the homomorphism between syntax and semantics. The semantic output as such is a higher-order logical expression, but it&apos;s reduced to a Horn clause expression in its final form that constitutes a logic database on which a logical inference is executed.
FREE WORD ORDER IN A CONSTRAINT-BASED IMPLEMENTATION OF DEPENDENCY GRAMMAR Computational linguists in China [9] [26] [27] have been studying and experimenting with Chinese sentence parsing based on the tradition of Gaifman, Hays and Robinson [4][7] [21] in the development of Dependency Grammar, [25] which is different from other traditions of the grammar formalism [10] [24][19] [6] in that single-headedness and projectivity are required for dependency relations holding between governor-dependent pairs of words in a sentence. Using different algorithms for the parsing process, they differ in whether syntactic categories are considered in lieu of the actual words and in whether and how word order is taken into consideration, but they all observe the requirements of singleheadedness and projectivity. They also label dependency relations with functional labels like subject and object as is done in practically all schools of Dependency Grammar.Unification [22] has been used in Dependency Grammar implementation and theory (e.g. [8][2] [18]). The authors have distinguished between a single-headed and projective dependency (constituent) structure and a much less constrained functional structure. [14][15] [16][17] Dependency rules [7] are annotated with functional annotation, [1] and a unification-based parser [22] [5] has been adapted to produce dependency and functional structures for Chinese (and English) sentences. They have discussed how syntactic phenomena like control are dealt with. They make a distinction between obligatory complements prescribed for by the syntactic properties of the governing verb and optional adjuncts that are taken care of by non-lexical rules in the syntax of the language. A pair of subcategorization lists residing in the lexical entry of the governing verb are used to take care of obligatory verbal complements, making use of constraints in Chinese (and English) that the subject is to the left of the verb while the other verbal complements are on the other side, and that optional adjuncts must not come between the verb and the non-subject complements.Though basically respected in Chinese (and English), these constraints are obviously too restrictive. In Japanese, for example, the subject, obligatory verbal complements and optional verbal adjuncts are all on the same side of the verb, complements can be "scrambled", and adjuncts often come between complements and the verb.In this paper, the authors' mechanisms for dealing with verbal complements and adjuncts mentioned above [17] are modified and extended to take care of free word order. Syntactic properties of the governing verbs are encoded in their dictionary entries, and, free complement and adjunct word order are taken care of by making modifications to the subcategorization lists and introducing special processing mechanism, while inherent order of elements in the subcategorization list is still useful in capturing default or preferred word order. Parsing of sentences based on Dependency Grammar is emulated with a constraint-and unification-based mechanism that preserves single-headedness and projectivity in syntactic dependency. Working on Chinese, and referring to English at times, the authors have treated subcategorization properties of verbs on the assumptions that relative positions of obligatory complements of a verb are fixed, and that optional verbal adjuncts, while relatively free in their choice of positions in the sentence, are constrained to be placed farther away from the governing verb than any complements. A pair of subcategorization lists residing in the governing verb have been used to capture these properties. However when one tries to extend the model to languages with relatively free word order like Japanese, for which the assumptions mentioned above are not valid, straightforward list-manipulations on subcategorization lists will not be adequate. This paper discusses how additional subcategorization handling mechanisms can be introduced to deal with verbal complements scrambling and intervening adjuncts, while retaining the mechanisms to capture default word order.
WHAT IS THE LEXICAL FORM OF &apos;BEI&apos;? The Chinese word 'It' is an unusual lexical unit as it has two different linguistic behaviours (cf. Section 2). Due to this distinctive characteristic, the lexical representation of`tkof`tk.' has been a frequently discussed subject. The Chinese word 'It' is frequently being mistaken as a preposition. However, as argued by Tan [9] and Her [7], 'It' cannot be a preposition; it is indeed a verb. Tan [9] treated 'It' and its following noun phrase (NP) as a subordinate clause for making the meaning of a sentence more specific. However, Her argued that Tan's argument structure of 'It' is incomplete. There are some forms of l'-sentences that Tan's argument structure failed to describe. The lexical form of 'It' suggested by Her can adequately predict and explain the different syntactic behaviours ofìbt' in Lexical-Functional Grammar (LFG) terms. However, a problem arises when applying lexical mapping theory defined in the LFG formalism to Her's lexical form.In LFG, different levels of linguistic information is represented by different structures: constituent structure (c-structure), functional structure (f-structure), argument structure (a-structure) and semantic structure (s-structure) [2,3,6]. The linguistic information encoded in these structures ranges from pure syntax towards a certain level of semantics:Semantic Information some semantic information thematic information higher syntactic and functional information syntactic structure s structure a structure f structure c-structure The lexical representation of the Chinese word &apos;4A: has been an issue of ongoing debate. The lexical form suggested by Her seemed to provide a complete representation of the different syntactic behaviours of`4htof`4ht.&apos; within a Lexical-Functional Grammar (LFG) account. However, when applying this representation in conjunction with the argument structure (a-structure) and the lexical mapping theory in LFG, this representation conflicts with the lexical mapping theory. This paper examines this problem and proposes a solution to the problem when dealing with the lexical representation of`16tof`16t&apos;.
Spoken Language Systems -Technical Challenges for Speech and Natural Language Processing  Speech is the most natural means of communication among humans. It is also believed that spoken language processing will play a major role in establishing a universal interface between humans and machines. Most of the existing spoken language systems are rather primitive. For example, speech synthesizers for reading unrestrict text of any language is only producing machine-sounding speech. Automatic speech recognizers are capable of recognizing spoken language from a selective population doing a highly restricted task. In this talk, we present some examples of spoken language translation and dialogue systems and examine the capabilities and limitations of current spoken language technologies. We also discuss technical challenges for language researchers to help realize the vision of natural human-machine communication to allow humans to converse with machines in any language to access information and solve problems.
IMPLEMENTATION AND EVALUATION OF SCALABLE APPROACHES FOR AUTOMATIC CHINESE TEXT CATEGORIZATION Text categorization is the problem of automatically assigning predefined categories to free text documents, and is gaining more and more importance as the amount of text data available on World Wide Web grows dramatically. A well classified text database will be very helpful for a user to identify interesting data from the huge collection of texts. There are many studies about the text categorization as well as web-page classification [11,3,7,8,21,25,26,18,6,5,2,10]. While there are a great number of researches on automatic text categorization for English texts, text categorization for Asian languages such as Chinese, Japanese, Korean and Thai has not been studied seriously until recently [17,29,1].It is well known that written Asian language consists of strings of ideograph separated by punctuation signs. An ideograph (or character) can function as a word with meaning(s), or it can act as an alphabet to form a "word" with one or more adjacent characters. Determining the boundaries of single or multi-character words in a string, a process called segmentation [4], is very difficult because no delimiter or while space is used in the text and one has to rely on the context contents. Because text segmentation is not straightforward, 1-grams, 2-grams and n-grams have been used as indexing terms to represent documents in Asian languages. Among them, 1-grambased approaches is the simplest one that uses single characters as indexing terms, and should be good for recall in information retrieval(IR) because it guarantees that if there are correct word matches between queries and documents, there will be 1-gram matches. However, single characters (1-grains) are ambiguous in meaning, which results in low precision in IR. A number of research have proposed to use n-grams, instead of 1-grams, as indexing terms. An n-gram is a sequence of n contiguous characters in the text. The 1-gram-based approaches [23] simply use every single character as a single term, and the 2-gram-based approaches use every 2 contiguous characters as indexing terms, and the general n-gram-based approaches use all 1-grams, 2-grams, 3-grams,. . ri-grams as indexing terms. Although 2-grain and n-gram perform similarly well as indicated in our experiment, in this research, we take n-grams, 1 &lt; n &lt; 10, as indexing terms because n-grams can catch the concept of a document. Notice that the possible number of n-grams in Chinese is dramatically huge, and furthermore many of them are meaningless and non-informative for text categorization. The major challenge is to develop approach that can reduce the number of n-grams to an acceptable level, while at the same time maintains similar categorization accuracy.The purpose of this research is to identify scalable approaches that can handle large amount of training data such as several years of news articles, and automatically assign predefined category to Chinese free text documents. Our approach consists of the following processes: (i) term extraction, (ii) term selection, and (iii) document classification. Identifying terms, or so-called word segmentation, from text documents is one of the most difficult problems in processing Chinese texts. In this research, we develop a scalable approach to identify terms from large amount of text data, which does not use a dictionary. The approach first builds a recently developed SB-tree [9,4,19] to identify all repeated substrings, called patterns, from the texts. We believe important terms will appear repeatedly in the articles. The SB-tree also gives the information such as the frequency of a pattern, the documents and the locations where a pattern appears which are then used to identify possible boundary of terms appearing in the same pattern, and to remove meaningless patterns which are substrings of some terms. Term boundaries are used to partition patterns into terms. After terms are extracted from the training articles, we run term selection algorithms to select the most representative terms and to reduce the number of terms to an acceptable level. The selected terms are used by the classifier to assign a predefined category to each text document.Our current experiment uses CNA one year news as training data, which consists of 73,420 articles and is far more than previous related research which use either one month news or sampled articles from the whole year news. Notice that although sampling methods are very interesting research issues, most of the commercial systems prefer to extract information from the original whole-set data as done in the recent data mining applications. We believe the whole year training data can make conclusions from our experiment more reliable than previous research. We implement and compare four term selection methods, the odds ratio method, the mutual information method, the information gain method and the x2-test method, when they are combined with the naive Bayes classifier [22]. Our experiment shows that X 2-test achieve the best performance.The remainder of this paper is organized as follows. Section 2 describes the process to remove meaningless and non-informative substrings, and to select the most representative terms. Section The purpose of this research is to identify scalable approaches that can handle large amount of training data such as several years of news articles, and automatically assign predefined category to Chinese free text documents. Our approach consists of the following processes: (i) term extraction, (ii) term selection, and (iii) document classification. The approach first builds a recently developed SB-tree to identify all repeated substrings, called patterns, from the text. We then proceed to identify possible boundary of terms appearing in the identified patterns. After terms are extracted from the training articles, we run term selection algorithms to select the most significant terms and to reduce the number of terms to an acceptable level. The selected terms are med by the classifier to assign a predefined category to each text document. Our current experiment uses CNA one year news as training data, which consists of 73,420 articles and is far more than previous related research. In the experiment, we implement and compare four term selection methods, the odds ratio method, the mutual information method, the information gain method and the x2-test method, when they are combined with the naive Bayes classifier.
MANDARIN LOANWORD PHONOLOGY AND OPTIMALITY THEORY: EVIDENCE FROM TRANSLITERATED AMERICAN STATE NAMES AND TYPHOON NAMES Mandarin transliterated loanwords borrowed from English are prevalent, such as Maryland [mt.n.lond] 4 Malkin [ma.li.lan] 1 . Since Mandarin and English have different syllable structures the latter allows onset and coda clusters and all almost consonants to be codas while the former does not, the syllable structures of the loanwords must be modified or converted while they are borrowed into Mandarin.Three repair strategies-featural change, epenthesis, and deletion-are generally found to operate on these Mandarin loanwords. A question arises: What is the motivation for these strategies? In the previous rule-based studies (Yin, 1984;Chang, 1996: 10-17), several arbitrary rules of deletion and epenthesis are proposed under the condition of context without phonological motivation. These rulebased analyses, as will be presented in section 3.3, also fail to show the connection of related facts.Different from the rule-based perspective, this study will investigate how the consonant clusters and illicit codas are dealt with in transliterated American state names and typhoon names in Mandarin in a constraint-based perspective. 2 It will be argued that the repair strategies on the syllable structures are triggered by high-ranked well-formedness constraints. These constraints interact, in order to increase the well-formedness of the surface forms of the loanwords. It will also be argued that the epenthesis and the disyllabicity effect found in these loanwords result from universal constraints that are present in all grammars, but are masked by the effects of higher-ranked constraints in Mandarin Chinese. The questions addressed are: What constraints are involved in generating the candidate set in Mandarin loanwords? How do these constraints interact to leave one candidate as the optimal output to surface? In what ways is this constraint-based analysis better than the rule-based one in explaining this issue?The Pinyin romanization is used in this study. The pronunciation is transcribed in IPA. The symbol "." indicates syllabification.2 The term "illicit coda" in this study means the coda in (C)VC. Take, Beth [W], for example. The [0] is not allowed to be a coda in Mandarin and is termed as "illicit coda". The coda cluster, such as [bz] in Babs [bxbz], is categorized as a consonant cluster but not an illicit coda in this study.In the next section, the sound inventories and syllable structures of Mandarin and English will be briefly presented and compared. Section 3 presents the data collected in this study and reviews the previous rule-based studies. Section 4 introduces the theoretical background of this study-Optimality Theory (henceforth, OT), and illuminates how constraints interact to generate the surface forms of Mandarin loanwords, followed by conclusion in section 5. This paper aims to examine how the consonant clusters and illicit codas are modified in Mandarin loanwords transliterated from English, and to argue that no rules need to be involved and that a purely constraint-based approach-within the framework of Optimality Theory-can explain the data. The data collected from transliterated American state names and state names display the onset-coda inconsistency in Mandarin loanwords. All the onset consonant clusters in the data are faithfully parsed into Mandarin syllables, with inserting vowels to shun consonant cluster. The coda clusters and illicit codas are generally parsed. However, the coda liquids may be parsed in some cases but unparsed in others. The preference of insertion in Mandarin loanwords can be explained by the interaction between the constraints-*COMPLEX, CODACON, MAX-I0, and DEMO. The distinctive behavior of coda liquids can be accounted for by the effect of MINwD.
CONSONANTAL WEAKENING AND LICENSING IN OPTIMALITY THEORY  This paper explores several patterns of consonantal weakening such as voicing and spirantization. Given that such weakening usually applies to either intervocalic or intersonorant obstruents, this paper contends that inherently redundant features such as [voice] for sonorants or [continuant] for vowels, cannot license their syllabic constituents, thus triggering consonantal weakening. In addition, based on the typology of consonantal weakening such as one-step weakening with voicing or spirantization and full weakening with both voicing and spirantization, this paper maintains that several patterns of consonantal weakening is the consequence of licensing and more importantly, of constraint relations such as conjunction. Finally, this paper provides unified analysis of all types of consonantal weakening relying on constraints and constraint relations within the framework of Optimality Theory.
THE LEXICON IN FCIDB: A FRIENDLY CHINESE INTERFACE FOR DBMS A natural language interface helps users communicate with computers in their own natural language. Natural language interfaces are an application of natural language processing. The goal of a natural language interface to database management systems (DBMSs) is to help users get information from a DBMS without knowing about databases or their contents.Some natural language interface systems have actually appeared, e.g., LADDER [3], DATALOG [2], and TELI [1]. Today, some domain-independent English interface systems for database are available as commercial products, e.g., English Wizard. English Wizard was developed by the Linguistic Technology Corp. [4]. It allows you to talk to several database management systems, which support Open DataBase Connectivity (ODBC), in the English language. FMDSIC (Friendly Medical Database System Interface in Chinese) is a domain-dependent interface [6]. A domain-independent Chinese interface system, as far as we know, has not appeared before. FCIDB (Friendly Chinese Interface for DataBase management systems) is a domain-independent interface system. It allows users to communicate with database management systems in Chinese. FCIDB translates Chinese questions into SQL, so that your DBMS receives the SQL it expects.The architecture of FCIDB includes the processing components: Look-Up &amp; Word Segmentation, Parsing, Translating to SQL Commands, and Dialogue; and the information components: Lexicon, Chinese grammar rules, SQL grammar rules, and Mapping Dictionary. Figure 1 shows the architecture of FCIDB. The function of the Look-Up and Word Segmentation component is to look up Chinese character strings in a lexicon and determine the word boundary. At the same time, this component obtains information about words from the lexicon, and sends them to the Parser. The Parser applies a set of Chinese grammar rules and a bottom-up parsing algorithm to parse user queries. The Parser must also choose the proper meaning for a word with multiple meanings. During translation the Parser output is reorganized into a legal SQL command. The legal SQL command will be sent to the Dialogue component. Dialogue sends the SQL command to the DBMS and gets the results back. Dialogue must then decide how to phrase the response to the users. If the result is only one value, then the value is included in a sentence. Otherwise, the result is given to the user in table form.We will focus on the lexicon of FCIDB in this paper. In the following sections, we will talk about the function of the lexicon, the structure of the lexicon, and the contents of the lexicon. FCIDB (Friendly Chinese Interface for DataBase management systems) can understand users&apos; queries in the Chinese language. It works like a translator that translates Chinese queries into SQL commands. In the translation process, the lexicon of FCIDB plays a key role in both parsing and word segmentation. We designed some questionnaires to collect the frequently occurring words and add them to the public &apos;lexicon in FCIDB. FCIDB will produce a private lexicon for every new connected database. This paper will focus on the words included in the public lexicon and in the private lexicon. We also discuss the function, the structure, and the contents of the lexicon in FCIDB.
SIMPLE SYLLABLE STRUCTURE AND ENGLISH PRE-NUCLEUS GLIDES  
SPEECH INVERSE FILTERING BY SIMULATED ANNEALING ALGORITHM Articulatory synthesis is the production of speech sounds using a model of the vocal tract, which directly or indirectly simulates the movements of the speech articulators. It provides a means for gaining an understanding of speech production and for studying phonetics. Articulatory synthesis usually consists of two separate components. In the articulatory model, the vocal tract is divided into many small sections and the corresponding cross-sectional areas are used as parameters to represent the vocal tract characteristics. In the acoustic model, each crosssectional area is approximated by an electrical analog transmission line to simulate the speech sound propagation through the vocal system as well as the physics of the physiological-toacoustic transformation.To simulate the movement of the vocal tract, the area functions must change with time. Each sound is designated in terms of a target configuration and the movement of the vocal tract is specified by a separate fast or slow motion of the articulators. The recovery of articulatory movements from the speech signal, known as the speech inverse filtering problem, is difficult due to the non-uniqueness of the solution. This problem has been the subject of research for several applications, including articulatory synthesis, speech recognition, low-bit-rate speech coding, and text-to-speech synthesis. Here we attempt a new solution using the simulated annealing algorithm, which is a "constrained multidimensional nonlinear optimization problem." The coordinates of the jaw, tongue body, tongue tip, lips, velum, and hyoid compose the multidimensional articulatory vector. A comparison between the model-derived and the targetframe first four formant frequencies forms the cost function. There are two constraints: (1) the articulatory-to-acoustic transformation function, and (2) the boundary conditions for the articulatory parameters. The optimum articulatory vector is obtained by finding the minimum cost function. Once the optimum articulatory vector is determined, the articulatory model determines the vocal tract cross-sectional area function which in turn is used by the articulatory speech synthesizer. The purpose of this study is to develop one solution to the speech inverse filtering problem. A new efficient articulatory speech analysis scheme, identifying the articulatory parameters from the acoustic speech waveforms, was induced. The algorithm is known as simulated annealing, which is constrained to avoid non-unique solutions and local minima problems. The constraints are determined by the articulatory-to-acoustic transformation function and the boundary conditions for the articulatory parameters. The cost function is defined as a percentage of the weighted least-absolute-value error distance between the first four formant frequencies of the articulatory model and the first four formant frequencies determined from speech analysis. It is used to optimize the vocal tract parameters to match a specified set of formant characteristics. A 1% error criterion was found to be both practical and achievable.
A CLASSIFICATION TREE APPROACH TO AUTOMATIC SEGMENTATION OF JAPANESE COMPOUND SENTENCES It is well known that direct parsing of a long Japanese compound sentence, comprising many coordinate clauses, is extremely difficult. Various pre-processing methods have been proposed to segment such a sentence into shorter, simpler ones prior to parsing [1]. Sentence segmentation have also been discussed from a view point of document revision support system [2], because a long compound sentence is difficult to understand even for humans. The techniques of sentence segmentation reported so far can be summarized as follows:(1) Segmentation points are estimated by matching between prescribed segmentation patterns and an input sentence. The segmentation patterns are described by using the part-of-speech and the orthographic representation of morphemes obtained by morphological analysis [1].(2) Dependency analysis on a clause sequence is conducted based on subordination relation among clauses [3]. Then dependency structure candidates are ordered by using heuristic dependency scores between clauses. Finally the segmentation points are determined in accordance with the top candidate for the dependency structure [2].These techniques have been reported effective. However, the problem with these conventional methods is that the segmentation patterns or the heuristic dependency scores must be given manually, hence no guarantee for optimality. This paper proposes a new method of automatic segmentation of long compound sentences using a classification tree technique [4,5,6,7] based on the surface information obtained by morphological analysis. In this method, optimal segmentation patterns and the optimal order of their application are automatically acquired from training data, linguistic phenomena together with their occurrence frequencies being taken into account. The rest of the paper describes the details of the method, and reports the experimental results on an EDR corpus, including the effects of pruning. It is well known that direct parsing of a long Japanese compound sentence is extremely difficult. Various pre-processing methods have been proposed to segment such a sentence into shorter, simpler ones prior to parsing. The problem with the conventional methods is that some kind of segmentation patterns or heuristic preference scores must be given manually , hence no guarantee for optimality. This paper proposes a new method of sentence segmentation based on a classification tree technique. In this method, optimal segmenta-tion patterns and the optimal order of their application are automatically acquired from training data, linguistic phenomena together with their occurence frequencies being taken into account. Generation of a classification tree is conducted on an EDR corpus, and evaluation results are reported. It is shown that pruning reduces the tree size by a factor of about 1/4 without affecting the performance.
DETERMINING THE ANTECEDENT OF NOUN PHRASE CONTAINING THE DETERMINER KONO OR SONO IN JAPANESE Reference problems are a central issue in natural language processing. For instance, we need to understand the antecedents of pronouns in translating one language into another. Consider:Someone killed Jim. The police have no suspect, but they think that he or she needed money and knew that he was a wealthy man. He walked in the house with a big suitcase and put the money in it.We will not be able to translate the sentences into, say, Japanese when we are uncertain of what the pronouns like he, she, they, and it in this text are referring to.A pronoun refers to a linguistic object in the preceding or succeeding sentences. In the sequence, Here, the noun president refers to Clinton.In this paper we try to devise a method for determining antecedent of the noun phrase containing determiner "kono (this)" or "sono (that, its)" in Japanese. This paper offers a method for determining referent (antecedent) of the noun phrase containing determiner &quot;kono (this)&quot; or &quot;sono (that, its)&quot; in Japanese. It uses in the determination of the antecedent a statistical measure of conceptual similarities, taken from a corpus, between each candidate for the antecedent and the modeficand of kono or sono. We describe our method and an algorithm. We then show an experiment that has given us an overall success rate of 85.2%. Our method is applicable to solve other semantic problems than that for finding the antecedent of noun phrase containing the determiner kono or sono.
GENERATING SUPPLEMENTARY INDEX RECORDS USING MORPHOLOGICAL ANALYSIS FOR HIGH-SPEED PARTIAL MATCHING Higashida [4] has been developing a fully automated Japanese telephone directory assistance system using touch-tone telephones as input devices [3]  [4]. In the written Japanese language, there are no spaces between words. Therefore, Japanese morphological analysis is necessary to recognize word boundaries for input strings. However, since morphological analysis requires high-cost processing, it is not suitable for the telephone directory retrieval task in which processing speed is very important.Let' s consider that a company name is input as the retrieving keyword by a telephone directory user. If the user knows and inputs the complete company name, the system can exactly match the keywords input against the index records in the DB. However, many users know only part of the company name. In this case, a partial string of the company name is input as the retrieving keyword. Namely, it is unusual for there to be an exact match between retrieving keywords and index records and partial matching is the fundamental operation for the telephone directory retrieval task.Consequently, the requirements for telephone directory retrieval task are as follows:• High-speed partial matching:In the telephone directory retrieval task, it is necessary to obtain retrieval results from incomplete input strings. In order to satisfy this requirement, we have to perform partial matching. In the telephone directory retrieval task, the number of records in the subject DB is of the order of several million. Therefore, we have to develop an extremely high-speed partial matching method.• Supporting many processes concurrently by reducing resource requirements:Since many tasks must be handled at the same time in the telephone directory retrieval task, the system must invoke many processes concurrently. Therefore, it is indispensable to reduce resource requirements such as CPU power, memory, and disk I/O.To satisfy the above requirements, this paper proposes a method for generating supplementary index records using Japanese morphological analysis for high-speed partial matching. The supplementary index records are managed by a commercial DBMS. This allows us to realize high-speed partial matching since the DBMS can handle the supplementary index records by traversing the index tree efficiently. The telephone directory retrieval task requires high-speed partial matching, and many processes run concurrently so there is a need to reduce resource requirements such as CPU power. To realize the high-speed partial matching that satisfies these requirements, this paper proposes a method for generating supplementary index records. This method enables us to retrieve all candidates including all words within strings input by users from a DB without applying morphological analysis to the input strings on the fly. Moreover, this paper describes the results of evaluation experiments. The experimental results show that the proposed method requires about 60% less CPU power and 80% less logical disk I/0 than the simple partial matching, and that the proposed method takes less than about 5 seconds when fewer than 100 records are retrieved. Namely, the experimental results show that the proposed method satisfies the requirements mentioned above quite well. This means that the proposed method is practical when applied to the telephone directory retrieval task.
EXTRACTION OF SIMPLE SENTENCES FROM MIXED SENTENCES FOR BUILDING KOREAN CASE FRAMES In NLP, the Case frames of a language are very important for a correct syntactic and semantic analysis of the language. The term Case frames is originated from the Case grammar of Fillmore. However, the term may currently refer to the syntactic part of a lexical entry in grammars such as HPSG, LFG, and the like, or in other place it sometimes includes the semantic part, too.To confirm the common deficiency of the recent approaches to the acquisition of Case frames, let us review some of the related works. Chae-Deug Park studied on learning the Case frames of English without any consideration of preparing sufficient simple sentences as training data [7]. Chae-Kwan Song tried to automatically extract sentence patterns and the information of semantic attributes from the corpus manually tagged with parts-of-speech [8]. Tanaka used sentences analyzed by means of a full parser as training data [9].Most of such researches so far have used the corpus tagged either by hand or by a parser. Experimental studies in a small scale manage to prepare training data manually. Such a manual arrangement, however, always results in a barrier to doing practical researches on the entire language. On the other hand, the use of any full parser, as it is without any additional processing, brings about a contradiction because the training data are obtained from the unreliable parser. Notice that Case frames are the very information for a further reliable syntactic analysis. Furthermore, currently available parsers for Korean are not even as good as those for English.The training data needed to construct the Case frames for the entire Korean verbs are nothing but a large amount of simple sentences. Unfortunately, however ordinary sentences are not in the form of simple sentences but mixed ones. If we might extract only originally simple sentences from a given corpus, hence a corpus of tremendous size would be required, which is not expected to be available in the near future [11]. This implies that we have to extract simple sentences from mixed ones. Concerning this, assuming that the Case structures and argument structures for all Korean verbs are available, Kwang-Jin Kim extracted simple sentences from embedded ones, though the ultimate goal of his study was a machine This research was funded by the Ministry of Information and Communication of Korea under contract 98-86. translation [4]. However, such linguistic information might not be available for practical NLP until sufficient simple sentences are available.Summing up, we do not rely on such unrealistic assumptions in this study. We just use the output of NM-KTS morphological analyzer, whose rate of accuracy is 96% and probability of guessing unregistered words is 0.75, and hence it is comparatively reliable. As already discussed, the use of a full parser results in consistent reflection of the internal algorithm and Case frames of the parser. Therefore, this study proposes a partial parsing algorithm. Also, to increase the accuracy of analysis we exclude sentences that might bring about fallacy in actual analyses. The approach of partial parsing enables a large amount of incompletely (but not incorrect) analyzed sentences to be used for machine learning. This implies that we may adopt a quite different approach from full parsers. A large number of simple sentences are needed to construct practical Case frames automatically. Until now, most studies have assumed that there are already extensive training data (especially here simple sentences) and linguistic information for their work. However, this is not true at least of Korean. Furthermore, Korean syntactic structures are significantly different from those of English. So, this paper first of all, compares Korean with English in relation to extracting simple sentences from mixed ones. Second, we suggest fundamental and detailed principles. For convenience and practicality, however, we deliberately exclude some linguistic phenomena. Finally, we attempt to develop a reliable algorithm to extract simple sentences with the ultimate goal of building Case frames.
SUB-SENTENTIAL ALIGNMENT METHOD BY ANALOGY Being able to define accurately and efficiently word correspondences between parallel texts is an issue in corpus-based machine translation. Translation patterns can be extracted very easily if word correspondences between pairs of translation sentences are defined. For sub-sentential alignment, a simple consultation of a machine readable dictionary seems to be a very obvious method. However, the presence of unregistered words, and the difference which may be seen in the surface form of words, or after being processed by a lexical analyzer cause problems. In addition, compound words, which often appear in one-to-many or many-to-many word correspondences, are not covered by single word entry-based dictionaries. The use, not of dictionaries, but of the parallel corpus itself, has been therefore suggested in order to search word correspondences [4,5,6]. Sub-sentential alignment methods which have been proposed are based on statistics. The problem with statistical methods is that they are not able to produce reliable results when the corpus size is limited. For unexplored languages whose huge parallel corpus are still not available, these approaches cannot be applied. In addition, correspondences involving multiple tokens have not yet been entirely resolved [4]. Melamed avoids these cases and considers only one-to-one correspondence [5]. On the other hand, although Kitamura' s [6] main goal is not the sub-sentential alignment, but the extraction of translation patterns, possible correspondences between functional words are not considered. It weakens the method since it decreases the number of translation patterns which could be extracted.We have proposed a French-Japanese example-based machine translation which uses word correspondence-included translation examples [1,2]. As a relatively new field, a huge parallel corpus is not available. Manual construction of translation examples are very time consuming. An automatic method which can efficiently be applied to size-limited corpus is necessary.In this paper, we propose a method for estimating the word correspondences between new pairs of translation sentences by analogy. Similar pairs are selected from a word correspondences-included initial translation examples. Word correspondences between the new pairs are predicted according to the word correspondences between these similar pairs. By doing so, the method can work with size-limited corpus. Besides, it is considered to be able to resolve efficiently correspondences involving multiple tokens, since solution models are given beforehand within the translation examples. For example, if "voulez -vous"  PRV: pronoun, DTN: determinant, SBC: common noun, ACJ: verb "avoir" 1: punctuation, 2: verb, 6: noun, 9: particle, 14: suffix (would you) and "itadake masu ka" appear in a pair of translation sentences, aligning "voulez" with "itadake", or "voulez -vous" with "itadake", or else "voulez -vous" with 'itadake masu ka" are all conceivable. Observation of sentence meaning sometimes produces results which differ from statistic results. Besides, results depend on whether only one-to-one correspondences are considered or not. However, if there is a translation example mapping "voulez -vous" or its similar phrase "pouvez -vous" (could you) with "itadake masu ka", that map can be used as a reference to determine the correspondence maps of the new pair.We introduce, in addition, a learning process-based method for constructing gradually the translation examples. Experiments were performed with French-Japanese spoken language texts, but the method is designed for any unexplored language pairs.The overview of the system is immediately presented and detailed step by step in the next. section. Next, the method of the experiments and the results are described, discussed, and at last, few words are given as a conclusion. This paper describes a method for searching word correspondences between pairs of translation sentences. In the Example-Based Machine Translation, translation patterns can be extracted easily if word correspondences between pair of translation sentences are defined. The popular methods for aligning bilingual corpus at a sub-sentential level are unable to produce reliable result when the size of the corpus is limited, because they are based on statistics. We propose a method for incrementing a word correspondence-included initial corpus automatically. It is appropriate for new languages whose huge corpus as well as machine readable dictionaries are still not available. The method was evaluated with French-Japanese spoken language texts. As the number of translation examples goes beyond 1,000, more than 80.0% of correct word correspondence rates were earned.
A STUDY OF PERFORMANCE EVALUATION FOR GA-ILMT USING TRAVEL ENGLISH Recently, the internet has come to be used by many people around the world. As a result, it is necessary to communicate correctly and quickly, information that is expressed in different languages. Machine translation is a very effective method of responding this need. Therefore, substantial research has been carried out in the machine translation field, and many machine translation systems have been developed. However, their correct translation rates and quality of translation are not sufficient to date. In the translation of conversation, the correct translation rate and the quality of translation are particularly low. This is due to machine translation systems not being able to generate translation results which fit the context of the conversation. Machine translation systems must be able to translate well to generate translation results which will also fit the context of the conversation taking place..To date, machine translation systems have been rule-based machine translation [1]. However, this method has several problems. Rule-based machine translation cannot deal adequately with various linguistic phenomena due to its use of limited rules. Moreover, it also has difficulty in dealing with words that it does not recognize due to the use of it's non-dynamic dictionary. It is difficult for the rule-based machine translation method to translate conversational sentences which have a certain context. To resolve these problems, an example-based machine translation method is being researched [2,3,4]. This method can improve the correct translation rate and the quality of translation by incorporating any new translation examples that it is given. It becomes possible to translate conversational sentences through having many translation examples.However, serious obstacles remain before the realization of a practical translation system, as the example-based machine translation method requires both a large base of translation examples and a lot of CPU time. As a result, it is difficult to produce a machine translation system based on this method. We previously proposed a method of Machine Translation Using Inductive Learning with Genetic Algorithms toward the realization of a practical machine translation system. We call this method GA-ILMT. In GA-ILMT, translation rules are inductively acquired from only translation examples. Thereafter, many translation rules which apply to specific fields are acquired. Translation results are then generated by using these translation rules. As a result, GA-ILMT can improve the correct translation rate and the quality of translation by incorporating new translation examples. Moreover, a greater variation of translation examples is generated from the initial small number of translation examples, by applying genetic algorithms [6] to machine translation using inductive learning.Our research goal is to design a computer system with the same capability of language and knowledge acquisition as found in human beings [7]. We believe that GA-ILMT can imitate the learning process which human beings have. GA-ILMT can generate variety translation examples because it uses only given translation examples without any analytical knowledge. For example, the system automatically generate the translation example "He likes tennis." ; Kare wa tenisu ga suki desu. 1 " by replacing the word "tea; ocha" in the given translation example "He likes tea.;Kare wa ocha ga suki desu." with the word "tennis;tenisu." This process corresponds to the process in which a little child replaces words in sentences with other words.hi order to show the practical effectiveness of GA-ILMT, we performed experiments that give a practical evaluation of GA-ILMT. We used translation examples of travel conversations [8]. In the experiments, we used travel conversations as data to confirm that GA-ILMT can generate translation results which fit the context of the conversation. These results showed that GA-ILMT is a more practical machine translation system than machine translation systems that are rule-based machine translations. However, consideration of the experiment's results was still not sufficient. Therefore, we performed further experiments using more data, evaluating the effectiveness of GA-ILMT in more detail. In this paper, we describe the consideration of these experiment's results in detail. Moreover, we confirm GA-ILMT can generate translation results which fit a certain context, and can adapt to various filed data through its learning capability.1.1 Outline of the GA-ILMT Figure 1: Outline of GA-ILMT 1 Japanese words are written in italics GA-ILMT can translate many different languages by simply changing to the language of the translation examples, as this method aims to acquire translation rules from character strings in each of the languages it is translating. Figure 1 shows the outline of GA-ILMT.First, in the case of English-to-Japanese, the user inputs a source sentence in English. Second, in the translation process, the system generates several candidates as translation results using translation rules extracted from the learning process. Third, the user proofreads the translated sentences if they include any errors. Fourth, in the feedback process, the system determines the fitness value of translation rules used in the translation process and performs a selection process removing erroneous translation rules. In the learning process, new translation examples are automatically produced through crossover and mutation, and various translation rules are acquired from the translation examples through inductive learning. Repetition of the abovementioned process is the equivalent of generation replacement within the system, thus the system is continuously evolving to a higher-quality translation system.In the feedback process, the system evaluates the translation results using translated sentences which have been proofread. The system determines the fitness value of the translation rules used in the translation process through the use of correct and erroneous translation frequencies. The fitness value is calculated by the fitness function as follows:The correct translation frequency x 100 (1) The number of uses The system performs the selection process using the fitness value.(1)Selection form, giving translation examples  Figure 2: Example of one-point crossoverIn the learning process, new translation examples are automatically generated through crossover and mutation. In crossover, two translation examples which have common parts are selected, the crossover position occurring at the common part. Crossover uses the common parts from the English and Japanese sentences for translation examples. Figure 2 shows examples of a one-point crossover. In Figure 2, "likes" is the common part of the two English sentences, and " wa" and "ga suki desu" are the common parts of the two Japanese sentences. Therefore, "likes" and "wa" are the crossover positions. New translation examples are generated using the one-point crossover. Next, one-point crossover is performed for "likes" and "ga suki desu". However, the generated translation examples have the same character strings as the source sentences. Therefore, these translation examples are not input into the dictionary. The system extracts the common and (3)One-point crossover of Japanese sentence Kare wa Kanojyo wa different parts from the character strings of all translation examples which incorporating translation examples and generated translation examples inputed into the dictionary. These become translation rules. There are two kinds of translation rules: those for sentences and those for words. The former are called sentence translation rules and the latter word translation rules.In the translation process, the system generates several candidates as translation results for a source sentence using extracted translation rules. This process also uses genetic algorithm. The system substitutes words using word translation rules, for the variables in the sentence translation rules. The system can generate the Japanese sentence for the English sentence when the English sentence has the same character string as the source sentence. The Japanese sentence generated is the translation result. When there are several translation results, the system selects the correct translation result according to two criteria: the one that uses the translation rule more similar to the source sentence, and the one that uses the translation rule with a higher fitness value. Recently, many machine translation systems have been developed. However, for translation of conversation, correct translation rates and quality of translation are particularly low. This is due to machine translation systems not being able to generate translation results which fit the context of the conversation. We previously proposed a method of Machine Translation Using Inductive Learning with Genetic Algorithms(GA-ILMT). We compare this system&apos;s results to two others that use rule-based translation method, and evaluate the results of experiments done with GA-ILMT, measuring it&apos;s performance when applied to travel English. As a result of the evaluation experiments, we confirmed that GA-ILMT can generate translation results which are more appropriate to the context of the conversation.
ANAPHORA RESOLUTION AS LEXICAL COHESION IDENTIFICATION It is realized that pieces of information in a text are highly interconnected and links between them are mainly local, i.e., linguistic items connect adjacent or near adjacent clues. One of the principal devices indicating such interconnections is anaphora. Anaphora is the linguistic phenomenon of abbreviated subsequent reference. It is also a device for referring back to an entity which has been introduced with more fully descriptive phrasing earlier in the text. The entity may be an object, a concept, an individual, or state of being. In the linguistic studies of anaphora, although some comprehensive studies have already presented and a coherent classification of anaphora has been laid, much of them is still in a theoretical basis and no much computational algorithms are released [1,13,15]. Among a few, systematic methods have been proposed for resolving anaphora, however, as in work on other syntactic processing, most anaphora resolution algorithms are conceptualized entirely within symbolic frameworks [3,10,11,9]. Syntactic information plays a central role in establishing appropriate referents, nevertheless, it has long been recognized the traditional symbolic syntactic approach which tries to capture the meaning from antecedent linguistic items in text cannot be translated into a processing theory [2]. Anaphoric relations could be hidden in the context. Interpretation of anaphora is crucial in making use of syntactic and semantic preferences of each linguistic item in sentences, moreover, it is a great necessity to incorporate previously analyzed sentences which is regarded as an important knowledge source in modelling prior specific context effect.Psycholinguistic accounts of discourse comprehension often assume anaphora resolution proceeds by search-and-match. Interpretation of an anaphora is not resolved by decoding, but inferred, by a nondemonstrative inference process -a process of hypothesis formation and evaluation -in which linguistic constituents and contextual assumptions determine the class of possible hypotheses. These hypotheses are then evaluated in the light of certain general principles which authors and readers are expected to obey. The semantic inferences during anaphora resolution are those based on easily available information and those required making statements in a text locally coherent. To acquire a correct interpretation of a sentence, the reader must first identify the general interpretation of the input sentence, and combine these with the contextual assumptions generated in the preceding sentences to obtain the contextual effects. These contextual effects will govern the possible contextual implications for the anaphora. In other words, a necessary step towards identifying the appropriate referent would be to search for linguistic items that could serve as antecedents. By comparing the coreferential devices and the linguistic items as to a match of various factors, including syntactic, semantic and case-role constraints, the set of potential referents might be considerably narrowed down.In this article, we address the issue by treating semantic ambiguities and anaphora resolution in a same conceptual framework. The principle idea is that on encountering an anaphoric device, some sort of backward search is initialized in the framework which integrates knowledge sources from various paradigms. A collective knowledge source is used to determine the antecedent from a set of antecedent candidates. Disparate linguistic sources with weight assignments are used to encode the different constraints that are relevant for the resolution. The remainder of this paper is organized as follows. Section 2 explores in depth the rationale behind our framework. Anaphora resolution as well as semantic disambiguation are discussed in Section 3. The modelling of context effects in anaphora resolution is also exhibited. The system is implemented and simulations have been done as shown in Section 4, followed by a conclusion. Anaphora,. an important indicator in lexical cohesion, is a discourse level linguistic phenomenon. Most theoretical linguistic approaches to the interpretation of anaphoric expressions propose a treatment on the basis of purely syntactic information. In this article, what we proposed is to cast anaphora resolution as a semantic inference process in which combination of multiple strategies, each exploiting a different linguistic knowledge, is employed to resolve anaphora into a coherent one. We also exhibit how to embed an anaphora resolution into a framework which captures all the salient parameters as well as to remedy, to a certain extent, the inadequacies found in any monolithic resolution systems. The effectiveness of the anaphora resolution considered in this work is exemplified through a set of simulations.
A Large-Vocabulary Bilingual Speech Recognition System for Chinese and Japanese Language  Bilingual or Multilingual speech recognition gradually becomes an attractive research topic because bilingual writings appear almost everywhere in present day. In this paper, we propose a continuous word-based speech recognition system to dictate the Mandarin and Japanese speech simultaneously. We find that there are about 62 basic phoneme like units(PLUs) among the mixed Mandarin and Japanese syllables. The 62 HA/Ms are used to decode the input speech into word hypotheses based on a fast tree-beam searching algorithm. In the language model, the bigram model and trigram model are used to select the most likely word from the word candidates. We also have a bilingual dictionary to deal with the cross language information. Our proposed system architecture can not only dictate Mandarin and Japanese speech simultaneously but also provide a possible solution to recognize any other bilingual speech.
THE SYNTACTIC PROCESSING OF PARTICLES IN JAPANESE SPOKEN LANGUAGE The treatment of particles is essential for the processing of the Japanese language for two reasons. The first reason is that these are the words that occur most frequently. The second reason is that particles have various central functions in the Japanese syntax: case particles mark subcategorized verbal arguments, postpositions mark adjuncts and have semantic attributes, topic particles mark topicalized phrases and no marks an attributive nominal adjunct. Their treatment is difficult for three reasons: 1) despite their central position in Japanese syntax the omission of particles occurs quite often in spoken language. 2) One particle can fulfill more than one function. 3) Particles can cooccur, but not in an arbitrary way. In order to set up a grammar that accounts for a larger amount of spoken language, a comprehensive investigation of Japanese particles is thus necessary. Such a comprehensive investigation of Japanese particles was missing up to now. Two kinds of solutions have previously been proposed: (1) the particles are divided into case particles and postpositions. The latter build the heads of their phrases, while the former do not (cf. [6], [12]). (2) All kinds of particles build the head of their phrases and have the same lexical structure (cf. [1]). Both kinds of analyses lead to problems: if postpositions are heads, while case particles are nonheads, a sufficient treatment of those cases where two or three particles occur sequentially is not possible, as we will show. If on the other hand there is no distinction of particles, it is not possible to encode their different behaviour in subcategorization and modification. We carried out an empirical investigation of cooccurrences of particles in Japanese spoken language. As a result, we could set up restrictions for 25 particles. We show that the problem is essentially based at the lexical level. Instead of assuming different phrase structure rules we state a type hierarchy of Japanese particles. This makes a uniform treatment of phrase structure as well as a differentiation of subcategorization patterns possible. We therefore adopt the 'all-head' analysis, but extend it by a type hierarchy in order to be able to differentiate between the particles. Our analysis is based on 800 Japanese dialogues of the VERBMOBIL data concerning appointment scheduling. 2 The Type Hierarchy of Japanese Particles Japanese noun phrases can be modified by more than one particle at a time. There are many examples in our data where two or three particles occur sequentially. On the one hand, this phenomenon must be accounted for in order to attain a correct processing of the data. On the other hand, the discrimination of particles is  -particle  complementizer  modifying particle wa ga ni-case to noun-modifying particle verb-modif ing particle no topic-article adverbial particle postpositions wa ga-top mo kola ni-adv-p to-adv-p de Figure 1: Type Hierarchy of Japanese Particles. Postpositions include e, naNka, sonota, tomo, kara, made, soshite, nado, bakari, igai, yori, toshite, toshimashite, nitsuite, nikaNshite and nikakete motivated by their modificational and subcategorizational behaviour. We carried out an empirical analysis, based on our dialogue data. Table 1 shows the frequency of cooccurrence of two particles in the dialogue data. There is a tendency to avoid the cooccurrence of particles with the same phonology, even if it is possible in principal in some cases. The reason is obvious: such sentences are difficult to understand.   [4] treats wa, ga, wo, ni, de, to, made, kara and ya as 'particles'. They are divided into those that are in the deep structure and those that are introduced through transformations. An example for the former is kara, examples for the latter are ga(SBJ), wo(OBJ), ga(OBJ) and ni(0 BJ2).[1] assigns all particles the part-ofspeech P. Examples are ga, wo, ni, no, de, e, kara and made. All particles are heads of their phrases. Verbal arguments get a grammatical relation [GR OBJ/SBJ]. In [2] the part-of-speech class P contains only ga, wo and ni. [12] defines postpositions and case particles such that postpositions are the Japanese counterpart of prepositions in English and cannot stand independently, while case particles assign case and can follow postpositions. Her case particles include ga, wo, ni, no and wa. [7] divides case markers (ga, wo, ni and wa) from copula forms (ni, de, na and no). He argues that ni, de, na and no are the infinitive, gerund and adnominal forms of the copula.In the class of particles, we include case particles, complementizers, modifying particles and conjunctional particles. We thus assume a common class of the several kinds of particles introduced by the other authors. But they are further divided into subclasses, as can be seen in figure 1. We assume not only a differentiation between case particles and postpositions, but a finer graded distinction that includes different kinds of particles not mentioned by the other authors. de is assumed to be a particle and not a copula, as [7] proposes. It belongs to the class of adverbial particles. One major motivation for the type hierarchy is the observation we made of the cooccurrence of particles. Case particles (ga, wo, ni) are those that attach to verbal arguments. A complementizer marks complement sentences. Modifying particles attach to adjuncts. They are further divided into noun-modifying particles and verb-modifying particles. Verb modifying particles can be topic particles, adverbial particles, or postpositions. Some particles can have more than one function, as for example ni has the function of a case particle and an adverbial particle. Figure 1 shows the type hierarchy of Japanese particles. The next sections examine the individual types of particles. Particles fullfill several distinct central roles in the Japanese language. They can mark arguments as well as adjuncts, can be functional or have semantic funtions. There is, however, no straightforward matching from particles to functions, as, e.g., ga can mark the subject, the object or an adjunct of a sentence. Particles can cooccur. Verbal arguments that could be identified by particles can be eliminated in the Japanese sentence. And finally, in spoken language particles are often omitted. A proper treatment of particles is thus necessary to make an analysis of Japanese sentences possible. Our treatment is based on an empirical investigation of 800 dialogues. We set up a type hierarchy of particles motivated by their subcategorizational and modificational behaviour. This type hierarchy is part of the Japanese syntax in VERBMOBIL.
Automatic Selection of Synthesis Units from A Large Speech Database In past years, many studies have focused on TTS systems for different languages [1]- [2]. Also, TTS systems and synthesis technology for the Chinese language have been developed in the last two decades [3]- [4]. A detailed overview of TTS systems for English and Chinese were introduced by Klatt [1] and Shih and Sproat [5], respectively. Potential applications include aids for the handicapped, teaching aids, speech-to-speech translation, and any applications for text reading, such as email reader, news reader, and so on.General speaking, a TTS system could be logically composed of three main parts: text/linguistic analysis, prosodic information generation, and speech synthesis. Text analysis is first invoked to analyze the input text. The prosodic information generation employs the linguistic features to generate prosodic features including pitch contour, energy contour, and duration. Finally, speech synthesis is performed to modify the prosodic parameters of the synthesis units and generate intelligible and natural speech based on the above features. There are three modern approaches to speech synthesis: articulatory synthesis, formant synthesis, and concatenative synthesis [5]. The concatenative synthesis is the simplest and effective approach which uses real recorded speech as the synthesis units and concatenates them back together during synthesis.In concatenative speech synthesis, unit selection plays a prominent role of synthesizing intelligible, natural, and high-quality speech. In past years, many kinds of synthesis units have been proposed [1 ]. The phonemes have been adopted as the basic synthesis units. Such units take advantage of small storage. However, it needs to improve the accuracy of intra-syllable coarticulation and the spectral discontinuity between adjacent units. Consequently, longer synthesis units, such as diphone, demi-syllable, syllable, triphone and polyphone, are appropriately incorporated to reduce the effect of spectral distortion [2]. Recently, the approaches to unit selection from a large speech database or using non-uniform units [6] have been appreciated and proved to obtain natural and high quality speech. In this paper, a novel method for the selection of synthesis unit is proposed. The monosyllables are adopted as the basic synthesis units. A set of high-quality synthesis units is selected from a large continuous speech database based on four procedures: pitch period detection and smoothing, speech unit filtering, unit selection, and manual examination. Two cost functions are proposed for obtaining the synthesis units, which minimize the inter-and intra-syllable distortion. The cost functions estimate the parameters including the prosodic features, the LSP frequencies, and types of syllable concatenation. Experimental results showed that a match rate of 48.9% was achieved. It indicates that about half of the &quot;best&quot; synthesis units can be automatically obtained. Also, a replacement rate of 4.8% was obtained.
PATTERN-BASED MACHINE TRANSLATION FOR ENGLISH-THAI Various efforts have been made in developing machine translation (MT) systems for practical use. Historically, there are many approaches on MT research: transfer-based, interlingua-based, and etc. Among these approaches, the most distinctive are rule-based and corpus-based methods. Research on the corpus-based approach has emphasized on the importance of text corpora used as a source for linguistic and knowledge databases. There have been two major approaches among the corpus-based MT known as statistics-based and example-based. It might be said that all approaches have their own pros and cons. Therefore some MT researchers have selected and combined them together for creating a new effective model. We also combine two potential approaches to produce our own strategy; namely, rule-based and example-based. This paper proposes a new model of machine translation system in which rule-based and example-based approaches are applied for English-to-Thai sentence translation. The proposed method has 4 steps :1) analyze an English sentence into a string of grammatical nodes, based on Phrase Structure Grammar, 2) map the input pattern with a table of English-Thai sentence patterns, 3) look up the bilingual dictionary for the equivalent Thai words, reorder and then generate output sentences and 4) rank the possible combinations and eliminate the ambiguous output sentences by using a statistical method. The translated sentences will then be stored in a bilingual corpus to serve as a guide or template for imitating the translation, i.e., the example-based approach. The future work will focus on disambiguation by using semantic features to make a more precise translation.
